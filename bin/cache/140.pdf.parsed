[[[ ID ]]]
140
[[[ INDEX ]]]
0
[[[ TITLE ]]]
Limiting Distribution of Lempel Ziv’78 Redundancy
[[[ AUTHORS ]]]
Philippe Jacquet
Wojciech Szpankowski
[[[ ABSTR ]]]
Abstract—We show that the Lempel Ziv’78 redundancy rate tends to a Gaussian distribution for memoryless sources. We accomplish it by extending ﬁndings from our 1995 paper [3]. We present a new simpliﬁed proof of the Central Limit Theorem for the number of phrases in the LZ’78 algorithm. As in our 1995 paper, here we ﬁrst analyze the asymptotic behavior of the total path length in a digital search tree (a DST) built from independent sequences. Then we present simpliﬁed proofs and extend our analysis of LZ’78 algorithm to include new results on the convergence of moments, moderate and large deviations, and redundancy analysis.
[[[ BODY ]]]
The Lempel-Ziv compression algorithm [12] is a universal compression scheme. It partitions the text to be compressed into consecutive phrases such that the next phrase is the unique largest preﬁx of the uncompressed text not seen before in the compressed text. The compression code for a word w over the alphabet A we denote as C(w). It is known that for a large class of sources the average compression rate ρ(w) = |C(w)| |w|
tends to the source entropy rate h when |w| → ∞. Our goal is to prove that the redundancy rate r(w) = ρ(w) − h tends in probability and in moments to a normal distribution. In particular, we prove that
It is convenient to organize the phrases (dictionary) of the Lempel-Ziv scheme in a digital search tree (DST) [6], [11] which is really a parsing tree. The root represents an empty phrase. The ﬁrst phrase is the ﬁrst symbol, say “a” which is stored in a node appended to the root. The next phrase is either “aa” stored in another node that branches out from the node containing the ﬁrst phrase “a” or a new symbol that is stored in a node attached to the root. This process repeats recursively until the text is parsed into full phrases (last incomplete phrase is ignored). A detailed description can be found in [3], [11].
Let a text w be generated over an alphabet A, and let T (w) be the associated digital search tree constructed by the algorithm. Each node in T (w) corresponds to a phrase in the parsing algorithm. Let L(w) be the (total) path length in T (w), that is, the sum of all paths from the root to all nodes. We should have L(w) = |w| (if all phrases are full). If we know the order of nodes creation in the tree T (w), then we can reconstruct the original text w.
The compression code C(w) is a description of T (w), node by node in the order of creation; each node being identiﬁed by a pointer to its parent node in the tree and the symbol that labels the edge linking it to the parent node. The pointer to the kth node requires at most log k nats, and the next symbol costs log |A| nats. We just assume that the total cost is log(k|A|) . The compressed code length is
where M (w) is the number of full phrases needed to parse w. Clearly, M (w) is also the number of nodes in the associated tree T (w). Notice that the code is self-consistent and does not need a priori knowledge of the text length, since the length is a simple function of the nodes sequence. We conclude from (1) that
In fact, different implementation may add O(M (w)) to the code length. Throughout, we shall assume that |C(w)| = M (w) (log(M (w)) + log(|A|)).
In this paper we study the limiting distribution, large devi- ations, and moments of the number of phrases M (w) and the redundancy when the text of length |w| = n is generated by a memoryless source. We prove the Central Limit Theorem (CLT) for the number of phrases and establish the LZ’78 code redundancy (excess of the code length over the optimal length). The former result was already proved in our 1995 paper [3] while the latter was presented in [5]. However, the proof of the CLT in our 1995 paper was quite complicated; it involves a generalized analytic depoissonization over convex cones in the complex plane. In this paper we simpliﬁed and generalized it to present new comprehensive large deviations results. It should be pointed out that since our 1995 paper [3] no simpler, in fact, no new proof of CLT was presented except the one by Neininger and Ruschendorf [8] but only for unbiased memoryless sources (as in [1]). The proof of [8] applies the so called contraction method and should generalize to biased memoryless sources.
Let n be a nonnegative integer. We denote by M n the number of phrases M (w) when the original text w is of ﬁxed
length n. We shall assume throughout that the text is generated by a memoryless source over A such that the entropy rate is h = − a∈A p a log p a > 0 where p a is the probability of symbol a. We also deﬁne h 2 = a∈A p a (log p a ) 2 and
+ 1 m
log m + h 2 2h
log p a − 1 2
We prove the following theorem which improves our pre- vious result from [3] by adding the convergence of moments.
Theorem 1. Consider the LZ’78 algorithm over a sequence of length n generated by a memoryless source. Let
The number of phrases M n has mean E[M n ] and variance Var(M n ) satisfying for all 1 2 < δ < 1
∼ (h 2 − h 2 )n log 2 n
Furthermore, the normalized number of phrases converges in distribution and moments to the the standard normal distribution N (0, 1). More precisely, for any given real x:
We also have large and moderate deviations results. To the best of our knowledge these results are new (see also [3], [7]).
Theorem 2. Consider the LZ’78 algorithm over a sequence of length n generated by a memoryless source.
(i) [Large Deviations]. For all 1 2 < δ < 1 there exist ε > 0, B > 0 and β > 0 such that for all y > 0
(ii) [Moderate Deviation]. Let δ < 1 6 and A > 0. There exists B > 0 such that for all non-negative real number x < An δ :
As direct consequence of these large deviations result, we conclude that the average compression rate converges to the entropy rate. Furthermore, our large deviation results allow us also to estimate the average redundancy E[r n ] = E[C(w)]/|w| − h and its limiting distribution when |w| = n.
Corollary 1. The redundancy rate r n for sequences of length n satisﬁes for all 1 2 < δ < 1:
E(r n ) = E(M n log M n + M n log(|A|)) n
log(|A|) − β h n log n log n
converges in distribution and moments to N (0, 1).
The average redundancy estimate was ﬁrst proved in [5], [10] but we provide a new proof. The limiting distribution of the redundancy is new.
In this section we make a connection between the Lempel- Ziv algorithm and digital search trees using a renewal argu- ment [2].
Our goal is to derive an estimate on the probability distri- bution of M n . We assume that our original text is a preﬁx of an inﬁnite sequence X generated by a memoryless source over the alphabet A. We build a Digital Search Tree (DST) by parsing the inﬁnite sequence X up to the mth phrase. Thus the DST is constructed over m strings (phrases).
Let L m be the total path length in the associated DST after inserting m (independent) strings. The quantity M n is exactly the number of strings needed to be inserted to increase the path length of the associated DST to n. This observation leads to the following identity valid for all integers n and m:
P (M n > m) = P (L m < n) . 	 (9) We now use generating functions to ﬁnd a functional
equation for the distribution of L m . Let L m (u) = E(u L m ) be the moment generating function of L m . In the following, k is a tuple in N |A| and k a for a ∈ A is the component of k for symbol a. Since inserted strings are independent, we conclude that
m k
. Next, we introduce the exponential generating function L(z, u) = m z m m! L m (u) leading to
It is clear from the construction that L(z, 1) = e z , since L m (1) = 1 for all integer m. Via the cumulant formula, we also know that for all integers m and for t complex sufﬁciently small for which log(L m (e t ) exists, we have
Notice that the term O(t 3 ) is not uniform in m. In passing we remark that E(L m ) = L m (1) and Var(L m ) = L m (1) + L m (1) − (L m (1)) 2 .
Theorem 3. Consider a digital search tree built over m independent strings. Then
We aim now at showing that the limiting distribution of the path length is normal for m → ∞. In order to accomplish it, we need one technical result presented next to be proved only in the ﬁnal version of this paper.
Theorem 4. For all δ > 0 and for all δ < δ there exists ε > 0 such that for |t| ≤ ε: log L m (e tm −δ ) exists, and
Provided Theorem 4 is true, we are ready to prove our main results concerning the path length L m .
Theorem 5. Consider a digital search tree built over m sequences generated by a memoryless source. The random variable L m −E(L m ) √ Var
tends to a normal distribution with mean 0 and variance 1 in probability and in moments. More pre- cisely, for any given real number x:
Proof: We apply Levy’s continuity theorem or equiva- lently Goncharov’s result [11] asserting that L m −E(L m ) √ Var
tends to the standard normal distribution if for complex τ
To prove it we apply several times our main technical re- sult Theorem 4 with t = 	 τ m δ √ Var
< δ < 1 and δ > δ such that 1 − 3δ < 0, we obtain
(16) for some ε > 0. Thus by (15) the normality result follows.
To establish the convergence in moments, we use (16) in the Cauchy formula applied on a circle of radius R encircling the origin, that is,
We also have large deviation results for the path length presented next.
Theorem 6. Consider a digital search tree built over m sequences generated by a memoryless source.
(i) [Large deviation]. Let 1 2 < δ < 1. Then there exist ε > 0, B > 0, and β > 0 such that for all x ≥ 0:
(ii) [Moderate deviation]. Let δ < 1 6 and A > 0. Then there exists B > 0 such that for non-negative real number x < An δ :
Proof: We apply the Chernoff bound. We take t as being a non negative real number. We have the identity:
(19) Using Markov’s inequality we ﬁnd
such that the estimate log L m (e t ) = tE(L m ) + O(t 2 m 1+ε ) is valid. Therefore we have
which tends to zero. We complete the proof by noticing that tm δ x = βm ε x.
To obtain an upper bound we follow the same route only considering −t instead of t. Indeed,
To prove part (ii) of moderate deviation, we apply again Theorem 4 with t = xm δ √ Var
2 +o(1). (21)
Observe 	 that 	 the 	 error 	 term 	 is at 	 most O(m 1− 3 2 +3δ−3δ (log m) −3 ) = o(1), as needed. Therefore, by Markov inequality for all t > 0,
2 ) . This completes the proof.
In this section we prove our main results, namely Theo- rems 1 and 2. We start with the large deviation results.
P (M n > −1 (n) + yn δ ) = P (M n > −1 (n) + yn δ ) = P (L −1 (n)+yn δ < n) .
Since the function (·) is convex and (0) = 0, we have for all real numbers a > 0 and b > 0
a b, 	 (23) (a − b) ≤ (a) −
Applying inequality (23) to a = −1 (n) and b = yn δ we arrive at
P (L −1 (n)+yn δ < n) ≤ P (L m − E(L m ) < −xm δ + O(1)) (26)
We now apply several times Theorem 6 from the previous section regarding the path length L m . That is, for all x > 0 and for all m, there exist ε > 0 and A such that
P (L m − E(L m ) < xm δ + O(1)) ≤ Ae −βxm ε +O(n ε−δ) ) ≤ A e −βxm ε
Observe that this case is easier since we have now m < −1 (n) and we don’t need the correcting term (1 + yn ε ) −δ .
Now we can turn our attention to moderate deviation expressed in Theorem 2(ii). It is essentially the same proof except that we consider y s n ( −1 (n)) with s n = v( −1 (n)) instead of yn δ , and we assume y = O(n δ ) for some δ < 1 6 . Thus y s n ( −1 (n)) = O(n 1 2 +δ ) = o(n). If y > 0, then
Since v(m) = s n +O(1) we have n = E(L m )−y v(m)+ o(1). Referring again to Theorem 6: we know that
where the term O(1) inducing a term exp(O( y 2 v(m) )) = exp(o(1)) that is absorbed by A. The proof for y < 0 follows a similar path.
Indeed, noticing that for any random variable X: |E(X)| ≤ E(|X|) = ∞ 0 P (|X| > y)dy, we set X = M n − −1 (n) to ﬁnd from Theorem 2(i)
Let us now move our attention to large deviations results. For a given y, we have
since for all y lim m→∞ P L m < E(L m ) + y Var(L m ) = Φ(y ) and therefore by continuity of Φ(x)
and following the same footsteps we also establish the match- ing lower bound
tends to the normal distribution in probability. Second, since by the moderate deviation result the normalized random vari-
has bounded moments, and then by the virtue of the dominated convergence:
This work was supported by the NSF Science and Tech- nology Center for Science of Information Grant CCF- 0939370, NSF Grants DMS-0800568 and CCF-0830140, and the MNSW grant N206 369739.
[[[ REFS ]]]
D. Aldou
P. Shield
--
A Diffusion Limit for a Class of Random- Growing Binary Trees, Probab
----
P. Billingsle
--
Convergence of Probability Measures, John Wiley & Sons, New York 1968
----
P. Jacque
W. Szpankowsk
--
Asymptotic behavior of the Lempel-Ziv parsing scheme and digital search trees, Theoretical Computer Science, 144, 161–197, 1995
----
P. Jacque
W. Szpankowsk
J. Tan
--
Average Proﬁle of the Lempel- Ziv Parsing Scheme for a Markovian Source, Algorithmica, 31, 318–360, 2001
----

--
G Louchard, W Szpankowski, On the average redundancy rate of the Lempel-Ziv code
----
D. Knut
--
The Art of Computer Programming
----
N. Merha
--
Universal Coding with Minimum Probability of Codeword Length Overﬂow, IEEE Trans
----
R. Neininge
L. Ruschendor
--
A General Limit Theorem for Recur- sive Algorithms and Combinatorial Structures, The Annals of Applied Probability , 14, No
----
E. Plotni
J. Weinberge
J. Zi
--
M
----
S. Savar
--
Redundancy of the Lempel-Ziv Incremental Parsing Rule, IEEE Trans
----
W. Szpankowsk
--
Average Case Analysis of Algorithms on Sequences, Wiley, New York, 2001
----
J. Zi
A. Lempe
--
Compression of individual sequences via variable- rate coding, IEEE Transactions on Information Theory, 24, 530–536, 1978
[[[ META ]]]
parsed -> yes
file -> E:\isit2011\140.pdf
[[[ LINKS ]]]

