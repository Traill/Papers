{"id":"1569566869","paper":{"title":{"text":"Compressive Binary Search"},"authors":[{"name":"Mark A. Davenport"},{"name":"Ery Arias-Castro"}],"abstr":{"text":"Abstract\u2014In this paper we consider the problem of locating a nonzero entry in a high-dimensional vector from possibly adaptive linear measurements. We consider a recursive bisection method which we dub the compressive binary search and show that it improves on what any nonadaptive method can achieve. We also establish a non- asymptotic lower bound that applies to all methods, regard- less of their computational complexity. Combined, these results show that the compressive binary search is within a double logarithmic factor of the optimal performance."},"body":{"text":"How should one approach the problem of ﬁnding a needle in a haystack ? Speciﬁcally, suppose that a high- dimensional vector x ∈ R n is known to have a single nonzero entry\u2014how can we efﬁciently ﬁnd the location of the nonzero? We will assume that we can learn about x by taking m noisy linear measurements of the form\nwhere the measurement vectors a 1 , . . . , a m have Eu- clidean norm at most 1 and z 1 , . . . , z m are i.i.d. ac- cording to N (0, 1). Our question reduces to the problem of choosing the vectors a 1 , . . . a m and constructing an algorithm to estimate the location of the nonzero from the measurements y 1 , . . . , y m .\nThis is a special case of support recovery in compres- sive sensing (CS) [1, 2], since (1) is equivalent to the linear model\nwhere y = (y 1 , . . . , y m ), A is the m × n matrix with row vectors a 1 , . . . , a m and z = (z 1 , . . . , z m ). (Note that the rows of A are normalized, as opposed to the columns, which is another common convention in the CS literature.) There are a variety of results on support recovery in the context of (2) where the measurement matrix A is ﬁxed in advance (i.e., is nonadaptive) and satisﬁes certain desirable properties [3\u20139]. As an example, one can show that if A is generated by drawing i.i.d. ± 1/\n1-sparse with nonzero entry equal to µ > 0, then the Lasso and Orthogonal Matching Pursuit (OMP) recover the support of x with high probability provided that\nwith C sufﬁciently large. Moreover, any method based on such measurements requires µ to satisfy this lower bound for some constant C > 0 [10]. This is essentially the whole story when the measurements are nonadaptive.\nIn contrast, suppose now that the system implementing (1) can provide feedback in such a way as to allow for the measurements to be taken adaptively, meaning that a i may be chosen as a function of the observations up to time i − 1, that is, (y 1 , . . . , y i−1 ). (This implicitly assumes that a i is a deterministic function of this vector, but there is no loss of generality in this assumption. See [11] for details.) This instance of active or online learning has received comparatively far less attention to date. However, in recent work [11] we have established lower bounds showing that any support recovery method under any adaptive sampling scheme (satisfying the conditions above) will be unable to recover the correct support unless the nonzero entry satisﬁes\nOur contribution in this paper is twofold. In Section II, we propose a compressive binary search algorithm which recursively tests whether the nonzero entry is on the left or right half of the current interval. We show that the method reliably recovers the support of a 1-sparse vector when the nonzero entry satisﬁes\nwith a constant C > 2. We then verify this analysis via numerical simulations. Note that by using an adaptive measurement scheme we are able to improve upon the requirement in (3) by reducing the log n to log log 2 n, but our scheme does not eliminate the logarithmic factor entirely as in (4). A corollary of this result is that in contrast to the results of [11], which argued that in gen- eral adaptive strategies do not improve over nonadaptive strategies in terms of our ability to accurately recover x, we see that when µ satisﬁes (5), adaptive strategies can signiﬁcantly outperform nonadaptive ones by ﬁrst identifying the location of the nonzero and then reserving a set of measurements to more accurately estimate the value of the nonzero.\nIn contrast to this upper bound, in Section III, we provide a simple proof that µ ≥ C n/m is necessary for any method to work. This novel proof is in some sense tailored to this binary method as it too is based on testing whether the nonzero component is in the left or right half of x. In Section IV, we discuss related work in more detail and directions for future work.\nThe algorithm is designed assuming that the target vector x has exactly one nonzero entry equal to µ > 0; both the location and magnitude are unknown. The methodology described here can be easily adapted to the case where the sign of the nonzero entry is unknown. For simplicity, we assume that n is dyadic, and let s 0 = log 2 n, where log 2 denotes the logarithm in base 2.\nWith a budget of m ≥ 2 log 2 n measurements of the form (1), the binary search method proceeds as follows. We divide our m measurements into a total of s 0 stages, allocating m s measurements to stage s, where\nwhere a denotes the largest integer not greater than a. Note that we do not exceed our total measurement budget since\nWe also have m s ≥ 1 for all s, which is necessary for our algorithm to be able to run to completion. Starting with J (1) 0 := {1, . . . , n}, at stage s = 1, . . . , s 0 , we have a dyadic interval J ( s) 0 and consider its left and right halves denoted J ( s) 1 and J ( s) 2 . For example, J (1) 1 := {1, . . . , n 2 } and J (1) 2 := { n 2 + 1, . . . , n}. Let u ( s) denote the vector with entries indexed by J ( s) 1 equal to 2 −(s 0 −s+1)/2 and with entries indexed by J ( s) 2 equal to − 2 −(s 0 −s+1)/2 . Note that u ( s) = 1, since |J ( s) 1 | = |J ( s) 2 | = 2 s 0 −s . We measure m s times with u ( s) , meaning that we observe\nBased on these measurements, we decide between going left or right, meaning we test whether the nonzero entry is in J ( s) 1 or J ( s) 2 . We do so by simply computing\nSpeciﬁcally, we set J ( s+1) 0 \t = J ( s) 1 if w ( s) > 0, and J ( s+1) 0 \t = J ( s) 2 otherwise.\nThe binary search improves on methods based on non- adaptive measurements by by weakening the requirement (3) to (5).\nTheorem 1. In our setting, with a single nonzero entry equal to µ > 0 and a measurement budget of m ≥ 2 log 2 n, the probability that binary search fails to locate the nonzero entry (denoted P e ) satisﬁes\n8n . \t (7) Proof: Since the binary search algorithm is equiv-\nariant with respect to the ordering of the entries, we can begin by assuming without loss of generality that x = (µ, 0, . . . , 0) T , i.e., the nonzero is located in the ﬁrst entry of x. Thus, we can use a simple union bound to argue that\nwhere J ( s) 1 = {1, . . . , 2 −s n} and J ( s) 2 = {2 −s n + 1, . . . 2 1− s n}. Under our assumptions, we have that\nn \t , \t (9) since for all t > 0 we have\n2 exp(−t 2 /2). We next note that by construction,\nm s 2 s ≥ (m s + 1)2 s ≥ m − s 0 ≥ m/2. Plugging m s 2 s ≥ m/2 into (9), we obtain\n8n . Plugging this into (8) we arrive at\n8n , as desired.\n2 for the upper bound on P e in (7) to actually tend to zero as n increases. However, by taking additional measurements beyond the 2 log 2 n required by this theorem, we could loosen this requirement to be able to set C arbitrarily close to 2.\nTo validate our theory, we perform some simple numerical experiments. Speciﬁcally, we compare the per- formance of the compressive binary search procedure to that of OMP (with A constructed with random ± 1/\nentries). Note that in the 1-sparse case, OMP simply reduces to identifying the column of A most highly correlated with the measurements y. The performance of these two algorithms is shown in Figure 1, which shows the empirical probability of error as a function of µ computed by averaging over 100, 000 trials. For these experiments, we set n = 4096 and m = 256. Note that for these values of n and m, we have that\nn/m = 4 and (n/m) log log 2 n ≈ 6.3. Thus, ignoring the constant terms in (4) and (5), we see that the performance of the compressive binary search is largely consistent with our theory\u2014namely, it cannot reliably identify the location of the nonzero when µ ≤ 4 but can for µ \t 6.3. Moreover, recall that as noted in (3), the nonadaptive OMP algorithm requires that µ exceed (n/m) log n ≈ 11.5 to succeed. Again ignor- ing constants, in our case this corresponds to requiring µ to be roughly 1.8 times larger than is required for the compressive binary search procedure, and this is precisely the behavior we observe in Figure 1.\nWe now establish an explicit, non-asymptotic lower bound for adaptive support recovery, valid for any recov- ery method based on adaptive measurements satisfying the conditions required here. Though such bounds were recently derived in [11], we provide a slightly simpler proof here for the case of 1-sparse signals that closely\naligns with the core idea of the compressive binary search.\nLet y [ i] = (y 1 , . . . , y i ) denote the information avail- able after taking i measurements. Let P x denote the dis- tribution of these measurements when the target vector is x. Without loss of generality, we assume that a i is a deterministic function of y [ i−1] . In that case, using the fact that y i is independent of y [ i−1] when a i is given, we have\nFor a subset K ⊂ {1, . . . , n}, let K c := {1, . . . , n} \\ K and let x K be the part of x indexed by K.\nLet P − Q TV denote the total variation metric between distributions P and Q, and K(P, Q) their Kullback-Leibler divergence [12], related by Pinsker\u2019s inequality\nLemma 1. Suppose that n is even and let J 1 = {1, . . . , n/2} and J 2 = {n/2 + 1, . . . , n}. For r = 1, 2, let π r denote the uniform prior on the vectors x ∈ R n having a single nonzero entry equal to µ > 0, located in J r . Let P r denote the distribution of y [ m] when x ∼ π r . Then\nProof: Let P 0 denote the distribution of y [ m] when x = 0, which is multivariate normal with zero mean and covariance I. Using Pinsker\u2019s inequality (11), we have\nLet P ( j) denote the distribution of y [ m] when the nonzero entry (equal to µ) is at j ∈ {1, . . . , n}. By the law of total probability,\nwhich allows us to use the convexity of the KL diver- gence [13], to obtain\nUnder P ( j) , y i = µa i,j + z i , while under P 0 , y i = z i , so that\nThe ﬁrst line is by deﬁnition; the second and third are consequences of (10) and the deﬁnition of the normal likelihood; the fourth line is because, under P 0 , y i is independent of a i,j and has zero mean. Hence,\nLemma 1 implies a lower bound on the risk of the problem of testing whether a vector x ∈ R n with a single nonzero entry equal to µ is supported on the ﬁrst half or second half of the index set { 1, . . . , n}. Proving this result by directly looking at the likelihood ratio, which would be the standard approach, seems quite delicate as we are testing a mixture (supported on the ﬁrst half) versus a mixture (supported on the second half).\nTheorem 2. In the setting of Lemma 1, consider testing H 1 versus H 2 , where H r is the hypothesis by which x is supported in J r . Then under the uniform prior, for any test T ,\nNote that the lower bound is also valid in the minimax sense. In fact, the uniform prior is least favorable by invariance consideration [14, Sec. 8.4].\nProof: Under the uniform prior, we are effectively testing P 1 versus P 2 . The likelihood ratio test, which rejects when L > 1, with L := P 2 / P 1 , has minimum risk, bounded by\n(See Lemma 1 of [11].) We then apply Lemma 1 to bound the total variance distance on the RHS.\nOur main results can be cast as follows: Theorem 1 implies that, with probability at least 1/4, the binary search method locates the nonzero entry (for n ≥ 4) if\nwhile Theorem 2 shows that any method for locating the nonzero entry fails with probability at least 1/4 when\nClearly, the bounds do not match. Numerically, for n ≤ 10 6 , log log 2 n ≤ 3, in which case the discrepancy is a multiplicative factor of 8\nWe will return to the issue of whether this gap can be closed below, but ﬁrst we wish to discuss an additional implication of Theorem 2. Speciﬁcally, one can show that Theorem 2 implies that for any estimator S of the support of x, E|S∆S| ≥ (1 − µ m/n). Following the same argument as in Theorem 3 of [11], this implies that under the measurement model in (2) we have\nwhere C = 1/27. In contrast, Theorem 1 implies that for sufﬁciently large µ, there exist estimators that do far better than this bound (by a factor of n).\nWhile the problems of estimating or detecting the support of a 1-sparse vector might seem to have only limited applications, in fact one can extend any algorithm that identiﬁes the support of a 1-sparse vector to one that works for vectors with k ≥ 2 nonzero entries. This can be done by ﬁrst exploiting a simple hashing scheme which will (with high probability) isolate each nonzero, and then applying the method for 1-sparse vectors to each hash separately. For an overview of this approach in a similar context, see [4].\nWe also note that [4] independently proposes a method very similar to the compressive binary search approach we describe. Though [4] considers a different setting with continuous signals (instead of vectors as we do), the method proposed is essentially the same, except that the measurement budget is partitioned differently. In particular, it is not obvious to us that the strategy in [4] will always succeed, since it does not account for rounding effects or enforce that a base number of measurements are reserved for each scale (stage) and so (to the best of our understanding) the method might exhaust its measurement budget before the algorithm terminates. Another key difference is that by considering the simpler setting of a vector in R n , we can signiﬁcantly simplify the analysis. That being said, the conclusions of [4] are broadly similar to our own.\nFinally, we also note that there a few other adap- tive algorithms that have been proposed in this setting. For example, [15] proposes an algorithm similar to the compressive binary search procedure but using a different procedure for allocating measurements to each stage. As another example, the Compressive Distilled Sensing (CDS) algorithm proposed in [16] considers a CS sampling algorithm which performs sequential subset selection via the random projections typical of CS. In a different direction, [17, 18] suggest Bayesian approaches where the measurement vectors are sequentially chosen so as to maximize the conditional differential entropy of y i given (y 1 , . . . , y i−1 ). While it remains a challenge to obtain performance bounds for the Bayesian methods suggested in [17, 18], CDS is analyzed in detail in [16] for the task of estimating a k-sparse vector x. Following the proof with a view on support recovery, one can establish that CDS is reliable in our context when\nwith C n → ∞ arbitrarily slowly, coming extremely close to the lower bound of (4). However, the algorithm seems to require that m ≥ n α for a constant α > 0 ﬁxed, while binary search only requires m ≥ 2 log 2 n.\nAn important question would seem to be whether there exist methods which allow for both small m and µ approaching the bound in (4). After the submission of this paper, Malloy and Nowak proposed a slight modiﬁcation of the compressive binary search approach (involving a different allocation of the measurements to each stage) which answers this question [19]. Speciﬁ- cally, [19] removes the log log 2 n term at the cost of a slightly worse constant. Thus, the gap between the lower bound in (4) and the upper bound in [19] differs only by a constant factor. It would be interesting to know whether either of these bounds can be tightened.\nThanks to E. Cand`es for many insightful discussions. M. D. is supported by NSF grant DMS-1004718. E. A-C. is partially by ONR grant N00014-09-1-0258.\n[1] R. Baraniuk, \u201cCompressive sensing,\u201d IEEE Signal Processing Mag. , vol. 24, no. 4, pp. 118\u2013120, 124, 2007.\n[2] E. Cand`es and M. Wakin, \u201cAn introduction to compressive sampling,\u201d IEEE Signal Processing Mag. , vol. 25, no. 2, pp. 21\u201330, 2008.\n[3] N. Verzelen, \u201cMinimax risks for sparse regressions: Ultra-high-dimensional phenomenons,\u201d Electron. J. Statist. , vol. 6, pp. 38\u201390, 2012.\n[4] M. Iwen, \u201cGroup testing strategies for recovery of sparse signals in noise,\u201d in Proc. Asilomar Conf.\nSignals, Systems, and Computers , Paciﬁc Grove, CA, Nov. 2009.\n[5] S. Aeron, V. Saligrama, and M. Zhao, \u201cInformation theoretic bounds for compressed sensing,\u201d IEEE Trans. Inform. Theory , vol. 56, no. 10, pp. 5111\u2013 5130, 2010.\n[6] M. Wainwright, \u201cInformation-theoretic limits on sparsity recovery in the high-dimensional and noisy setting,\u201d IEEE Trans. Inform. Theory, vol. 55, no. 12, pp. 5728\u20135741, 2009.\n[7] T. Cai and L. Wang, \u201cOrthogonal matching pursuit for sparse signal recovery with noise,\u201d IEEE Trans. Inform. Theory , vol. 57, no. 7, pp. 4680\u20134688, 2011.\n[8] P. Zhao and B. Yu, \u201cOn model selection consistency of lasso,\u201d J. Mach. Learn. Res., vol. 7, pp. 2541\u2013 2563, 2006.\n[9] E. Cand`es and Y. Plan, \u201cNear-ideal model selection by 1 minimization,\u201d Ann. Stat., vol. 37, no. 5A, pp. 2145\u20132177, 2009.\n[10] E. Cand`es and M. Davenport, \u201cHow well can we estimate a sparse vector?\u201d Arxiv preprint arXiv:1104.5246 , 2011.\n[11] E. Arias-Castro, E. J. Cand`es, and M. A. Dav- enport, \u201cOn the fundamental limits of adaptive sensing,\u201d Arxiv preprint arXiv:1111.4646, 2011.\n[12] P. Massart, Concentration inequalities and model selection , ser. Lecture Notes in Mathematics. Berlin: Springer, 2007, vol. 1896.\n[13] T. Cover and J. Thomas, Elements of information theory . Hoboken, NJ: Wiley-Interscience, 2006.\n[14] E. Lehmann and J. Romano, Testing statistical hypotheses , ser. Springer Texts in Statistics. New York: Springer, 2005.\n[15] J. Haupt, R. Nowak, and R. Castro, \u201cAdaptive sensing for sparse signal recovery,\u201d in Proc. Digital Signal Processing Workshop , Marco Island, FL, Jan. 2009.\n[16] J. Haupt, R. Baraniuk, R. Castro, and R. Nowak, \u201cCompressive distilled sensing: Sparse recovery using adaptivity in compressive measurements,\u201d in Proc. Asilomar Conf. Signals, Systems, and Com- puters , Paciﬁc Grove, CA, Nov. 2009.\n[17] R. Castro, J. Haupt, R. Nowak, and G. Raz, \u201cFind- ing needles in noisy haystacks,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, and Signal Processing (ICASSP) , Las Vegas, NV, Apr. 2008.\n[18] S. Ji, Y. Xue, and L. Carin, \u201cBayesian compressive sensing,\u201d IEEE Trans. Signal Processing, vol. 56, no. 6, pp. 2346\u20132356, 2008.\n[19] M. Malloy and R. Nowak, \u201cNear-optimal compressive binary search,\u201d Arxiv preprint arXiv:1203.1804 , 2012."},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566869.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T8.1","endtime":"11:50","authors":"Mark Davenport, Ery Arias-Castro","date":"1341401400000","papertitle":"Compressive Binary Search","starttime":"11:30","session":"S10.T8: Group Testing and Detection","room":"Stratton (491)","paperid":"1569566869"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
