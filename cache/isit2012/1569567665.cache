{"id":"1569567665","paper":{"title":{"text":"Universal Rateless Coding with Finite Message Set"},"authors":[{"name":"Navot Blits"},{"name":"Meir Feder"}],"abstr":{"text":"Abstract\u2014Universal rateless coding over unknown dis- crete memoryless channels (DMC) is considered. In rateless codes each codeword is inﬁnitely long, and the decoding time depends on the conﬁdence level of the decoder. This work considers the ﬁnite message set case where a ﬁnite number of bits are transmitted over an unknown discrete memoryless channel, with a ﬁx allowed error probability. Using rateless codes along with sequential universal decoding that utilizes a mixture probability law instead of the unknown channel law, an optimal universal scheme is obtained. The analysis speciﬁes explicitly the attainable rate, at each message set size and allowed error probability, which reﬂects the cost of universality."},"body":{"text":"In traditional channel coding the code rate, which is the ratio between the lengths of the encoder\u2019s input and output blocks, is an integral part of the code deﬁnition. If one of M messages is to be encoded at rate R, then the corresponding codeword has length n = (log M )/R. Provided that the rate is chosen properly, the error proba- bility decreases as M grows. The capacity of the channel C is deﬁned as the largest value of R for which the error probability can vanish.\nAn alternative approach to ﬁxed-rate channel coding is rateless codes. In this approach, we abandon the basic assumption of a ﬁxed coding rate, and allow the codeword length, and hence the rate, to depend on the channel conditions. When the encoder wants to send a certain message, it starts transmitting symbols from an inﬁnite- length codeword. The decoder receives the symbols that passed through the channel and when it is conﬁdent enough about the message, it makes a decision. Perhaps the simplest example of a rateless code (see e.g. [1, Ch.3] or [2, Ch.7]) appears in communication over a binary erasure channel (BEC) with erasure probability δ, and feedback. In this case a rateless code with a message set of size 2 (a single bit) is a simple repetition coding, in which the binary symbol is retransmitted until the decoder receives an unerased symbol. Since the erasure probability is δ, the expected number of transmissions until an unerased symbol is received is 1/(1 − δ). This\ntransmission time implies a rate of 1 − δ, which is exactly the BEC capacity.\nThis simple scheme exempliﬁes some important con- cepts of rateless codes. First, the transmission time is not ﬁxed, but rather a random variable (geometrically- distributed in the above case); second, when the length of the transmission is set dynamically, the error probability may be controllable. In the BEC example the transmission is only terminated once the decoder knows what message has been transmitted, so the error probability is zero; third, the code design is rate-independent. In fact, this code can be used for any binary erasure channel; fourth, the continuity of the transmission requires feedback to the encoder. Indeed, as we shall see in the sequel, when rateless codes are used for point-to-point communication, some form of feedback, which can be limited to decision feedback, must exist to enable continuity. Rateless codes, however, are also invaluable for other settings such as multicast or broadcast communications, in which the ex- istence of feedback is not explicitly required. Shulman [1] introduced the concept of Static Broadcasting, in which the transmitter sends a message to multiple users. The scheme does not requires feedback - each user remains connected until it retrieved enough symbols to make a conﬁdent decision, and its rate is determined by the time it spent online.\nIn this paper we explore universal rateless coding over unknown discrete memoryless channel (DMC), with a ﬁnite size message set M and with an allowed small (but ﬁxed) error probability . We investigate the dependence between the rate, the error probability and the size of the message set. The entire analysis is done for a ﬁnite message set, and we show that when the size of the message set is taken to inﬁnity, our results agree with the classic results. Our work essentially combines two previous works. For the case of a ﬁnite message set, but under the assumption that the decoder knows the channel law, Polyanskiy [5] introduced a coding scheme that uses information density function as the decoding metric, and analyzed its performance. That work as well as ours utilizes Wald\u2019s results (see [3, Ch.3]) on Sequential\nProbability Ratio Test (SPRT). For the universal case, where the decoder is ignorant of the channel law, Shulman [1] introduced a sequential version of the maximal mutual information (MMI) decoder [4] for universal channel decoding and joint-source channel coding, including the case of side information at the decoder. However, the results in [1] are asymptotic in the size of the message set. We introduce here a universal decoder, which does not require knowledge of the channel law at the receiver, and analyzed its optimal performance.\nThe rest of this paper is organized as follows. In Section II we deﬁne rateless codes and provide related deﬁnitions and notation. The main result of this paper, a rateless coding theorem for an unknown channel is presented in Section III. In Section IV we extend our scope to and show how rateless coding can be efﬁciently used for uncompressed sources, and obtain a universal joint source-channel coding scheme. Section V concludes the paper.\nThroughout this paper, random variables will be de- noted by capital letters and their realizations by the corresponding lowercase letters. Vectors are denoted by superscript that indicate their length, for instance X n = [X 1 , . . . , X n ]. Unless otherwise stated, all logarithms are taken to the base of 2. We focus on communication over a discrete memoryless channel (DMC) characterized by a transition probability p(y|x), x ∈ X , y ∈ Y, where X and Y are the input and output alphabets of the channel, respectively. With a slight abuse of notation, we use p(·|·) also to denote the joint transition probabilities of the channel, thus p(y n |x n ) = n i=1 p(y i |x i ). The capacity of the channel (in bits per channel use) in conventionally deﬁned as C = max q(x) I(X; Y ), where I(X; Y ) is the mutual information between the input of the channel and its output, and the maximization is over all channel input priors q(x). If |X | = |Y|, and p(y|x) = 1 if x = y and p(y|x) = 0 otherwise, then the channel is said to be noiseless, and in that case C = log |X |. We also assume that a noiseless feedback exists from the receiver to the transmitter.\n1) Message set W containing M messages. With- out the loss of generality we assume that W = {1, . . . , M }, with corresponding probabili- ties π(1), . . . , π(M ). Occasionally, we deﬁne K = log M as the number of bits conveyed in a message.\n2) Codebook C = {c i } M i=1 , where each codeword c i ∈ X ∞ is generated by drawing i.i.d. symbols according to a prior q(x), x ∈ X .\n4) Set of decoding function g n : Y n → W ∪ {0}, n ≥ 1.\nUnlike conventional codes, for which the rate is a funda- mental property, the above description does not specify a working rate; hence the term rateless code. To encode a message w ∈ W, the encoder starts transmitting the codeword c w over the channel. Upon receiving each channel output, the decoder can either decide on one of the messages ˆ w or decide to wait for further channel outputs, returning \u20180\u2019. Through feedback, the decoder\u2019s decision is known to the encoder, which correspondingly decides whether to transmit further symbols from c w or to proceed to the next message. We note that two different forms of feedback can be assumed here: channel feedback and decision feedback. In channel feedback, the encoder at time instance t observes Y t−1 , the channel outputs so far, and by imitating the decoder\u2019s operation it becomes aware of any decision made by the decoder. In decision feedback, the encoder is only informed that a decision has been made, and it can proceed to the next message. While channel feedback requires no intervention from the decoder in the feedback process, it essentially assumes that the feedback channel has the same bandwidth as the main channel. Decision feedback, in contrast, requires only one feedback bit per symbol.\nWe conclude this section with a few deﬁnitions re- quired for the next sections.\nDeﬁnition 1. A stopping time T of a rateless code is a random variable deﬁned as\nDeﬁnition 2. An effective rate R of a rateless code is deﬁned as\nR = log M E{T }\nwhere E{T } = E q {E p {T }}, i.e. the averaging is done over all possible codebooks and channel realizations.\nUsing the deﬁnition of stopping time, we can deﬁne the error event as the case in which the decoder stops, decid- ing on the wrong message. The error event conditioned on a particular message is deﬁned as\nwhere ˆ W = g T (Y T ). The average error probability for the entire message set is therefore\nDeﬁnition 3. For a given DMC, an (R, M, )-code is a rateless code with effective rate R, containing M\nConsider a discrete memoryless source with a set of M equiprobable messages, i.e. π(i) = 1/M , i = 1, . . . , M . We use a rateless code as deﬁned in Section II, where each codeword c i , i = 1, . . . , M is generated by drawing i.i.d. symbols according to q(x), the capacity-achieving prior of the channel. The source of randomness generating the codewords is shared by the encoder and the decoder, so that the codebook in known at both ends. If the channel were known at the receiver end, one could have used the following decision rule, originally introduced by Burnashev [6]:\np(c w,k |y k ) ≥ A · n k=1 q(c w,k ) 0, if no such w exists\n(5) where {c w,k } ∞ k=1 are the symbols in c w . If the threshold crossing condition in (5) is satisﬁed by more than one codeword, we randomly choose one of them and declare an error. The decoding rule can be also written in a logarithmic form, which is equivalent to (5) due to the monotonicity of the logarithm function:\n|y k ) q(c w,k )\nIt should be noted, however, that while the above scheme does not require that the rate is predeﬁned, they all assume that the channel law is fully known at the re- ceiver end. An interesting question, from both theoretical and practical points of view, is whether similar rateless coding schemes are applicable also when the decoder is completely ignorant of the channel law. Clearly, if the channel law is unknown at the receiver, the decoding rule at (8) must change, as is uses the transition probability of the channel. To circumvent this, we introduce a novel universal decoder, in which the unknown transition prob- ability is replaced by a mixture probability. As we shall see shortly, if the weight function of the mixture is chosen properly, the universal decoder will preserve two desirable characteristics of the decoder for the known channel. First, the probability that the decoder will err, identifying an\nindependently-chosen codeword as the transmitted code- word, remains as small as in the original decoder; and second, the excess delay in the decision on the correct codeword is not only bounded, but also decays as the size of the message set grows.\nSuppose that we wish to communicate over a DMC with unknown (backward) transition probabilities\nθ ij = Pr{X = i|Y = j}, i = 1, . . . , |X | j = 1, . . . , |Y| (9)\nWe use a coding scheme similar to (5) with the following modiﬁcation. Assume, ﬁrst, a predeﬁned prior q(x). In- stead of using the true transition probability p θ (x t |y t ), which is unknown to the decoder, we use a universal probability assignment deﬁned as\n \n \nand the weight function w(·) is chosen to be Jeffreys\u2019 prior 1 , i.e.,\nRemark: While the unknown channel is usually char- acterized by a set of transition probabilities\nthe entire derivation here is done for the backward channel parameterization given in (9). However, this does not need to bother us since the entire analysis assumes a known input prior q(x), and therefore given {˜ θ ij }, the parameters in (9) are well-deﬁned. Moreover, the region ˜ Λ, induced by {˜ θ ij } and q(x), is clearly contained in the region Λ. Therefore, if a coding scheme is universal with respect to all possible realizations of the backward channel, it is also universal with respect to all possible realizations of the forward channel.\nThe universal probability assignment implies the fol- lowing decoding rule, which is the universal counterpart of (5):\nThe following theorem, proven in [7], is the main result of this paper. It shows that using the rateless coding scheme deﬁned in Section II with the sequential decoder (14), reliable communication can be achieved, and the rate degradation with respect to the case of a known channel decays as O(log log M/ log M ).\nTheorem 1. For the decoder in (14) with P e ≤ , the following effective rate is achievable:\n(15) where I(q, p) is the mutual information induced by the prior q(·) and the channel law p(·|·), and we deﬁne\nIf q(·) is known to be the capacity-achieving prior of the channel, I(q, p) can be replaced by C, the channel capacity. Generally, ﬁnding the capacity-achieving prior requires knowledge of the channel, which limits the uni- versality of this decoding scheme. However, in many prac- tical cases the capacity-achieving prior can be found even when the channel law is unknown. For example, if the channel is known to be symmetric, the capacity-achieving prior is uniform. Furthermore, using the uniform prior in binary channels guarantees that I(q, p) ≥ 0.94C, and the degradation from the optimum rate C − I(q, p) ≤ 0.011 bits [1]. In general, a universal approach to determining the optimum input distribution is Prior Prediction [8]. Introduced for the case of Arbitrarily-Varying Channels (AVC), this approach uses a sequential estimation process for the prior, and is proved to approach the best rate achievable using any ﬁxed prior.\nEvidently, as the size of the massage set it taken to inﬁnity, the rate approaches the channel capacity, in agree- ment with Shannon\u2019s theory. Moreover, it is interesting to compare the above rate to the one obtained by using rateless coding over a known channel, and evaluate the rate degradation due to the lack of channel knowledge at the receiver. For the case of a known channel, the effective rate at (8) can be approximated by\n(18) Hence, the penalty for lack of channel knowledge\nThe leading term in the latter expression behaves as O(log log M/ log M ) = O(log K/K), where K is the number bits in the ﬁnite message set, factorized by the product of the cardinalities of input and output of the channel. It is interesting to compare this result with known results from universal source coding, where the redun- dancy 2 is dominated by the cardinality of the alphabet of the source, and a term that behaves as O(log n/n), where n is the source length [9].\nTheorem 1 (Eq (15)) also implies the following error exponent:\n(20) As in the case of a known channel, we see that the error exponent is a linear function of the rate, but an additional term of order O(log log M/ log M ) is added. Here again, we interpret this term as a penalty for the lack of channel knowledge at the receiver. Furthermore, by taking M → ∞, we can also see that (20) coincides with [10, Proposition 1].\nSo far, we assumed that the messages conveyed over the channel were equiprobable. This is the case if, for instance, the source of information has been compressed and the message W is the output of the source encoder. Assume now, that the messages have arbitrary proba- bilities π(1), . . . , π(M ). Each message now contains a different amount of information, which would translate into different codeword length at the output of the source encoder. However, in rateless codes the codeword as- signed to each message is always inﬁnite, and the actual transmission length is determined by the decoder. (The effective length of the message depends on the decoder\u2019s stopping time.) It is therefore tempting to use rateless\ncodes for uncompressed sources and try to achieve good compression rate and reliable communication simulta- neously. To simplify matters, we begin by tackling the case of known channel and postpone the formulation for unknown channel to the end of this section. We use the following generalized version of the encoder (6).\nwhere a w is a threshold that depends on the message w, and we deﬁne a w = log A w . By choosing\nThus, for an appropriate choice of message-dependent threshold values, the average probability of error for the entire message set is bounded by . Recall, however, that the effective rate depends on the threshold value and therefore needs to be reexamined here. When different thresholds are used for different messages, the stopping time depends on which message crosses the threshold. Suppose that we want to convey blocks of L bits from a source with entropy rate of H(W ). It is shown in [7] that the effective rate in this case satisﬁes\nwhere we deﬁne H (W ) = H(W )/ log M as the per-bit entropy of the source.\nInterestingly, it should be noted that the encoder, as well as the codebook, used for joint source-channel coding are the same as these deﬁned in Section II for channel coding only. The only difference is the decoder, which utilizes the knowledge on the source law (or, see below, universally learns it).\nCombining the results for joint source-channel coding in this section with the analysis of the universal case, we obtain the following effective rate for an uncompressed source over an unknown channel:\n(26) We note the far-reaching implications of the latter\nof the source or the capacity of the channel, the rate approaches the optimum rate achievable by an informed encoder. From practical point of view, compression algo- rithms can be implemented and maintained at the decoder, while the encoder remains simple and source-independent.\nIn this paper we introduced a universal coding scheme based in rateless coding and universal sequential de- coding. Throughout the analysis, the coding schemes is allowed to have a ﬁxed error probability, while aiming to achieve shortest mean transmission time, or equivalently, the highest rate. This approach is different than the preva- lent one, in which the communication rate is held ﬁxed and the codebook is enlarged indeﬁnitely so that the error probability vanishes. We demonstrated how rateless codes, combined with universal sequential decoding, attains the optimal performance. The decoding methods and analysis introduced here is novel it that it enabled obtaining results for a ﬁnite size message set and an unknown channel, while previous studies were restricted to either a ﬁnite message set with a known channel, or vise-versa.\nThe key point to be noticed is the following. To attain the precise ﬁnite length performance for universal rateless coding we proposed a universal decoding metric based on a mixture probability assignment, that enabled bounding the difference between the universal metric and the one used by an informed decoder. That concept follows easily to universal rateless coding for uncompressed sources over an unknown channel, showing that even when the encoder is uninformed on the statistics of the source or the channel, the achievable rate approaches the optimal rate with precise account of the ﬁnite length."},"refs":[{"authors":[{"name":"N. Shulman"}],"title":{"text":"Communication over an unknown channel via com- mon broadcasting"}},{"authors":[{"name":"T. Cove"},{"name":"J. M. Thoma"}],"title":{"text":"Elements of Information Theory, 2nd ed"}},{"authors":[{"name":"A. Wal"}],"title":{"text":"Sequential Analysis"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orne"}],"title":{"text":"Information Theory"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"H. Poor"},{"name":"S. Verdu"}],"title":{"text":"Feedback in the non- asymptotic regime"}},{"authors":[{"name":"M. V. Burnashev"}],"title":{"text":"Data transmission over a discrete channel with feedback. random transmission time"}},{"authors":[{"name":"N. Blits"}],"title":{"text":"Rateless codes for ﬁnite message set"}},{"authors":[{"name":"Y. Lomnitz"},{"name":"M. Feder"}],"title":{"text":"Prediction of priors for communication over arbitrarily varying channels"}},{"authors":[{"name":"M. Feder"},{"name":"N. Merhav"}],"title":{"text":"Universal prediction"}},{"authors":[{"name":"A. Tchamkerten"},{"name":"I. E. Telatar"}],"title":{"text":"Variable length coding over an unknown channel"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569567665.pdf"},"links":[{"id":"1569566527","weight":5},{"id":"1569565383","weight":5},{"id":"1569566725","weight":16},{"id":"1569564635","weight":11},{"id":"1569566875","weight":5},{"id":"1569559617","weight":16},{"id":"1569566943","weight":5},{"id":"1569566571","weight":16},{"id":"1569552245","weight":16},{"id":"1569566415","weight":11},{"id":"1569566469","weight":5},{"id":"1569566081","weight":11},{"id":"1569565931","weight":11},{"id":"1569551535","weight":11},{"id":"1569565461","weight":5},{"id":"1569564245","weight":5},{"id":"1569566119","weight":5},{"id":"1569564233","weight":11},{"id":"1569559541","weight":5},{"id":"1569565291","weight":11},{"id":"1569562685","weight":5},{"id":"1569566467","weight":5},{"id":"1569566579","weight":5},{"id":"1569556091","weight":5},{"id":"1569565347","weight":5},{"id":"1569566497","weight":5},{"id":"1569564989","weight":16},{"id":"1569565953","weight":5},{"id":"1569566985","weight":5},{"id":"1569566095","weight":5},{"id":"1569559565","weight":5},{"id":"1569566905","weight":22},{"id":"1569558681","weight":11},{"id":"1569566643","weight":5},{"id":"1569565841","weight":5},{"id":"1569566531","weight":11},{"id":"1569561143","weight":11},{"id":"1569562867","weight":5},{"id":"1569567015","weight":5},{"id":"1569566437","weight":5},{"id":"1569553537","weight":5},{"id":"1569565427","weight":5},{"id":"1569565915","weight":5},{"id":"1569552251","weight":16},{"id":"1569553519","weight":5},{"id":"1569554881","weight":5},{"id":"1569554971","weight":5},{"id":"1569566209","weight":5},{"id":"1569565151","weight":11},{"id":"1569566629","weight":5},{"id":"1569565633","weight":11},{"id":"1569565219","weight":11},{"id":"1569566037","weight":5},{"id":"1569565357","weight":5},{"id":"1569567033","weight":5},{"id":"1569566603","weight":11},{"id":"1569566233","weight":5},{"id":"1569560997","weight":11},{"id":"1569566407","weight":5},{"id":"1569560503","weight":11},{"id":"1569565439","weight":16},{"id":"1569562551","weight":16},{"id":"1569566901","weight":5},{"id":"1569551347","weight":11},{"id":"1569565415","weight":5},{"id":"1569565571","weight":5},{"id":"1569565885","weight":5},{"id":"1569566929","weight":11},{"id":"1569565549","weight":5},{"id":"1569566873","weight":5},{"id":"1569565765","weight":5},{"id":"1569557275","weight":5},{"id":"1569566129","weight":5},{"id":"1569565093","weight":16},{"id":"1569565919","weight":11},{"id":"1569566267","weight":11},{"id":"1569565421","weight":5},{"id":"1569566595","weight":5},{"id":"1569565013","weight":11},{"id":"1569565375","weight":5},{"id":"1569565541","weight":5},{"id":"1569566771","weight":5},{"id":"1569559035","weight":5},{"id":"1569551905","weight":16},{"id":"1569556759","weight":11},{"id":"1569566619","weight":5},{"id":"1569565271","weight":5},{"id":"1569561185","weight":5},{"id":"1569565669","weight":5},{"id":"1569566817","weight":5},{"id":"1569564281","weight":22},{"id":"1569565805","weight":5},{"id":"1569566577","weight":11},{"id":"1569557851","weight":5},{"id":"1569565389","weight":5},{"id":"1569559919","weight":5},{"id":"1569566147","weight":11},{"id":"1569555891","weight":11},{"id":"1569566583","weight":5},{"id":"1569565337","weight":11},{"id":"1569565889","weight":16},{"id":"1569561397","weight":11},{"id":"1569565113","weight":11},{"id":"1569566375","weight":16},{"id":"1569564257","weight":11},{"id":"1569566555","weight":11},{"id":"1569564141","weight":16},{"id":"1569566449","weight":11},{"id":"1569565031","weight":5},{"id":"1569564755","weight":5},{"id":"1569551541","weight":5},{"id":"1569564419","weight":5},{"id":"1569566067","weight":5},{"id":"1569566113","weight":16},{"id":"1569566417","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T5.5","endtime":"13:10","authors":"Navot Blits, Meir Feder","date":"1341406200000","papertitle":"Universal Rateless Coding with Finite Message Set","starttime":"12:50","session":"S10.T5: Rateless Codes","room":"Kresge Little Theatre (035)","paperid":"1569567665"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
