{"id":"1569561143","paper":{"title":{"text":"A Simple Technique for Bounding the Redundancy of Source Coding with Side Information"},"authors":[{"name":"Shigeaki Kuzuoka"}],"abstr":{"text":"Abstract\u2014A simple technique for bounding the redundancy of Slepian-Wolf coding is given. We demonstrate that our simple technique gives the tight bound established by He et al. Our proof is so simple that it can be easily extended to the case where the source (X n , Y n ) has an n-fold product distribution (i.e., (X 1 , Y 1 ), . . . , (X n , Y n ) are independent but not necessarily iden- tically distributed). It can be also applied to Wyner-Ahlswede- K¨orner coding and gives novel bounds of the redundancies of the coding rates of the encoder and the helper."},"body":{"text":"Let {(X i , Y i ) } ∞ i=1 be a discrete memoryless multiple source (DMMS) with two components X and Y . That is, {(X i , Y i ) } ∞ i=1 is a sequence of independent replicas of a pair of correlated random variables (X, Y ) taking values in ﬁnite alphabets X and Y, respectively. We consider the following multiterminal source coding problem (Fig. 1). The encoder compresses X n = (X 1 , . . . , X n ) and sends the compressed data to the decoder so that the decoder, given the side informa- tion Y n = (Y 1 , . . . , Y n ), can recover X n with asymptotically zero error probability. This problem was ﬁrst introduced and studied by Slepian and Wolf [1], and is known as the Slepian- Wolf (SW) coding problem (with the full side information at the decoder).\nRecently, He et al. [2] investigated the redundancy of SW coding. They proved that under mild assumptions about the error probability ε n , the redundancy of ﬁxed-rate coding is d SW\nn /n), where d SW is the constant completely determined by the joint distribution of the DMMS.\nIn this paper, we give a conceptually simple proof for the direct part of He et al [2, Theorem 4]. The key of our proof is the fact that the behavior of the conditional-log-likelihood − log P X n |Y n (X n |Y n ) determines the error probability of\nSW code. To bound the probability of the event such that − log P X n |Y n (X n |Y n ) exceeds a threshold, we use the large deviations result due to Bernstein (known as Bernstein\u2019s in- equality). This simple argument allows us to extend the result to the case where the source (X n , Y n ) has an n-fold product distribution P X n Y n = P X 1 Y 1 × P X 2 Y 2 × · · · × P X n Y n (i.e., (X 1 , Y 1 ), . . . , (X n , Y n ) are independent but not necessarily identically distributed) 1 . While a similar idea is used in the proof of Theorem 1 of [4], our proof derives the bound estab- lished by He et al, which was proved to be tight. Moreover, our proof indicates that the optimal redundancy is achieved by linear codes.\nOur technique can be also applied to the case where the side information Y n is encoded by the helper (Fig. 2; the problem is known as the Wyner-Ahlswede-K¨orner (WAK) coding prob- lem [5], [6]). It gives novel bounds on the redundancies of the coding rates of the encoder and helper of WAK coding.\nThis paper is organized as follows. In Sect. II, we introduce some notations used in this paper. A new proof of the bound of the redundancy of SW coding is given in Sect. III, and then, it is applied to WAK coding in Sect. IV. Sect.V concludes the paper.\nThroughout this paper, log means the natural logarithm. For any ﬁnite set S, |S| and S n denote the cardinality and the n-th Cartesian product of S, respectively. For a random variable Z, E[Z] and Var(Z) denote the expectation and the variance of Z, respectively.\nLet us be given a pair of correlated random variables (X, Y ) taking values in ﬁnite alphabets X and Y, respectively. Let {(X i , Y i ) } ∞ i=1 be a sequence of independent replicas of (X, Y ). {(X i , Y i ) } ∞ i=1 is called a discrete memoryless multiple\nsource (DMMS) with two components. Considered separately, the sequences {X i } ∞ i=1 and {Y i } ∞ i=1 are two discrete mem- oryless sources ith output of which, X i and Y i , are not necessarily independent. For brevity, {X i } ∞ i=1 and {Y i } ∞ i=1 are referred to as the sources X and Y , respectively. The ﬁrst n variables (X 1 , X 2 , . . . , X n ) of the source X is denoted by X n . Y n = (Y 1 , . . . , Y n ) is deﬁned similarly.\nLet P XY denote the joint distribution of (X, Y ). Let P X and P Y be the marginals of P XY over X and Y, respectively. The conditional distribution of Y given X (resp. X given Y ) is denoted by P Y |X (resp. P X |Y ) 2 . We will use standard notations H(X) denoting the entropy, H(X |Y ) denoting the conditional entropy, and I(X; Y ) denoting the mutual information, etc., as in [7].\nLet a DMMS {(X i , Y i ) } ∞ i=1 be given. A ﬁxed-rate Slepian- Wolf (SW) code of order n is a pair of an encoder\nφ n : X n → {1, 2, . . . , M n } and a decoder\nTheorem 1: Let {ε n } ∞ n=1 be a sequence of positive real numbers such that − log ε n = o(n) and lim n →∞ ε n = 0. Then there exists (n, M n , ε n )-code such that\n(1) where\nRemark 1: The bound (1) is given by He et al. [2] (Note that log is to the base 2 in [2], while log means the natural logarithm in this paper).\nOur main contribution is to show that the tight bound (1) is easily deviled by just combining the following two lemmas 3 .\nLemma 1 (Bernstein\u2019s inequality): Let Z 1 , Z 2 , . . . , Z n be independent (not necessarily identically distributed) real- valued random variables such that E[Z i ] = 0 and |Z i | ≤ c with probability one. Let\n1 n\nLemma 2: For any n = 1, 2, . . . and γ n > 0, there exists an (n, M n , ϵ n )-code such that\n− nγ n } + e −nγ n . (2)\nIt should be noted here that Lemma 2 holds for general correlated sources {(X n , Y n ) } ∞ n=1 , i.e. the sources which are not stationary nor ergodic [9]. Since our proof below depends only on Lemmas 1 and 2, it can be easily extended to the case where (X 1 , Y 1 ), · · · , (X n , Y n ) are independent but not necessarily identically distributed. In that case, the right hand side of (1) becomes (1/n)[log M n −\nH(X i |Y i )] and d SW in the left hand side is replaced with\n2 n\nMoreover, it should also be noted that we can show there exists a linear code satisfying (2) 4 . This implies that the optimal redundancy (1) of SW coding is achieved by linear codes.\nProof of Theorem 1: For each i = 1, 2, . . . , let Z i be a random variable such that\nNote that Z 1 , Z 2 , . . . , Z n are independent and satisfy that E[Z i ] = E [ − log P X |Y (X i |Y i ) ] − H(X|Y ) = 0\nwhere p min = min {P X |Y (x |y) : P X |Y (x |y) > 0, x ∈ X , y ∈ Y} and p max = max {P X |Y (x |y) : x ∈ X , y ∈ Y}.\nσ 2 1 n\n2n \t . \t (4) Further, ﬁx M n so that\n− nγ n } + e −nγ n = Pr\nHence, by Lemma 2, there exists an (n, M n , ε)-code for which M n satisﬁes (5). To prove the theorem, it is sufﬁcient to give an upper-bound on the difference between (1/n) log M n and H(X |Y ).\nThus, by (5), we have 1\n= δ n + log(2/ε n ) n ≤\nIn this section, we apply the technique developed in Sect. III to Wyner-Ahlswede-K¨orner coding (Fig. 2).\nencoder φ n : X n → {1, 2, . . . , M n }, helper ˜ φ n : Y n → {1, 2, . . . , ˜ M n },\nWe call a code (φ n , ˜ φ n , ψ n ) with the error probability ε n the (n, M n , ˜ M n , ε n )-code.\nBefore stating the result, we introduce a sequence {U i } ∞ i=1 of auxiliary random variables. Let U be an arbitrarily ﬁnite\nset, and then, ﬁx an conditional distribution 5 P U |Y of U on U given Y . Then, {U i } ∞ i=1 is deﬁned by P U |Y so that\nP X n Y n U n (x n , y n , u n ) = P X n Y n (x n , y n )P U n |Y n (u n |y n ), ∀(x n , y n , u n ) ∈ X n × Y n × U n .\nNow, we give the bounds on the redundancies of the coding rates (1/n) log M n of the encoder and (1/n) log ˜ M n of the helper.\nTheorem 2: Fix P U |Y and let {ε n } ∞ n=1 be a sequence of positive real numbers such that − log ε n = o(n) and lim n →∞ ε n = 0. Then there exists an (n, M n , ˜ M n , ε n )-code such that\n1 n\n(7) and\n(8) where\nTo show the theorem, we use the following Lemma 3. It gives a bridge between the error probability ε n of WAK coding and the behavior of the conditional log- likelihood − log P X n |U n (x n |u n ) and the self-mutual informa- tion log P U n |Y n (u n |y n )/P U n (u n ). To bound the probabilities π n of the event − log P X n |U n (x n |u n ) ≥ log M n − nγ n and ˜ π n of log P U n |Y n (u n |y n )/P U n (u n ) ≥ log ˜ M n − nγ n , we can use the Bernstein\u2019s inequality. The proof of the lemma shall be given in the appendix.\nπ n Pr {(X n , U n ) / ∈ T n (γ n ) } ˜ π n Pr {(Y n , U n ) / ∈ ˜ T n (γ n ) }\nProof of Theorem 2: For i = 1, 2, . . . , let us consider random variables\n|Y i ) P U (U i )\nBy applying the Bernstein\u2019s inequality to Z 1 , . . . , Z n and ˜ Z 1 , . . . , ˜ Z n , we can evaluate the upper bound (9) of the\nerror probability, and then, guarantee the existence of an (n, M n , ˜ M n , ε n ) code satisfying (7) and (8). The details are omitted, since it is quite similar to the proof of (1).\nIn this paper, it is demonstrated that the Bernstein\u2019s inequal- ity gives the tight bound on the redundancy of the coding rate of Slepian-Wolf codes. While only ﬁxed-rate coding is considered in this paper, our simple proof can also derive the tight bound of the redundancy of variable-rate Slepian-Wolf coding given in Theorem 3 of [2] (see [11]). An important future work is to give a lower bound on the redundancies of the coding rates of the encoder and the helper of Wyner- Ahlswede-K¨orner coding.\nAt ﬁrst, we introduce a key lemma. The main idea of the lemma is quite similar to Wyner\u2019s lemma [5, Lemma 4.3].\nLemma 4: There are ˜ M n sequences {u n (i) } ˜ M n i=1 (u n (i) ∈ U n , i = 1, 2 . . . , ˜ M n ) and a function\nThen, randomly generate a set U = {u n (i) } ˜ M n i=1 , where u n (i) (i = 1, . . . , ˜ M n ) is generated according to the probability dis- tribution P U n independently. Given U, we deﬁne the function\nF U : Y n → U as in the following manner: Let ˜i be the smallest i such that (y n , u n (i)) ∈ A n (If there exists no such u n (i) ∈ U then let ˜i = 1). Then, let us deﬁne F U (y n ) = u n (˜i).\nThe ﬁrst term of the right hand side of (10) is rewritten as ∑\nTo evaluate the second term of the right hand side of (10), let us consider the expectation of the value with respect to the distribution of the random set U.\n   \n   \n      \n      \nwhere E U denotes the expectation with respect to the distribu- tion of U and\n1 if (y n , u n ) ∈ A n ∩ ˜ T n (γ n ), 0 otherwise.\n   \n   \nBy using the inequality (1 −xy) n ≤ 1−x+e −yn (see, e.g. [7]), we have\n   \n   \nwhere the inequality (a) follows from the Markov inequality. The inequalities (10), (11), (12), and (13) imply that there\nAt ﬁrst, we specify the encoder and the helper. We consider the random bin encoding. In other words, the encoder φ n assigns to each x n ∈ X n a codeword from M n at random. On the other hand, let {u n (i) } ˜ M n i=1 and f n be the sequences and the function whose existence is guaranteed by Lemma 4. The helper ˜ φ n is deﬁned so that ˜ φ n (y n ) = ˜ m ∈ ˜ M n if f n (y n ) = u n ( ˜ m).\nNext, we specify decoding process. Assume that the decoder ψ n receives m ∈ M n and ˜ m ∈ ˜ M n . Then, the decoder provides output ψ n (m, ˜ m) = x n if and only if there exists only one x n ∈ X n satisfying (i) φ n (x n ) = m and (ii) (x n , u n ( ˜ m)) ∈ T n (γ n ).\nA standard random coding argument shows that the average error probability ¯ ϵ with respect to random encoding is bounded by\nBy Lemma 4, this implies the existence of at least one code satisfying (9)."},"refs":[{"authors":[{"name":"D. Slepian"},{"name":"J. K. Wolf"}],"title":{"text":"Noiseless coding of correlated information sources"}},{"authors":[{"name":"D.-K. He"},{"name":"L. A. Lastras-Monta˜no"},{"name":"E.-H. Yang"},{"name":"A. Jagmohan"},{"name":"J. Chen"}],"title":{"text":"On the redundancy of Slepian-Wolf coding"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems"}},{"authors":[{"name":"T. Holenstein"},{"name":"R. Renner"}],"title":{"text":"On the randomness of independent experiments"}},{"authors":[{"name":"A. D. Wyner"}],"title":{"text":"On source coding with side information at the decoder"}},{"authors":[{"name":"R. F. Ahlswede"},{"name":"J. K¨orner"}],"title":{"text":"Source coding with side information and a converse for degraded broadcast channels"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, 2nd ed"}},{"authors":[{"name":"P. Massar"}],"title":{"text":"Concentration Inequalities and Model Selection"}},{"authors":[{"name":"T. S. Ha"}],"title":{"text":"Information-spectrum methods in information theory"}},{"authors":[{"name":"H. Koga"}],"title":{"text":"Source coding using families of universal hash functions"}},{"authors":[{"name":"S. Kuzuoka"}],"title":{"text":"On the redundancy of variable-rate Slepain-Wolf coding"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569561143.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S6.T1.1","endtime":"11:50","authors":"Shigeaki Kuzuoka","date":"1341315000000","papertitle":"A Simple Technique for Bounding the Redundancy of Source Coding with Side Information","starttime":"11:30","session":"S6.T1: Finite Blocklength Analysis for Source Coding","room":"Kresge Rehearsal B (030)","paperid":"1569561143"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
