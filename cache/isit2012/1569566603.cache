{"id":"1569566603","paper":{"title":{"text":"Secrecy Is Cheap if the Adversary Must Reconstruct"},"authors":[{"name":"Curt Schieler"},{"name":"Paul Cuff"}],"abstr":{"text":"Abstract\u2014A secret key can be used to conceal information from an eavesdropper during communication, as in Shannon\u2019s cipher system. Most theoretical guarantees of secrecy require the secret key space to grow exponentially with the length of communication. Here we show that when an eavesdropper attempts to reconstruct an information sequence, as posed in the literature by Yamamoto, very little secret key is required to effect unconditionally maximal distortion; speciﬁcally, we only need the secret key space to increase unboundedly, growing arbitrarily slowly with the blocklength. As a corollary, even with a secret key of constant size we can still cause the adversary arbitrarily close to maximal distortion, regardless of the length of the information sequence."},"body":{"text":"In this work, we consider the Shannon cipher system, ﬁrst investigated in [1]. The cipher system is a communication system with the addition of secret key that the legitimate parties share and use to encrypt messages. A classic result by Shannon in [1] states that to achieve perfect secrecy, the size of the secret key space must be at least the size of the message space. As in [1], we consider the secrecy resource to be shared secret key, but we relax the requirement of perfect secrecy and instead look at the minimum distortion that an adversary attains when attempting to reproduce the source sequence. The joint goal of Alice (transmitter) and Bob (receiver) is to communicate a source sequence almost losslessly while maximizing the adversary\u2019s minimum attainable distortion. In contrast to equivocation, a max-min distortion measure pro- vides guarantees about the way in which any adversary could use his knowledge; equivocation does not give much insight into the structure of the knowledge or how the knowledge can be used.\nThis measure of security was investigated by Yamamoto in the general case where distortion is allowed at the legiti- mate receiver. In [2], Yamamoto established upper and lower bounds on the tradeoff between the rate of secret key and the adversary\u2019s distortion.\nIn this paper, we solve the problem studied in [2], in the case that almost lossless communication is required. We show that any positive rate of secret key sufﬁces for Alice and Bob to cause the adversary unconditionally maximal distortion (i.e., the distortion incurred by only knowing the source distribution and nothing else). A positive rate of secret key R 0 means the number of secret keys is exponential in the blocklength n, because there are 2 nR 0 secret keys available. However, if the secret key space is merely growing unboundedly with n, we show that the adversary still suffers maximal distortion.\nWe also show that a constant amount of secret key can yield nontrivial distortion at the adversary.\nThe system under consideration, shown in Figure 1, operates on blocks of length n. Alice is given an i.i.d. source sequence X n = (X 1 , . . . , X n ) consisting of symbols drawn from a ﬁnite alphabet X according to P X . Alice and Bob share secret key in the form of a uniform random variable K taking values in an alphabet K. Eve knows the source distribution and the operations of Alice and Bob, but does not have access to the secret key. At the transmitter, Alice sends M ∈ M based on the source sequence X n and secret key K; at the other end, Bob observes M and K and produces a sequence ˆ X n . Eve produces a sequence Z n from M and her knowledge of the source distribution and system operations (encoder and decoder).\nDeﬁnition 1. Let k : N → N. An (n, k(n), R) code consists of an encoder f and a decoder g:\nf : X n × K → M g : M × K → X n ,\nwhere the size of the message set is |M| = 2 nR , and the number of secret keys available is |K| = k(n).\nWe measure secrecy by the distortion between the source sequence and an adversary\u2019s estimate of the source sequence. Given a per-letter distortion measure d : X × Z → [0, ∞), we deﬁne the distortion between two sequences as the average of the per-letter distortions:\nWithout loss of generality, we assume that for all x ∈ X , there exists a z ∈ Z such that d(x, z) = 0.\nFor a given amount of secret key, we are interested in the rate of communication and the distortion incurred by the cleverest adversary.\nDeﬁnition 2. For a given sequence k(n) and measure of distortion d(x, z), we say that the pair (R, D) is achievable if there exists a sequence of (n, k(n), R) codes such that\nThe requirement in (1) is that the probability of commu- nication error between Alice and Bob vanishes. In (2), the minimum is taken over all functions z n : M → Z n , i.e., all possible strategies that Eve can employ. Although not explicit in the notation, it should be understood that Eve\u2019s strategy is a function of not only the message M , but also the source distribution P X and the (n, k(n), R) code.\nThe main result is the following theorem. The restriction on R, the communication rate, is the same as the classic result for source coding. Notice that min z E[d(X, z)] is the distortion between X n and the constant sequence (z ∗ , . . . , z ∗ ), where z ∗ = argmin z E[d(X, z)].\nTheorem 1. Let k(n) be an increasing, unbounded sequence. Then (R, D) is achievable if and only if 1\nIf we wanted to consider rates of secret key, we would set k(n) = 2 nR 0 and deﬁne (R, R 0 , D) to be achievable if there exists a sequence of codes such that (1) and (2) hold. Then, by Theorem 1, we would have that (R, R 0 , D) is achievable if and only if\nR 0 > 0 \t or \t R 0 = 0 D ≤ min\nThis is the solution to the lossless case of the problem posed in [2]. It should be noted that with the proper choice of auxilliary random variables, the converse bound on R 0 in [2] is actually the trivial bound, R 0 ≥ 0.\nWith Theorem 1 in hand, we are able to say something about the usefulness of a ﬁnite amount of secret key. The following corollary asserts that the cleverest adversary suffers close to maximal distortion even if the number of secret keys stays constant as blocklength increases.\nCorollary 1. Fix P X and d(x, z), and denote D max = min z E[d(X, z)]. For all D < D max and R > H(X), there exists k ∗ ∈ N such that (R, D) is achievable under k(n) = k ∗ .\nProof of Corollary 1: Suppose the contrary. That is, assume there exists D < D max or R > H(X) such that for all ˜ k ∈ N, (R, D) is not achievable under k(n) = ˜ k. If we denote the minimum attainable distortion for blocklength n and ˜ k secret keys by\nIn particular, all (n, ˜ k, R) codes not satisfying (3) must satisfy (4), which implies that for all ˜ k ∈ N, the sequence d n,˜ k is strictly less than D inﬁnitely often. To arrive at a contradiction, we will deﬁne an increasing unbounded sequence ˆ k(n) such that d n,ˆ k(n) is strictly less than D inﬁnitely often. Since D < D max , such a ˆ k(n) will imply\ncontradicting Theorem 1 and completing the proof. To that end, ﬁrst deﬁne the increasing sequence {N } recursively by\nThe proof of Theorem 1 is presented in the next section, but ﬁrst we provide some intuition for why the result holds by brieﬂy addressing some of the proof ideas. In designing a code, Alice and Bob can use the secret key K to apply a one-time pad to part of the message so that the adversary effectively knows that the source sequence X n lies in a subset B ⊂ X n , but is unsure which sequence the true one is. The number of sequences in B is |K| = k(n), the number of secret keys. Under such a scheme, the adversary\u2019s optimal strategy for minimizing distortion is to output the following symbol on the ith step:\nNote that (5) is the expected value of d(X i , z) conditioned on the event {X n ∈ B}. Now, if each of the sequences in A were equally likely to be the source sequence, (5) becomes\n0 0 1 0 0 1 2 2 0 2 0 1 1 2 0 0 1 0 2 2 0 0 1 0 2 1 0 0 2 0 0 1\nwhere Q i (x) denotes the empirical distribution of the ith symbols of the sequences in B, i.e.,\nIf we could also guarantee that Q i (x) = P X (x) for all x ∈ X , then (5) would become\nIn the light of this discussion, we want to design a codebook and an encryption scheme so that, roughly speaking,\nFigure 2 gives an example of (7) and (8). These ideas are borne out in the proof of Theorem 1, which we now turn to.\nIn preparation for the proof of achievability, we ﬁrst deﬁne ε-typicality for a distribution P with ﬁnite support X :\nwhere Q x n (x) = 1 n i 1{x i = x} is the empirical distribu- tion, or \u201ctype\u201d, of x n . Denote the set of types of sequences x n ∈ X n by P n , and let P n ε ⊂ P n denote the set of types of those sequences x n ∈ X n satisfying x n ∈ T n ε (P X ). For P ∈ P n , use |P | to denote the number of sequences of type P . Finally, deﬁne the variational distance between distributions P and Q by\nWe will need a few lemmas. The ﬁrst three lemmas will aid us in asserting (8).\nLemma 1. Let P ∈ P n . Form a matrix whose columns are the sequences with type P , with the columns arranged in any order. Then each of the rows of the matrix also has type P .\nProof: Any permutation applied to the rows of the matrix simply permutes the columns. Therefore all the rows have identical type. Since the matrix as a whole has type P , each of the rows must be of type P as well.\nLemma 2 (see [3]). Suppose an urn U contains n balls, each marked by an element of the set S, whose cardinality c is ﬁnite. Let H be the distribution of k draws made at random without replacement from U , and M be the distribution of k draws made at random with replacement. Thus, H and M are two distributions on S k . Then\nThus, sampling without replacement is close in variational distance to sampling with replacement (i.e, i.i.d.) provided the sample size is small enough and the number of balls is large enough. The rate at which the distance vanishes is important to our problem. The next lemma is a lower bound on the size of a type class.\nThe ﬁnal lemma concerns sufﬁcient statistics in the context of our measure of secrecy.\nLemma 4. Let X, Y , and Z be random variables that form a markov chain X − Y − Z and let g be a function on A × Z. Deﬁne two sets of functions, F = {f : X × Y → A} and F = {f : Y → A}. Then\nProof of Lemma 4: (≤) follows from F ⊂ F . As for (≥), we have\nProof of Theorem 1: The proof of the converse is straightforward: the converse for lossless source coding gives us R > H(X), and Eve can always produce the constant sequence (z ∗ , . . . , z ∗ ) so that her distortion never exceeds min z E[d(X, z)].\nTo begin the proof of achievability, ﬁx P X , d(x, z), and an increasing, unbounded sequence k(n). Let ε > 0 and R > H(X). We will show that there exists a codebook of 2 nR sequences and an encryption scheme such that\nOur codebook, the set of sequences that Alice encodes uniquely, consists of the ε-typical sequences; thus, (9) is satisﬁed by the law of large numbers. For blocklength n, we want to consider a partition of the set of typical sequences into equally sized subsets (or \u201cbins\u201d) of length k(n). A partition will let us encode the message in two parts: in the ﬁrst part, we will reveal the identity of the bin that contains the source sequence, and in the second part we will encrypt the location within the bin by using the secret key to apply a one-time pad. Effectively, the second part of the message will be useless to Eve. We will denote the set of bins by B, so that each element of B is a bin of k(n) sequences.\nFor a given partition of the typical sequences, the encoder operates as follows. If X n is typical and is the Lth sequence in bin J , then transmit the pair (J, L ⊕ K), where K is the secret key and ⊕ is addition modulo k(n). If X n is not typical, transmit a random message.\nIn addition to requiring equal-sized bins, we further restrict our attention to partitions in which each bin only contains sequences of the same type 2 , and denote the set of bins of type P by B P ; thus, B = P ∈P n\nB P . This restriction addresses (7). We claim that there exists a partition so that (10) is satisﬁed.\nTo do this, we ﬁrst select a partition uniformly at random and average the minimum attainable distortion over all partitions. We use E π to indicate that expectation is being taken with respect to a random partition. If (10) holds for the average, then it must hold for at least one partition. This use of the probabilistic method should be distinguished from \u201crandom binning\u201d that is often used in information theory. In random binning, each sequence is assigned to a random bin; in particular, the bin sizes are random, whereas here they are of size k(n).\nSelecting a partition at random is the same as drawing typical sequences without replacement to ﬁll equal-sized bins of uniform type. This is also equivalent to ﬁrst ﬁxing a partition B that meets the criteria, then for each P ∈ P n ε randomly permuting the sequences in B P , selecting the |P n ε | random permutations independently.\nNote that although x n is deterministic when inside the summa- tion above, the bin J (x n ) that it belongs to is random because we are considering a random partition. Summing over bins and moving the summation outside, we have\nNext, we sum over types as well, and use the fact that all sequences of type P have probability\nE π [D(n)] ≥ E π\nApplying the separability of d n (x n , z n ) and moving the ex- pectation inside, we have\nKeep in mind that the elements of B are random codewords because the partition is random.\nNow we analyze the expectation in (11). Viewing B P as a matrix with the constituent sequences forming the columns, we denote the ith row by the random sequence (Y 1 , . . . , Y |P | ). Furthermore, we let (Y 1 , . . . , Y k(n) ) denote the ith row of B ∈ B P ; this is acceptable because the forthcoming analysis is the same for each row of each bin. For ease of exposition, we now refer to k(n) as simply k with the dependence on n understood. Thus, we have\nwhere Q Y k is the type of Y k . Denoting the event {Y k ∈ T k ε (P )} by A, we have by the towering property of expectation that\nFocusing attention on the conditional expectation in (12), we use the deﬁnition of typicality and the triangle inequality to get\nwhere δ 1 (ε) = 2ε min z x d(x, z) goes to zero as ε → 0 because the distortion measure d is bounded. Now we bound P[A] in (12). We can assume that k(n) ∈ o(|X | nH(P ) ) without loss of generality because Alice and Bob can simply ignore extra secret key. Invoking Lemmas 1-3 to address (8), we have\n(16) ≤ ε \t (17)\nfor large enough n, where (14) follows from Lemma 1, (15) follows from Lemma 2, and (16) follows from Lemma 3. By the deﬁnition of variational distance and the law of large numbers, (17) gives\n≥ 1 − 2ε \t (18) for large enough n. The notation P P indicates that the probability is evaluated with respect to the i.i.d. distribution\nUpon substituting (19) into (11), we conclude the proof by noting that\n1 n\nIf an eavesdropper is trying to reconstruct an information sequence in the Shannon cipher system, we have shown that even small amounts of secret key enable the cipher to cause maximal distortion in the eavesdropper\u2019s estimate. Any positive rate of secret key will sufﬁce. However, the rate of secret key, implying exponential growth in the number of secret key assignments, is not even the right way to discuss the theoretical limits. Corollary 1 shows that the proper question to address is the tradeoff between secret key size and guaranteed distortion, irrespective of the transmission length.\nThis work was supported by the National Science Founda- tion (NSF) through the grant CCF-1116013 and by the Defense Advanced Research Projects Agency (DARPA) through the award HR0011-07-1-0002."},"refs":[{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"Communication theory of secrecy systems"}},{"authors":[{"name":"H. Yamamoto"}],"title":{"text":"Rate-distortion theory for the Shannon cipher system"}},{"authors":[{"name":"P. Diaconis"},{"name":"D. Freedman"}],"title":{"text":"Finite exchangeable sequences"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems "}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566603.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S1.T4.2","endtime":"10:30","authors":"Curt Schieler, Paul Cuff","date":"1341223800000","papertitle":"Secrecy Is Cheap if the Adversary Must Reconstruct","starttime":"10:10","session":"S1.T4: Secrecy Models in Wiretap Channels","room":"Stratton 20 Chimneys (306)","paperid":"1569566603"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
