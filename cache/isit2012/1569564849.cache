{"id":"1569564849","paper":{"title":{"text":"Efﬁcient Decoding of Partial Unit Memory Codes of Arbitrary Rate"},"authors":[{"name":"Antonia Wachter-Zeh"},{"name":"Markus Stinner"},{"name":"Martin Bossert"}],"abstr":{"text":"Abstract\u2014Partial Unit Memory (PUM) codes are a special class of convolutional codes, which are often constructed by means of block codes. Decoding of PUM codes can take advantage of existing block decoders. The Dettmar\u2013Sorger algorithm is an efﬁcient decoding algorithm for PUM codes, but allows only low code rates. The same restriction holds for several known PUM code constructions. In this paper, an arbitrary-rate construction, the analysis of its distance parameters and a generalized decoding algorithm for these PUM codes of arbitrary rate are provided. The correctness of the algorithm is proven and it is shown that its complexity is cubic in the code length.\nIndex Terms\u2014Convolutional codes, Partial Unit Memory Codes, Bounded Minimum Distance Decoding"},"body":{"text":"The algebraic description and the distance calculation of convolutional codes is often difﬁcult. By means of block codes, special convolutional codes of memory m = 1 can be constructed, which enable the estimation of the distance parameters. Moreover, the existing efﬁcient block decoders can be taken into account in order to decode the convolutional code. There are constructions of these so-called Partial Unit Memory (PUM) codes [1], [2] based on Reed\u2013Solomon (RS) [3]\u2013[5], BCH [6], [7] and \u2013 in rank metric \u2013 Gabidulin [8], [9] codes. Decoding of these PUM codes uses the algebraic structure of the underlying RS, BCH or Gabidulin codes.\nIn [10], Dettmar and Sorger constructed low-rate PUM codes and decoded them up to half the extended row distance. Such a decoder is called Bounded Minimum Distance (BMD) decoder for convolutional codes. Winter [11] gave ﬁrst ideas of an arbitrary rate construction.\nIn this contribution, we construct PUM codes of arbi- trary rate, prove their distance properties and generalize the Dettmar\u2013Sorger algorithm to PUM codes of arbitrary rate. We prove the correctness of the decoding algorithm and show that the complexity is cubic in the length. To our knowledge, no other construction and efﬁcient decoding of PUM codes of arbitrary rate exist. Due to space limitations, we consider only PUM codes, but all results apply also to Unit Memory codes.\nThis paper is organized as follows. In Section II, we give basic deﬁnitions, Section III provides the arbitrary rate construction and calculates its parameters. In Section IV, we explain and prove the BMD decoding algorithm and Section V concludes this contribution.\nLet q be a power of a prime and let F denote the ﬁnite ﬁeld of order q. We denote by F n = F 1×n the set of all row vectors of length n over F and the elements of a vector a j ∈ F n by a j = (a (j) 0 , a (j) 1 , . . . , a (j) n−1 ). Let us deﬁne a zero- forced terminated convolutional code C for some integer L by the following Lk × (n(L + m)) generator matrix G over the ﬁnite ﬁeld F\nwhere G i , i = 0, . . . , m are k × n\u2013matrices and m denotes the memory of C. In the following, N def = L + m.\nLet C r (j) denote the set of all codewords corresponding to paths in the minimal code trellis that diverge from the zero state at depth 0 and return to the zero state for the ﬁrst time at depth j. The extended row distance of order j is deﬁned as the minimum Hamming weight of all codewords in C r (j):\nSimilarly, let C c (j) denote the set of all codewords leaving the zero state at depth 0 and ending in any state at depth j and let C rc (j) denote the set of all codewords starting in any state at depth 0 and ending in the zero state in depth j, both without zero states in between. The extended column distance and the extended reverse column distance are:\nThe free distance is the minimum (Hamming) weight of any non-zero codeword of C and can be determined by d free = min j {d r j }. The extended row distance d r j can be lower bounded by a linear function with slope α:\nPUM codes are convolutional codes of memory m = 1. Therefore, the semi-inﬁnite generator matrix consists of two k × n sub-matrices G 0 and G 1 . Both matrices have full rank if we construct an (n, k) UM code. For an (n, k | k 1 ) PUM code, rank(G 0 ) = k and rank(G 1 ) = k 1 < k hold, such that:\nwhere G 00 and G 10 are k 1 × n matrices and G 01 is a (k − k 1 ) × n-matrix. The encoding rule for a code block of length n is given by c j = i j · G 0 + i j−1 · G 1 , for i j , i j−1 ∈ F k .\nThe free distance of UM codes is upper bounded by d free ≤ 2n − k + 1 and of PUM codes by d free ≤ n − k + k 1 + 1. For both the slope is upper bounded by α ≤ n − k [4], [12]. For ﬁxed k 1 , let R = k/n denote the code rate. As notation, let the generator matrices\ndeﬁne the block codes C 0 , C 1 , C 01 and C α with the minimum Hamming distances d 0 , d 1 , d 01 and d α and the BMD block decoders BMD(C 0 ), BMD(C 1 ), BMD(C 01 ) and BMD(C α ),\nSince each code block of length n of the PUM code can be seen as a codeword of the block code C α , d α directly inﬂuences the distance parameters of the convolutional code and the decoding capability. One approach is to deﬁne by G α a Maximum Distance Separable (MDS) code, i.e., d α = n − k − k 1 + 1. This is basically the construction from [6], [10] which designs low-rate PUM codes. The rate is restricted since the (k + k 1 ) × n matrix G α can deﬁne an MDS code only if k + k 1 ≤ n. Otherwise (as observed by [11]), there are linear dependencies between the rows of G α , what we have to consider when constructing PUM codes of arbitrary rate. In the following, we provide a construction of arbitrary k 1 < k and calculate its distance parameters.\nLet k + k 1 − ϕ ≤ n, for some ϕ < k 1 , and let the (k + k 1 − ϕ) × n matrix\n  \n  \nA : (k 1 − ϕ) × n Φ : ϕ × n\nG 01 : (k − k 1 ) × n B : (k 1 − ϕ) × n\ndeﬁne an MDS (e.g. RS) code. We deﬁne the sub-matrices of the semi-inﬁnite generator matrix of the PUM code as follows in order to enable arbitrary code rates.\nDeﬁnition 1 (PUM Code of Arbitrary Rate) Let k 1 < k < n and let G tot be deﬁned as in (3). Then, we deﬁne the PUM code by the following submatrices (2):\nΦ B\nSince G tot deﬁnes an MDS code, C 0 , C 1 and C 10 (compare Section II for the notations) are also MDS codes. We restrict ϕ < k 1 since otherwise all rows in G 1 are rows of G 0 . Note that any rate k/n in combination with any k 1 is feasible with this restriction since k + 1 ≤ k + k 1 − ϕ ≤ n and hence, we have only the trivial restriction k < n.\nWe calculate the extended row distance of the construction from Deﬁnition 1 by cutting the semi-inﬁnite generator matrix into parts. Each code block of length n can be seen as a codeword of C α with minimum distance\nHowever, due to the linear dependencies between the sub-generator matrices, a non-zero information block can result in a zero code block. The following lemma bounds the maximum number of such consecutive zero code blocks.\nLemma 1 (Consecutive Zero Code Blocks) The maximum number of zero code blocks c j , c j+1 , . . . , c j+ −1 , which have no edge in common with the zero state, is\nProof: If ϕ = 0, there is no zero code block obtained from a non-zero information block and = 0.\n) .. .\nIn the non-binary case, each second block i j , i j+2 , . . . has to be multiplied by −1. Then,\nIn each step, we shift the information vector to the right by k 1 − ϕ positions, where this shift size is determined by the size of A. Since Φ has ϕ rows, this right-shifting can be done\nϕ/(k 1 − ϕ) times. We ceil the fraction since the last block i j+ −1 can contain less than k 1 − ϕ information symbols.\nTherefore, after zero code blocks there is at least one block of weight d α and the slope can be lower bounded by:\nTheorem 1 (Extended Distances) The extended distances of order j for the PUM code of Deﬁnition 1 are:\nd r 1 ≥ d r 1 = d 01 , d r j ≥ d r j = d 0 + (j − 2) · α + d 1 , j > 1, d c j ≥ d c j = d 0 + (j − 1) · α, j > 0,\nwith d 01 = n−k +k 1 +1, d 0 = d 1 = n−k +1 and α as in (5) and d r j , d c j and d rc j denote the designed extended distances.\nProof: For the calculation of the extended row distance, we start in the zero state, hence, the previous information is i 0 = 0. We obtain d r 1 for an information block i 1 = (0, . . . , 0, i (1) k\n, . . . , i (1) k−1 ), then c 1 ∈ C 01 . The extended row distance of order j follows from (5) and a last information block i j = (0, . . . , 0, i (j) k\n, . . . , i (j) k−1 ). The second-last block i j−1 is arbitrary and thus c j = i j · G 0 + i j−1 · G 1 is in C 1 .\nThe calculation of the extended column distance starts in the zero state, hence, i 0 = 0, but we end in any state, thus, d c 1 ≥ d 0 . For higher orders, each other block is in C α .\nThe reverse extended column distances considers all code blocks starting in any state, hence there is no restriction on i 0 , i 1 and c 1 ∈ C α . In order to end in the zero state, i j = (0, . . . , 0, i (j) k\n, . . . , i (j) k−1 ) and as for the extended row distance c j ∈ C 1 .\nNote that if d free = n − k + k 1 + 1, then the free distance is optimal since the upper bound is achieved [4].\nLet the received sequence r = c + e = (r 0 , r 1 , . . . , r N −1 ) be given, where r h = c h + e h , h = 0, . . . , N − 1 is in F n , c = (c 0 , c 1 , . . . , c N −1 ) is a codeword of the (terminated) PUM code as in Deﬁnition 1 and e h is an error block of Hamming weight wt(e h ). A BMD decoder for convolutional codes is deﬁned as follows.\nDeﬁnition 2 (BMD Decoder for Convolutional Codes [10]) A BMD decoder for convolutional codes guarantees to ﬁnd the Maximum Likelihood (ML) path as long as\nThe main idea of our algorithm is to take advantage of the efﬁcient BMD block decoders for C α , C 0 , C 1 and C 01 . With the results of the block decoders, we build a reduced trellis and ﬁnally use the Viterbi algorithm to ﬁnd the ML path. Since this trellis has only very few edges, the overall decoding complexity is only cubic in the length. Figure 1 illustrates the decoding principle for = 1.\nSince each code block of the PUM code of length n is a codeword of the block code C α , the ﬁrst step of the algorithm is decoding with BMD(C α ). Due to the termination, the ﬁrst and the last block can be decoded with BMD(C 0 ), respectively BMD(C 1 ). The decoding result of BMD(C α ) is c j . Assume it is correct, then c j = c j = i j G 0 + i [k 1 ] j−1 G 10 ,\n) is a part of the previous information block. Now, we want to reconstruct the information i j = (i (j) 0 , . . . , i (j) k−1 ) and i [k 1 ] j−1 . For this, we need + 1 consecutive decoded code blocks since the linear dependencies \u201cspread\u201d to the next blocks (see Example 1).\nϕ = 2/3k 1 , where \t = 2 and Φ has twice as much rows as A. Assume, we have decoded c 0 , c 1 and c 2 and we want to reconstruct i 1 . Decompose i 0 , i 1 , i 2 into: i j = (i [1] j | i [2] j | i [3] j | i [4] j ) for j = 0, 1, 2, where the ﬁrst three sub-blocks have length k 1 − ϕ and the last k − k 1 . Then,\n    \n    \nand Φ 1 , Φ 2 have k 1 − ϕ rows. Since we know c 1 and G tot deﬁnes an MDS code, we can reconstruct the vector i 1 . This directly gives us i [1] 1 and i [4] 1 . This can be done in the same way for c 0 and we also directly obtain (among others) i [1] 0 . To obtain i [2] 1 , we substract i [1] 0 from the known sum i [2] 1 + i [1] 0 . For c 2 , this reconstruction provides i [3] 1\nand we have the whole i 1 . This principle also gives us i [k 1 ] 0 = (i [1] 0 | i [2] 0 | i [3] 0 ). This is why + 1 consecutive decoded blocks are necessary to reconstruct an information block. Note that it does not matter if the other decoded blocks precede or succeed the wanted information, this principle works the same way.\nAfter this decoding and reconstruction, we build an edge in a reduced trellis for each block with the metric:\n− c j ) if Step 1 ﬁnds c j and i j , (d α + 1)/2 else.\nAssume, in Step 1, we decoded c j and reconstructed i j and a part of the previous information i [k 1 ] j−1 , then we calculate:\nHence, as a second step, we decode (j) F blocks forward with BMD(C 0 ) respectively (j) B blocks backward in BMD(C 1 ),\nThese codes have higher minimum distances than d α and close (most of) the gaps between two sequences of correctly decoded blocks in C α . Lemma 3 in Section IV-B proves that after Step 2, the size of the gap between two correctly reconstructed blocks is at most one block.\n) from Step 1 and i j−2 from Step 1 or 2, then similar to (8):\nwhich shows that we can use BMD(C 01 ) to close the remaining gap at j − 1. After Step 3, assign as metric to each edge\n  \n(11) where again c j denotes the result of a successful decoding. Note that there can be more than one edge in the reduced trellis at depth j.\nFinally, we use the Viterbi algorithm to search the ML path in this reduced trellis. As in [10], we use m j as edge metric and the sum over different edges as path metric. Algorithm 1 shows the basic principle of our generalization of the Dettmar\u2013 Sorger algorithm to arbitrary rate.\nSection IV-B proves that if (6) is fulﬁlled, after Steps 1\u20133, all gaps are closed and Algorithm 1 ﬁnds the ML path. It is a generalization of the Dettmar\u2013Sorger algorithm to arbitrary rates, which results in linear dependencies between the submatrices of the PUM code (see Deﬁnition 1). This requires several non-trivial modiﬁcations of the algorithm. Namely these are: the reconstruction of the information requires + 1 consecutive code blocks (see Example 1), the path extensions (9), (10) have to be prolonged and the\nassigned metric has to be adapted appropriately (7), (11) since the smallest error causing a non-reconstructable sequence is explained in the following remark.\nRemark 1 The error of minimum weight causing a sequence of non-reconstructed information blocks in Step 1 is as follows:\nwhere the × marks blocks with at least d α /2 errors. Also the information of the error-free blocks cannot be reconstructed, since we need + 1 consecutive decoded blocks. The last\nerror-free blocks are the reason why we substract in the deﬁnitions of (j) F and L (j) F . This corresponds to additional decoding steps in forward direction. The (minimum) average weight in a sequence of non-reconstructed information blocks (without the last blocks) is therefore d α /(2( + 1)).\nIn this subsection, we prove that Algorithm 1 ﬁnds the ML path if (6) is fulﬁlled. For this purpose, Lemma 2 shows that the size of the gaps after Step 1 is not too big and in Lemma 3 we prove that after Step 2, the gap size is at most one block. Finally, Theorem 2 shows that we can close this gap and that the ML path is in the reduced trellis. Then, the Viterbi algorithm will ﬁnd it. The complexity of the decoding algorithm is stated in Theorem 4.\nLemma 2 The length of any gap between two correct reconstructions in Step 1, i j , i j+i , is less than min(L (j) F , L (j+i) B ) if (6) holds, with\nProof: Step 1 fails if there occur at least d α /2 errors in every ( + 1)-th block, followed by correct ones (compare Remark 1). Assume there is a gap of at least L (j) F blocks after Step 1. Then,\n(d α − m j+h ) + 1\ncontradicting (6). We prove this similarly for L (j) B without substracting in the limit of the sum, since we directly start left of the correct blocks on the right. Therefore, the gap size is less than min(L (i) F , L (i) B ).\nLemma 3 Let i j and i j+i be reconstructed in Step 1. Let Step 2 decode (j) F blocks in forward and (j+i) B \t blocks in backward direction (see (9), (10)). Then, except for at most one block, the ML path is in the reduced trellis if (6) holds.\nProof: First, we prove that the ML path is in the reduced trellis if (6) holds and in each block less than min{d 0 /2, d 1 /2} errors occurred. In this case, BMD(C 0 ) and BMD(C 1 ) will always yield the correct decision. The ML path is in the\nreduced trellis if (j) F + (j+i) B \t ≥ i − 1, since the gap is then closed. Assume that (j) F + (j+i) B \t < i − 1 and at least d α /2 errors occur in every ( + 1)-th block in the gap, since Step 1 was not successful (compare Remark 1). Then,\nSecond, we prove that at most one error block e h , j < h < j + i has weight at least d 0 /2 or d 1 /2. To fail in Step 1, there are at least d α /2 errors in every ( + 1)-th block. If two error blocks have weight at least d 0 /2 = d 1 /2, then\nin contradiction to (6). Thus, the ML path is in the reduced trellis except for a gap of one block.\nProof: Lemma 3 guarantees that after Step 2, the gap length is at most one block. This gap can be closed in Step 3 with C 01 , which is always able to ﬁnd the correct solution since d 01 ≥ d r 1 = d free .\nSimilar to [10], we give a weaker BMD condition to guarantee ML decoding of a single block. This condition shows how fast the algorithm returns to the ML path after a sequence where (6) is not fulﬁlled. A BMD decoder for convolutional codes guarantees the correct decoding of a block r j of a received sequence r = c + e if the error e satisﬁes\nTo guarantee (12) for a certain block if (6) is not fulﬁlled for the whole sequence, we introduce an erasure node in each step j as in [7], representing all nodes which are not in the reduced trellis. Let j , j−1 denote erasure nodes at time j, j −1 and let s j , s j−1 be nodes found by BMD decoding in Steps 1 and 2. Let t F , t B denote the minimum number of errors of any edge starting from s j−1 and s j in forward, respectively backward direction. t α denotes the minimum number errors of any edge between nodes at time j − 1 and j. We set the metric of the connections with the erasure nodes as follows.\nTheorem 3 If (12) holds for r j , the Viterbi algorithm for the reduced trellis with erasure nodes ﬁnds the correct block c j .\nProof: The metric of the erasure nodes is always at least d r i /2. All nodes of a state are connected with the erasure nodes of the previous and the next state. As soon as (12) is fulﬁlled, the metric of a correct edge is better than all other edges and the ML path will be chosen.\nThe complexity is determined by the complexity of the BMD block decoders, which are all in the order O(n 2 ), if the construction is based on RS codes of length n.\nSimilar as Dettmar and Sorger [10], we can give the following bound on the complexity. Due to space restrictions, the proof is omitted here.\nTheorem 4 Let C be a PUM code as in Deﬁnition 1, where G tot is the generator matrix of an RS code. Then, the decoding complexity of Algorithm 1 of one block is upper bounded by\nWe presented a construction of PUM codes of arbitrary rate and provided and proved an efﬁcient decoding algorithm. The algorithm corrects all error patterns up to half the designed extended row distance, where the complexity is cubic in the length of a block. For = 0, the Dettmar\u2013Sorger algorithm [10] is a special case of Algorithm 1.\nThe authors thank Alexander Zeh and Vladimir Sidorenko for the valuable discussions."},"refs":[{"authors":[{"name":"L.-N. Lee"}],"title":{"text":"Short Unit-Memory Byte-Oriented Binary Convolutional Codes Having Maximal Free Distance"}},{"authors":[{"name":"G. S. Lauer"}],"title":{"text":"Some Optimal Partial-Unit Memory Codes"}},{"authors":[{"name":"V. Zyablov"},{"name":"V. Sidorenko"}],"title":{"text":"On Periodic (Partial) Unit Memory Codes with Maximum Free Distance"}},{"authors":[{"name":"F. Pollara"},{"name":"R. J. McEliece"},{"name":"K. A. S. Abdel-Ghaffar"}],"title":{"text":"Finite-state codes"}},{"authors":[{"name":"J. Justesen"}],"title":{"text":"Bounded distance decoding of unit memory codes"}},{"authors":[{"name":"U. Dettmar"},{"name":"S. Shavgulidze"}],"title":{"text":"New Optimal Partial Unit Memory Codes"}},{"authors":[{"name":"U. Dettmar"},{"name":"U. Sorger"}],"title":{"text":"New optimal partial unit memory codes based on extended BCH codes"}},{"authors":[{"name":"A. Wachter"},{"name":"V. Sidorenko"},{"name":"M. Bossert"},{"name":"V. Zyablov"}],"title":{"text":"Partial Unit Memory Codes Based on Gabidulin Codes"}},{"authors":[],"title":{"text":"On (Partial) Unit Memory Codes Based on Gabidulin Codes"}},{"authors":[{"name":"U. Dettmar"},{"name":"U. K. Sorger"}],"title":{"text":"Bounded minimum distance decoding of unit memory codes"}},{"authors":[{"name":"J. Winter"}],"title":{"text":"Blockcodedarstellung von Faltungscodes"}},{"authors":[{"name":"C. Thommesen"},{"name":"J. Justesen"}],"title":{"text":"Bounds on distances and error expo- nents of unit memory codes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564849.pdf"},"links":[{"id":"1569565551","weight":5},{"id":"1569565775","weight":5},{"id":"1569566739","weight":5},{"id":"1569558859","weight":5},{"id":"1569565817","weight":5},{"id":"1569565847","weight":5},{"id":"1569566275","weight":5},{"id":"1569565665","weight":5},{"id":"1569565177","weight":5},{"id":"1569566819","weight":5},{"id":"1569566397","weight":5},{"id":"1569565769","weight":5},{"id":"1569566273","weight":21},{"id":"1569565165","weight":5},{"id":"1569558697","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T6.2","endtime":"15:20","authors":"Antonia Wachter-Zeh, Markus Stinner, Martin Bossert","date":"1341500400000","papertitle":"Efficient Decoding of Partial Unit Memory Codes of Arbitrary Rate","starttime":"15:00","session":"S13.T6: Convolutional and Turbo Codes","room":"Kresge Rehearsal A (033)","paperid":"1569564849"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
