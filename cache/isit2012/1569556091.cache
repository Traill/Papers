{"id":"1569556091","paper":{"title":{"text":"An Optimal Sampling Technique for Distinguishing Random S-boxes"},"authors":[{"name":"Paul Stankovski"},{"name":"Martin Hell"}],"abstr":{"text":"Abstract\u2014The nonrandom behavior of the outputs of a ran- dom S-box can be exploited when constructing distinguishers for cryptographic primitives. Different methods of constructing samples from the outputs have been used in the literature. However, it has been unclear exactly how these methods differ and which method is optimal. We analyze four different sampling techniques. We prove that two of these sampling techniques result in dependent samples. We further show one sampling technique that is optimal in terms of error probabilities in the resulting distinguisher. However, this sampling technique is quite impractical as it requires very large storage. We further show a fourth sampling technique that is much more practical, and we prove that it is equivalent to the optimal one. We also show an improved algorithm for calculating the associated probability distributions that are required for the attack."},"body":{"text":"Random S-boxes can appear in cryptanalysis when obser- vations, e.g., linear sums of keystream bits in stream ciphers, can be regarded as outputs of a large table. In this paper we study such random S-boxes. More speciﬁcally, we study how to perform an optimal distinguisher from the observations. A random S-box is an a-to-b-bit mapping which can be seen as a table containing n = 2 a random entries of b bits each. Our work is motivated by the analysis of the HC-128 stream cipher as performed in [2], [5], but the results are applicable to all random S-boxes. HC-128 is a stream cipher in the eSTREAM portfolio, and is thus considered to be one of the most promising stream ciphers today. Indeed, it is very fast in software and it has been shown to resist cryptanalytic attacks very well. There are no attacks (not relying on side- channel information) that are more efﬁcient than exhaustive key search. The distinguishing attack given in [2] is currently the most efﬁcient non-generic attack, and that attack is based on the attack given in [5]. The improvement comes from a more efﬁcient sampling technique, reducing the number of keystream bits required by the distinguisher.\nWe analyze different sampling techniques. We show that the sampling technique used in [5] signiﬁcantly underestimates the number of samples needed by the distinguisher as the samples are not independent. We further prove that it is not possible to take two independent biased samples at all, unless the S-box is reinitialized. The optimal sampling technique is thus to take one sample containing all information, i.e., to consider a vector\nof all outputs. The problem with this vector is that, for large vector sizes, its probability distribution is infeasible both to compute and to store. Due to this, a shortcut was used in [2], namely to consider only the weight of the vector. We show that this weight probability distribution is equivalent to the optimal probability distribution and, as a result, it is not possible to further improve the sampling used in [2]. Finally, we give a new algorithm for computing the vector weight distribution that improves the one given in [2]. Our new algorithm uses much less memory (optimal) and saves 80-85% in time.\nTwo outputs from an S-box of size n are equal with probability (at least) 1 n since the same entry may have been used twice. This simple observation can be used to construct a distinguisher for random S-boxes.\nAs noted above, it is known (see [5]) that the xor sum of a pair of output bits is biased, and this bias stems from the fact that the same S-box entry may have been probed for both outputs. More speciﬁcally, for i = j,\nPr(s i = s j ) = 1 2\n+ 1 n\n= 1 2\n(1 + 1 n\n) = 1 2\nand the bias in (1) can be used to construct a distinguisher. The main problem we study in this paper is exactly how to construct this distinguisher when the number of S-box obser- vations is more than two. That is, how should a cryptanalyst use the observations to construct an optimal distinguisher?\nThe empirical probability distribution as deﬁned by the sampling is denoted P ∗ . The corresponding (theoretical) prob- ability distribution of the S-box is denoted P 1 while its uniform distribution is denoted P 2 . The optimal hypothesis test is given by the Neyman-Pearson lemma, see e.g., [1].\nLemma 1 (Neyman-Pearson): Let X 1 , X 2 , . . . , X t be iid random variables according to P ∗ . Consider the decision prob- lem corresponding to the hypotheses P ∗ = P 1 vs. P ∗ = P 2 .\nLet α t = P t 1 (A c t (Q)) and β t = P t 2 (A t (Q)) be the error probabilities corresponding to the decision region A t . Let B t be any other decision region with associated error probabilities α ∗ and β ∗ . If α ∗ ≤ α, then β ∗ ≥ β.\nIf we want the error probabilities to be equal we set Q = 1. In other words, we decide P ∗ = P 1 if\nP 1 (x 1 , . . . , x t ) P 2 (x 1 , . . . , x t )\nlog P 1 (x i ) P 2 (x i )\nand P ∗ = P 2 otherwise. The equivalence in (2) is valid when the samples x 1 , . . . , x t are independent.\nIn our case, the samples x i will be constructed from the observations s j . Note that the Neyman-Pearson lemma, which gives the optimal distinguisher, requires that the samples x i are independent. By sampling technique we mean how to use the observations to build the samples used in the distinguisher.\nIf the samples are very easy to construct from the observa- tions, we can say that the online computational complexity of the attack is given by the number of terms t in the sum (2). The ofﬂine complexity is the time needed to compute P 1 .\n\u2022 All-Pairs Sampling (APS) Take all pairs of observations (s i , s j ), 1 ≤ i < j ≤ as samples. Let P 1 be the distribution corresponding to (1), i.e., Pr(s i = s j ) =\n(1 + 1 n ) and Pr(s i = s j ) = 1 2 (1 − 1 n ). P 2 is the uniform distribution, Pr(s i = s j ) = Pr(s i = s j ) = 1 2 .\n\u2022 Linear-Pairs Sampling (LPS) Take the pairs of obser- vations (s i , s i+1 ), 1 ≤ i < as samples and let P 1 and P 2 be as for APS above.\n\u2022 Vector Sampling (VS) Take the vectors (s 1 , s 2 , . . . , s ) as samples and perform the hypothesis test with the corresponding vector probability distributions as P 1 and P 2 .\n\u2022 Weight Sampling (WS) Take the vector weights (s 1 , s 2 , . . . , s ) = i=1 s i as samples and perform\nthe hypothesis test with the corresponding vector weight probability distributions as P 1 and P 2 .\nIt is clear that Vector Sampling (VS) is optimal since it preserves all information in the samples. The drawbacks are that the distribution is very large in storage (2 ) and that it is demanding to compute. APS was applied in [4], [5]. It uses the easily computed bias in (1) and produces many samples. For observations, 2 samples are produced. Due to the dependency between samples, LPS was suggested in [2] and WS was also applied as an improvement. However, it was an open question whether it was possible to improve over WS as it appears that not all sample information is retained in the vector weight samples.\nThe rest of the paper is organized as follows. In Sec- tions III-A and III-B we prove that APS and LPS are faulty. In Sections III-C and III-D we give algorithms for computing the required distributions for VS and WS, respectively. We also prove that VS and WS are equivalent in terms of the performance of the resulting distinguisher. Section IV explic- itly compares APS, LPS and WS. The paper is concluded in Section V.\nThe Neyman-Pearson lemma assumes that all samples are independent and identically distributed. In APS sampling, all possible bit pairs in an -bit chunk are taken as samples, producing in total k 2 samples. It is very easy to prove that these samples are not independent. Consider a chunk with = 3, where we take the samples (s 1 , s 2 ), (s 1 , s 3 ) and (s 2 , s 3 ). If we know the ﬁrst two samples, then we also know the last sample, i.e.,\nwhere H(·) denotes the entropy function, ⊕ denotes xor and S 1 , S 2 and S 3 are random variables corresponding to the three observations. This argument is easily extended to the general case with arbitrary , which also serves as a direct motivation for deﬁning and using LPS sampling.\nEven though the samples are dependent, APS is very easy to apply. Computing and storing the P 1 requires negligible memory and can be trivially done by hand, see (1). However, the large number of samples gives an online computational complexity of k 2 = O(k 2 ).\nIn LPS sampling we take (s 1 , s 2 ) as the ﬁrst sample and then only take one new sample for each new observation. This procedure produces − 1 samples per chunk for a total of k( −1) samples. In order to show that this sampling technique also gives dependent samples, for P 1 we calculate and compare Pr(s 3 = s 2 |s 2 = s 1 ) and Pr(s 3 = s 2 |s 2 = s 1 ) to see that the probability of pair equality in one sample depends on the pair equality of the previous one.\nWe regard the S-box as a table with n entries. The ﬁrst time we read a speciﬁc entry in the table, we say that we \u201copen\u201d the entry. Consider Pr(s 3 = s 2 |s 2 = s 1 ) ﬁrst. Given that s 2 = s 1 , we must have opened precisely two entries in the table, one 0 and one 1. We can now have s 3 = s 2 in two different ways, by reading s 3 from either an \u201cold\u201d entry (same as s 2 ) or a \u201cnew\u201d previously unopened one. Thus, we have\nIn total we get Pr(s 3 = s 2 |s 2 = s 1 ) = p\n· b = 1 2\n) > 1 2\nThis proves that LPS is also erroneous in assuming indepen- dence between samples.\nOne may further note that the same probabilities are valid for any other overlapping pair, i.e., for Pr(s k = s j |s j = s i ) and Pr(s k = s j |s j = s i ) for all distinct indices i, j and k.\nThis dependency may seem natural since the two sam- ples are overlapping in one of the observations. Collect- ing samples in a non-overlapping fashion according to (s 1 , s 2 ), (s 3 , s 4 ), (s 5 , s 6 ), and so on, may at ﬁrst glance seem better. However, by performing similar calculations we can also prove that\nThis means that the probability of pair equality in one sample depends on the previous one in this case as well. This immediately generalizes to all non-overlapping pairs, i.e., the same holds for Pr(s j = s i |s v = s u ) and Pr(s j = s i |s v = s u ) for all distinct indices i, j, u and v. Since the overlapping and non-overlapping cases are exhaustive, we can conclude that any two samples will be dependent. An intuitive explanation for this is that a sample leaks information about the entries in the S-box. This information will affect the probability of the next sample since we may read the same entries as before. We summarize this result in Theorem 2.\nTheorem 2 (Random S-box Sampling Theorem): It is not possible to extract more than one independent sample from a chunk s 1 , . . . , s of observations from a random S-box.\nThe advantage of LPS over APS is that fewer samples are used. The computational complexity of the online phase of LPS is k( − 1) = O(k ).\nIn order to correctly apply the Neyman-Pearson lemma, we need to ﬁnd the probability of the complete chunk. Thus, we collect all observations in one vector (s 1 , s 2 , . . . , s ). The vector probability distributions P 1 and P 2 both have a domain of size 2 , which determines the storage cost for handling P 1 and P 2 with VS.\nFor P 2 , all vectors are equally likely, resulting in identical probability values P 2 (v) = 2 − for all vectors v.\nThe S-box vector probability distribution P 1 can be calcu- lated according to Algorithm I, which is stated recursively for simplicity. The main idea here is simply to keep track of how many entries in the S-box that have revealed zeros and ones, as this information will enable us to compute the associated probabilities at each stage.\n(probability entries), and since this amount of memory is necessary to store the resulting probability distribution, no other algorithm can do better in terms of memory. The time complexity of Algorithm I is also exponential in , at most 4 = 2 2 , because every call at depth d results in at most 4 calls at depth d + 1. By employing dynamic programming, see e.g., [3], it is possible to improve this time complexity to O(n 2 2 ) at the expense of increased storage, O(n 2 2 ), but the running time must still necessarily be exponential in .\nFor large , i.e., when many observations are generated before the S-box is reinitialized, the vector sampling technique is infeasible since the distribution P 1 is both too large to store and too demanding to compute.\nNow consider WS, for which we take vector weights (s 1 , s 2 , . . . , s ) = i=1 s i as samples. The corresponding\nvector probability distributions P 1 and P 2 have domains of size + 1, which is much more manageable than those for VS.\nFor WS we begin with P 2 . Every vector is equally likely in the ideal case, so the resulting vector weight probability distribution is combinatorially determined by\nP 1 can be calculated according to Algorithm II, which is just a simple modiﬁcation of Algorithm I. Instead of passing on a (partial) vector we now pass on the (accumulated) vector weight. The algorithm is, again, stated recursively for simplic- ity, but it can also be implemented in a dynamic programming fashion as detailed in [2]. Upper bound formulas for memory and computational complexity for handling vectors of size\nderived from an S-box of size n are given by n 2 2 and n 2 2 4 , respectively.\nWe now explicitly prove that VS and WS are equivalent in terms of keystream complexity of the resulting distinguisher. We ﬁrst present Algorithm III which calculates the proba- bility of an S-box outputting a speciﬁc vector \u2013 the vector probability. The correctness of Algorithm III follows from its relationship to Algorithm I.\nTheorem 3 (WS is optimal): WS is equivalent to VS in terms of the Neyman-Pearson test (Lemma 1).\nProof: The proof follows if we can show that all vectors of equal weight are equiprobable, because the probability distributions P 1 and P 2 for WS can then be derived from those of VS by grouping all probabilities for vectors of equal weight. In such a case the Neyman-Pearson test is equal for both sampling techniques, showing that no information is lost when considering WS over VS.\nIt is sufﬁcient to show that the vector probability is invariant under pairwise bit ﬂips. That is, we need to show that the vector probability does not change if a neighboring pair of bits in a vector are ﬂipped from 10 to 01 (or from 01 to 10).\nLet v = (s 1 , s 2 , . . . , s ) be a vector for which s i = 0 and s i+1 = 1 for some i, and let v be the corresponding vector with s i = 1 and s i+1 = 0. Let v j denote the vector (s j , s j+1 , . . . , s ). We need to show that vp(p, v, a 0 , a 1 ) =\nvp(p, v , a 0 , a 1 ) (we omit some of the less interesting param- eters).\nAll recursive calls to vp(p, v, a 0 , a 1 ) and vp(p, v , a 0 , a 1 ) are identical up to depth i, so it is enough to consider any two such calls vp(p, v i , a 0 , a 1 ) and vp(p, v i , a 0 , a 1 ) at depth i. We need to show that both these calls give rise to the same quadruple of function calls at depth i + 2, two levels deeper.\nvp(p · a 0 n\n2n \t , v i+1 , a 0 + 1, a 1 ) at level i + 1, and then into\n− a 1 ) 2n 2\n− a 1 )a 1 2n 2\nvp(p · (n − a 0 − a 1 )(n − (a 0 + 1) − a 1 ) (2n) 2\nvp(p · a 1 (n − a 0 − a 1 ) 2n 2\n2n 2 \t , v i+2 , a 0 , a 1 + 1) and vp(p ·\nA direct consequence of Theorem 3 is that, although VS is highly impractical to use due to its exponential time- and memory complexities, WS will provide the same result as VS at a much lower cost, allowing much larger -values. The computational complexity of the distinguisher is given by O(k).\nWe can also use Theorem 3 to improve the efﬁciency of computing P 1 with Algorithm II. This is also true for the dynamic programming variant of the algorithm presented in [2]. Since all vectors of equal weight are equiprobable, we need only consider vectors of type 00 . . . 011 . . . 1. The improved algorithm is to calculate the probabilities for all\n+1 such vectors by using a dynamic programming version of Algorithm III. Recall that the time and memory complexities given in [2] are O(n 2 2 ) and O(n 2 ), respectively, so memory usage is limiting in practice. For the new algorithm we need only O(min(n, )) memory for storing intermediate probabil- ity values and O( ) storage to hold the resulting probability distribution. An additional improvement is to recognize that the distribution is symmetric, so we need only compute half of it.\nWhile the time required is still O(n 2 2 ), the constants are better. Our simulations show that we save 80-85% in time, and\nthe memory usage is O( ), i.e., it no longer depends on the size of the S-box. This is optimal since it equals the length of the vector.\nWe have shown above that both APS and LPS are erroneous as the corresponding samples are not taken independently. Still, both techniques are very simple to apply. The distribution P 1 is very easy to compute in each case, and checking if (s i = s j ) is trivial, but the drawback is that the resulting distinguisher will not be optimal. For optimality, WS (or VS) must be used. This optimality comes at the cost of a larger precomputational complexity, i.e., for computing P 1 . Table I summarizes the important parameters corresponding to each sampling technique. Note that we assume that the best dy- namic programming variant is used to compute the probability distributions P 1 for VS and WS. The actual performance of the attack using each of the sampling techniques has been simulated. As VS and WS give the exact same distinguisher performance, only WS is included in the simulations. For a fair comparison, we assume that the number of chunks is the same for all variants, i.e., APS and LPS are allowed to use many more samples than WS, but all use the same number of observations. The two plots in Fig. 1 show the error probabilities for APS, LPS and WS when the size of the S-box is n = 64 and the number of observations in each chunk is = 20 and = 500, respectively. Similarly, Fig. 2 shows the error probabilities when the size of the S-box is n = 512.\nFrom the simulations we can see that both LPS and APS are outperformed by WS. It is interesting to see that APS is not very much worse when the same number of chunks is considered. However, we stress again that APS uses a factor\nmore samples than WS. This clearly shows the problem of assuming independent samples when they are in fact very dependent. Also, the factor 2 makes APS impractical for large .\nSpeciﬁcally for HC-128, each observation is a linear combi- nation of keystream bits. In this case, the comparison assumes an equal number of keystream bits for all sampling techniques.\nLooking at Fig. 1 and Fig. 2 it seems that the performance of APS approaches that of WS when the S-box size n increases and when the chunk size decreases. Thus, for large n and small their performances are practically equal, while for small n and large , WS clearly outperforms APS. We have\nsimulated many other choices of n and , and all simulations show this same tendency.\nFour different sampling techniques for random S-box out- puts have been considered and analyzed. We have proved that it is not possible to take two independent samples from one chunk of a random S-box, which implies that APS and LPS are sub-optimal as they impose a higher error probability in the resulting distinguisher. We have also proved that WS is equivalent to the optimal VS, and that WS is much more prac- tical than VS. WS is thus the preferred sampling technique. We have also presented an improved algoritm for the ofﬂine computation of P 1 for WS.\nEven though APS and LPS are not optimal, they are very simple to apply. For large S-boxes that are frequently rerandomized, the APS technique is very close to optimal."},"refs":[{"authors":[{"name":"T. Cove"},{"name":"J. A. Thomas"}],"title":{"text":"Elements of Information Theory"}},{"authors":[{"name":"P. Stankovsk"},{"name":"S. Ru"},{"name":"M. Hel"},{"name":"T. Johansson"}],"title":{"text":"Improved Distinguishers for HC-128"}},{"authors":[{"name":"R. Rivest T"},{"name":"C. Leiserso"},{"name":"C. Stein"}],"title":{"text":"Cormen,   Introduction to Algorithms, Third Edition "}},{"authors":[{"name":"H. Wu"}],"title":{"text":"A New Stream Cipher HC-256"}},{"authors":[{"name":"H. Wu"}],"title":{"text":"The Stream Cipher HC-128"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569556091.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T6.3","endtime":"10:50","authors":"Paul Stankovski, Martin Hell","date":"1341311400000","papertitle":"An Optimal Sampling Technique for Distinguishing Random S-boxes","starttime":"10:30","session":"S5.T6: Boolean Functions and Related Topics","room":"Kresge Rehearsal A (033)","paperid":"1569556091"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
