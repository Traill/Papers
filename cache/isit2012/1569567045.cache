{"id":"1569567045","paper":{"title":{"text":"Non-entropic Inequalities from Information Constraints"},"authors":[{"name":"Siu-Wai Ho"},{"name":"Terence Chan"},{"name":"Alex Grant"}],"abstr":{"text":"Abstract\u2014This paper investigates a new method in proving converses in secure communication problems. The method gives a converse result in terms of the logarithm of support size instead of entropy. The results are connected to constrained information inequalities involving three random variables. A new constrained non-Shannon type inequality is shown."},"body":{"text":"The capacity of a system is usually written in terms of Shannon\u2019s information measures (e.g., entropy and mutual information). However, the system constraints may involve parameters other than entropy, such as error probability. In the typical approach for proving converses, we ﬁrst rephrase these constraints in terms of entropy. For example, Fano\u2019s inequality is usually used to transform a decoding constraint in terms of error probability into a constraint on entropy. Then information inequalities can be applied to show the desired results. Here, information inequalities refer to inequalities involving only functions of entropy on both sides.\nRecently, we have investigated a security system, in which the capacity of a system cannot be expressed in terms of entropy while the system requirements are in terms of only entropy or mutual information [1]. Therefore, typical approach cannot give our desired results. The security system involves a set of three random variables {U, X, R} which satisﬁes\nI(U ; R) = 0, \t (1) I(U ; X) = 0, \t (2)\nHere, U is a secret message, R is a secret key shared by the transmitter and receiver, and X is the ciphertext. The constraint (1) is due to the independence between the secret message and the secret key. We assume that the ciphertext is transmitted over a public and insecure channel. The constraint (2) thus ensures that an eavesdropper who observes only X can infer no information about the secret message U . Finally, the constraint (3) guarantees that the receiver can reconstruct the secret message with zero error. Intuitively, one may expect that H(U ) ≤ H(R) and H(U ) ≤ H(X). In this paper, we will show that both H(R) and H(X) are lower bounded by log |U|, where U is the support of U . Our proving technique has been recently applied in other problems [2].\nThe results in this paper can also contribute to information inequalities. An information inequality is called Shannon-type if it can be proved by the nonnegativity of Shannon informa- tion measures. Otherwise, it is called non-Shannon-type [3] [4]. So far, it was proved that all unconstrained information inequalities involving three random variables are Shannon- type inequalities [3, Ch. 15]. For four or more random vari- ables, there are inﬁnite number of unconstrained information inequalities [5] [6]. While it seems that a characterization of information inequalities involving three random variables is complete, [7] proved an interesting result about piece- wise linear constrained information inequality involving three random variables. This result suggested that characterization of constrained information inequalities involving three random variables has not been completely solved.\nIn this paper, our tighter bounds on H(R) and H(X) are new constrained information inequalities and can be reduced to the existing results. More importantly, it presents the relations among random variables by a simpler form.\nIn addition to the above case, we will consider some special cases in Section II. While in Case (a), {U, X, R} satisﬁes only (1)\u2013(3), the random variables are also required to satisfy\nin Case (b). In Case (c), {U, X, R} is further required to satisfy (1)\u2013(4) and\nThese cases allow us to rederive the non-Shannon-type in- equalities obtained in [7] and [3, Ch. 15]. Finally, Case (d) is considered in Section III where {U, X, R} satisﬁes only (1)\u2013 (3) and (5). We will show that the support sizes of the key R and the ciphertext X greatly depend on whether the probability distribution of secret message U has only rational probability masses. Information diagrams of {U, X, R} in the four cases are shown in Fig. 1. These diagrams pictorially illustrate the constraints {U, X, R} required in each case.\nNotations: Random variables will be denoted by a capital letter such as A, U, V . For random variables A and B, the probability of the event \u201c A = a\u201d and the conditional proba- bility of the event \u201c A = a\u201d given that B = b will be speciﬁed by P A (a) and P A |B (a|b) respectively. The support of a random variable A is the set of outcomes a such that P A (a) > 0. We will denote the support by the calligraphic letter A.\nWe ﬁrst give an alternative proof of Theorem 1 in [1], i.e., Case (a) and then we will compare the results with Cases (b) and (c). Instead of giving a formal proof, we will see more insights from this alternative proof.\nTheorem 1 (C ase (a): H o et al. [1 ]): L et X , R and U be the supports of random variables X, R, and U , respectively. Suppose |U| is ﬁnite. If {U, X, R} satisﬁes (1)\u2013(3), then\nlog |U| ≤ H(X), \t (8 ) log |U| ≤ H(R). \t (9 )\nConsider a unit square as shown in Fig. 2(a). We divide it into |U| rows and |R| columns such that the width of rows and columns are proportional to P U (u) and P R (r),\nrespectively. The area of the cell with coordinate (u, r) is equal to P U (u)P R (r) = P U R (u, r) due to (1). Now, we partition the cell (u, r) proportional to P X |U =u,R=r and assign values for each partition. For example, 1 and 2 are assigned the cell (1, 1) in Fig. 2(b) because P X |U R (x|1, 1) > 0 for x = 1 or 2. Since the same ciphertext x can be generated from the same message U with different key R, the same number x can appear more than once in the same row but in different columns. However, the same number cannot appear more than once in the same column but in different rows due to (3). Otherwise, the decoder side will be confused. Therefore, if we sum the widths of the partitions with same label, say x, the summation is less than the width of the unit square which is equal to 1 (i.e.,\nP R (r)). This can be seen in Fig. 2(c). Finally, the total widths of the partitions with label x is the same for different rows due to (2). Therefore, P X (x) ≤ |U| −1 .\nSince a uniform distribution with support U always ma- jorizes P X from (6), (8 ) can be easily veriﬁed by the theory of majorization [8 ] even if X may be deﬁned on a countably inﬁnite alphabet [9 ]. Finally, (7) and (9 ) follow from the symmetric roles of X and R in (2)\u2013(3).\nR emark : The inequalities (6) and (7) can be restated as information inequalities involving R´enyi entropies [10 ]. L et A be a discrete random variable whose support is A. Its α th order R´enyi entropy, denoted by H α (A), is deﬁned as follows:\nH 0 (A) = log |A|, H 1 (A) = −\nα − 1 \t if 0 < α < 1 or 1 < α < ∞. Note that H 1 (A) is simply the Shannon entropy of A. It is easy to see that (6) and (7) are equivalent to\nH 0 (U ) ≤ H ∞ (X) \t (10 ) H 0 (U ) ≤ H ∞ (R). \t (11)\nFurthermore, as H α (A) is a monotonic decreasing function of α for any given random variable A (i.e., H α (A) ≥ H β (A) if α ≤ β [11]), (8 ) and (9 ) follow accordingly.\nC orollary 2 (E x tension of Theorem 1 in [7 ]): If the set of random variables {U, R, X} satisﬁes (1)\u2013(3), then\nR emark : Corollary 2 extends [7, Theorem 1] in two aspects: First, {U, R, X} in [7, Theorem 1] is required to satisfy (1)\u2013 (4) in order to prove (12). Corollary 2 extends [7, Theorem 1] by showing that only the constraints (1)\u2013(3) are needed. Furthermore, the inequality (12) (which is also the same as that used in [7, Theorem 1]) in Corollary 2 can in fact be strengthened to\nfor any α, β ≥ 0. If α = β = 1, then (16) is equivalent to (12). On the other hand, if α = 0 and β = ∞, then (16) is equivalent to (6).\nC orollary 3 (N on-ex istence): If U takes values from a countably inﬁnite alphabet (i.e., |U| = ∞), then one cannot construct any auxiliary random variables {X, R} such that (1)\u2013 (3) hold simultaneously.\nNow, we will study the second case where {U, R, X} satisﬁes not only (1)\u2013(3) but also (4).\nL emma 1 : Suppose U , X, and R satisfy (1)\u2013(4). For any r ∈ R and x ∈ X such that P RX (r, x) > 0,\nP roof: If P RX (r, x) > 0, then (3) implies the existence of a unique u ∈ U such that P XRU (x, r, u) > 0. B y (4),\nTheorem 4 (C ase (b)): If U , X, and R satisfy (1)\u2013(4), then there exists a random variable Y such that\n2) for any Y = y, the conditional distributions of X and R are uniform over their supports. In particular, for all y ∈ Y, there exists an integer θ y ≥ |U| such that P X |Y (x|y) and P R |Y (r|y) are either equal to zero or θ −1 y .\nR emark : a) The random variable Y constructed in Theorem 4 is the common information between X and R [13]. b) The second statement in Theorem 4 can be rewritten as\nHere, H α (X|Y = y) is deﬁned as the α th order R´enyi entropy of a random variable A is deﬁned on the set X such that\nIn the following example, random variables satisfying the constraints in Theorems 1 and 4 are compared. After that,\nCorollary 5 shows that Theorem 4 subsumes the results in [3, p. 364].\n1) probability distributions of U and R are respectively { 1 2 , 1 4 , 1 4 } and { 3 11 , 3 11 , 2 11 , 1 11 , 1 11 , 1 11 }, 2) U and R are inde- pendent, and 3) X is randomly generated according to U and R as deﬁned in Fig. 3(a). Then {U, X, R} satisﬁes (1)\u2013(3).\nE x ample 2 (C ase (b)): Suppose the probability distribu- tion of R is changed to { 3 11 , 2 11 , 2 11 , 2 11 , 1 11 , 1 11 }, and X = g(U, R) where the function g is deﬁned in Fig. 3(b). Then {U, X, R} satisﬁes (1)\u2013(4).\nC orollary 5 (C ase (c): C hapter 1 5 in [3 ]): If the ran- dom variables U, X, R satisfy (1)\u2013(5), then P U , P X and P R are all uniform distributions with the same support size.\nIn this section, we will consider the fourth case where the set of random variables {U, X, R} satisﬁes (1)\u2013(3) and (5). In [1], we have characterized the expected key consumption is I(R; U X). Under the conditions (1)\u2013(3), I(R; U X) ≥ H(U ). Furthermore, the minimum expected key consumption, i.e., I(R; U X) = H(U ), can be achieved if (5) is satisﬁed. There- fore, the fourth case is when the security system satisfying (1)\u2013(3) achieves minimum key consumption as well.\nTheorem 6 (C ase (d)): L et X , R and U be the supports of random variables X, R, and U , respectively. If the random variables satisfy (1)\u2013(3) and (5), then\nP roof: Consider any u ∈ U and x ∈ X . As U and X are supports, P U (u) > 0 and P X (x) > 0. From (2), we have\nConsequently, there exists r ∈ R such that P U XR (u, x, r) > 0.\nP U XR (u, x, r) = P XR (x, r) \t (22) = P X (x)P R (r), \t (23)\nwhere (22) is due to (3) and (25) is due to (5). On the other hand, we have\nP U XR (u, x, r) ≤ P U R (u, r) \t (24) = P U (u)P R (r), \t (25)\nwhere (25) is due to (1). Finally, as P R (r) > 0, we have P X (x) ≤ P U (u)\nR emark s: a) Comparing (19 ) with (6) in Theorem 1, the requirement on P X is more difﬁcult to achieve if we want to achieve minimum key consumption. b) Instead of expressing everything in terms of entropy, the bounds in Theorems 1 and 6 give simpler descriptions of the consequences due to the constraints in (1)\u2013(3) or (5).\nC orollary 7 : Suppose {X n , R n , U n } satisfy (1)\u2013(3), (5) and H(U n ) > 0 for all n. If lim n → ∞ H(U n ) = 0, then\nRoughly speaking, Corollary 7 illustrates that if U has a small positive entropy, then the entropies of X and R must be large in order to satisfy the constraints (1)\u2013(3) and (5).\nIn the following, we show that the supports of X and R depend on whether P U (u) is rational for all u ∈ U. Note that P U (u) is rational for all u if and only if there exists an integer θ such that θ · P U (u) is an integer for all u.\nL emma 2 : There exists an integer θ such that θ · P U (u) is an integer for all u if and only if there exists a random variable U such that\nTheorem 8 (E x istence): If there exists an integer θ such that θ · P U (u) is an integer for all u, then X and R can be constructed such that\n|X | = |R| = θ \t (27) H(X) = H(R) = log θ, \t (28 )\nP roof: Assume that there exists an integer θ such that θ · P U (u) is an integer for all u. We can construct a random variable U as described in L emma 2. Assume without loss of generality that U takes values from the set {0, 1, . . . , θ − 1}. L et R be uniformly distributed over {0, 1, . . . , θ −1} and X = U + R m od θ. Then\nFurthermore, |X | = |R| = θ and H(X) = H(R) = log θ. Since U is a function of U , {U, X, R} still satisﬁes (1)\u2013(3) and (5) so that the theorem follows.\nTheorem 8 shows that for any P U having only rational probability masses, it is always possible to construct X and R which are deﬁned on ﬁnite alphabets and satisfy (1)\u2013(3) and (5). In the following theorem, we will prove that the rationality condition on P U (u) is an important condition. In other words, it is impractical to construct a security system satisfying (1)\u2013 (3) with minimum key consumption when P U has irrational probability masses.\nTheorem 9 : Suppose U , X, and R satisfy (1)\u2013(3) and (5). If P U (u) is irrational for any u ∈ U, then\n|X | = ∞, \t (30 ) |R| = ∞. \t (31)\nP roof: Suppose there exists u o ∈ U such that P U (u o ) is irrational. Deﬁne a new random variable U ∗ such that\nThen P U ∗ (0) and P U ∗ (1) are irrational. As U ∗ is a function of U , by (1)\u2013(3) and (5),\nTherefore, it sufﬁces to consider U deﬁned on the set {0, 1} to prove the theorem.\nSuppose to the contrary ﬁrst that |X | and |R| are both ﬁnite. We can assume without loss of generality that\nX = {1, . . . , n} \t (33) R = {1, . . . , m}. \t (34)\nx i = P X (i), i = 1, . . . , n \t (35) r j = P R (j), j = 1, . . . , m. \t (36)\nL et x be a n-row vector such that its i th entry is x i . Similarly, we can deﬁne the column vector r.\nAs X and R are independent and H(U |XR) = 0, there exists a function f such that U = f (X, R). Hence, we can deﬁne a n × m matrix G such that for any i = 1, . . . , n and j = 1, . . . , m, the (i, j) th entry of G is given by\nWe call G the decoding m atrix induced by X and R. L et G i be the i th row of G. Then\nHere, 1) is due to that I(U ; X) = 0, 2) and 3) are due to that P X and P R are probability distributions, and 4) is due to that I(U ; R) = 0.\nIn fact, for any x, r and binary matrix G satisfying the above four conditions, one can construct random variables {U, R, X} such that\nwhere U = f (X, R) and the probability distributions of X and R are speciﬁed by the vectors x and r respectively.\nIn the following, we will prove that if the rows of G are not independent, then we can construct another random variable X ∗ such that\nTo prove the claim, let A and B be two disjoint subsets of {1, . . . , n} and α i , i ∈ A ∪ B be positive numbers such that\nL et \t m in i ∈ A ∪ B x i /α i . Assume without loss of generality that n ∈ A and that = x n /α n . Deﬁne\n \n \nx i − α i if i ∈ A x i + α i if i ∈ B\nNote that x ∗ n = 0. Suppose that X\u2019s probability distribution is changed such that P X (i) = x ∗ i . Then it can be checked easily that the set of random variables {U, X, R} still satisﬁes the constraint (1)\u2013(3) and (5). Furthermore, the size of the support of P X is |X | ≤ n − 1.\nRepeating this procedure, we can prove that for any random variable U , if there exists auxiliary random variables {X, R}\nsatisfying (1)\u2013(3) and (5), then there exists another set of auxiliary random variables {X ∗ , R ∗ } such that (32) is satisﬁed and the rows and columns of the decoding matrix induced by X ∗ and R ∗ are all linearly independent. Hence, the decoding matrix G induced by X ∗ and R ∗ must be a square matrix (and thus m = n). Consequently,\nClearly, z i = x i /P U (1). As all the entries in G are either 0 or 1, all the z i are rational numbers. Therefore, 1 =\nx i = P U (1) n i =1 z i . Hence, P U (1) must be rational and a contradiction occurs. We have proved that X and R cannot be both ﬁnite. The case when only X or R is ﬁnite can be proved similarly.\nB y using an alternative approach to prove converse, bounds on entropy in terms of support size or Renyi entropy have been given, leading to new constrained information inequalities involving three random variables. Our inequalities can present the relations among random variables by a simple form. These inequalities can be applied to coding theorems in secure transmission problems (as studied in [1]). In particular, we have seen that a message cannot be transmitted securely if it has inﬁnite support. Minimum key consumption is impractical if the source distribution has irrational numbers."},"refs":[{"authors":[{"name":"S.-W. Ho"},{"name":"T. Chan"},{"name":"C. Uduwerelle"}],"title":{"text":"Tradeoff between K ey Rate and Number of Channel Use"}},{"authors":[{"name":"S.-W. Ho"},{"name":"A. Grant"}],"title":{"text":"Multiuser P roblems in B iometric Security Systems"}},{"authors":[{"name":"R. W. Y eun"}],"title":{"text":"Inform ation T heory and Netw ork C oding , Springer, 20 0 8 "}},{"authors":[{"name":"T. Chan"}],"title":{"text":"Recent P rogresses in Characterising Information Inequalities"}},{"authors":[{"name":"F. Inﬁnitely Many Information Inequalities"}],"title":{"text":"Matus,  In P roceedings ISIT 20 0 7, Nice, France, J une 20 0 7"}},{"authors":[{"name":"T. Chan"},{"name":"A. Grant"}],"title":{"text":"Non-linear Information Inequalities"}},{"authors":[{"name":"F. Mat´uˇs"}],"title":{"text":"P iecewise L inear Conditional Information Inequality."}},{"authors":[{"name":"S.-W. Ho"},{"name":"T. Chan"},{"name":"A. Grant"}],"title":{"text":"Inequalities from Information Constraints"}},{"authors":[],"title":{"text":"Common information is far less than mutual information"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569567045.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T8.2","endtime":"15:20","authors":"Siu-Wai Ho, Terence H. Chan, Alex Grant","date":"1341327600000","papertitle":"Non-entropic Inequalities from Information Constraints","starttime":"15:00","session":"S7.T8: Information Inequalities","room":"Stratton (491)","paperid":"1569567045"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
