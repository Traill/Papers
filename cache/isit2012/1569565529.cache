{"id":"1569565529","paper":{"title":{"text":"Information-Theoretically Optimal Compressed Sensing via Spatial Coupling and Approximate Message Passing"},"authors":[{"name":"David L. Donoho"},{"name":"Adel Javanmard"},{"name":"Andrea Montanari"}],"abstr":{"text":"Abstract\u2014We study the compressed sensing reconstruction problem for a broad class of random, band-diagonal sensing matrices. This construction is inspired by the idea of spatial coupling in coding theory. As demonstrated heuristically and numerically by Krzakala et al. [11], message passing algorithms can effectively solve the reconstruction problem for spatially coupled measurements with undersampling rates close to the fraction of non-zero coordinates.\nWe use an approximate message passing (AMP) algorithm and analyze it through the state evolution method. We give a rigorous proof that this approach is successful as soon as the undersam- pling rate δ exceeds the (upper) R´enyi information dimension of the signal, d(p X ). More precisely, for a sequence of signals of diverging dimension n whose empirical distribution converges to p X , reconstruction is with high probability successful from d(p X ) n+o(n) measurements taken according to a band diagonal matrix.\nFor sparse signals, i.e. sequences of dimension n and k(n) non-zero entries, this implies reconstruction from k(n) + o(n) measurements. For \u2018discrete\u2019 signals, i.e. signals whose coordi- nates take a ﬁxed ﬁnite set of values, this implies reconstruction from o(n) measurements. The result is robust with respect to noise, does not apply uniquely to random signals, but requires the knowledge of the empirical distribution of the signal p X ."},"body":{"text":"Assume that m linear measurements are taken of an un- known n-dimensional signal x ∈ R n , according to the model\nThe reconstruction problem requires to reconstruct x from the measured vector y ∈ R m , and the sensing matrix A ∈ R m×n .\nIt is an elementary fact of linear algebra that the reconstruc- tion problem will not have a unique solution unless m ≥ n. This observation is however challenged within compressed sensing. A large corpus of research shows that, under the assumption that x is sparse, a dramatically smaller number of measurements is sufﬁcient [6], [2]. Namely, if only k entries of x are non-vanishing, then roughly m 2k log(n/k) mea- surements are sufﬁcient for A random, and reconstruction can be solved efﬁciently by convex programming. Deterministic sensing matrices achieve similar performances, provided they satisfy a suitable restricted isometry condition [4]. On top of this, reconstruction is robust with respect to the addition of noise [3], [5], i.e. under the model\nwith \u2013say\u2013 w ∈ R m a random vector with i.i.d. components w i ∼ N(0, σ 2 ). In this context, the notions of \u2018robustness\u2019 or \u2018stability\u2019 refers to the existence of universal constants C such that the per-coordinate mean square error in reconstructing x from noisy observation y is upper bounded by C σ 2 .\nFrom an information-theoretic point of view it remains however unclear why we cannot achieve the same goal with far fewer than 2 k log(n/k) measurements. Indeed, we can interpret Eq. (1) as describing an analog data compression process, with y a compressed version of x. From this point of view, we can encode all the information about x in a single real number y ∈ R (i.e. use m = 1), because the cardinality of R is the same as the one of R n . Motivated by this puzzling remark, Wu and Verd´u [15] introduced a Shannon-theoretic analogue of compressed sensing, whereby the vector x has i.i.d. components x i ∼ p X . Crucially, the distribution p X is available to, and may be used by the reconstruction algorithm. Under the mild assumptions that sensing is linear (as per Eq. (1)), and that the reconstruction mapping is Lipschitz continuous, they proved that compression is asymptotically lossless if and only if\nHere d(p X ) is the (upper) R´enyi information dimension of the distribution p X . We refer to Section II for a deﬁnition of this quantity. Sufﬁces to say that, if p X is ε-sparse (i.e. if it puts mass at most ε on nonzeros) then d(p X ) ≤ ε. Also, if p X is the convex combination of a discrete part (sum of Dirac\u2019s delta) and an absolutely continuous part (with a density), then d(p X ) is equal to the weight of the absolutely continuous part.\nThis result is quite striking. For instance, it implies that, for random k-sparse vectors, m ≥ k + o(n) measurements are sufﬁcient. Also, if the entries of x are random and take values in \u2013say\u2013 {−10, −9, . . . , −9, +10}, then a sublinear number of measurements m = o(n), is sufﬁcient! At the same time, the result of Wu and Verd´u presents two important limitations. First, it does not provide robustness guarantees of the type described above. It therefore leaves open the possibility that reconstruction is highly sensitive to noise when m is signiﬁcantly smaller than the number of measurements re- quired in classical compressed sensing, namely Θ(k log(n/k)) for k-sparse vectors. Second, it does not provide any com- putationally practical algorithms for reconstructing x from measurements y.\nveloped an approach that leverages on the idea of spatial coupling . This idea was introduced for the compressed sens- ing literature by Kudekar and Pﬁster [12]. Spatially coupled matrices are \u2013roughly speaking\u2013 random sensing matrices with a band-diagonal structure. The analogy is, this time, with channel coding. In this context, spatial coupling, in conjunction with message-passing decoding, allows to achieve Shannon capacity on memoryless communication channels. By analogy, it is reasonable to hope that a similar approach might enable to sense random vectors x at an undersampling rate m/n close to the R´enyi information dimension of the coordinates of x, d(p X ). Indeed, the authors of [11] evaluate this approach numerically on a few classes of random vectors and demonstrate that it indeed achieves rates close to the fraction of non-zero entries. They also support this claim by insightful statistical physics arguments.\nFinally, let us mention that robust sparse recovery of k- sparse vectors from m = O(k log log(n/k)) measurement is possible, using suitable \u2018adaptive\u2019 sensing schemes [10].\nIn this paper, we ﬁll the gap between the above works, and present the following contributions:\n\u2022 Construction. We describe a construction for spatially coupled sensing matrices A that is somewhat broader than the one of [11] and give precise prescriptions for the asymptotics of various parameters. We also use a somewhat different reconstruction algorithm from the one in [11], by building on the approximate message passing (AMP) approach of [8], [9]. AMP algorithms have the advantage of smaller memory complexity with respect to standard message passing, and of smaller computational complexity whenever fast multiplication procedures are available for A.\n\u2022 Rigorous proof of convergence. Our main contribution is a rigorous proof that the above approach indeed achieves the information-theoretic limits set out by Wu and Verd´u [15]. Indeed, we prove that, for sequences of spatially cou- pled sensing matrices {A(n)} n∈N , A(n) ∈ R m(n)×n with asymptotic undersampling rate δ = lim n→∞ m(n)/n, AMP reconstruction is with high probability successful in recovering the signal x, provided δ > d(p X ).\n\u2022 Robustness to noise. We prove that the present approach is robust 1 to noise in the following sense. For any signal dis- tribution p X and undersampling rate δ, there exists a constant C such that the output x(y) of the reconstruction algorithm achieves a mean square error per coordinate n −1 E{ x(y) − x 2 2 } ≤ C σ 2 . This result holds under the noisy measurement model (2) for a broad class of noise models for w, including i.i.d. noise coordinates w i with E{w 2 i } = σ 2 < ∞.\n\u2022 Non-random signals. Our proof does not apply uniquely to random signals x with i.i.d. components, but indeed to more general sequences of signals {x(n)} n∈N , x(n) ∈ R n indexed\nby their dimension n. The conditions required are: (1) that the empirical distribution of the coordinates of x(n) converges (weakly) to p X ; and (2) that x(n) 2 2 converges to the second moment of the asymptotic law p X . Interestingly, the present framework changes the notion of \u2018structure\u2019 that is relevant for reconstructing the signal x. Indeed, the focus is shifted from the sparsity of x to the information dimension d(p X ).\nIn the next section we state formally our results, and discuss their implications as well as the basic intuition behind them. Section III provides a precise description of the matrix construction and the reconstruction algorithm. Due to space limitations, the proofs of the theorems are removed from this version of the paper and can be found in [7].\nWe consider the noisy model (2). An instance of the prob- lem is therefore completely speciﬁed by the triple (x, w, A). We will be interested in the asymptotic properties of se- quence of instances indexed by the problem dimensions S = {(x(n), w(n), A(n))} n∈N . We recall a deﬁnition from [1]. (More precisely, [1] introduces the B = 1 case of this deﬁnition.)\nDeﬁnition II.1. The sequence of instances S = {x(n), w(n), A(n)} n∈N indexed by n is said to be a B-converging sequence if x(n) ∈ R n , w(n) ∈ R m , A(n) ∈\nR m×n with m = m(n) is such that m/n → δ ∈ (0, ∞), and in addition the following conditions hold:\n(a) The empirical distribution of the entries of x(n) con- verges weakly to a probability measure p X on R with bounded second moment. Further n −1 n i=1 x i (n) 2 → E p X {X 2 }.\n(b) The empirical distribution of the entries of w(n) con- verges weakly to a probability measure p W on R with bounded second moment. Further m −1 m i=1 w i (n) 2 → E p W {W 2 } ≡ σ 2 .\n(c) If {e i } 1≤i≤n , e i ∈ R n denotes the canonical basis, then lim sup\nmax i∈[n] A(n)e i 2 ≤ B, lim inf\nWe further say that S is a converging sequence if it is B- converging for some B. We say that {A(n)} n≥0 is a converg- ing sequence of sensing matrices if they satisfy condition (c) above for some B.\nFinally, if the sequence {(x(n), w(n), A(n))} n≥0 is ran- dom, the above conditions are required to hold almost surely.\nGiven a sensing matrix A, and a vector of measurements y, a reconstruction algorithm produces an estimate x(A; y) ∈ R n of x. In this paper we assume that the empirical distribution p X , and the noise level σ 2 are known to the estimator, and hence the mapping x : (A, y) → x(A; y) implicitly depends on p X and σ 2 . Since however p X , σ 2 are ﬁxed throughout, we avoid the cumbersome notation x(A, y, p X , σ 2 ).\nGiven a converging sequence of instances S = {x(n), w(n), A(n)} n∈N , and an estimator x, we deﬁne the asymptotic per-coordinate reconstruction mean square error as\nNotice that the quantity on the right hand side depends on the matrix A(n), which will be random, and on the signal and noise vectors x(n), w(n) which can themselves be random. Our results hold almost surely with respect to these random variables.\nIn this paper we study a speciﬁc low-complexity estimator, based on the AMP algorithm ﬁrst proposed in [8]. This pro- ceed by the following iteration (initialized with x 1 i = E p X X for all i ∈ [n]).\nx t+1 = η t (x t + (Q t A) ∗ r t ) , \t (5) r t = y − Ax t + b t r t−1 . \t (6)\nHere, for each t, η t : R n → R n is a differentiable non- linear function that depends on the input distribution p X . Further, η t is separable, namely, for a vector v ∈ R n , we have η t (v) = (η 1,t (v 1 ), . . . , η n,t (v n )). The matrix Q t ∈ R m×n and the vector b t ∈ R m can be efﬁciently computed from the current state x t of the algorithm, indicates Hadamard (entrywise) product and X ∗ denotes the transpose of matrix X. Further Q t does not depend on the problem instance and hence can be precomputed. Both Q t and b t are block-constants. This property makes their evaluation, storage and manipulation particularly convenient. We refer to the next section for explicit deﬁnitions of these quantities. In particular, the speciﬁc choice of η i,t is dictated by the objective of minimizing the mean square error at iteration t + 1, and hence takes the form of a Bayes optimal estimator for the prior p X . In order to stress this point, we will occasionally refer to this as to the Bayes optimal AMP algorithm.\nWe denote by MSE AMP (S; σ 2 ) the mean square error achieved by the Bayes optimal AMP algorithm, where we made explicit the dependence on σ 2 . Since the AMP esti- mate depends on the iteration number t, the deﬁnition of MSE AMP (S; σ 2 ) requires some care. The basic point is that we need to iterate the algorithm only for a constant number of iterations, as n gets large. Formally, we let\nAs discussed above, limits will be shown to exist almost surely, when the instances (x(n), w(n), A(n)) are random, and almost sure upper bounds on MSE AMP (S; σ 2 ) will be proved. (Indeed MSE AMP (S; σ 2 ) turns out to be deterministic.)\nWe will tie the success of our compressed sensing scheme to the fundamental information-theoretic limit established in [15]. The latter is expressed in terms of the (upper) R´enyi information dimension of the probability measure p X [13], denoted by d(p X ). Further, our \u2018stability\u2019 result is expressed in terms of the (upper) MMSE dimension of the probability\nmeasure p X [16], denoted by D(p X ). It is convenient to recall the following result in this regard.\nProposition II.2 ([13], [16]). Consider the Lebesgue\u2019s decom- position of probability p X as p X = (1−α)ν d +α 1 ν ac +α 2 ν sc , where ν d is a discrete distribution, ν ac is an absolutely continuos measure with respect to Lebesgue measure, and ν sc is a singular continuos measure with respect to Lebesgue measure. Then, d(p X ) ≤ D(p X ) ≤ α = α 1 + α 2 , and if α 2 = 0, then d(p X ) = D(p X ) = α.\nTheorem II.3. Let p X be a probability measure on the real line and assume δ > d(p X ). Then there exists a random converging sequence of sensing matrices {A(n)} n≥0 , A(n) ∈ R m×n , m(n)/n → δ, for which the following holds. For any ε > 0, there exists σ 0 such that for any converging sequence of instances {(x(n), w(n))} n≥0 with parameters (p X , σ 2 , δ) and σ ∈ [0, σ 0 ], we have, almost surely\nTheorem II.4. Let p X be a probability measure on the real line and assume δ > D(p X ). Then there exists a random converging sequence of sensing matrices {A(n)} n≥0 , A(n) ∈ R m×n , m(n)/n → δ and a ﬁnite stability constant C = C(p X , δ), such that the following is true. For any converging sequence of instances {(x(n), w(n))} n≥0 with parameters (p X , σ 2 , δ), we have, almost surely\nNotice that, by Proposition II.2, D(p X ) ≥ d(p X ), and D(p X ) = d(p X ) for a broad class of probability measures p X . Further, the noiseless model (1) is covered as a special case of Theorem II.3 by taking σ 2 ↓ 0.\nwhere q is a measure that is absolutely continuous with respect to Lebesgue measure, i.e. q(dx) = f (x) dx for some measurable function f . Then, by Proposition II.2, we have d(p X ) = D(p X ) = α. Now let {x(n)} n≥0 be a sequence of vectors with i.i.d. components x(n) i ∼ p X . Denote by k(n) the number of nonzero entries in x(n). Then, almost surely as n → ∞, Bayes optimal AMP recovers the signal x(n) from m(n) = k(n) + o(n) spatially coupled measurements.\nUnder the regularity hypotheses of [15], no scheme can do substantially better, i.e. reconstruct x(n) from m(n) measure- ments if lim sup\nOne way to think about this result is the following. If an oracle gave us the support of x(n), we would still need m(n) ≥ k(n) − o(n) measurements to reconstruct the signal. Indeed, the entries in the support have distribution q, and\nd(q) = 1. Theorem II.3 implies that the measurements overhead for estimating the support of x(n) is sublinear, o(n), even when the support is of order n.\nIn the next section we describe the basic intuition behind the surprising phenomenon in Theorems II.3 and II.4, and why are spatially-coupled sensing matrices so useful.\nSpatially-coupled sensing matrices A are \u2013roughly speaking\u2013 band diagonal matrices. It is convenient to think of the graph structure that they induce on the reconstruction problem. Associate one node (a variable node in the language of factor graphs) to each coordinate i in the unknown signal x. Order these nodes on the real line R, putting the i-th node at location i ∈ R. Analogously, associate a node (a factor node ) to each coordinate a in the measurement vector y, and place the node a at position a/δ on the same line. Connect this node to all the variable nodes i such that A ai = 0. If A is band diagonal, only nodes that are placed close enough will be connected by an edge. See Figure 1 for an illustration.\nIn a spatially coupled matrix, additional measurements are associated to the ﬁrst few coordinates of x, say coordinates x 1 , . . . , x n 0 with n 0 much smaller than n. This has a negligible impact on the overall undersampling ratio as n/n 0 → ∞. Although the overall undersampling remains δ < 1, the coordi- nates x 1 , . . . , x n 0 are oversampled. This ensures that these ﬁrst coordinates are recovered correctly (up to a mean square error of order σ 2 ). As the algorithm is iterated, the contribution of these ﬁrst few coordinates is correctly subtracted from all the measurements, and hence we can effectively eliminate those nodes from the graph. In the resulting graph, the ﬁrst few variables are effectively oversampled and hence the algorithm will reconstruct their values, up to a mean square error of order σ 2 . As the process is iterated, variables are progressively reconstructed, proceeding from left to right along the node layout.\nWhile the above explains the basic dynamics of AMP reconstruction algorithms under spatial coupling, a careful consideration reveals that this picture leaves open several challenging questions. In particular, why does the overall un- dersampling factor δ have to exceed d(p X ) for reconstruction to be successful? Our proof is based on a potential function argument. We will prove that there exists a potential function for the AMP algorithm, such that, when δ > d(p X ), this function has its global minimum close to exact reconstruction. Further, we will prove that, unless this minimum is essentially achieved, AMP can always decrease the function.\nIn this section, we deﬁne an ensemble of random matrices, and the corresponding choices of Q t , b t , η t that achieve the reconstruction guarantees in Theorems II.3 and II.4.\nThe sensing matrix A will be constructed randomly, from an ensemble denoted by M(W, M, N ). The ensemble depends\non two integers M, N ∈ N, and on a matrix with non-negative entries W ∈ R R×C + , whose rows and columns are indexed by the ﬁnite sets R, C (respectively \u2018rows\u2019 and \u2018columns\u2019). The matrix is roughly row-stochastic, i.e.\n1 2\nWe will let |R| ≡ L r and |C| ≡ L c denote the matrix di- mensions. The ensemble parameters are related to the sensing matrix dimensions by n = N L c and m = M L r .\nIn order to describe a random matrix A ∼ M(W, M, N ) from this ensemble, partition the columns and rows indices in, respectively, L c and L r groups of equal size. Explicitly\n[n] = ∪ s∈C C(s) , |C(s)| = N , [m] = ∪ r∈R R(r) , |R(r)| = M .\nHere and below we use [k] to denote the set of ﬁrst k integers [k] ≡ {1, 2, . . . , k}. Further, if i ∈ R(r) or j ∈ C(s) we will write, respectively, r = g(i) or s = g(j). In other words g( · ) is the operator determining the group index of a given row or column.\nWith this notation we have the following concise deﬁnition of the ensemble.\nDeﬁnition III.1. A random sensing matrix A is distributed according to the ensemble M(W, M, N ) (and we write A ∼ M(W, M, N )) if the entries {A ij , i ∈ [m], j ∈ [n]} are independent Gaussian random variables with\nState evolution allows an exact asymptotic analysis of AMP algorithms in the limit of a large number of dimensions. As in- dicated by the name, it bears close resemblance to the density evolution method in iterative coding theory [14]. Somewhat surprisingly, this analysis approach is asymptotically exact despite the underlying factor graph being far from locally tree- like.\nState evolution recursion is used in deﬁning the parameters Q t , b t , η t and also plays a crucial role in the algorithm analysis [7]. In the present case, state evolution takes the following form.\nDeﬁnition III.2. Given W ∈ R L r ×L c + \t roughly row-stochastic, the corresponding state evolution sequence is the sequence of\nvectors {φ(t), ψ(t)} t≥0 , φ(t) = (φ a (t)) a∈R ∈ R R + , ψ(t) = (ψ i (t)) i∈C ∈ R C + , deﬁned recursively by\nIn order to fully deﬁne the AMP algorithm (5), (6), we need to provide constructions for the matrix Q t , the nonlinearities η t , and the vector b t . In doing this, we exploit the fact that the state evolution sequence {φ(t)} t≥0 can be precomputed.\nNotice that Q t is block-constant: for any r ∈ R, s ∈ C, the block Q t R(r),C(s) has all its entries equal.\nAs mentioned in Section I, the function η t : R n → R n is chosen to be separable, i.e. for v ∈ R N :\nWe take η t,i to be a conditional expectation estimator for X ∼ p X in gaussian noise:\nη t,i (v i ) = E X X + s g(i) (t) −1/2 Z = v i , s r (t) ≡\nwhere Z ∼ N(0, 1) and independent of X. Finally, in order to deﬁne the vector b t i , let us introduce the quantity\n1 δ\nwhere we deﬁned Q t i,j = Q t r,u for i ∈ R(r), j ∈ C(u). Again b t i is block-constant: the vector b t C(u) has all its entries equal.\nIn order to prove our main Theorem II.3, we use a sensing matrix from the ensemble M(W, M, N ) for a suitable choice of the matrix W ∈ R R×C . Our construction depends on parameters ρ ∈ R + , L, L 0 ∈ N, and on the \u2018shape function\u2019 W. As explained below, ρ will be taken to be small, and hence we will treat 1/ρ as an integer to avoid rounding (which introduces in any case a negligible error).\nDeﬁnition III.3. A shape function is a function W : R → R + continuously differentiable, with support in [−1, 1] and such that R W(u) du = 1, and W(−u) = W(u).\nWe let C ∼ = {−2ρ −1 , . . . , 0, 1, . . . , L − 1}, so that L c = L + 2ρ −1 . The rows are partitioned as follows:\nwhere R 0 ∼ = {−ρ −1 , . . . , 0, 1, . . . , L−1+ρ −1 }, and |R i | = L 0 . Hence L r = L c + 2ρ −1 L 0 .\nFinally, we take N so that n = N L c , and let M = N δ so that m = M L r = N (L c + 2ρ −1 L 0 )δ. Notice that m/n = δ(L c + 2ρ −1 L 0 )/L c . Since we will take L c much larger than L 0 /ρ, we in fact have m/n arbitrarily close to δ.\nGiven these inputs, we construct the corresponding matrix W = W (L, L 0 , W, ρ) as follows.\n1) For i ∈ {−2ρ −1 , . . . , −1}, and each a ∈ R i , we let W a,i = 1. Further, W a,j = 0 for all j ∈ C \\ {i}.\n2) For all a ∈ R 0 ∼ = {−ρ −1 , . . . , 0, . . . , L − 1 + ρ −1 }, we let W a,i = ρ W ρ (a − i) for i ∈ {−2ρ −1 , . . . , L − 1}.\nA.J. is supported by a Caroline and Fabian Pease Stanford Graduate Fellowship. Partially supported by NSF CAREER award CCF- 0743978 and AFOSR grant FA9550-10-1-0360. The authors thank the reviewers for their insightful comments."},"refs":[{"authors":[{"name":"M. Bayat"},{"name":"A. Montanari"}],"title":{"text":"The LASSO risk for gaussian matrices"}},{"authors":[{"name":"E. Cande"},{"name":"J. K. Romber"},{"name":"T. Tao"}],"title":{"text":"Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency informa- tion"}},{"authors":[{"name":"E. Cande"},{"name":"J. K. Romber"},{"name":"T. Tao"}],"title":{"text":"Stable signal recovery from incomplete and inaccurate measurements"}},{"authors":[{"name":"E. J. Cand´e"},{"name":"T. Tao"}],"title":{"text":"Decoding by linear programming"}},{"authors":[{"name":"D. Donoh"},{"name":"A. Malek"},{"name":"A. Montanari"}],"title":{"text":"The Noise Sensitivity Phase Transition in Compressed Sensing"}},{"authors":[{"name":"D. L. Donoho"}],"title":{"text":"Compressed sensing"}},{"authors":[{"name":"D. L. Donoh"},{"name":"A. Javanmar"},{"name":"A. Montanari"}],"title":{"text":"Information- theoretically optimal compressed sensing via spatial coupling and ap- proximate message passing"}},{"authors":[{"name":"D. L. Donoh"},{"name":"A. Malek"},{"name":"A. Montanari"}],"title":{"text":"Message Passing Algorithms for Compressed Sensing"}},{"authors":[{"name":"D. L. Donoh"},{"name":"A. Malek"},{"name":"A. Montanari"},{"name":"I. Motivatio"}],"title":{"text":"Message Passing Algorithms for Compressed Sensing:  Construction"}},{"authors":[{"name":"P. Indy"},{"name":"E. Pric"},{"name":"D. Woodruff"}],"title":{"text":"On the Power of Adaptivity in Sparse Recovery"}},{"authors":[{"name":"F. Krzakal"},{"name":"M. M´ezar"},{"name":"F. Sausse"},{"name":"Y. Su"},{"name":"L. Zdeborova"}],"title":{"text":"Statistical physics-based reconstruction in compressed sensing"}},{"authors":[{"name":"S. Kudeka"},{"name":"H. Pﬁster"}],"title":{"text":"The effect of spatial coupling on compressive sensing"}},{"authors":[{"name":"A. R´enyi"}],"title":{"text":"On the dimension and entropy of probability distributions"}},{"authors":[{"name":"T. Richardso"},{"name":"R. Urbanke"}],"title":{"text":"Modern Coding Theory"}},{"authors":[{"name":"Y. W"},{"name":"S. Verd´u"}],"title":{"text":"R´enyi Information Dimension: Fundamental Limits of Almost Lossless Analog Compression"}},{"authors":[{"name":"Y. W"},{"name":"S. Verd´u"}],"title":{"text":"MMSE dimension"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565529.pdf"},"links":[{"id":"1569565383","weight":3},{"id":"1569565883","weight":10},{"id":"1569565223","weight":3},{"id":"1569566385","weight":3},{"id":"1569564635","weight":3},{"id":"1569565867","weight":6},{"id":"1569559617","weight":3},{"id":"1569566321","weight":3},{"id":"1569566605","weight":3},{"id":"1569566683","weight":6},{"id":"1569566855","weight":10},{"id":"1569566227","weight":3},{"id":"1569565551","weight":10},{"id":"1569552245","weight":3},{"id":"1569565227","weight":10},{"id":"1569567005","weight":3},{"id":"1569566469","weight":3},{"id":"1569565355","weight":3},{"id":"1569565461","weight":6},{"id":"1569564245","weight":3},{"id":"1569566207","weight":3},{"id":"1569564227","weight":3},{"id":"1569565123","weight":10},{"id":"1569562685","weight":3},{"id":"1569566751","weight":3},{"id":"1569565771","weight":3},{"id":"1569566999","weight":3},{"id":"1569566497","weight":3},{"id":"1569566963","weight":3},{"id":"1569564989","weight":3},{"id":"1569565897","weight":6},{"id":"1569564613","weight":6},{"id":"1569567009","weight":6},{"id":"1569566865","weight":3},{"id":"1569564337","weight":3},{"id":"1569566167","weight":6},{"id":"1569566679","weight":3},{"id":"1569566617","weight":3},{"id":"1569558681","weight":3},{"id":"1569555999","weight":10},{"id":"1569566657","weight":3},{"id":"1569567015","weight":3},{"id":"1569566437","weight":3},{"id":"1569565735","weight":10},{"id":"1569559111","weight":3},{"id":"1569565427","weight":3},{"id":"1569552251","weight":3},{"id":"1569564441","weight":3},{"id":"1569566425","weight":3},{"id":"1569554971","weight":6},{"id":"1569566209","weight":3},{"id":"1569562821","weight":3},{"id":"1569566913","weight":3},{"id":"1569566629","weight":3},{"id":"1569566661","weight":3},{"id":"1569566223","weight":6},{"id":"1569566505","weight":3},{"id":"1569565393","weight":3},{"id":"1569562207","weight":3},{"id":"1569566191","weight":3},{"id":"1569567033","weight":3},{"id":"1569565545","weight":6},{"id":"1569566245","weight":20},{"id":"1569565463","weight":3},{"id":"1569562551","weight":3},{"id":"1569551347","weight":3},{"id":"1569565885","weight":3},{"id":"1569566805","weight":6},{"id":"1569559199","weight":3},{"id":"1569557715","weight":3},{"id":"1569566983","weight":6},{"id":"1569565397","weight":3},{"id":"1569566873","weight":16},{"id":"1569565765","weight":3},{"id":"1569565093","weight":10},{"id":"1569566711","weight":3},{"id":"1569566267","weight":3},{"id":"1569566737","weight":10},{"id":"1569565353","weight":6},{"id":"1569564305","weight":3},{"id":"1569566547","weight":3},{"id":"1569566595","weight":3},{"id":"1569552025","weight":6},{"id":"1569565013","weight":3},{"id":"1569566715","weight":3},{"id":"1569566755","weight":10},{"id":"1569566641","weight":3},{"id":"1569565425","weight":6},{"id":"1569564437","weight":3},{"id":"1569551905","weight":3},{"id":"1569566487","weight":3},{"id":"1569566619","weight":3},{"id":"1569565271","weight":3},{"id":"1569566397","weight":3},{"id":"1569566001","weight":3},{"id":"1569565769","weight":3},{"id":"1569567691","weight":6},{"id":"1569565861","weight":6},{"id":"1569562367","weight":16},{"id":"1569565997","weight":13},{"id":"1569559597","weight":3},{"id":"1569565337","weight":3},{"id":"1569566341","weight":3},{"id":"1569565889","weight":3},{"id":"1569566635","weight":3},{"id":"1569566611","weight":6},{"id":"1569561397","weight":3},{"id":"1569565707","weight":3},{"id":"1569566375","weight":3},{"id":"1569565143","weight":3},{"id":"1569564257","weight":3},{"id":"1569566067","weight":3},{"id":"1569566825","weight":16},{"id":"1569566443","weight":16},{"id":"1569566727","weight":6},{"id":"1569565315","weight":10}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T7.1","endtime":"15:00","authors":"David Donoho, Adel Javanmard, Andrea Montanari","date":"1341326400000","papertitle":"Information-Theoretically Optimal Compressed Sensing via Spatial Coupling and Approximate Message Passing","starttime":"14:40","session":"S7.T7: Approximate Belief Propagation","room":"Stratton (407)","paperid":"1569565529"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
