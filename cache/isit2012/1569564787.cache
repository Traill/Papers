{"id":"1569564787","paper":{"title":{"text":"The Bethe Approximation of the Pattern Maximum Likelihood Distribution"},"authors":[{"name":"Pascal O. Vontobel"}],"abstr":{"text":"Abstract\u2014Among all memoryless source distributions, the pat- tern maximum likelihood (PML) distribution is the distribution which maximizes the probability that a memoryless source produces a string with a given pattern. Equivalently, the PML distribution maximizes the permanent of a certain non-negative matrix. We reformulate this maximization problem as a double minimization problem of a suitable Gibbs free energy function. Because ﬁnding the minimum of this function appears intractable for practically relevant problem sizes, one must look for tractable approximations.\nOne approach is to approximately ﬁnd a minimum (or at least a local minimum) of the Gibbs free energy function by applying an alternating minimization algorithm where the steps are based on quantities that are obtained by Markov chain Monte Carlo sampling. One can show that this approach is equivalent to an algorithm that was proposed by Orlitsky et al..\nAn alternative approach is to replace the Gibbs free energy function by a tractable approximation like the Bethe free energy function and to apply an alternating minimization algorithm to this function. As it turns out, empirically, this latter approach gives very good approximations to the PML distribution (or at least a locally optimal PML distribution), and, for the same level of accuracy, is two to three orders of magnitude faster than the former approach for practically relevant problem sizes.\nMoreover, the above free energy framework allows us to simplify some earlier proofs of properties of the PML distribution and to derive some new properties of the PML distribution, along with obtaining similar results for its Bethe approximation."},"body":{"text":"Consider a memoryless source with alphabet X of ﬁnite size |X | and with distribution π = (π x ) x ∈X , i.e., the prob- ability that the source outputs the symbol x ∈ X is π x . Therefore, the probability that the source outputs the sequence x x 1 , x 2 , . . . , x n of length n equals\nsymbol appears in x, we can write Pr π (X = x) =\nLet us denote the number of distinct symbols in x by m. The pattern ψ ψ (x) ψ 1 , ψ 2 , . . . , ψ n of a sequence x is deﬁned to be the integer sequence derived by replacing each symbol in x by its order of appearance.\nExample 1 Consider the alphabet X = {a, b, c, . . . , z} of size |X | = 26. Then the sequence x = c, c, a, y, c, y, y, t, a\nhas length n = 9, contains m = 4 distinct symbols, and has pattern ψ = 1, 1, 2, 3, 1, 3, 3, 4, 2.\nClearly, the length of ψ is n, i.e., the same as the length of x, and the number of distinct integers in ψ equals m, i.e., the same as the number of distinct symbols in x.\nLet the multiplicity µ ψ (ψ) of the integer ψ in the pattern ψ be the number of times that ψ appears in ψ, i.e., µ ψ (ψ)\n{i | ψ i = ψ}. All multiplicities are collected in the multiplicity vector µ (ψ). Obviously, ψ µ ψ (ψ) = n.\nExample 2 We continue Example 1. The multiplicities of the integers in the pattern ψ = 1, 1, 2, 3, 1, 3, 3, 4, 2 are µ 1 = 3, µ 2 = 2, µ 3 = 3, and µ 4 = 1.\nThe probability P (ψ; π) of a pattern ψ is deﬁned to be the probability that the memoryless source outputs a sequence of length n with the pattern ψ: P (ψ; π) Pr π ψ (X) = ψ . In the following, we will assume that X has size k for some k m, and, without loss of generality, we also assume that X = {1, 2, . . . , k}.\nDeﬁnition 3 For a given pattern ψ, the pattern maximum likelihood (PML) distribution is deﬁned to be\nwhere p varies over all distributions over X . Actually, because of symmetry considerations, we allow only distributions that satisfy p 1 p 2 · · · p k .\nThe PML distribution has many interesting properties and uses; for an in-depth discussion we refer the interested reader to the relevant literature (in particular [1], but also [2]\u2013[9]).\nThe pattern probability P (ψ; p) of a pattern ψ with m distinct integers and multiplicity vector µ µ (ψ) can be written as follows\nDeﬁnition 4 (see, e.g., [10]) Let θ = (θ i,j ) i,j be a real matrix of size k × k. The permanent of θ is deﬁned to be the scalar perm(θ) = σ i ∈[k] θ i,σ (i) , where the summation is over all k! permutations of the set [k] {1, 2, . . . , k}.\nWith this, one can then verify (see [11]) that the pattern probability P (ψ; p) can be written in terms of the permanent of a suitably deﬁned square matrix, i.e.,\n   \np µ 1 1 p µ 2 1 · · · p µ m 1 1 · · · 1 p µ 1 2 p µ 2 2 · · · p µ m 2 1 · · · 1\n.. . .. . \t .. . .. . \t .. . p µ 1 k p µ 2 k · · · p µ m k 1 · · · 1\n   \nWe simplify the notation by deﬁning µ j 0, m < j k: the entries of θ = θ(p, µ) are then θ i,j = p µ j i , (i, j) ∈ [k] 2 .\nDeﬁnition 5 (Restatement of Deﬁnition 3) For a given pat- tern ψ, the PML distribution is deﬁned to be\nwhere p varies over all distributions over X that satisfy p 1 p 2 · · · p k and where θ p , µ(ψ) is deﬁned in (1).\nBecause ﬁnding the PML distribution of a pattern appears to be intractable for practically relevant problem sizes, it is desirable to devise efﬁcient algorithms that ﬁnd approxima- tions to the PML distribution. This is the topic of the rest of this paper that presents two different approximations.\n\u2022 In the ﬁrst approach, discussed in Sec. II, we express p PML (ψ) as the solution of a certain Gibbs free energy minimization problem. We will then approximately ﬁnd a minimum (or at least a stationary point) of this Gibbs free energy function by applying an alternating minimization algorithm where the steps are based on quantities that are obtained by Markov chain Monte Carlo (MCMC) sampling. One can show that this approach is equivalent to an algorithm that was proposed by Orlitsky et al. (an early version of that algorithm was mentioned in [12]; for a detailed discussion see [8]).\n\u2022 In the second approach, discussed in Sec. III, we replace the above Gibbs free energy function by a tractable ap- proximation like the Bethe free energy function and apply an alternating minimization algorithm to this function.\nAlong the way we will show (as far as space constraints allow us) that the above free energy framework allows us to simplify some earlier proofs of properties of the PML distribution and to derive some new properties of the PML distribution. Moreover, we will see that the Bethe approxima- tion of the PML distribution behaves in many ways similar to the PML distribution.\nThe Bethe free energy approach that is pursued in this paper leverages results from [13]\u2013[15] on how to approximate the permanent of a non-negative matrix with the help of the sum- product algorithm (SPA) (see also [16]\u2013[19]). Note that there are some high-level similarities between the problem setup in [13], [14] and the problem setup here: namely, in both setups one tries to learn some model parameters (ﬂow parameters\nin [13], [14] vs. source distribution here) that maximize a model ﬁtness function that is expressed in terms of some permanent. The conclusions in both setups are very similar: for the same level of accuracy, the Bethe free energy based approach is much faster than the Gibbs free energy / MCMC based approach. There are two notable differences between the setups though. First, our model has typically many more parameters than the setup in [13], [14]. Second, the structure of the matrix θ (p, µ) can be used towards signiﬁcantly speeding up the SPA computations.\nScalars are denoted by non-boldface characters, whereas vectors and matrices are denoted by boldface characters. The size of a set S is denoted by |S|. For any positive integer L we deﬁne [L] {1, . . . , L}. Moreover, 0 · log(0) 0.\nFor a non-negative matrix θ = (θ i,j ) i,j , i.e., a matrix with only non-negative entries, we deﬁne log ◦ (θ) to be the matrix where the logarithm function is applied component-wise to θ, i.e., log ◦ (θ) \t log(θ i,j ) i,j . The inner product of two matrices U = (U i,j ) i,j and V = (V i,j ) i,j of the same size is deﬁned to be U , V \t i,j U i,j V i,j . Let U be a matrix of size L 1 × L 2 ; for integers i ∈ [L 1 ], j ∈ [L 2 ], the matrix U ∼i,∼j is the submatrix of U where the i-th row and the j-th column are omitted.\nMoreover, for a ﬁnite set S, we deﬁne Π S to be the set of probability mass functions over S. If there is a total order on the set S then we deﬁne Π S to be the set of monotone probability mass functions over S, i.e., Π S \t p ∈ Π S\np s p s \u2032 if s s \u2032 , where s, s \u2032 ∈ S . For any positive integer L, we let Γ L ×L be the set of doubly stochastic matrices of size L × L.\nIn this section we show an alternative derivation of a PML distribution estimation algorithm that was proposed by Orlitsky et al. [8], [12]. The main ideas behind our approach, which is based on Gibbs free energy functions, are: 1\n\u2022 Set up a graphical model N whose partition function equals perm(θ) with θ θ p , µ(ψ) , where ψ and p are auxiliary parameters. Here we will use the graphical model N N(θ) that was deﬁned in [17, Sec. II].\n\u2022 Express the partition function of N as the solution of a maximization problem.\n\u2022 There is a tuning parameter T 0, called the temperature. As we will see, T = 1 yields the PML distribution, whereas T = 0 yields the so-called sequence maximum likelihood (SML) distribution.\nActually, because of the way that things are typically expressed in the Gibbs variational approach, we will end up with a double minimization problem, not a double maximization problem.\nU G ( · ; θ) : Γ k ×k → R, γ → − γ, log ◦ (θ) , H G ( · ) : Γ k ×k → R, γ → min\nwith ¯ H G : R n ×n → R, ¯ γ → log c exp(− c, ¯ γ ) . Here, c ranges over all k×k permutation matrices, T 0 is called the temperature, and F G , U G , and H G are called the Gibbs free energy function, the Gibbs average energy function, and the Gibbs entropy function, respectively. The permanent of θ can then be expressed as the solution of an optimization problem, namely,\nUsually, the Gibbs free energy function is a function over the probability polytope over all valid conﬁgurations of a graphical model. However, in Lemma 6 we have used a version whose domain is projected onto Γ k ×k . (See also the comments after the upcoming Deﬁnition 12.)\nThe reformulation of the permanent in Lemma 6 allows us to rewrite the expression that deﬁnes p PML .\nDeﬁnition 7 (Restatement of Deﬁnitions 3 and 5) For a given pattern ψ, the pattern maximum likelihood (PML) distribution is deﬁned to be\nwhere p varies over Π S , where γ varies over Γ k ×k , and where θ p , µ(ψ) is deﬁned in (1). (Note that the maximization over p in Deﬁnitions 3 and 5 turned into a minimization over p.)\nNote that the function (p, γ) → F G γ ; θ p, µ(ψ) is convex in p for ﬁxed γ, and convex in γ for ﬁxed p, but in general it is not convex in (p, γ).\nIn the following, we will distinguish between the case T > 0 and the case T = 0.\nThe expression for p PML in Deﬁnition 7 makes it clear that we can use an alternating minimization approach to ﬁnd the minimum of (p, γ) → F G γ ; θ p, µ(ψ) , or at least a stationary point thereof, i.e., we ﬁx p and optimize over γ, then we ﬁx γ and optimize over p, etc..\n\u2022 For t = 1, 2, . . . alternatingly apply the following two steps until a suitable convergence criterion is met.\n\u2013 Compute γ (t) according to γ (t) arg min\n\u2013 Compute p (t) according to p (t) arg min\n\u2022 For the special case T = 1, one can show that Algo- rithm 8 is equivalent to the expectation maximization (EM) type algorithm that was proposed by Orlitsky et al. [8], [12].\n\u2022 For the special case T = 1, one can verify that the expression in (2) is equivalent to\n\u2022 It appears that the update equation in (2) (or, alternatively, (4)) is not tractable for practically relevant problem sizes. A way out of this dilemma is to approximate the permanent-based quantities in (4) by quantities that are obtained by MCMC based techniques; for a detailed discussion of such an approach see [8].\n\u2022 For proving convergence of p (t) and γ (t) to a stationary point of (p, γ) → F G γ ; θ p, µ(ψ) , one can use standard results for alternating minimization algorithms (see, e.g., [21, Proposition 2.7.1]).\nAs we show next, Algorithm 8 has the pleasing property that the ordering of the p (t) i values is maintained from iteration to iteration.\nTheorem 9 For p (t) as deﬁned in Algorithm 8 it holds that p (t) ∈ Π [k] for all t = 0, 1, . . . .\nProof: Follows from certain symmetries of the permanent as a function of the entries of a matrix. We omit the details.\nFor T = 0, ﬁnding the p and γ that minimize F G γ ; θ p, µ(ψ) is equivalent to ﬁnding the p that max-\n. The resulting probability vector p SML is called the sequence maximum likelihood (SML) distribution and can be expressed explicitly as follows: p SML = µ(ψ)/n. C. Majorization\nLet p and q be two non-negative vectors of length k such that i ∈[k] p i = i ∈[k] q i , and let p ↓ and q ↓ be vectors with the same components as p and q, respectively, but sorted in decreasing order. One says that the vector p is majorized by q if i ∈[k \u2032 ] p ↓ i \t i ∈[k \u2032 ] q ↓ i for all k \u2032 ∈ [k] (see, e.g., [22]).\nTheorem 10 Fix some T > 0. For p (t) as deﬁned in Al- gorithm 8, it holds that p (t) is majorized by p SML for all t = 1, 2, . . . .\nProof: From (3) and Sec. II-B it follows that p (t) = γ (t) ·p SML for some doubly stochastic matrix γ (t) . It is well known that this implies that p (t) is majorized by p SML .\nCorollary 11 Fix some T > 0. It holds that p PML is ma- jorized by p SML .\nProof: Apply Theorem 10 with p (0) p PML and use the fact that p (t) = p PML for t = 1, 2, . . . .\nNote that the majorization statement in Corollary 11 appears already in [9]. However, the proof therein is much longer and less intuitive.\nAs we have discussed in the previous sections, some of the quantities that appear in Algorithm 8 do not seem to be tractable for practically relevant problem sizes. In Sec. II we have also discussed an approach to circumvent these com- putational problems by approximately computing the relevant quantities.\nIn this section, we explore an alternative approach. Namely, we replace the ﬁtness function that is optimized in Deﬁnitions 5: instead of maximizing the permanent perm θ p, µ(ψ) , we will maximize the Bethe approxi- mation of the permanent perm θ p, µ(ψ) , i.e., we will maximize the Bethe permanent perm B θ p , µ(ψ) . Al- though there are potentially other functions that could be optimized, the Bethe permanent has the advantage of being reasonably close to the permanent [17], [18], yet being algo- rithmically tractable. Equivalently, this approach can be seen as replacing the cost function that is optimized in Deﬁni- tions 7: instead of minimizing the Gibbs free energy function F G γ ; θ(p, µ(ψ)) , we will minimize the Bethe free energy function F B γ ; θ(p, µ(ψ)) .\nU B ( · ; θ) : Γ k ×k → R, γ → − γ, log ◦ (θ) H B ( · ) : Γ k ×k → R, γ → − γ, log ◦ (γ)\nHere, T 0 is called the temperature, and F G , U G , and H G are called the Bethe free energy function, the Bethe average energy function, and the Bethe entropy function, respectively. The Bethe permanent of a non-negative matrix θ of size k × k is then deﬁned to be\nwhere the maximization is over all γ ∈ Γ k ×k . (See [17, Corollary 15]; alternatively, see also [13]\u2013[15].)\nUsually, the Bethe free energy is deﬁned over the local marginal polytope of a graphical model (see, e.g., [23], [24]). However, for the factor graph under consideration one can show that there is a bijective mapping between the local marginal polytope and Γ k ×k . The fact that the Gibbs and the Bethe free energy functions can be expressed as functions over the same domain allows one to compare these functions more easily. In particular, we observe that the Gibbs average\nenergy function equals the Bethe average energy function, i.e. , U B (γ; θ) = U G (γ; θ), whereas in general H B (γ) is an approximation of H G (γ).\nDeﬁnition 13 For a given pattern ψ, the Bethe approximation of the PML (BPML) distribution p BPML is deﬁned to be\n\u2022 For t = 1, 2, . . . alternatingly apply the following two steps until a suitable convergence criterion is met.\n\u2013 Compute γ (t) according to γ (t) arg min\nThe matrix γ (t) can for example be obtained with the help of the sum-product algorithm ( cf. [17, Sec. V]).\n\u2013 Compute p (t) according to p (t) = arg min\nThe Bethe free energy function (and the corresponding Algorithm 14) behave in many ways similar to the Gibbs free energy function (and the corresponding Algorithm 8):\n\u2022 The function (p, γ) → F B γ ; θ p, µ(ψ) \t is convex in p for ﬁxed γ, and convex in γ for ﬁxed p (cf. [17, Corollary 23]), but in general it is not convex in (p, γ).\n\u2022 For showing convergence of (p (t) , γ (t) ) in Algorithm 14 to a stationary point of of (p, γ) → F B γ ; θ p, µ(ψ) , one can for example use standard results for alternat- ing minimization algorithms (see, e.g., [21, Proposi- tion 2.7.1]).\n\u2022 A statement analogous to Theorems 9 can be proven using a combinatorial characterization of the Bethe per- manent that is presented in [17, Sec. VI], in particular by using its symmetries.\n\u2022 Statements analogous to Theorem 10 and Corollary 11 can easily be proven.\n\u2022 A naive implementation of the SPA to solve (5) in round t for a given vector p (t−1) has a time and space complexity proportional to k 2 per iteration. However, using the structure of the matrix θ p (t−1) , µ(ψ) , this complexity can be reduced to be proportional to k times the number of distinct entries in the vector µ (ψ). More- over, typically only few iterations are necessary because the SPA messages can be initialized with the messages of the SPA run in round t − 1.\n\u2022 Empirically, the BPML gives very good approximations to the PML distribution (or at least a locally optimal PML distribution), cf. Figs. 1 and 2. Moreover, for the same level of accuracy, Algorithm 14 is two to three orders\nof magnitude faster than the algorithm of the previous section for problem sizes as in the setups of Figs. 1 and 2.\n\u2022 It is worthwhile to also consider other permanent approxi- mations (see, e.g., [19]) and, more generally, other source distribution estimation setups, along with comparing the PML/BPML to the setup and algorithm in [25]; however, due to space limits we have to omit these aspects.\nI am indebted to my colleague Krishna Viswanathan for introducing me to the topic of the PML distribution and for answering all my subsequent questions."},"refs":[{"authors":[{"name":"A. Orlitsky"},{"name":"N. P. Santhanam"},{"name":"K. Viswanathan"},{"name":"J. Zhang"}],"title":{"text":"On modeling proﬁles instead of values"}},{"authors":[{"name":"A. Orlitsky"},{"name":"N. P. Santhanam"},{"name":"J. Zhang"}],"title":{"text":"Universal compression of memoryless sources over unknown alphabets"}},{"authors":[{"name":"G. I. Shamir"}],"title":{"text":"Universal lossless compression with unknown alphabets \u2014 the average case"}},{"authors":[{"name":"A. B. Wagner"},{"name":"P. Viswanath"},{"name":"S. R. Kulkarni"}],"title":{"text":"A better Good-Turing estimator for sequence probabilities"}},{"authors":[{"name":"J. Acharya"},{"name":"A. Orlitsky"},{"name":"S. Pan"}],"title":{"text":"Recent results on pattern maximum likelihood"}},{"authors":[{"name":"A. Orlitsky"},{"name":"S. Pan"}],"title":{"text":"The maximum likelihood probability of skewed patterns"}},{"authors":[{"name":"A. B. Wagner"},{"name":"P. Viswanath"},{"name":"S. R. Kulkarni"}],"title":{"text":"Probability estimation in the rare-events regime"}},{"authors":[{"name":"A. Orlitsky"},{"name":"S. Pan"},{"name":"N. P. Santhanam"},{"name":"K. Viswanathan"},{"name":"J. Zhang"}],"title":{"text":"Approximation of the pattern maximum linkelihood distribu- tion"}},{"authors":[{"name":"A. Orlitsky"},{"name":"N. P. Santhanam"},{"name":"K. Viswanathan"},{"name":"J. Zhang"}],"title":{"text":"On estimating the probability multiset"}},{"authors":[{"name":"H. Min"}],"title":{"text":"Permanents"}},{"authors":[{"name":"K. Viswanathan"}],"title":{"text":"Pattern maximum-likelihood,\u201d Talk at Workshop on \u201cPermanents and modeling probability distributions"}},{"authors":[{"name":"A. Orlitsky"},{"name":"N. P. Santhanam"},{"name":"K. Viswanathan"},{"name":"J. Zhang"}],"title":{"text":"Algorithms for modeling distributions over large alphabets"}},{"authors":[{"name":"M. Chertkov"},{"name":"L. Kroc"},{"name":"M. Vergassola"}],"title":{"text":"Belief propagation and beyond for particle tracking"}},{"authors":[{"name":"M. Chertkov"},{"name":"L. Kroc"},{"name":"F. Krzakala"},{"name":"M. Vergassola"},{"name":"L. Zdeborov´a"}],"title":{"text":"Inference in particle tracking experiments by passing messsages be- tween images"}},{"authors":[{"name":"B. Huang"},{"name":"T. Jebara"}],"title":{"text":"Approximating the permanent with belief prop- agation"}},{"authors":[{"name":"Y. Watanabe"},{"name":"M. Chertkov"}],"title":{"text":"Belief propagation and loop calculus for the permanent of a non-negative matrix"}},{"authors":[{"name":"P. O. Vontobel"}],"title":{"text":"The Bethe permanent of a non-negative ma- trix"}},{"authors":[{"name":"L. Gurvits"}],"title":{"text":"Unleashing the power of Schrijver\u2019s permanental inequality with the help of the Bethe approximation"}},{"authors":[{"name":"A. B. Yedidia"},{"name":"M. Chertkov"}],"title":{"text":"Computing the permanent with belief propagation"}},{"authors":[{"name":"J. S. Yedidia"},{"name":"W. T. Freeman"},{"name":"Y. Weiss"}],"title":{"text":"Constructing free-energy approximations and generalized belief propagation algorithms"}},{"authors":[{"name":"D. Bertseka"}],"title":{"text":"Nonlinear Programming, 2nd ed"}},{"authors":[{"name":"A. Marshal"},{"name":"I. Olki"}],"title":{"text":"Inequalities: Theory of Majorization and Its Applications "}},{"authors":[{"name":"M. J. Wainwright"},{"name":"M. I. Jordan"}],"title":{"text":"Graphical models, exponential families, and variational inference"}},{"authors":[{"name":"P. O. Vontobel"}],"title":{"text":"Counting in graph covers: a combinatorial characteriza- tion of the Bethe entropy function"}},{"authors":[{"name":"G. Valiant"},{"name":"P. Valiant"}],"title":{"text":"Estimating the unseen: a sublinear-sample canonical estimator of distributions"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564787.pdf"},"links":[{"id":"1569566527","weight":3},{"id":"1569566485","weight":3},{"id":"1569565377","weight":3},{"id":"1569566875","weight":6},{"id":"1569564605","weight":3},{"id":"1569566683","weight":3},{"id":"1569566761","weight":9},{"id":"1569552245","weight":3},{"id":"1569564481","weight":3},{"id":"1569566415","weight":3},{"id":"1569566469","weight":3},{"id":"1569564469","weight":3},{"id":"1569551535","weight":3},{"id":"1569565547","weight":12},{"id":"1569566821","weight":3},{"id":"1569560613","weight":3},{"id":"1569565809","weight":3},{"id":"1569564903","weight":3},{"id":"1569566963","weight":3},{"id":"1569566709","weight":3},{"id":"1569566523","weight":6},{"id":"1569564189","weight":3},{"id":"1569563981","weight":3},{"id":"1569566311","weight":9},{"id":"1569558681","weight":3},{"id":"1569559995","weight":3},{"id":"1569565199","weight":3},{"id":"1569566643","weight":3},{"id":"1569566719","weight":3},{"id":"1569559805","weight":6},{"id":"1569566811","weight":3},{"id":"1569565735","weight":6},{"id":"1569566513","weight":3},{"id":"1569565559","weight":3},{"id":"1569566371","weight":3},{"id":"1569558985","weight":3},{"id":"1569566809","weight":3},{"id":"1569565929","weight":3},{"id":"1569565357","weight":3},{"id":"1569561245","weight":3},{"id":"1569566505","weight":3},{"id":"1569565363","weight":3},{"id":"1569566695","weight":3},{"id":"1569565909","weight":3},{"id":"1569565741","weight":3},{"id":"1569551347","weight":3},{"id":"1569559199","weight":9},{"id":"1569565665","weight":42},{"id":"1569566831","weight":9},{"id":"1569566983","weight":3},{"id":"1569565435","weight":3},{"id":"1569566129","weight":3},{"id":"1569565093","weight":3},{"id":"1569566711","weight":3},{"id":"1569565661","weight":3},{"id":"1569566887","weight":3},{"id":"1569565319","weight":6},{"id":"1569564919","weight":3},{"id":"1569566917","weight":6},{"id":"1569566595","weight":3},{"id":"1569565375","weight":6},{"id":"1569566639","weight":6},{"id":"1569566819","weight":9},{"id":"1569566713","weight":3},{"id":"1569565541","weight":6},{"id":"1569566619","weight":3},{"id":"1569561185","weight":6},{"id":"1569558779","weight":3},{"id":"1569566817","weight":3},{"id":"1569566299","weight":6},{"id":"1569566171","weight":6},{"id":"1569559919","weight":3},{"id":"1569562367","weight":3},{"id":"1569565997","weight":3},{"id":"1569564961","weight":3},{"id":"1569567013","weight":3},{"id":"1569563725","weight":24},{"id":"1569556327","weight":3},{"id":"1569565113","weight":3},{"id":"1569566375","weight":3},{"id":"1569566825","weight":3},{"id":"1569564807","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T8.1","endtime":"10:10","authors":"Pascal Vontobel","date":"1341481800000","papertitle":"The Bethe Approximation of the Pattern Maximum Likelihood Distribution","starttime":"09:50","session":"S11.T8: Patterns, Estimation, Hypothesis Testing","room":"Stratton (491)","paperid":"1569564787"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
