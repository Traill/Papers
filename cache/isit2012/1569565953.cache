{"id":"1569565953","paper":{"title":{"text":"Short Message Noisy Network Coding for Multiple Sources"},"authors":[{"name":"Jie Hou"},{"name":"Gerhard Kramer"}],"abstr":{"text":"Abstract\u2014Short message noisy network coding (SNNC) trans- mits independent short messages in blocks rather than using long message repetitive encoding. SNNC is shown to achieve the same rates as noisy network coding (NNC) for discrete memoryless networks where each node transmits a multicast message. One advantage of SNNC is that backward decoding may be used which simpliﬁes the analysis and understanding of the achievability proof. The analysis reveals that each decoder may ignore certain other nodes rather than including their message in the decoding procedure. Additionally, SNNC enables early decoding at nodes if the channel quality happens to be good."},"body":{"text":"Recently, Lim et al. [1] proposed Noisy Network Coding (NNC) for communications over noisy networks and devel- oped a lower bound on the capacity. Their result recovers previous results in [2]\u2013[5] as special cases.\n\u2022 Relays quantize without hashing (or binning), which sim- pliﬁes the relay operation and is referred to as quantize- forward (QF).\n\u2022 Destinations decode messages and quantization bits jointly rather than sequentially.\nOne drawback of long messages is that they inhibit decode- forward (DF) at the relays even if the channel conditions are good [6]. For example, if one relay is close to the source and has a strong source-relay link, the natural operation is DF which removes the noise at the relay. But this is generally not possible with a long message because of its high rate.\nFortunately, a short message variant called SNNC gives the same rate as NNC for one unicast session [7] (see also [8]). SNNC has:\n1) Sources transmit independent short messages in every block (see [9, Appendix D]).\n3) Destinations decode with backward decoding or joint decoding or any other type of decoding.\nNote that the name SNNC does not depend on what kind of decoder is used but is a generic name for the encoding procedure. Indeed, we will see that the choice of decoder is important for certain problems.\nThe main contribution of this work is to show that SNNC achieves the same rates as NNC for general discrete memory-\nless networks (DMNs) with multiple multicast sessions. How- ever, we will see that SNNC seems to have extra constraints that do not appear for NNC. We show that these constraints are redundant by using the same approach as in [8].\nThis paper is organized as follows. In Section II, we state the problem and our main result. In Section III, we present SNNC for DMNs with multicast-sessions and show that SNNC achieves the same rates as NNC.\nRandom variables are written with upper case letters and their realizations with the corresponding lower case letters. Bold upper and lower case letters refer to random vectors with speciﬁed dimensions and their respective realizations. Subscripts on a symbol denote the symbol\u2019s source and the po- sition of the symbol in a sequence. For instance, X ki denotes the i-th output of the k-th encoder. Superscripts denote ﬁnite- length sequences of symbols, e.g., x n k = {x k 1 , . . . , x kn }. Set subscripts denote vectors of letters, e.g., X S = [X k , k ∈ S]. We use T n ǫ (P X ) to denote the set of letter-typical sequences with respect to P X .\nConsider the K-Node discrete memoryless network (DMN) depicted in Fig. 1, where each node has one message only. Node k ∈ K, K = {1, . . . , K} has message W k destined for nodes in the set D k , D k ⊆ K, while acting as a relay for messages of the other nodes. The messages are statistically\nindependent and each W k , k ∈ K, is uniformly distributed over the set {1, . . . , 2 nR k }, where nR k is assumed to be an integer. We write R(S) = k ∈S R k and D(S) = ∪ k ∈S D k .\nNode k transmits x kt ∈ X k at time t and receives y kt ∈ Y k . The channel is memoryless in the sense that\n= P (y 1t , . . . , y Kt |x 1t , . . . , x Kt ) \t (1) for all t.\nWe could also represent the DMN as a directed graph G = {K, E}, where E ⊆ K × K is the set of edges. Edges (links) are denoted as (i, j) ∈ E, i, j ∈ K, i = j. Suppose that every edge (i, j) is associated with a non-negative real number C ij = max\nI(X i ; Y j |X K\\i ) called the capacity of the link. Let Path (i,j) be a path that starts from node i and ends at node j. Let Γ (i,j) to be the set of all Path (i,j) . We assume |Γ (i,j) | ≥ 1 and write (k, ℓ) ∈ Path (i,j) if (k, ℓ) lies on the path Path (i,j) . We may communicate reliably between nodes i and j if R (i,j) = \t max\nC kl is positive. We assume that every node k ∈ K can communicate to any other node in the network with a positive rate in such a multihop fashion.\n\u2022 n encoding functions f n k = (f k 1 , . . . , f kn ) that generate channel inputs based on the message and past channel outputs\n\u2022 One decoding function g k (Y n k , W k ) = [ ˆ W (k) i , i ∈ D k ] that puts out estimates of the messages, where D k = {i ∈ K, k ∈ D i }, that is the set of nodes whose signals node k is interested in decoding correctly.\nwhere D = ∪ K k =1 D k . A rate tuple (R 1 , . . . , R K ) is achievable for the DMN if for any ǫ > 0, there exist some integer n and some functions {f n k } K k =1 and {g k } K k =1 such that (s.t.) P (n) e < ǫ. The capacity region is the closure of the set of achievable rate tuples.\nTheorem 1: For a K-node DMN with multiple multicast sessions, SNNC achieves rate tuples R K SNNC = (R 1 , . . . , R K ) satisfying\nfor all subsets S ⊂ K s.t. S ∩ D k = ∅ and k ∈ S c for any joint distribution that factors as\nFor clarity, we set the time-sharing random variable T to be a constant below. The encoding process of SNNC is depicted in table I.\nThe message w k , k ∈ K, of 2 nBR k bits is split into B equally sized blocks, w k 1 , . . . , w kB , each of nR k bits and is transmitted over B + K · (K − 1) blocks.\nFix a distribution K k =1 P (x k )P (ˆ y k |y k x k ). For each block j = 1, . . . , B and k ∈ K, generate 2 n (R k + ˆ R k ) codewords x kj (w kj , l k (j −1) ), w kj = 1, . . . , 2 nR k , l k (j −1) = 1, . . . , 2 n ˆ R k , according to n i =1 P X k (x (kj)i ). For each w kj and l k (j −1) , generate 2 n ˆ R k ˆ y kj (l kj |w kj , l k (j −1) ), l kj = 1, . . . , 2 n ˆ R k , ac- cording to n i =1 P ˆ Y\n|X k (ˆ y (kj)i |x (kj)i (w kj , l k (j −1) )). This de- ﬁnes the codebook\nThere is a minor modiﬁcation of the codebooks used in block j = B + 1, . . . , B + K · (K − 1). In block j = B+(k−1)·(K −1)+1, . . . , B+k·(K −1), k = 1, . . . , K, gen- erate for all nodes 2 n \u2032 ˆ R k independently identically distributed (i.i.d.) codewords x kj (1, l kj ), l kj = 1, . . . , 2 n \u2032 ˆ R k , according to n \u2032 i =1 P X k (x (kj)i ). We choose\nn ˆ R k min\nEncoding: Each node k upon receiving y kj at the end of the block j, tries to ﬁnd an index l kj s.t. the following event occurs\nwhere l k 0 = 1 by convention. If there is no such index, set l kj = 1. If there is more than one, choose one. Each node k transmits x kj (w kj , l k (j −1) ) in block j = 1, . . . , B.\nIn block j = B + (k − 1)(K − 1) + 1, . . . , B + k · (K − 1), k = 1, . . . , K, node k tries to convey l kB reliably to all other nodes by sending x kj (1, l kB ) with multihop at rate ˆ R k through the network.\nAt the end of block B + K · (K − 1), every node k ∈ D has reliably recovered l B = (l 1B , . . . , l KB ), via the multihopping of the last K(K − 1) blocks.\nFor block j = B, . . . , 1, node k tries to ﬁnd tuples ˆ w (k) j = ( ˆ w (k) 1j , . . . , ˆ w (k) Kj ) and ˆl (k) j −1 = (ˆl (k) 1(j −1) , . . . , ˆl (k) K (j −1) ) s.t. the following event occurs\nwhere l j = (l 1j , . . . , l Kj ) has already been reliably recovered from the previous block j + 1.\nError Probability: Assume without loss of generality that w j = (w 1j , w 2j , . . . , w Kj ) = (1, 1, . . . , 1) and l j −1 = (l 1(j −1) , l 2(j −1) , . . . , l K (j −1) ) = (1, 1, . . . , 1). In each block j, the error events at node k ∈ D are as follows:\nPr E (kj)0 can be made small with large n, as long as [10] ˆ R k > I( ˆ Y k ; Y k |X k ) + 3ǫ.\nTo bound Pr E (kj)2 , for each w j and l j −1 deﬁne S j (w j , l j −1 ) ⊂ K where S j (w j , l j −1 ) = {i ∈ K : w ij = 1 or l i (j −1) = 1}. Further, we deﬁne M(w j ) = {i ∈ K : w ij = 1} and Q(l j −1 ) = {i ∈ K : l i (j −1) = 1}. By deﬁnition, we have M(w j ) ⊆ S j (w j , l j −1 ), Q(l j −1 ) ⊆ S j (w j , l j −1 ), S j (w j , l j −1 ) = M(w j ) ∪ Q(l j −1 ) and k ∈ S c j (w j , l j −1 ).\nDeﬁne X S j to be the set of X ij (w ij , l i (j −1) ), i ∈ S j (w j , l j −1 ), where w ij and l i (j −1) are the corresponding elements in w j and l j −1 .\nSimilarly deﬁne X S c j , ˆ Y S j and ˆ Y S c j . Then, (X S j , ˆ Y S j ) is independent of (X S c j , ˆ Y S c j , Y kj ) and the probability that they\n \n \nwhere (a) is because all error events are disjoint and (b) is because for every node i ∈ S, we must have one of the following three cases occur:\nDeﬁning |S| to be the size of S , there are 3 |S| different ways choosing of M and Q s.t. M ∪ Q = S.\nn − 3ǫ = I(S) −\nSince we require ˆ R k ≥ I( ˆ Y k ; Y k |X k ), ∀k ∈ K, we have I(S) −\nWe can split the bounds in (8) for two different kinds of subsets S ⊂ K and B ⊂ K:\nfor all subsets S ⊂ K s.t. S ∩ D k = ∅ and k ∈ S c and all subsets B ⊆ D c k ⊂ K s.t. k ∈ B c .\nWith (10), it is guaranteed that the quantization indices l j −1 for the next (backward) decoding step are reliably decoded.\nContinuing this way, node k ∈ D successively decodes all messages w j and quantization indices l j −1 , j = 1, . . . , B.\nNote that for any S ⊂ K and k ∈ D s.t. k ∈ S c , we have k ∈ S c ∩ D(S) if and only if S ∩ D k = ∅:\n2) If k ∈ S c ∩ D(S), there exists at least one i ∈ S s.t. k ∈ D i which implies that i ∈ D k . We hence have S ∩ D k = ∅.\nHence, in [1, Theorem 2], with long-message NNC the error probability at node k ∈ D tends to zero as n → ∞ if\nThe difference between the SNNC and NNC bounds are the constraints (10). If all constraints in (10) are satisﬁed, it is obvious that SNNC achieves the same rates as NNC. In this case the decoder should decode the signals from the nodes in D c k , since they can be decoded with the signals from the nodes in D k and thereby help remove interference.\nSuppose that for one subset B ⊆ D c k , the constraint (10) is violated, i.e., R(B) ≥ R K (B). Using the same argument as in [8], we have for any B ⊂ S ⊂ K s.t. S ∩ D k = ∅ and k ∈ S c\nThe bounds in (12) are the NNC rate bounds describing the rates for the nodes in K \\ B while treating the signals from the nodes in B as noise. This suggests that even for NNC, if any of the constraints is violated, the destination node should treat the signals from the corresponding nodes as noise rather than decoding them. In this way, we get better rates.\nNow repeat the above argument for the nodes in K \\B, until we ﬁnally reach a set K k ⊆ K, for which we have no more violation of constraints, which means\nfor all subsets B ⊆ (K k \\ D k ) ⊂ K k s.t. k ∈ B c , where B c is now the complement set of B in K k .\nfor all subsets S ⊂ K k s.t. S ∩ D k = ∅ and k ∈ S c , where S c is the complement set of S in K k .\nWe remark that there is a subtle addition to [8] and differ- ence to [11], namely that each k ∈ D may have a different set K k of nodes whose messages and quantization indices it includes in its typicality test. But we can achieve the same rates as in (13) at node k with SNNC using backward decoding by treating the signals from the nodes in K \\ K k as noise. Hence we may ignore the constraints in (10) associated with SNNC at node k.\nBy the union bound, the error probability for all destina- tions tends to zero as n → ∞ if the rate tuple R K SNNC = (R 1 , . . . , R K ) satisﬁes\nfor all subsets S ⊂ K s.t. S ∩ D k = ∅ and k ∈ S c for any joint distribution that factors as\nIf D = D 1 = · · · = D K , then SNNC achieves the same rates as in [1, Theorem 1] (see also [11, Chapter 19, Theorem 5])\nRemark 1: Intuitively, for some channels it may not be so surprising that R K SNNC and R K NNC are equivalent. Consider the following network in Fig. 2. If both nodes 1 and 2 act as sources as well as relays for each other in transmitting information to the destination node d (Fig. 2(a)), referring to Theorem 1 and [1, Theorem 2 ], the SNNC and NNC bounds are the same (see Fig. 3).\nHowever, interesting cases arise when one of the two nodes acts only as a relay node and the constraints kick in. Suppose now node 2 acts as a relay node only (Fig. 2(b)), we have\nsubject to I(X 2 ; Y d |X 1 ) − I( ˆ Y 2 ; Y 2 |X 1 X 2 Y d ) > 0 (16) R 1(NNC) <min I(X 1 ; ˆ Y 2 Y d |X 2 ),\nNow we ask whether R 1(SNNC) in (15) with the constraint (16) is equivalent to R 1(NNC) in (17). This is equivalent to asking whether R K SNNC = R K NNC in point 1 and point 2 in Fig. 3. To see this, choose R 2 = ǫ, ǫ > 0, we have R K SNNC = R K NNC . Let ǫ → 0, we must have R K SNNC → R K NNC in point 1. We also have R K SNNC → R K NNC in point 2 with the same argument. Therefore, it suggests that we might ignore the quantization constraint in (16).\nIt turns out that SNNC with joint decoding also achieves the same rates as in (2). Recently, the authors of [12] claimed that SNNC with joint decoding fails to achieve the NNC rates for multi-source networks because long message repetition encoding is needed to do so. This is not true. With a minor modiﬁcation of the decoding procedure, namely, ﬁrst decoding the last quantization indices and then performing joint decod- ing with the messages and remaining quantization bits, SNNC with joint decoding performs as well as SNNC with backward decoding and NNC. This makes sense, since intuitively one would expect joint decoding to perform at least as well as backward decoding. Due to the similarity to the proof with backward decoding, details are omitted.\nJ. Hou and G. Kramer were supported by an Alexander von Humboldt Professorship endowed by the German Federal Ministry of Education and Research. G. Kramer was also supported by NSF Grant CCF-09-05235."},"refs":[{"authors":[{"name":"H. Li"},{"name":"Y.-H. Ki"},{"name":"A. El Gama"},{"name":"S.-Y. Chung"}],"title":{"text":"S"}},{"authors":[{"name":"R. Ahlswed"},{"name":"S.-Y"},{"name":"R. L"},{"name":"W. Yeung"}],"title":{"text":"Ning Cai,  and R"}},{"authors":[{"name":"F. Dan"},{"name":"R. Gowaika"},{"name":"R. Palank"},{"name":"B. Hassib"},{"name":"M. Effros"}],"title":{"text":"A"}},{"authors":[{"name":"N. Ratnaka"},{"name":"G. Kramer"}],"title":{"text":"The multicast capacity of deterministic relay networks with no interference"}},{"authors":[{"name":"S. Avestimeh"},{"name":"N. Diggav"},{"name":"C. Tse"}],"title":{"text":"A"}},{"authors":[{"name":"G. Krame"},{"name":"J. Hou"}],"title":{"text":"Short-message quantize-forward network coding"}},{"authors":[{"name":"X. W"},{"name":"L.-L. Xie"}],"title":{"text":"On the optimal compressions in the compress- and-forward relay schemes"}},{"authors":[{"name":"G. Krame"},{"name":"J. Hou"}],"title":{"text":"On message lengths for noisy network coding"}},{"authors":[{"name":"G. Krame"},{"name":"M. Gastpa"},{"name":"P. Gupta"}],"title":{"text":"Cooperative strategies and capac- ity theorems for relay networks"}},{"authors":[{"name":"T. Cove"},{"name":"J. Thomas"}],"title":{"text":"Elements of Information Theory "}},{"authors":[{"name":"A. El Gama"},{"name":"Y.-H. Kim"}],"title":{"text":"Network Information Theory"}},{"authors":[{"name":"P. Zhon"},{"name":"A. Haij"},{"name":"M. Vu"}],"title":{"text":"A"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565953.pdf"},"links":[{"id":"1569566381","weight":12},{"id":"1569566485","weight":12},{"id":"1569565383","weight":3},{"id":"1569565223","weight":3},{"id":"1569566725","weight":3},{"id":"1569566385","weight":3},{"id":"1569565867","weight":3},{"id":"1569564669","weight":3},{"id":"1569559617","weight":3},{"id":"1569559259","weight":6},{"id":"1569566597","weight":3},{"id":"1569566571","weight":3},{"id":"1569566081","weight":3},{"id":"1569565613","weight":3},{"id":"1569565931","weight":3},{"id":"1569564245","weight":9},{"id":"1569565837","weight":6},{"id":"1569566119","weight":3},{"id":"1569563411","weight":3},{"id":"1569559541","weight":3},{"id":"1569566319","weight":3},{"id":"1569566941","weight":9},{"id":"1569564203","weight":21},{"id":"1569562685","weight":3},{"id":"1569566467","weight":3},{"id":"1569566157","weight":3},{"id":"1569566999","weight":3},{"id":"1569566843","weight":3},{"id":"1569556091","weight":3},{"id":"1569565347","weight":3},{"id":"1569564387","weight":6},{"id":"1569565455","weight":9},{"id":"1569561679","weight":6},{"id":"1569566709","weight":9},{"id":"1569551763","weight":6},{"id":"1569564189","weight":6},{"id":"1569566193","weight":3},{"id":"1569564311","weight":3},{"id":"1569566617","weight":3},{"id":"1569566063","weight":3},{"id":"1569558681","weight":6},{"id":"1569555999","weight":3},{"id":"1569566643","weight":3},{"id":"1569566511","weight":3},{"id":"1569565841","weight":3},{"id":"1569566531","weight":6},{"id":"1569567665","weight":3},{"id":"1569565833","weight":12},{"id":"1569564611","weight":3},{"id":"1569565535","weight":3},{"id":"1569566325","weight":3},{"id":"1569566423","weight":3},{"id":"1569553519","weight":3},{"id":"1569566231","weight":6},{"id":"1569554971","weight":3},{"id":"1569566445","weight":3},{"id":"1569566209","weight":3},{"id":"1569566649","weight":3},{"id":"1569565655","weight":3},{"id":"1569566473","weight":3},{"id":"1569564333","weight":3},{"id":"1569566629","weight":6},{"id":"1569565033","weight":15},{"id":"1569566447","weight":3},{"id":"1569563897","weight":3},{"id":"1569565633","weight":3},{"id":"1569555879","weight":3},{"id":"1569566003","weight":3},{"id":"1569565185","weight":3},{"id":"1569566553","weight":3},{"id":"1569565469","weight":6},{"id":"1569564969","weight":9},{"id":"1569566043","weight":3},{"id":"1569565029","weight":3},{"id":"1569566505","weight":3},{"id":"1569566695","weight":3},{"id":"1569566051","weight":3},{"id":"1569566673","weight":3},{"id":"1569566233","weight":3},{"id":"1569566667","weight":3},{"id":"1569560997","weight":3},{"id":"1569566501","weight":9},{"id":"1569566481","weight":3},{"id":"1569560503","weight":3},{"id":"1569565439","weight":3},{"id":"1569563395","weight":6},{"id":"1569555367","weight":3},{"id":"1569565571","weight":3},{"id":"1569566929","weight":3},{"id":"1569565611","weight":6},{"id":"1569566779","weight":3},{"id":"1569566479","weight":3},{"id":"1569565397","weight":3},{"id":"1569566873","weight":6},{"id":"1569566129","weight":3},{"id":"1569565919","weight":3},{"id":"1569565181","weight":3},{"id":"1569565661","weight":6},{"id":"1569564131","weight":3},{"id":"1569565511","weight":3},{"id":"1569561221","weight":3},{"id":"1569564595","weight":3},{"id":"1569565353","weight":3},{"id":"1569566823","weight":21},{"id":"1569566595","weight":3},{"id":"1569566529","weight":3},{"id":"1569565375","weight":6},{"id":"1569565293","weight":3},{"id":"1569566641","weight":6},{"id":"1569566487","weight":3},{"id":"1569556759","weight":6},{"id":"1569566301","weight":3},{"id":"1569565233","weight":3},{"id":"1569566817","weight":3},{"id":"1569566299","weight":9},{"id":"1569564769","weight":6},{"id":"1569565769","weight":3},{"id":"1569566601","weight":3},{"id":"1569565805","weight":3},{"id":"1569557851","weight":3},{"id":"1569565389","weight":3},{"id":"1569565537","weight":6},{"id":"1569566457","weight":3},{"id":"1569567013","weight":6},{"id":"1569561861","weight":6},{"id":"1569561397","weight":3},{"id":"1569566797","weight":3},{"id":"1569566555","weight":3},{"id":"1569566973","weight":3},{"id":"1569566449","weight":3},{"id":"1569565031","weight":3},{"id":"1569564755","weight":3},{"id":"1569551751","weight":3},{"id":"1569565139","weight":3},{"id":"1569564419","weight":6},{"id":"1569565315","weight":3},{"id":"1569566417","weight":3},{"id":"1569560581","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T1.5","endtime":"13:10","authors":"Jie Hou, Gerhard Kramer","date":"1341406200000","papertitle":"Short Message Noisy Network Coding for Multiple Sources","starttime":"12:50","session":"S10.T1: Network Coding: Capacity and Bounds","room":"Kresge Rehearsal B (030)","paperid":"1569565953"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
