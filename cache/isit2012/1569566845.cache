{"id":"1569566845","paper":{"title":{"text":"An Afﬁne Invariant k -Nearest Neighbor Regression Estimate"},"authors":[{"name":"G´erard Biau"},{"name":"Luc Devroye"},{"name":"Vida Dujmovi´c"}],"abstr":{"text":"Abstract\u2014We propose a new k-NN regression estimate based on a data-dependent metric in R d which is used to deﬁne the k-nearest neighbors of a given point. The metric is invariant under all afﬁne transformations. With this metric, the standard k-nearest neighbor regression estimate is asymptotically consistent under the usual conditions on k, and minimal requirements on the input data."},"body":{"text":"The prediction error of standard nonparametric regression methods may be adversely affected by a linear transformation of the coordinate axes. It typically happens with the popular k-nearest neighbor (k-NN) regression estimate deﬁned by Fix and Hodges [8], [9], Cover and Hart [4], Cover [2], [3], where a mere rescaling of the coordinate axes changes the output of the estimate. This is clearly undesirable, especially in applications where data measurements represent differ- ent physical items, for instance, weight, blood pressure, sugar level, and the age of the patient. In this example, a simple change in, say, the unit of weight will completely alter the results, and will thus force the statistician to preprocess the input data prior to computing the k-NN estimate.\nIn this paper, we propose a version of the k-NN regression estimate that is not affected by afﬁne transformations of the coordinate axes. Such a modiﬁcation could save the user a subjective preprocessing step and would simplify computation of the estimate.\nWe assume that the data set is a collection of independent and identically distributed R d × R-valued random variables D n = {(X 1 , Y 1 ), . . . , (X n , Y n )}, independent of and with the same dis- tribution as a pair (X, Y ) satisfying E|Y | < ∞. The space R d is equipped with the standard Euclidean norm . . For ﬁxed x ∈ R d , our goal is to estimate the regression function r(x) = E[Y |X = x] from the data D n . The usual k-NN regression estimate is given by\nwhere (X (1) (x), Y (1) (x)), . . . , (X (n) (x), Y (n) (x)) is a reordering of the data according to increasing distances X i − x of the X i \u2019s to x. (If distance ties occur, a tie-breaking strategy is used. For example, if X i − x = X j − x , X i may be declared \u201ccloser\u201d if i < j, i.e., the tie-breaking is done by indices.) For simplicity, we will suppress D n in the notation and write r n (x) instead of r n (x; D n ). Stone [29] showed that, for all p ≥ 1, E[r n (X) − r(X)] p → 0\nfor all possible distributions of (X, Y ) with E|Y | p < ∞, whenever k n → ∞ and k n /n → 0 as n → ∞. Thus, the k-NN estimate is asymptotically optimal, for all distributions, or as we say, it is L p universally consistent.\nThe drawback of k-NN estimate is that any afﬁne transformation of the coordinate axes affects the k-NN estimate through the norm\n. . Thus, we want to construct a regression estimate r n with the following property\nwhere D n = (T (X 1 ), Y 1 ), . . . , (T (X n ), Y n ). We call r n afﬁne invariant. In R d , this invariance of k-NN estimates, it obtained by deﬁning data-dependent afﬁne invariant distance measure. In the next section we deﬁne an afﬁne invariant estimation procedure featuring (I.1) which coincides with the k-NN estimate, and discuss its consistency in Section 3.\nThe literature on afﬁne invariance in nonparametric estimation has been quite rich. The easiest approach is to compute ˆ M n , the standard estimate of the d × d covariance matrix of X, and to replace the data {(X 1 , Y 1 ), . . . , (X n , Y n )} by {( ˆ M −1 n X 1 , Y 1 ), . . . , ( ˆ M −1 n X n , Y n )}.\nAny nonparametric procedure that uses the new data is afﬁne in- variant. That method has been discussed in [6] and [14] for pattern recognition and regression, respectively, but it has also been used in kernel density estimation (see, e.g., Samanta [28]). Computational issues aside, this procedure requires at the very least that the second moment of X exists, which is something we are not willing to assume. Our approach takes ideas from the classical nonparametric literature using concepts such as data depth or multivariate ranks.\nThere have been attempts to obtain invariance to other transforma- tions. They include invariance under monotone transformations of the coordinate axes, e. g., based on the coordinatewise ranks of the X i \u2019s. One can show using an L p norm on the d-vectors of differences between ranks that the classical k-NN regression function estimate is universally consistent in the sense of Stone [29], see Olshen [23] and Devroye [5], Gordon and Olshen [12], [13], Devroye and Krzy˙zak [7], and Biau and Devroye [1]. Other important examples include the rules based upon statistically equivalent blocks, see Quesenberry and Gessaman [26], Gessaman [10], Gessaman and Gessaman [11], and Devroye, Gy¨orﬁ, and Lugosi [6].\nThe k-NN estimate discussed in this paper is based upon the notion of empirical distance. We assume that the distribution of X is absolutely continuous with respect to the Lebesgue measure on R d and that n ≥ d. Thus any collection X i 1 , . . . , X i d (1 ≤ i 1 < i 2 < . . . < i d ≤ n) of d points among X 1 , . . . , X n are in general position with probability one. Consequently, there exists with probability one a unique hyperplane in R d containing these d random points, and we denote it by H(X i 1 , . . . , X i d ).\nWe can now deﬁne the empirical distance between d-vectors x and x by\nIn other words, ρ n (x, x ) counts the number of hyperplanes in R d passing through d out of the points X 1 , . . . , X n , that are separating x and x . Intuitively, \u201cnear\u201d points have fewer intersections.\nThis concept of distance based on hyperplanes is known in the multivariate rank tests literature as the empirical lift-interdirection function (Randles [27], Oja [21], Hallin and Paindaveine [15], Oja and Paindaveine [22]). It was introduced in Hettmansperger, M¨ott¨onen, and Oja [16] (but not investigated), and independently suggested as an afﬁne invariant alternative to ordinary metrics in the monograph of Devroye, G¨yorﬁ, and Lugosi [6, Section 11.6]. We use the notion of distance even though, for a ﬁxed sample of size n, ρ n is only deﬁned with probability one and strictly speaking is not a distance measure. Nevertheless, this empirical distance is invariant under afﬁne transformations x → Ax + b, where A is some arbitrary nonsingular linear map and b any offset vector (see, for instance, Oja and Paindaveine [22, Section 2.4]).\nLet us now deﬁne our k-NN regression estimate. Pick x ∈ R d and let ρ n (x, X i ) be the empirical distance between x and some observation X i in the sample X 1 , . . . , X n , i. e., the number of hyperplanes in R d passing through d out of the observations X 1 , . . . , X n , that are cutting the segment (x, X i )). Thus our k-NN estimate still takes the familiar form\nwith the important difference that now the data set (X 1 , Y 1 ), . . . , (X n , Y n ) is reordered according to increasing values of the empirical distances ρ n (x, X i ), not the original Euclidean metric. By construction, the estimate r n has the desired afﬁne invariance property and, moreover, it coincides with the standard (Euclidean) estimate in dimension d = 1. We have the following universal consistency result, where µ denotes the distribution of the random variable X.\nTheorem 2.1 (Pointwise L p consistency): Assume that X has a probability density, that Y is bounded, and that the regression function r is µ-almost surely continuous. Then, for µ-almost all x ∈ R d and all p ≥ 1, if k n → ∞ and k n /n → 0,\nCorollary 2.1 (Global L p consistency): Assume that X has a prob- ability density, that Y is bounded, and that the regression function r is µ-almost surely continuous. Then, for all p ≥ 1, if k n → ∞ and k n /n → 0,\nhold anymore when out transformation is applied, so a single data point may have considerable inﬂuence on the estimate. Whether our regression estimate remains universally consistent is an open problem. The consistency can be shown with additional constraints, namely that X has a density (not necessarily continuous) and that r is µ- almost surely continuous. The outline of the proof is given in the next section. The complete proof will be presented elsewhere.\nLet X the random variable with distribution µ and let f be its density with respect to the Lebesgue measure. For every ε > 0, let B x,ε = {y ∈ R d : y − x ≤ ε} be the closed Euclidean ball with center at x and radius ε. Let A c be the complement of a subset A of R d .\nWe use Jensen\u2019s inequality to bound the L p error of the regression estimate\nThe ﬁrst term converges to 0 by k n /n → 0 while the second therm can be made arbitrarily small by letting ε → 0 since r is continuous at x. Thus we have shown that J n → 0 as n → ∞.\nWe next use Marcinkiewicz and Zygmund [18] inequality to bound I n .\nwhere C p is some positive constant depending only on p. Conse- quently, I n → 0 as k n → ∞, and this concludes the proof of the theorem."},"refs":[{"authors":[{"name":"G. Bia"},{"name":"L. Devroye"}],"title":{"text":"On the layered nearest neighbour estimate, the bagged nearest neighbour estimate and the random forest method in regression and classiﬁcation"}},{"authors":[{"name":"M. Cover"}],"title":{"text":"T"}},{"authors":[{"name":"M. Cover"}],"title":{"text":"T"}},{"authors":[{"name":"M. Cove"},{"name":"E. Hart"}],"title":{"text":"T"}},{"authors":[{"name":"L. Devroye"},{"name":"V. Dasarath"}],"title":{"text":"A universal k-nearest neighbor procedure in discrimina- tion"}},{"authors":[{"name":"L. Devroy"},{"name":"L. Gy¨or"},{"name":"G. Lugosi"}],"title":{"text":"A Probabilistic Theory of Pattern Recognition "}},{"authors":[{"name":"L. Devroy"},{"name":"A. Krzy˙zak"}],"title":{"text":"New multivariate product density estima- tors"}},{"authors":[{"name":"E. Fi"},{"name":"L. Hodges"}],"title":{"text":"J"}},{"authors":[{"name":"E. Fi"},{"name":"L. Hodges"}],"title":{"text":"J"}},{"authors":[{"name":"M. Gessaman"}],"title":{"text":"A consistent nonparametric multivariate density estimator based on statistically equivalent blocks"}},{"authors":[{"name":"M. Gessama"},{"name":"P. Gessaman"}],"title":{"text":"A comparison of some multivariate dis- crimination procedures"}},{"authors":[{"name":"L. Gordo"},{"name":"A. Olshen"}],"title":{"text":"R"}},{"authors":[{"name":"L. Gordo"},{"name":"A. Olshen"}],"title":{"text":"R"}},{"authors":[{"name":"L. Gy¨or"},{"name":"M. Kohle"},{"name":"A. Krzy˙za"},{"name":"H. Walk"}],"title":{"text":"A Distribution-Free Theory of Nonparametric Regression "}},{"authors":[{"name":"M. Halli"},{"name":"D. Paindaveine"}],"title":{"text":"Optimal tests for multivariate location based on interdirections and pseudo-Mahalanobis ranks"}},{"authors":[{"name":"P. Hettmansperge"},{"name":"J. M¨ott¨one"},{"name":"H. Oja"}],"title":{"text":"T"}},{"authors":[{"name":"W. Hoeffding"}],"title":{"text":"Probability inequalities for sums of bounded random variables"}},{"authors":[{"name":"J. Marcinkiewic"},{"name":"A. Zygmund"}],"title":{"text":"Sur les fonctions ind´ependantes"}},{"authors":[{"name":"C. McDiarmid"},{"name":"J. Siemon"}],"title":{"text":"On the method of bounded differences"}},{"authors":[{"name":"S. Miller"}],"title":{"text":"K"}},{"authors":[{"name":"H. Oja"}],"title":{"text":"Afﬁne invariant multivariate sign and rank tests and correspond- ing estimates: A review"}},{"authors":[{"name":"H. Oj"},{"name":"D. Paindaveine"}],"title":{"text":"Optimal signed-rank tests based on hyperplanes"}},{"authors":[{"name":"R. Olshen"},{"name":"J. Stone"}],"title":{"text":"Comments on a paper by C"}},{"authors":[{"name":"R. Parthasarathy"}],"title":{"text":"K"}},{"authors":[{"name":"V. Petrov"}],"title":{"text":"V"}},{"authors":[{"name":"C. Quesenberr"},{"name":"M. Gessaman"}],"title":{"text":"Nonparametric discrimination using tolerance regions"}},{"authors":[{"name":"H. Randles"}],"title":{"text":"R"}},{"authors":[{"name":"M. Samanta"}],"title":{"text":"A note on uniform strong convergence of bivariate density estimates"}},{"authors":[{"name":"J. Stone"}],"title":{"text":"C"}},{"authors":[{"name":"L. Wheede"},{"name":"A. Zygmund"}],"title":{"text":"R"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566845.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S8.T8.1","endtime":"17:00","authors":"Gerard Biau, Luc Devroye, Vida Dujmović, Adam Krzyżak","date":"1341333600000","papertitle":"An Affine Invariant k-Nearest Neighbor Regression Estimate","starttime":"16:40","session":"S8.T8: Estimation and Detection","room":"Stratton (491)","paperid":"1569566845"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
