{"id":"1569566673","paper":{"title":{"text":"Lossy Common Information of Two Dependent Random Variables"},"authors":[{"name":"Kumar Viswanatha"},{"name":"Emrah Akyol"},{"name":"Kenneth Rose"}],"abstr":{"text":"Abstract\u2014The two most prevalent notions of common infor- mation are due to Wyner and Gács-Körner and both the notions can be stated as two different characteristic points in the lossless Gray-Wyner region. Although these quantities can be easily evaluated for random variables with inﬁnite entropy (eg. contin- uous random variables), the operational signiﬁcance underlying their deﬁnition is applicable only to the lossless framework. The primary objective of this paper is to generalize these two notions of common information to the lossy Gray-Wyner network, which extends the theoretical intuition underlying their deﬁnitions for general sources and distortion measures. We begin with the lossy generalization of Wyner\u2019s common information, deﬁned as the minimum rate on the shared branch of the Gray-Wyner network at minimum sum rate when the two decoders reconstruct the sources subject to individual distortion constraints. We derive a complete single letter information theoretic characterization for this quantity and use it to compute the common information of symmetric bivariate Gaussian random variables. We then derive similar results to generalize Gács-Körner\u2019s deﬁnition to the lossy framework. These two characterizations allow us to carry the practical insight underlying the two notions of common information to general sources and distortion measures."},"body":{"text":"The quest for a meaningful and useful notion of common information (CI) of two random variables (denoted by X and Y ) has been actively pursued by researchers in information theory for over three decades. An early seminal approach to quantify CI is due to Gács and Körner [1] (denoted here by C GK (X, Y ) ), who deﬁned it as the maximum amount of information relevant to both random variables, one can extract from the knowledge of either one of them. Their result was of considerable theoretical interest, but also fundamentally negative in nature. They showed that C GK (X, Y ) is typically much smaller than the mutual information and only depends on the zeros of the joint distribution. Unsatisﬁed with the negative implication, Wyner proposed an alternative idea of CI [2] (denoted here by C W (X, Y ) ) inspired by earlier work in multi-terminal source coding [3]. He deﬁned the CI as the minimum rate on the shared branch of the lossless Gray-Wyner network (described in section I-A and Fig. 1), when the sum rate is constrained to be the joint entropy. His conception was to look at the minimum amount of shared information that must be sent to both decoders, while restricting the overall\ntransmission rate to the minimum, H(X, Y ). He obtained the single letter characterization for C W (X, Y ) as,\nwhere the inﬁmum is over all random variables, U, such that X ↔ U ↔ Y form a Markov chain in that order. We note that although C GK (X, Y ) and C W (X, Y ) were deﬁned from theoretical standpoints, they play critical roles in understanding the performance limits in several practical networking and database applications, see for eg. [4]. We further note in passing that several other deﬁnitions of CI, with applications in different ﬁelds, have appeared in the literature [6], [7], but are less relevant to us here.\nAlthough the quantity in (1) can be evaluated for ran- dom variables with inﬁnite entropy (eg. continuous random variables), for such random variables it lacks the underlying theoretical interpretation of Wyner\u2019s notion of CI as one of the distinctive operating points in the Gray-Wyner region and thereby lacks the fundamental intuition behind the deﬁnition. This largely compromises its practical signiﬁcance and calls for a useful generalization which can be easily extended to inﬁnite entropy distributions. Our primary step is to char- acterize a lossy coding extension of Wyner\u2019s CI (denoted by C W (X, Y ; D 1 , D 2 ) , deﬁned as the minimum rate on the shared branch of the Gray-Wyner network at minimum sum rate when the sources are decoded at respective distortions of D 1 and D 2 . Note that the minimum sum rate at distortions D 1 and D 2 is given by the Shannon\u2019s rate distortion function, hereafter denoted by R X,Y (D 1 , D 2 ) . In this paper, our main objective is to derive an information theoretic characterization for C W (X, Y ; D 1 , D 2 ) for general sources and distortion measures. We note that although there is no prior work on characterizing C W (X, Y ; D 1 , D 2 ) , in a recent work [8], Xu et\nal. deﬁned the asymptotic quantity C W (X, Y ; D 1 , D 2 ) 1 and showed that there exists a region of distortions around the origin where C W (X, Y ; D 1 , D 2 ) is equal to the Wyner\u2019s single letter characterization in (1).\nWe note that much of the focus in the beginning of this paper in section II will be towards characterizing Wyner\u2019s deﬁnition of CI in the lossy Gray-Wyner setting. However, similar results and proof techniques are used in section III to extend Gács and Körner\u2019s deﬁnition to the lossy framework. We further note that there have been other physical interpre- tations of both the notions of common information besides the Gray-Wyner network, see for example [1], [2]. We are not yet sure of the relations between such interpretations and the lossy generalizations we consider in this paper. We will be investigating these connections are part of our future work.\nLet (X, Y ) be any two dependent random variables taking values in the alphabets X and Y respectively. Let ˆ X and ˆ Y be two given reconstruction alphabets for random variables X and Y respectively. We denote the set {1, 2 . . . M} by I M for any positive integer M, n iid samples of a random variable by X n and the corresponding alphabet by X n . In what follows, for any pair of random variables X and Y , R X ( ·), R Y ( ·) and R X,Y ( ·, ·) denote the respective rate distortion functions.\nA rate-distortion tuple (R 0 , R 1 , R 2 , D 1 , D 2 ) is said to be achievable for the Gray-Wyner network if for all  > 0, there exists encoder mappings f E : X n × Y n → I M 0 × I M 1 × I M 2 and decoder mappings f (X) D : I M 0 ×I M 1 → ˆ X n , f (Y ) D : I M 0 × I M 2 → ˆ Y n such that the following hold:\nd X (X i , ˆ X i ), ∆ Y = 1 n  n i=1 d Y (Y i , ˆ Y i ) for some well deﬁned single letter distortion measures d X ( ·, ·) and d Y ( ·, ·). The convex closure over all such achievable rate- distortion tuples is called the achievable region for the Gray- Wyner network. The set of all achievable rate tuples for any given distortion D 1 and D 2 is denoted here by R GW (D 1 , D 2 ) .\nGray and Wyner [3] gave the following complete charac- terization for R GW (D 1 , D 2 ) . Let (U, ˆ X, ˆ Y ) be any random variables jointly distributed with (X, Y ) and taking values over alphabets U, ˆ X and ˆ Y respectively, for some arbitrary U. Let the joint density be P (X, Y, U, ˆ X, ˆ Y ) . All rate-distortion tu- ples (R 0 , R 1 , R 2 , D 1 , D 2 ) satisfying the following conditions are achievable:\nif E(d X (X, ˆ X)) ≤ D 1 and E(d Y (Y, ˆ Y )) ≤ D 2 . The closure of the achievable rate distortion tuples over all such joint densities is the complete rate distortion region for the Gray- Wyner network.\nWe next deﬁne Wyner\u2019s CI generalized to the lossy frame- work denoted by C W (X, Y ; D 1 , D 2 ) . It is deﬁned as the inﬁmum over all shared rates R 0 , such that (R 0 , R 1 , R 2 ) ∈ {R GW (D 1 , D 2 ) ∩ Pangloss plane}, where for any distortion pair (D 1 , D 2 ) the plane R 0 + R 1 + R 2 = R X,Y (D 1 , D 2 ) is deﬁned as the Pangloss plane. We note that the above opera- tional deﬁnition of C W (X, Y ; D 1 , D 2 ) , has already appeared recently in [8]. However, a complete single letter information theoretic characterization of C W (X, Y ; D 1 , D 2 ) has never been considered in any earlier work. The primary objective of section II-B is to characterize C W (X, Y ; D 1 , D 2 ) for general sources and distortion measures. It is important to note that Wyner gave the complete single letter characterization of C W (X, Y ; 0, 0) when X and Y have ﬁnite joint entropy. His result is stated formally as:\nwhere the inﬁmum is over all U satisfying X ↔ U ↔ Y . B. Single Letter Characterization of C W (X, Y ; D 1 , D 2 )\nIn the following theorem, we characterize C W (X, Y ; D 1 , D 2 ) . We denote the set of all channels which achieve R X,Y (D 1 , D 2 ) by P X,Y D 1 ,D 2 , i.e.,\n∀P (X ∗ , Y ∗ |X, Y ) ∈ P X,Y D 1 ,D 2 , where the inﬁmum is over channels such that E(d X (X, ˆ X)) ≤ D 1 , E(d X (X, ˆ X)) ≤ D 2 . Hereafter we assume that, for every distortion pair (D 1 , D 2 ) , there exists at least one channel P (X ∗ , Y ∗ |X, Y ) such that I(X, Y ; X ∗ , Y ∗ ) = R X,Y (D 1 , D 2 ) . We note that, our results can be easily extended to all other \u2018well behaved\u2019 continuous joint densities using standard techniques from probability measures.\nTheorem 1. A single letter characterization of C W (X, Y ; D 1 , D 2 ) is given by:\nwhere the inﬁmum is over all joint densities (X, Y, X ∗ , Y ∗ , U ) such that the following Markov conditions hold:\nX ∗ ↔ \t U \t ↔ Y ∗ \t (7) (X, Y ) ↔ (X ∗ , Y ∗ ) ↔ U \t (8)\nand where P (X ∗ , Y ∗ |X, Y ) ∈ P X,Y D 1 ,D 2 is any joint distribution which achieves the rate distortion function at (D 1 , D 2 ) .\nRemark 1. If we set ˆ X = X , ˆ Y = Y and consider the Hamming distortion measure, at (D 1 , D 2 ) = (0, 0) , it is easy to show that Wyner\u2019s common information is obtained as a special case, i.e., C W (X, Y ; 0, 0) = C W (X, Y ) .\nProof: We note that, although there are arguably simpler methods to prove this theorem, we choose the following\napproach as it uses only the Gray-Wyner theorem without re- course to any supplementary results. We also assume that there exists a unique channel P ∗ (X ∗ , Y ∗ |X, Y ) which achieves R X,Y (D 1 , D 2 ) . The proof of the theorem when there are multiple channels in P X,Y D 1 ,D 2 follows directly.\nOur objective is to show that every point in the intersec- tion of R GW (D 1 , D 2 ) and the Pangloss plane has R 0 = I(X, Y ; U ) for some U jointly distributed with (X, Y, X ∗ , Y ∗ ) and satisfying conditions (7) and (8). We ﬁrst prove that every point in the intersection of the Pangloss plane and R GW (D 1 , D 2 ) is achieved by a joint density satisfying (7) and (8). Towards showing this, we begin with an alternate characterization of R GW (D 1 , D 2 ) (which is also complete) due to Venkataramani et.al in [10] 2 . Let (U, ˆ X, ˆ Y ) be any random variables jointly distributed with (X, Y ) such that E(d X (X, ˆ X)) ≤ D 1 and E(d Y (Y, ˆ Y )) ≤ D 2 . Then any rate tuple (R 0 , R 1 , R 2 ) satisfying the following conditions belongs to R GW (D 1 , D 2 ) :\nR 1 + R 0 ≥ I(X, Y ; U, ˆ X) R 2 + R 0 ≥ I(X, Y ; U, ˆ Y )\nIt is very easy to show that the above characterization is equiv- alent to (3). As the above characterization is complete, this implies that, if a rate-distortion tuple (R 0 , R 1 , R 2 , D 1 , D 2 ) is achievable for the Gray-Wyner network, then we can always ﬁnd random variables (U, ˆ X, ˆ Y ) such that E(d X (X, ˆ X)) ≤ D 1 , E(d Y (Y, ˆ Y )) ≤ D 2 and satisfying (9). We are further interested in characterizing the points in R GW (D 1 , D 2 ) which also lie on the Pangloss plane, i.e., R 0 + R 1 + R 2 = R X,Y (D 1 , D 2 ) . Therefore, for any rate tuple (R 0 , R 1 , R 2 ) on the Pangloss plane in R GW (D 1 , D 2 ) , we have the following series of inequalities:\n≥ I(X, Y ; U, ˆ X, ˆ Y ) − H( ˆ X, ˆ Y |U) +H( ˆ X |U) + H( ˆ Y |U)\n≥ I(X, Y ; ˆ X, ˆ Y ) − H( ˆ X, ˆ Y |U) +H( ˆ X |U) + H( ˆ Y |U)\nwhere (a) follows because ( ˆ X, ˆ Y ) satisfy the distortion constraints. As the LHS and RHS of the above series of inequalities are the same, all the inequalities must be equalities\nFrom our assumption, there is a unique channel, P ∗ (X ∗ , Y ∗ |X, Y ) which achieves I(X, Y ; ˆ X, ˆ Y ) = R X,Y (D 1 , D 2 ) . It therefore follows that every point in R GW (D 1 , D 2 ) that lies on the Pangloss plane satisﬁes (9) for some joint density satisfying (7) and (8).\nWhat remains is to show that any joint density (X, Y, X ∗ , Y ∗ , U ) satisfying (7) and (8) leads to a sub- region of R GW (D 1 , D 2 ) which has at least one point on the Pangloss plane with R 0 = I(X, Y ; U ) . Formally, de- note by R(U), the region (9) achieved by a joint density (X, Y, X ∗ , Y ∗ , U ) satisfying (7) and (8). Then we have to show that ∃(R 0 , R 1 , R 2 ) ∈ R(U) such that:\nConsider \t the \t point, \t (R 0 , R 1 , R 2 ) \t = (I(X, Y ; U ), I(X, Y ; X ∗ |U), I(X, Y ; Y ∗ |U, X ∗ )) for any\njoint density (X, Y, X ∗ , Y ∗ , U ) satisfying (7) and (8). Clearly the point satisﬁes the ﬁrst two conditions in (9). Next, we note that:\nR 0 + R 2 = I(X, Y ; U ) + I(X, Y ; Y ∗ |U, X ∗ ) ≥ (b) I(X, Y ; U ) + I(X, Y ; Y ∗ |U)\nR 0 + R 1 + R 2 = (c) I(X, Y ; X ∗ , Y ∗ , U ) \t (14) = I(X, Y ; X ∗ , Y ∗ ) = R X,Y (D 1 , D 2 )\nwhere (b) and (c) follow from the fact that the joint density satisﬁes (7) and (8). Hence, we have shown the existence of one point in R(U) satisfying (12) for every joint density (X, Y, X ∗ , Y ∗ , U ) satisfying (7) and (8) proving the theorem.\nWe note that, in general, it is hard to establish con- vexity/monotonicity of C W (X, Y ; D 1 , D 2 ) with respect to (D 1 , D 2 ) . This makes it hard to establish conclusive inequality relations between C W (X, Y ; D 1 , D 2 ) and C W (X, Y ) for all distortions. However, in the following lemma, we establish sufﬁcient conditions on (D 1 , D 2 ) for C W (X, Y ; D 1 , D 2 ) ≶ C W (X, Y ) .\n∃( ˜ D 1 , ˜ D 2 ) such that ˜ D 1 ≤ D 1 , ˜ D 2 ≤ D 2 and R X,Y ( ˜ D 1 , ˜ D 2 ) = C W (X, Y )\n\u2022 (ii) C W (X, Y ; D 1 , D 2 ) ≥ C W (X, Y ) if Shannon lower bound for R XY (D 1 , D 2 ) is tight at (D 1 , D 2 )\nProof: The proof of (i) is rather straightforward and hence we choose to omit it. Towards proving (ii), it is easy to show using standard techniques [11], [9] that the conditional\ndistribution P (X ∗ , Y ∗ |X, Y ) which achieves R X,Y (D 1 , D 2 ) when Shannon lower bound is tight has independent backward channels, i.e.:\nLet us consider any U which satisﬁes (X ∗ ↔ U ↔ Y ∗ ) and (X, Y ) ↔ (X ∗ , Y ∗ ) ↔ U. It is easy to verify that any such joint density also satisﬁes X ↔ U ↔ Y . As the inﬁmum for C W (X, Y ) is taken over a larger set of joint densities, we have C W (X, Y ; D 1 , D 2 ) ≥ C W (X, Y ) .\nThe above lemma highlights the anomalous behavior of C W (X, Y ; D 1 , D 2 ) with respect to the distortions. Determin- ing the conditions for equality in Lemma 1.(ii) is an interesting problem in its own right. For the symmetric setting, i.e., D 1 = D 2 = D , it was shown in [8] using a completely different approach leveraging prior results from conditional rate distortion theory (see for eg. [9]) that C W (X, Y ; D, D) = C W (X, Y ) iff D ≤ R −1 X,Y (C W (X, Y )) , where R −1 X,Y ( ·) de-\nnotes the distortion-rate function. We will further explore the underlying connections between these results as part of our future work.\nLet X and Y be jointly Gaussian random variables with zero mean, unit variance and a correlation coefﬁcient of ρ. We focus on the symmetric distortion scenario, i.e. D 1 = D 2 = D , under mean squared error distortion metric. The joint rate distortion function is given by [11]:\n(16) We ﬁrst consider the range 0 < D ≤ 1 − ρ. The RD-optimal random encoder is such that P (X|X ∗ ) and P (Y |Y ∗ ) are two independent zero mean Gaussian channels with variance D. It is easy to verify that the optimal reproduction distribution (for (X ∗ , Y ∗ ) ) is jointly Gaussian with zero mean. The covariance matrix for (X ∗ , Y ∗ ) is:\nAt these distortions, the Shannon lower bound is tight and hence from Lemma 1, C W (X, Y ; D, D) ≥ C W (X, Y ) . Fur- ther, it was shown in [8] that C W (X, Y ) = C W (X, Y ; 0, 0) =\nlog 1+ρ 1 −ρ and the inﬁmum achieving U ∗ is a standard Gaus- sian random variable jointly distributed with (X, Y ) as:\n(18) where N 1 and N 2 are independent standard Gaussian random variables. We can generate (X ∗ , Y ∗ ) by passing U ∗ through independent Gaussian channels as follows:\nwhere ˜ N 1 and ˜ N 2 are independent standard Gaussian ran- dom variables independent of both N 1 and N 2 . Therefore there exists a joint density over (X, Y, X ∗ , Y ∗ , U ∗ ) satisfying X ∗ ↔ U ∗ ↔ Y ∗ and (X, Y ) ↔ (X ∗ , Y ∗ ) ↔ U ∗ . This shows that C W (X, Y ; D, D) ≤ C W (X, Y ) . Therefore in the range 0 < D ≤ 1 − ρ, we have C W (X, Y ; D, D) = C W (X, Y ) . We\nHowever C W (X, Y ; D, D) in the range 1−ρ ≤ D ≤ 1, has never been considered to date. Note that the Shannon lower bound for R X,Y (D, D) is not tight in this range. However, the RD-optimal conditional distribution P (X ∗ , Y ∗ |X, Y ) in this distortion range is such that X ∗ = Y ∗ . Therefore the only U which satisﬁes (X ∗ ↔ U ↔ Y ∗ ) is U = X ∗ = Y ∗ . Therefore from Theorem 1, we conclude that C W (X, Y ; D, D) = R X,Y (D, D) for 1 − ρ ≤ D ≤ 1. Of\ncourse, for D > 1, C W (X, Y ; D, D) = 0 . Hence we have completely characterized C W (X, Y ; D, D) for (X, Y ) jointly Gaussian for all symmetric distortions D 1 = D 2 = D > 0 .\nGács and Körner [1] deﬁned CI of X and Y as the maxi- mum rate of the codeword that can be generated individually at two encoders observing X n and Y n separately. Formally:\nwhere sup is taken over all f 1 and f 2 such that P (f 1 (X n ) = f 2 (Y n )) → 0. They showed that C GK (X, Y ) is equal to the entropy of the random variable which deﬁnes the ergodic decomposition of the stochastic matrix of conditional proba- bilities P (X = x|Y = y), i.e., if J is a random variable such that J = j iff x ∈ X j ⇔ y ∈ Y j , where X ×Y =  j X j ×Y j , then C GK (X, Y ) = H(J) .\nGács-Körner\u2019s original deﬁnition of CI was naturally unre- lated to the Gray-Wyner network, which it predates. However, an equivalent and insightful characterization of C GK (X, Y ) was given by Ahlswede and Körner [5] in terms of R GW (0, 0) as follows:\nThough the original deﬁnition of Gács-Körner\u2019s CI does not have a direct lossy interpretation, the equivalent deﬁnition given by Ahlswede and Körner in terms of the lossless Gray- Wyner region can be easily extended to the lossy setting sim- ilar to Wyner\u2019s CI. These generalizations provide theoretical insight into the performance limits of practical databases for fusion storage of correlated sources as described in [4].\nWe deﬁne the lossy generalization of Gács-Körner\u2019s CI at (D 1 , D 2 ) , denoted by C GK (X, Y ; D 1 , D 2 ) as.\nWe provide an information theoretic characterization for C GK (X, Y ; D 1 , D 2 ) in the following theorem. We again as- sume that there exists channels which achieve R X (D 1 ) and R Y (D 2 ) respectively, noting that the results can be easily extended to more general source densities. We denote the set of all channels which achieve R X (D 1 ) and R Y (D 2 ) by P X D 1 and P X D 1 respectively.\nTheorem 2. A single letter characterization of C GK (X, Y ; D 1 , D 2 ) is given by:\nwhere the supremum is over all joint densities (X, Y, ˜ X, ˜ Y , U ) such that the following Markov conditions hold:\nX ↔ ˜ X ↔ U \t Y ↔ ˜ Y ↔ U \t (28) where P ( ˜ X |X) ∈ P X D 1 and P ( ˜ Y |Y ) ∈ P Y D 2 .\nProof: The proof follows in very similar lines to the proof of Theorem 1. The original Gray-Wyner\u2019s characterization is in fact sufﬁcient in this case. Again we ﬁrst assume that there are unique channels P ( ˜ X |X) and P ( ˜ Y |Y ) which achieve R X (D 1 ) and R Y (D 2 ) respectively. The proof extends directly to the case of multiple RD optimal channels.\nWe are interested in characterizing the points in R GW (D 1 , D 2 ) which lie on both the planes R 0 + R 1 = R X (D 1 ) and R 0 + R 2 = R Y (D 2 ) . Therefore we have the following series of inequalities:\n≥ I(X, Y ; U) + I(X; ˆ X |U) = I(X; ˆ X, U ) + I(Y ; U |X) ≥ I(X; ˆ X) ≥ R X (D 1 )\nWriting similar inequality relations for Y and following the same arguments as in Theorem 1, it follows that for all joint densities satisfying (28) and for which P ( ˜ X |X) ∈ P X D 1 and P ( ˜ Y |Y ) ∈ P Y D 2 , there exists at least one point in R GW (D 1 , D 2 ) which satisﬁes both R 0 + R 1 = R X (D 1 ) and R 0 + R 2 = R Y (D 2 ) and for which R 0 = I(X, Y ; U ) . This proves the theorem.\nProof: This corollary follows directly from Theorem 2 as conditions in (24) are a subset of the conditions in (28).\nIt is easy to show that if the random variables (X, Y ) are jointly Gaussian with a correlation coefﬁcient ρ < 1, then C GK (X, Y ) = 0 . Hence from Corollary 1, it follows that, for jointly Gaussian random variables with correlation coefﬁcient strictly less than 1, C GK (X, Y ; D 1 , D 2 ) = 0 ∀D 1 , D 2 under any distortion metric. It is well known that C GK (X, Y ) is typically very small (usually zero) and depends only on the zeros of the joint distribution. In the general setting, as C GK (X, Y ; D 1 , D 2 ) ≤ C GK (X, Y ) , it would seem that Theorem 2 has very limited practical signiﬁcance. However in a separate paper, currently under preparation, we show that C GK (X, Y ; D 1 , D 2 ) plays a central role in scalable coding of sources that are not successively reﬁnable.\nIn this paper we derived single letter information theoretic characterizations for the lossy generalizations of the two most prevalent notions of common information due to Wyner and Gács-Körner. These generalizations allow us to extend the theoretical interpretation underlying their original deﬁnitions to sources with inﬁnite entropy (eg. continuous random vari- ables). We use these information theoretic characterizations to derive the common information of symmetric bivariate Gaussian random variables."},"refs":[{"authors":[{"name":"P. Gács"},{"name":"J. Körner"}],"title":{"text":"Common information is far less than mutual information"}},{"authors":[{"name":"A. Wyner"}],"title":{"text":"The common information of two dependent random vari- ables"}},{"authors":[{"name":"R. Gray"},{"name":"A. Wyner"}],"title":{"text":"Source coding for a simple network"}},{"authors":[{"name":"K. Viswanatha"},{"name":"E. Akyol"},{"name":"K. Rose"}],"title":{"text":"An optimal transmit-receive rate tradeoff in Gray-Wyner network and its relation to common informa- tion"}},{"authors":[{"name":"R. Ahlswede"},{"name":"J. Körner"}],"title":{"text":"On common information and related characteristics of correlated information sources"}},{"authors":[{"name":"H. Yamamoto"}],"title":{"text":"Coding theorems for Shannon\u2019s cipher system with correlated source outputs and common information"}},{"authors":[{"name":"S. Kamath"},{"name":"V. Anantharam"}],"title":{"text":"A new dual to the Gács - Körner common information deﬁned via the Gray-Wyner system"}},{"authors":[{"name":"G. Xu"},{"name":"W. Liu"},{"name":"B. Chen; "}],"title":{"text":"Wyners common information for continuous random variables - A lossy source coding interpretation"}},{"authors":[{"name":"R. Gray"}],"title":{"text":"A new class of lower bounds to information rates of station- ary sources via conditional rate-distortion functions"}},{"authors":[{"name":"R. Venkataramani"},{"name":"G. Kramer"},{"name":"K. Goyal"}],"title":{"text":"Multiple description coding with many channels"}},{"authors":[{"name":"T. Berger"}],"title":{"text":"Rate distortion theory"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566673.pdf"},"links":[{"id":"1569566381","weight":4},{"id":"1569566485","weight":4},{"id":"1569566725","weight":4},{"id":"1569565867","weight":4},{"id":"1569566981","weight":4},{"id":"1569566683","weight":4},{"id":"1569566597","weight":9},{"id":"1569566943","weight":4},{"id":"1569566591","weight":9},{"id":"1569564481","weight":4},{"id":"1569566415","weight":9},{"id":"1569567005","weight":4},{"id":"1569565931","weight":4},{"id":"1569565547","weight":4},{"id":"1569564245","weight":4},{"id":"1569564227","weight":9},{"id":"1569558325","weight":4},{"id":"1569565837","weight":9},{"id":"1569563411","weight":4},{"id":"1569566941","weight":4},{"id":"1569564203","weight":4},{"id":"1569566843","weight":4},{"id":"1569565455","weight":4},{"id":"1569561679","weight":4},{"id":"1569566015","weight":4},{"id":"1569566523","weight":4},{"id":"1569565953","weight":4},{"id":"1569563981","weight":4},{"id":"1569566905","weight":4},{"id":"1569566753","weight":4},{"id":"1569566063","weight":4},{"id":"1569555999","weight":4},{"id":"1569565841","weight":9},{"id":"1569565833","weight":4},{"id":"1569564611","weight":4},{"id":"1569565667","weight":4},{"id":"1569561795","weight":9},{"id":"1569566325","weight":4},{"id":"1569566851","weight":4},{"id":"1569553909","weight":4},{"id":"1569553537","weight":4},{"id":"1569553519","weight":9},{"id":"1569566885","weight":4},{"id":"1569566231","weight":4},{"id":"1569554881","weight":4},{"id":"1569566445","weight":4},{"id":"1569564333","weight":4},{"id":"1569566809","weight":4},{"id":"1569566629","weight":4},{"id":"1569565033","weight":4},{"id":"1569564677","weight":4},{"id":"1569565633","weight":9},{"id":"1569565219","weight":4},{"id":"1569566037","weight":4},{"id":"1569564969","weight":4},{"id":"1569566043","weight":4},{"id":"1569565029","weight":9},{"id":"1569565357","weight":9},{"id":"1569566505","weight":4},{"id":"1569566603","weight":4},{"id":"1569566695","weight":19},{"id":"1569566233","weight":9},{"id":"1569566667","weight":4},{"id":"1569566297","weight":4},{"id":"1569560503","weight":4},{"id":"1569565439","weight":9},{"id":"1569563395","weight":14},{"id":"1569565415","weight":4},{"id":"1569555367","weight":4},{"id":"1569565571","weight":4},{"id":"1569565611","weight":4},{"id":"1569565397","weight":9},{"id":"1569557275","weight":4},{"id":"1569565919","weight":4},{"id":"1569565661","weight":4},{"id":"1569566267","weight":4},{"id":"1569564131","weight":4},{"id":"1569564919","weight":4},{"id":"1569566823","weight":9},{"id":"1569565375","weight":4},{"id":"1569566715","weight":4},{"id":"1569566813","weight":4},{"id":"1569565293","weight":4},{"id":"1569564247","weight":4},{"id":"1569563975","weight":4},{"id":"1569551905","weight":4},{"id":"1569565457","weight":4},{"id":"1569566619","weight":23},{"id":"1569561185","weight":4},{"id":"1569558779","weight":4},{"id":"1569566817","weight":47},{"id":"1569564923","weight":9},{"id":"1569566299","weight":14},{"id":"1569565769","weight":4},{"id":"1569565805","weight":4},{"id":"1569566933","weight":4},{"id":"1569565537","weight":4},{"id":"1569564961","weight":4},{"id":"1569567013","weight":4},{"id":"1569565853","weight":4},{"id":"1569550425","weight":4},{"id":"1569566611","weight":4},{"id":"1569565635","weight":9},{"id":"1569566413","weight":4},{"id":"1569566555","weight":4},{"id":"1569564141","weight":4},{"id":"1569565139","weight":4},{"id":"1569565315","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T8.3","endtime":"15:40","authors":"Kumar Viswanatha, Emrah Akyol, Kenneth Rose","date":"1341242400000","papertitle":"Lossy Common Information of Two Dependent Random Variables","starttime":"15:20","session":"S3.T8: Directed Information, Common Information, and Divergence","room":"Stratton (491)","paperid":"1569566673"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
