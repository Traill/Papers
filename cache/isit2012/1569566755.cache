{"id":"1569566755","paper":{"title":{"text":"The Sensitivity of Compressed Sensing Performance to Relaxation of Sparsity"},"authors":[{"name":"David Donoho"},{"name":"Galen Reeves"}],"abstr":{"text":"Abstract\u2014Many papers studying compressed sensing consider the noisy underdetermined system of linear equations: y = Ax 0 + z, with n × N measurement matrix A, n < N , and Gaussian white noise z ∼ N(0, σ 2 I). Both y and A are known, both x 0 and z are unknown, and we seek an approximation to x 0 ; we let δ = n/N ∈ (0, 1) denote the undersampling fraction. In the popular strict sparsity model of compressed sensing, such papers further assume that x 0 has at most a speciﬁed fraction ε of nonzeros.\nIn this paper, we relax the assumption of strict sparsity by assuming the vector x 0 is close in mean p-th power to a sparse signal. We study how this relaxation affects the performance of"},"body":{"text":"We study asymptotic mean-squared error (AMSE), the large- system limit of the MSE of ˆ x 1,λ . Using recently developed tools based on Approximate Message Passing (AMP), we develop expressions for minimax AMSE M ∗ ε,p (δ, ξ, σ) \u2013 max over all approximately sparse signals, min over penalizations λ, where ξ measures the deviation from strict sparsity. There is of course a phase transition curve δ ∗ = δ ∗ (ε); only above this curve, δ > δ ∗ (ε), can we have exact recovery even in the noiseless- data strict-sparsity setting. It turns out that the minimax AMSE can be characterized succinctly by a coefﬁcient sens ∗ p (ε, δ) which we refer to as the sparsity-relaxation sensitivity. We give explicit expressions for sens ∗ p (ε, δ), compute them, and interpret them.\nOur approach yields precise formulas in place of loose order bounds based on restricted isometry property and instance optimality results. Our formulas reveal that sensitivity is ﬁnite everywhere exact recovery is possible under strict sparsity, and that sensitivity to added random noise in the measurements y is smaller than the sensitivity to adding a comparable amount of noise to the estimand x 0 . Our methods can also treat the mean q-th power loss. The methods themselves are based on minimax decision theory and seem of independent interest.\nFig. 1. Contour lines of the sparsity-relaxation sensitivity sens ∗ 2 ( , δ) as a function of ε/δ and δ. The dotted black curve graphs the phase boundary: above this curve, the minimax MSE is unbounded.\nFig. 3. Contour lines of the minimax LASSO AMSE M ∗ ε,2 (δ, ξ, σ) as a function of ξ 2 and σ 2 with ε = 0.05 and δ = 0.3."},"refs":[{"authors":[{"name":"R. Tibshirani"}],"title":{"text":"Regression shrinkage and selection with the lasso"}},{"authors":[{"name":"S. Chen"},{"name":"D. Donoho"}],"title":{"text":"Examples of basis pursuit"}},{"authors":[{"name":"S. S. Chen"},{"name":"D. L. Donoho"},{"name":"M. A. Saunders"}],"title":{"text":"Atomic decomposition by basis pursuit"}},{"authors":[{"name":"D. Donoho"},{"name":"A. Maleki"},{"name":"A. Montanari"}],"title":{"text":"The Noise Sensitivity Phase Transition in Compressed Sensing"}},{"authors":[{"name":"M. Bayati"},{"name":"A. Montanari"}],"title":{"text":"The LASSO risk for gaussian matrices"}},{"authors":[{"name":"D. Donoho"},{"name":"I. Johnstone"},{"name":"A. Montanari"}],"title":{"text":"Compressed Sensing over p Balls: Minimax Mean Squared Error"}},{"authors":[{"name":"R. DeVore"},{"name":"G. Petrovaa"},{"name":"P. Wojtaszczyk"}],"title":{"text":"Instance-optimality in probability with an 1 -minimization decoder"}},{"authors":[{"name":"E. J. Candes"},{"name":"T. Tao"}],"title":{"text":"Decoding by linear programming"}},{"authors":[{"name":"J. D. Blanchard"},{"name":"C. Cartis"},{"name":"J. Tanner"}],"title":{"text":"Compressed sensing: How sharp is the restricted isometry property?"}},{"authors":[{"name":"Y. Kabashima"},{"name":"T. Wadayama"},{"name":"T. Tanaka"}],"title":{"text":"A typical reconstruction limit for compressed sensing based on lp-norm minimization"}},{"authors":[{"name":"D. Guo"},{"name":"D. Baron"},{"name":"S. Shamai"}],"title":{"text":"A single-letter characterization of optimal noisy compressed sensing"}},{"authors":[{"name":"S. Rangan"},{"name":"A. K. Fletcher"},{"name":"V. K. Goyal"}],"title":{"text":"Asymptotic analysis of map estimation via the replica method and applications to compressed sensing"}},{"authors":[{"name":"W. Xu"},{"name":"B. Hassibi"}],"title":{"text":"Precise stability phase transitions for 1 minimization: A uniﬁed geometric framework"}},{"authors":[{"name":"M. Herman"},{"name":"T. Strohmer"}],"title":{"text":"General deviants: An analysis of per- turbations in compressed sensing"}},{"authors":[{"name":"M. Herman"},{"name":"D. Needell"}],"title":{"text":"Mixed operators in compressed sensing"}},{"authors":[{"name":"Y. G. J. Ding"},{"name":"L. Chen"}],"title":{"text":"Performance analysis of orthogonal matching pursuit under general perturbations"}},{"authors":[{"name":"D. L. Donoho"},{"name":"I. M. Johnstone"}],"title":{"text":"Minimax risk over l p balls"}},{"authors":[{"name":"G. Reeves"},{"name":"M. Gastpar"}],"title":{"text":"Differences between observation and sampling error in sparse signal reconstruction"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566755.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T9.1","endtime":"11:50","authors":"David Donoho, Galen Reeves","date":"1341487800000","papertitle":"The Sensitivity of Compressed Sensing Performance to Relaxation of Sparsity","starttime":"11:30","session":"S12.T9: L1-Regularized Least Squares and Sparsity","room":"Stratton West Lounge (201)","paperid":"1569566755"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
