{"id":"1569566909","paper":{"title":{"text":"Expurgation for Discrete Multiple-Access Channels via Linear Codes"},"authors":[{"name":"Eli Haim"},{"name":"Yuval Kochman"},{"name":"Uri Erez ∗"}],"abstr":{"text":"Abstract\u2014We consider the error exponent of the memoryless multiple-access (MAC) channel. We show that if the MAC channel is modulo-additive, then any error probability, and hence any error exponent, achievable by a linear code for the corresponding single-user channel, is also achievable for the MAC channel. Speciﬁcally, for an alphabet of prime cardinality, where linear codes achieve the best known exponents in the single-user setting (and the optimal exponent above the critical rate), this performance carries over to the MAC setting. At least at low rates, where expurgation is needed, our approach strictly im- proves performance over previous results, where expurgation was used at most for one of the users. Even when the MAC channel is not additive, it may be transformed into such a channel. While the transformation is lossy, we show that the distributed structure gain in some \u201cnearly additive\u201d cases outweighs the loss, and thus we can improve upon the best known exponent for these cases as well. This approach is related to that previously proposed for the Gaussian MAC channel, and is based on \u201cdistributed structure\u201d."},"body":{"text":"The error exponent of the multiple access (MAC) channel is a long-standing open problem. While superposition and successive decoding methods lead to capacity, they may not be optimal in the sense of error probability: the decoding process may be improved by considering that the transmission of other users is a codeword, rather than noise. However, ﬁnding the optimal performance is a difﬁcult task. Early results include the works of Slepian and Wolf [1], Gallager [2] and Pokorny and Wallmeier [3]. Applying the results of [2] to the important special case of a (modulo) additive MAC channel, e.g., the binary symmetric case, it follows that the random- coding exponent of the corresponding single-user channel is achievable for the MAC channel. This exponent is optimal above the critical rate [4] and gives the best known perfor- mance above the expurgation rate. However, for yet lower rates it is outperformed by the expurgated exponent (in the single-user case). The reason that the expurgated exponent is not achieved in [2] is that the sum of such two good (expurgated) single-user codebooks does not result in a good single-user one, and in particular, the sum of two codebooks with good minimum-distance properties may not be good in that respect. Liu and Hughes [5] and recently Nazari et al. [6] have proposed improvements over the previous results.\nSpeciﬁcally, Nazari et al. suggest to use expurgation on one of the codebooks. While this certainly improves performance, it still does not allow to achieve the single-user expurgated exponent for additive MAC channels.\nIn [7] the exponent of a Gaussian MAC channel is consid- ered. It is suggested to use \u201cdistributed structure\u201d: the users use lattice codebooks, where the codebook of one user is nested in that of the other. This scheme has the advantage that the sum of the codebooks, as seen by the decoder, is a single linear code; since linear codes are inherently expurgated, the exponent at low rates is improved. However, the exponent obtained is inferior to the single-user exponent. It can be explained by viewing the joint codebook as tiling of the codebook of the user with weaker power (ﬁnest lattice) in the shaping region of the associated single-user channel. There is a loss since it is not a perfect tiling, i.e. not ﬁlling this whole shaping region.\nFavorably, when considering a discrete memoryless MAC channel without a cost constraint, the situation is more simple. For additive MAC channels we make the basic observation, that by \u201csplitting\u201d a linear codebook between the users, any error probability achievable in the corresponding single-user channel using linear codebooks is achievable for the MAC channel as well. This implies for prime (e.g. binary) alphabets, that the best currently known single-user error exponents for any code (not necessarily linear) are achievable for the MAC channel, including the random-coding and expurgated exponents. The improvement over previous results stems from the use of linear codes, which are inherently expurgated; thus using them provides \u201cjoint expurgation\u201d even in a distributed setting. In comparison, [2] performs no expurgation, and in [6] expurgation is performed only for one user.\nBut what happens outside the special case of additive channels? We are inspired by the fact that in the context of ﬁrst-order (capacity) analysis of networks, the advantage of linear codes has indeed been extended to some non-additive channels [8].\nThe application of linear codes to additive communication networks has a capacity advantage in many interesting scenar- ios, see e.g. [9], [10], [11]. In [8], a modulo-lattice transforma- tion is derived, that allows to obtain a virtual additive channel from any original MAC channel, albeit with a loss of capacity. It is shown in [8] that in some situations, the gain offered by the ability to use linear codes outweighs the loss inﬂicted by\nthe transformation. In this work we adopt the same ideas to the MAC exponent problem: we show that for MAC channels that are \u201cnearly additive\u201d, indeed the transformation improves over the best known exponents so far at low rates. We note that when one considers less symmetric channels, the results of [6] outperform those of the new scheme.\nConsider the single-user discrete memoryless channel (DMC) deﬁned by P Y |X (·|·), where X and Y are the channel input and output, respectively, with discrete alphabets X and Y. We recall some results regarding the error exponent of this channel, see [4].\nwhere ǫ n is the minimal possible error probability of codes (averaged over the codewords) with block length n and rate R.\nThe best known achievable error exponent for this channel, denoted by E SU (R), is given by the maximum between the expurgated error exponent E SU ex (R) and the random-coding error exponent E SU r (R). The expurgated exponent is larger than the random-coding exponent below some rate R ex (this range is thus called \u201cthe expurgation region\u201d). Above the critical rate R cr , the random-coding exponent is known to be optimal.\nConsider a two-user discrete memoryless MAC channel P Y |X 1 ,X 2 , where X 1 , X 2 are the channel inputs and Y is its output. Denote the codebook of user i by C i , and its rate by R i = 1 / n log|C i |.\nThe error event is deﬁned as the event that at least one of the messages from the message pair is decoded in error. 1 The error exponent of the MAC channel is deﬁned as\nwhere ǫ n is the minimal possible error probability for codes of length n, with the rate-pair (R 1 , R 2 ).\nGallager [2] found an achievable error exponent that is the minimum of three random coding error exponents cor- responding to different error events. The ﬁrst two correspond to making a erroneous decision on one message, by a \u201cgenie\u201d aided decoder, i.e., one that has knowledge of the message of the other user as side information. The third error event corresponds to making an erroneous decision regarding the two messages as a combined message. Each of these amounts to an error event over a single-user channel. Therefore, each exponent is equal to by Gallager\u2019s random coding error exponent [4] for the associated single-user channel.\nThe best known error-exponent of this channel is given by Nazari et al. [6]. This bound however is not given in closed form and is hard to compute.\nC. Additive-Noise Single-User Channel Consider the following channel:\nwhere all variables are deﬁned over the alphabet Z m = {0, 1, . . . , m − 1} and ⊕ denotes addition over this alphabet, i.e., modulo an integer number m. The noise N is additive, i.e., statistically independent of the channel input X.\nIn the expurgation region, the best known error exponent of this channel is larger than the random coding error exponent, as can be seen in Figure 1.\nIn the context of additive channels, it is important to consider linear codes. We deﬁne a linear code C via a k × n generating matrix G, 2 by\nThe rate is equal to R = k / n · log m. Clearly, every rate is possible asymptotically as n → ∞. We deﬁne E SU L (R) to be the error exponent of linear codes, i.e., as (1), except that ǫ n is the minimal possible error probability of linear codes only. We note that for single-user additive channels of the form (3), when the alphabet size m is a prime, the best known error exponent of linear codes, denoted by E SU L (R), is equal to the best known error exponent of the channel E SU (R) (see [13], [4], [14]), and in particular is optimal above the critical rate. In addition, we note that for linear codes the average error probability (over the codewords) is equal to the maximal error probability, due to their structure.\nA channel which is of particular interest in this work is the additive MAC channel\nwhere all variables are deﬁned over the alphabet Z m = {0, 1, . . . , m − 1} and ⊕ denotes addition over this alphabet, i.e., modulo an integer number m. The noise N is additive, i.e., statistically independent of the pair (X 1 , X 2 ).\nViewing the joint codebook X = X 1 ⊕ X 2 as a single- user codebook, we get the following channel (over the same alphabet)\nwhich we call the associated single-user channel of (5). Notice that when comparing the MAC channel to its associated single- user one,\nsince the last is equivalent to cooperation between the en- coders. In [2] it is shown that 3\nThus, it is equal to E SU (R 1 + R 2 ) above the expurgation rate and optimal above the critical rate. However, the best known error exponent for the associated single-user though, is larger in the expurgation region, as was shown in Figure 1.\nWe note that simple time sharing, where every user uses an expurgated codebook, improves on Gallager\u2019s random-coding bound in some cases, particularly for small enough rates and as the channel noise become smaller. Since [2], there were several improvements [5], [6] to the achievable error exponent. However these do not close the gap to the best known error exponent of the associated single-user channel. In the next section, we close this gap (for modulo-additive MAC channels) by attaining expurgation for all users.\nIn this section we ﬁrst describe a coding scheme for additive-noise MAC channels with prime alphabet size, which achieves the best known error exponent of its associated single-user channel. This is equivalent to full cooperation of the encoders, and thus it is optimal (in terms of error exponent) whenever the optimum is known for the single-user channel (i.e., above its critical rate).\nConsider the additive-noise MAC channel, as given in (5), with prime alphabet size m. We construct a codebook pair for the MAC channel using linear codes. We use a good linear code for the associated single-user channel (6), which we decompose into two sub linear codes, one for each user.\nLet G be a k × n generating matrix of a linear code C (see (4)) with rate R = k / n · log m. For some integers\nTherefore, the sum of codewords is indistinguishable from a codeword of the single-user code with R = R 1 + R 2 . Clearly, for every rate pair such a construction is possible asymptotically as n → ∞. A similar claim holds for a general number of users as well.\nProposition 1: The coding technique above achieves the best error probability of linear codes for the single-user channel. Thus, it achieves the exponent E SU L (R), and for prime m it achieves E SU (R) as well.\nThe previously best known error exponent for general mem- oryless discrete MAC channels is given by Nazari et al. [6]. In their derivation, codewords are expurgated from only one of the codebooks. Since our bound achieves the best known error exponent of the associated single-user channel (for the special case of additive MAC channels with prime alphabet size), it must be at least as good the one found by Nazari et al. Moreover, for small enough rate-pairs we expect our bound to be strictly better, since full expurgation is required in order to achieve the error exponent of the associated single- user channel. When considering more than two users, the gap is expected to increase since expurgation of one user becomes less signiﬁcant. In the sequel, we show how this advantage can be leveraged to non-additive MAC channels.\nWith the aim of applying a similar scheme to general (non- additive) discrete memoryless MAC channels P Y |X 1 ,X 2 , in this section we describe a method for transforming such channels into additive-noise MAC channels. We denote the obtained channel after the transformation as the resulting virtual chan- nel. The transformation is a discrete and scalar modiﬁcation of the Modulo-Lattice Transformation for continuous MAC channels [8].\nThe transformation is deﬁned for any ﬁnite alphabet size m. Let v i ∈ Z m be the input of the ith user to the virtual channel, and U i ∼ Uniform(Z m ) be its dither (i.e., common randomness at the ith transmitter and at the receiver), which is statistically independent of the dither of the other user and of v 1 , v 2 . Each encoder computes X \u2032 i = v i ⊕ U i and applies a scalar precoding function f i : Z m → X on it. The inputs to the channel are therefore given by\nNote that due to the dither, X \u2032 i is uniformly distributed over Z m and is statistically independent of v 1 , v 2 . Let\nwhere k i ∈ Z m , and let ˆ S = g(Y ) be some scalar estimator function of S from the channel output Y . Denote the estimation error by N = ˆ S − S. We deﬁne the output of the virtual channel as\n= [(k 1 (v 1 + U 1 ) mod m) + (k 2 (v 2 + U 2 ) mod m) + N − (k 1 U 1 + k 2 U 2 )] mod m\n= (k 1 v 1 + k 2 v 2 + N ) mod m = k 1 v 1 ⊕ k 2 v 2 ⊕ (N mod m) = k 1 v 1 ⊕ k 2 v 2 ⊕ ˜ N ,\nProposition 2 (The virtual MAC channel): Applying the transformation leads to the following virtual channel:\nwhere N = ˆ S − S is statistically independent of the channel inputs (v 1 , v 2 ), and ˜ N = N mod m.\nNotice that the transformation is not unique, and one is free to choose the alphabet size m, the precoding functions f i (·) and the estimator of S. We call any virtual MAC channel (11) that can be obtained by some choice of parameters, a feasible virtual MAC channel. Applying this transformation to any MAC channel, we have the following.\nProposition 3: Let ǫ n be the best error probability achiev- able with a code of length n on a MAC channel. Then\nwhere ˜ ǫ n is the best error probability achievable by a linear code of the same length on a feasible virtual MAC chan- nel (11).\nis the associated single-user channel of a feasible virtual MAC channel (11). Applying Proposition 3 to exponents, leads to our main result:\nTheorem 1: For any MAC channel, and any feasible single- user channel (12) of that channel,\nThis has great signiﬁcance when linear codes are known to perform well:\nCorollary 1: For any MAC channel, and any feasible single-user channel (12) of that channel with alphabet of prime cardinality,\n\u2022 Notice that this transformation is lossy in terms of capac- ity. However, since the resulting channel is an additive- noise channel, efﬁcient coding techniques and known bounds can be easily applied. In particular, for MAC channels, expurgation in all the users can be applied by using linear codes as in Section III.\n\u2022 We expect the beneﬁt from this coding technique to outweigh the loss when the channel is \u201cclose\u201d to additive. In the next section, after studying the binary case, we give a binary example which illustrates this property with a single parameter.\n\u2022 We note that this transformation is applicable to various non-additive network problems, where structure can im- prove the best-known achievable rate region (see e.g. [9], [10], [11]). In such settings, the gain will appear also as a \u201ccapacity gain\u201d rather than only in the error exponent.\nIn this section we conﬁne the discussion to binary MAC channels, i.e. channels with binary inputs and a binary output. We denote this general channel P Y |X 1 ,X 2 as:\nwhere the additive noise N = Y ⊕ (X 1 ⊕ X 2 ) depends on the channel input pair (X 1 , X 2 ).\nA natural choice for the parameter m of the transformation is clearly m = 2. We select f (x) = x, k 1 = k 2 = 1 and ˆ S = g(Y ) = Y . This selection leads to the following effective additive noise at the virtual channel is\nThe virtual channel is then an additive MAC channel given by:\nwhere ˜ N , given in (15)-(16), is statistically independent of (V 1 , V 2 ).\nWe now use the analysis of the previous subsection in order to study an example of an almost additive-noise binary MAC channel. Speciﬁcally, we consider the MAC channel charac- terized by Table I, which gives its transition probabilities.\n0 1 1 − [p(1 − q) + (1 − p)q] p(1 − q) + (1 − p)q 1 0 1 − [p(1 − q) + (1 − p)q] p(1 − q) + (1 − p)q 1 1 \t q \t q\nThe value of p determines the deviation of the channel from additivity. For small p the channel is nearly an additive MAC channel.\nIn Figure 2 we compare the resulting error exponent of the virtual channel with Gallager\u2019s [2] random coding exponent for symmetric rate pairs. 4 For the comparison we take the limit of zero rate-pair, where the gain due to expurgation is maximal. As p increases, the coding technique developed in this paper gains less since the channel transformation looses more as the channel is less additive.\nFor small enough p and for small enough rates we expect this bound to be strictly larger then the best known error expo- nent for this channel [6]. This is since [6] applies expurgation only to the user with larger rate, while the bound presented here achieves two-user expurgation. Nazari et al. [6] studied a non-symmetric example, where Pr(N = 1|X 1 = 1, X 2 = 1) = 1 2 , and all the other conditional probabilities of N are equal to 0.01. In this case the coding scheme described here is inferior to the one of [6], as expected since the channel is far from being an additive.\nBy using linear codes, we have shown that for modulo- additive MAC channels with a prime alphabet size, the achiev- able error exponent is equal to the best known exponent of the associated single-user channel. In addition, we have demonstrated that linear codes offer improvement to the best known MAC error exponent region for \u201calmost additive\u201d channels.\nWhile we chose to present the results using two-user MAC channels, the approach immediately extends to any number of users, allowing for full expurgation of the combined linear code. It is therefore reasonable to expect that at low rates and at least for modulo-additive channels, the gain over the best previously known achievable error exponent will increase with the number of users. The approach of transforming a MAC channel into an additive one is also applicable to a wide variety of non-additive network setups, where structure is beneﬁcial in terms of capacity."},"refs":[{"authors":[{"name":"D. Slepian"},{"name":"J. K. Wolf"}],"title":{"text":"A coding theorem for multiple access channels with correlated sources"}},{"authors":[{"name":"R. G. Gallager"}],"title":{"text":"A perspective on multiaccess channels"}},{"authors":[{"name":"J. Pokorny"},{"name":"H. Wallmeier"}],"title":{"text":"Random coding bound and codes produced by permutations for the multiple-access channel"}},{"authors":[{"name":"R. G. Gallage"}],"title":{"text":"Information Theory and Reliable Communication"}},{"authors":[{"name":"Y.-S. Liu"},{"name":"B. Hughes"}],"title":{"text":"A new universal random coding bound for the multiple-access channel"}},{"authors":[{"name":"A. Nazari"},{"name":"A. Anastasopoulos"},{"name":"S. S. Pradhan"}],"title":{"text":"Error exponent for multiple-access channels:lower bounds"}},{"authors":[{"name":"E. Haim"},{"name":"Y. Kochman"},{"name":"U. Erez"}],"title":{"text":"Improving the MAC error exponent using distributed structure"}},{"authors":[{"name":"U. Erez"},{"name":"R. Zamir"}],"title":{"text":"A modulo-lattice transformation for multiple- access channels"}},{"authors":[{"name":"D. Krithivasan"},{"name":"S. Pradhan"}],"title":{"text":"Lattices for distributed source coding: Jointly Gaussian sources and reconstruction of a linear function"}},{"authors":[{"name":"M. P. Wilson"},{"name":"K. R. Narayanan"},{"name":"H. D. Pﬁster"},{"name":"A. Sprintson"}],"title":{"text":"Joint physical layer coding and network coding for bidirectional relaying"}},{"authors":[{"name":"T. Philosof"},{"name":"R. Zamir"},{"name":"U. Erez"},{"name":"A. Khisti"}],"title":{"text":"Lattice strategies for the dirty multiple access channel"}},{"authors":[{"name":"L. Weng"},{"name":"S. S. Pradhan"},{"name":"A. Anastasopoulos"}],"title":{"text":"Error exponent regions for Gaussian broadcast and multiple-access channels"}},{"authors":[{"name":"R. L. Dobrushin"}],"title":{"text":"Asymptotic optimality of group and systematic codes for some channels"}},{"authors":[{"name":"A. Barg"},{"name":"J. Forney"}],"title":{"text":"Random codes: minimum distances and error exponents"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566909.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S1.T2.3","endtime":"10:50","authors":"Eli Haim, Yuval Kochman, Uri Erez","date":"1341225000000","papertitle":"Expurgation for Discrete Multiple-Access Channels via Linear Codes","starttime":"10:30","session":"S1.T2: Multiple Access Codes","room":"Kresge Auditorium (109)","paperid":"1569566909"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
