{"id":"1569566171","paper":{"title":{"text":"Convergence of Generalized Linear Coordinate-Descent Message-Passing for Quadratic Optimization"},"authors":[{"name":"Guoqiang Zhang"},{"name":"Richard Heusdens"}],"abstr":{"text":"Abstract\u2014We study the generalized linear coordinate-descent (GLiCD) algorithm for the quadratic optimization problem. As an extension of the linear coordinate-descent (LiCD) algorithm, the GLiCD algorithm incorporates feedback from last iteration in generating new messages. We show that if the amount of feedback signal from last iteration is above a threshold and the GLiCD algorithm converges, it computes the optimal solution. Based on the result, we further show that if the feedback signal is large enough, the GLiCD algorithm is guaranteed to converge."},"body":{"text":"diagonal dominant1. Later on, Johnson et al. discovered a more general convergence condition [4], [5]. They found that if the matrix J is walk-summable2, the min-sum algorithm is guaranteed to converge.\nRecently, the linear coordinate-descent (LiCD) algorithm has been proposed for the quadratic optimization problem [7]. The LiCD algorithm was motivated by the block coordinate- descent algorithms (see [8]) which were proposed for discrete MAP problems. The LiCD algorithm has the advantage that each message is described by only one parameter, as apposed to the min-sum algorithm of which each message is described by two parameters. A sufﬁcient convergence condition of the LiCD algorithm requires that the matrix J is walk-summable (like the min-sum algorithm). In order to ﬁx the convergence for a general matrix J (i.e., arbitrarily symmetric positive deﬁnite), the LiCD-based double-loop algorithm was also proposed in [7]. The double-loop algorithm performs diagonal- loading on J to obtain a new quadratic matrix that is walk- summable, thus allowing usage of the LiCD algorithm.\nInspired by the JOR algorithm, the generalized linear coordinate-descent (GLiCD) algorithm was proposed in [9] to solve a general convex optimization problem. The GLiCD algorithm extends the LiCD algorithm by incorporating feed- back (an estimate of x ∗ ) from last iteration in computing new messages. A sufﬁcient convergence condition of the algorithm was derived in [9].\nIn this work, we revisit the GLiCD algorithm for solving the quadratic optimization problem (1). Motivated by the convergence of the JOR algorithm, our primary concern is to study if the GLiCD algorithm converges for a general symmetric positive deﬁnite matrix J. The convergence of the GLiCD algorithm for a general matrix J is of great importance in practice. This is because both the JOR algorithm and the LiCD-based double-loop algorithm usually possess a slow convergence rate.\nWe study the convergence of the GLiCD algorithm in two steps. In the ﬁrst step, an algebraic relationship between the GLiCD and the LiCD-based double-loop algorithm is estab-\nwhere the new node and edge functions are given by f (t) i (x i ) = f i (x i ) +\nTo brieﬂy summarize, each node i at time t has the parameters {z (t) ui , ˆ x (t) i |u , u ∈ N (i)}. The estimates {ˆ x (t) i |u , u ∈ N (i)} provide information about the optimal solution x ∗ i . Thus, the estimates can be used as feedback in computing new messages and estimates in next iteration.\nIn this subsection, we use the estimates {ˆ x (t) i |u } as feedback in computing the new messages and estimates. As is presented in [9], the basic procedure is to ﬁrst construct a penalty function for each edge in the graph. The node and edge functions are then combined with the penalty functions in updating the messages and the estimates. In particular, we deﬁne the penalty function g (t) ij (x i , x j ) for (i, j) ∈ E to be a quadratic function:\nwhere the weighting factor α ≥ 0. The parameter α controls the amount of feedback in next iteration. As α increases, the penalty function enlarges the impact of the estimates when computing new estimates.\nWe note that the penalty function (7) is different than that of [9]. In [9], the function g (t) ij (x i , x j ) is constructed by using {ˆ x (t) i |u , u ∈ N (i)\\j} and {ˆ x (t) j |v , v ∈ N (j)\\i}. In this work, the particular form of (7) is essential to establish the convergence argument for the algorithm in Section IV.\nWe are now in a position to compute the new estimates and messages. Without loss of generality, we focus on computing {z (t+1) ij , z (t+1) ji } and {ˆ x (t+1) i |j , ˆ x (t+1) j |i } that are associated with the edge (i, j) ∈ E. From [9], a function L (t) ij (x i , x j ) is deﬁned to be\nThe function L (t) ij (x i , x j ) is in a quadratic form. The new estimates ˆ x (t+1) i |j and ˆ x (t+1) j |i are computed by minimizing the function L (t) ij (·, ·) over x i and x j [9]:\nCorrespondingly, the expressions for ˆ x (t+1) i |j and ˆ x (t+1) j |i are given by\nwhere s = α/(1 + α). By using the fact that α ≥ 0, we have 1 > s ≥ 0.\nUpon obtaining the estimates ˆ x (t+1) i |j and ˆ x (t+1) j |i , the remain- ing work is to compute z (t+1) ij and z (t+1) ji . From [9], z (t+1) ji is\nthe relationship between the GLiCD algorithm and the LiCD- based double-loop algorithm in [7]. We show that the updating expressions of the two algorithms can be derived from each other mathematically. As a consequence, some theoretical results obtained for the double-loop algorithm can be used to study the performance of the GLiCD algorithm.\nIn this subsection, we reformulate the two updating expres- sions (9) and (11) into a vector form. The vector form provides a big picture of the evolution of the algorithm.\nNote that (11) describes the relationship between the esti- mates and the message at each time step. By using (11), (9) can be rewritten as\nwhere ∀(i, j) ∈ E, t = 0, 1, . . . The new expression (12) only involves the estimates instead of the messages. We use ˆ x (t) edge to denote the vector of all the estimates at time t. ˆ x (t) edge is of dimension |E|, of which each component ˆ x (t+1) i |j\ncorresponds to an directed edge [i, j] ∈ E. With ˆ x (t) edge at hand, we denote the corresponding optimal solution as x ∗ edge , where the component associated with [i, j] ∈ E is set to be x ∗ i . The algorithm converges to the optimal solution if ˆ x (t) edge approaches to x ∗ edge as t → ∞. The expression (12) can be rewritten in a vector form as\nwhere the matrices D, E ∈ R |E|×|E| , and the vector y ∈ R |E| , are given by\n  \nu = j, k = i and [i, j], [u, k] ∈ E 0 \t otherwise\nwhere ˆ x (0) edge represents the initial estimation vector. It is seen that the matrix D(sI + (1 − s)E) plays an important role for the convergence of the GLiCD algorithm. If the spectral radius of the matrix D(sI + (1 − s)E) is less than 1 (i.e., ρ(D(sI + (1 − s)E)) < 1), the impact of the initial vector\nFrom (18), we let M s = (I − s(I − (1 − s)DE) −1 D) −1 . There are two matrix-inversions in M s . The inner matrix- inversion is obtained by implementing the LiCD algorithm for each time step in (17). On the other hand, the outer matrix- inversion is obtained by following the iteration (17) directly.\nC. Algebraic relation between the GLiCD and the LiCD-based double-loop algorithm\nIn this subsection, we establish the algebraic relation be- tween the GLiCD and the LiCD-based double-loop algorithm. Based on the result, we show that the ﬁxed point in (15) is identical to x ∗ edge when the parameter s is chosen properly.\nThe two expressions (15) and (18) carry a closed relation- ship. Suppose 1 > s ≥ ⌊1 − 1/ρ( ¯ R)⌋ + . From Lemma 3.2, the matrix I − (1 − s)DE in (18) is invertible. In this situation, the expression (15) can be readily derived from (18):\n(1 − s)(I − s(I − (1 − s)DE) −1 D) −1 ·(I − (1 − s)DE) −1 Dy\nIntuitively speaking, the derivation (20) compresses the LiCD- based double-loop algorithm into the GLiCD algorithm which has a single loop. Given a particular parameter s ∈ (⌊1 − 1/ρ( ¯ R)⌋ + , 1), if the GLiCD algorithm converges, it should converges faster than the double-loop algorithm.\nBased on the analysis above, we can conclude that the ﬁxed point in (15) is identical to x ∗ edge when s ∈ (⌊1 − 1/ρ( ¯ R)⌋ + , 1). We summarize the result in a theorem below.\nTheorem 3.3: If there exits s ∈ (⌊1 − 1/ρ( ¯ R)⌋ + , 1) such that the spectral radius of the matrix D(sI + (1 − s)E) is less than 1, then the GLiCD algorithm converges to the optimal solution. In other words, the ﬁxed point ˆ x (∞) edge in (15) is identical to x ∗ edge , i.e.,\nIt is now clear that when s ∈ (⌊1 − 1/ρ( ¯ R)⌋ + , 1), if the GLiCD algorithm converges, it computes the optimal solution. The remaining work is to show if there exits s ∈ (⌊1−1/ρ( ¯ R)⌋ + , 1) such that the GLiCD algorithm converges.\nFrom (7), the parameter α determines the amount of feed- back in computing new messages and estimates. We show that when α is large enough (or equivalently, the parameter s approaches to 1), the GLiCD algorithm converges. We use the Taylor expansions in the argument.\nAs indicated in Theorem 3.3, the key point in proving the algorithm convergence is to study the spectral radius of the\n= I + (1 − s)DB − (1 − s)D(I − E) = I − (1 − s)D(I − (B + E)) = sI + (1 − s)(B + E)\nThus, we can safely say that when 1 > s > ⌊1 − 1/ρ( ¯ R)⌋ + , the spectral radius of Q s is less than 1.\nThe above analysis shows that if s is sufﬁciently close to 1, the GLiCD algorithm converges, which we summarize in a theorem below.\nTheorem 4.1: If the parameter s is sufﬁciently close to 1 from below, the spectral radius of the matrix D(sI +(1−s)E) is less than 1. Consequently, the GLiCD algorithm converges to the optimal solution.\nRemark 4.2: We point out that the matrix Q s can be used to construct the message-updating expression of the JOR algorithm [1]. In particular, the expression takes the form\nCompared with JOR algorithm, the GLiCD algorithm updates the estimates nonlinearly in terms of the parameter s (see (12)), resulting in the last term in (24).\nIn this paper, we have studied the convergence of the GLiCD algorithm for the quadratic optimization problem. In particular, we show that the updating expression of the GLiCD algorithm can be alternatively derived from that of the LiCD-based double-loop algorithm. We then utilize the theoretical results of the double-loop algorithm to investigate the convergence of the GLiCD algorithm. Finally, we show that if the feedback signal is set to be large enough (i.e., large α), the GLiCD algorithm converges to the optimal solution."},"refs":[{"authors":[{"name":"D. P. Bertseka"},{"name":"J. N. Tsitsiki"}],"title":{"text":"Parallel and distributed Computation: Numerical Methods "}},{"authors":[{"name":"J. Pearl"}],"title":{"text":"Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference"}},{"authors":[{"name":"Y. Weiss"},{"name":"W. T. Freeman"}],"title":{"text":"Correctness of Belief Propagation in Gaussian Graphical Models of Arbitrary Topology"}},{"authors":[{"name":"J. K. Johnson"},{"name":"D. M. Malioutov"},{"name":"A. S. Willsky"}],"title":{"text":"Walk-sum Inter- pretation and Analysis of Gaussian Belief Propagation"}},{"authors":[{"name":"D. M. Malioutov"},{"name":"J. K. Johnson"},{"name":"A. S. Willsky"}],"title":{"text":"Walk-Sums and Belief Propagation in Gaussian Graphical Models"}},{"authors":[{"name":"N. Ruozzi"},{"name":"S. Tatikonda"}],"title":{"text":"Unconstrained Minimization of Quadratic Functions via Min-Sum"}},{"authors":[{"name":"G. Zhang"},{"name":"R. Heusdens"}],"title":{"text":"Linear Coordinate-Descent Message- Passing for Quadratic Optimization"}},{"authors":[{"name":"D. Sontag"},{"name":"A. Globerson"},{"name":"T. Jaakkola"}],"title":{"text":"Introduction to Dual Decomposition for Inference"}},{"authors":[{"name":"G. Zhang"},{"name":"R. Heusdens"}],"title":{"text":"Generalized Linear Coordinate-Descent Message-Passing for Convex Optimization"}},{"authors":[{"name":"J. K. Johnson"},{"name":"D. Bickson"},{"name":"D. Dolev"}],"title":{"text":"Fixing Convergence of Gaus- sian Belief Propagation"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566171.pdf"},"links":[{"id":"1569565377","weight":5},{"id":"1569564669","weight":5},{"id":"1569564605","weight":5},{"id":"1569565097","weight":5},{"id":"1569566761","weight":5},{"id":"1569566571","weight":5},{"id":"1569552245","weight":5},{"id":"1569565613","weight":5},{"id":"1569558681","weight":5},{"id":"1569565535","weight":5},{"id":"1569566505","weight":5},{"id":"1569566853","weight":5},{"id":"1569565467","weight":5},{"id":"1569567235","weight":15},{"id":"1569566233","weight":5},{"id":"1569565415","weight":5},{"id":"1569565665","weight":21},{"id":"1569565093","weight":5},{"id":"1569566595","weight":5},{"id":"1569566639","weight":36},{"id":"1569566819","weight":10},{"id":"1569564437","weight":5},{"id":"1569564787","weight":10},{"id":"1569567691","weight":5},{"id":"1569564807","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T7.2","endtime":"10:30","authors":"Guoqiang Zhang, Richard Heusdens","date":"1341483000000","papertitle":"Convergence of Generalized Linear Coordinate-Descent Message-Passing for Quadratic Optimization","starttime":"10:10","session":"S11.T7: Message Passing Algorithms","room":"Stratton (407)","paperid":"1569566171"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
