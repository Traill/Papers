{"id":"1569566497","paper":{"title":{"text":"Binary Graphs and Message Passing Strategies for Compressed Sensing in the Noiseless Setting"},"authors":[{"name":"Francisco Ramirez-Javega"},{"name":"Meritxell Lamarca"},{"name":"Javier Villares"}],"abstr":{"text":"Abstract\u2014We propose a scheme for Compressed Sensing in the noiseless setting that reconstructs the original signal operating on a binary graph where the samples are obtained sequentially. The proposed scheme has an affordable computational complexity and a large performance enhancement with respect to similar schemes in the literature, thanks to the proposed measurement matrix structure and enhanced decoding based on a message passing algorithm."},"body":{"text":"The Compressed Sensing (CS) problem considers the esti- mation of an unknown and sparse signal vector x ∈ 4 𝑛 from a vector of linear observations y ∈ 4 𝑚 , 𝑚 < 𝑛, y = Ax where A ∈ 4 𝑚×𝑛 is a ﬁxed randomly generated matrix known as measurement matrix and only a small number 𝑘 (the sparsity index), 𝑘 << 𝑛, of elements of x are non-zero. The set containing the positions of these elements is known as the support set, deﬁned as 𝒮 ≜ {𝑖 ∈ 1, ..., 𝑛 : 𝑥 𝑖 ∕= 0}, with cardinality ∣𝒮∣ = 𝑘. The solution to this system of equations is known to be given by the vector ˆx 0 that minimizes ∥x∥ 0 ( 𝑙 0 norm) subject to y = Aˆx 0 , which is a non-convex optimization problem.\nThe authors in [1], [2] established that the vector ˆx 1 with minimum 𝑙 1 norm subject to y = Aˆx 1 coincides with ˆx 0 whenever the measurement matrix satisﬁes the RIP condition [1]. Some examples of RIP matrices are given, with high probability, by matrices whose entries are i.i.d. Radamacher or Gaussian [3] [4]. These facts enabled the development of several 𝑙 1 -based reconstruction algorithms to solve the set of equations (see [5], [6] and references therein) and the characterization of the intrinsic limits, in terms of samples ( 𝑚), of 𝑙 1 -based algorithms [7].\nRecently, within this framework, the authors in [8] showed that 𝑚 ≥ 𝑘 samples sufﬁces to reconstruct a real-valued signal with 𝑘 non-zero components in the almost lossless compres- sion setting in the 𝑛 asymptotic regime. This result indicated that 𝑙 1 -based algorithms are non-optimal, since they require a number of samples larger than 𝑘 to succesfully recover the signal [7]. Simultaneously, in the literature some algorithms based on message propagation over graph representations of\nmeasurement matrices were shown to outperform 𝑙 1 -based theoretical limits, e.g. the scheme proposed in [9].\nIn this paper a new graph-based scheme is introduced that also outperforms the 𝑙 1 theoretical limits and has very low computational complexity, thanks to the use of binary sparse matrices. The proposed scheme stems from the original design of sudocodes [10], which has the distinctive features of taking inﬁnite reliability decisions (i.e. no errors are made), requiring binary sparse measurement matrices and its worst case compu- tational complexity at the decoder side is 𝑂 (𝑘 log(𝑘) log(𝑛)).\nThe scheme in this paper combines the introduction of a block-wise constructed binary measurement matrix structure with the proposal of a check node degree proﬁle design criterion and an extension of the existing veriﬁcation-based decoding algorithms [11], [12]. This enhancement of the veriﬁcation algorithm in [11] is particularly relevant for very sparse signals, since in this case length-four cycles cannot be avoided in the graph. The use of binary measurement matrices is a key point to reduce the computational complexity at the decoder side, as in this case the decoding process only performs additions and comparisons, enabling the use of both a sudocode-based or veriﬁcation-based decoder, which has an 𝑂(𝑛) computational complexity at the decoder side. The proposed algorithm outperforms the results in [10] and [11] in terms of number of measurements while having a similar decoding complexity than [11].\nThis paper is structured as follows. In section II we present the proposed scheme. We introduce a systematic method to construct binary matrices in section III. We propose in section IV an extension of the decoding algorithm proposed in [11] aimed at mitigating the effect of cycles of length 4 in dense binary graphs for the noiseless setting. In section V we give a closed-form expression to obtain the appropriate check node degree proﬁle. Finally, in section VI simulation results illustrate the performance enhancement obtained by the proposed compressed sensing system.\nWe assume that the signal vector x is strictly 𝑘-sparse, 0 < 𝑘 ≤ 𝑛, i.e. all the realizations have exactly 𝑘 non-null components and the components that are not indexed in 𝒮 are strictly zero. The coefﬁcients indexed in 𝒮 are randomly drawn according to a generic probability density function. We\nassume through all the work that the measures y = Ax are not corrupted by noise. A is a binary matrix where the number of ones depends on the sparsity ratio 𝑘/𝑛 and is sparse becoming denser for very low values of 𝑘/𝑛. Matrix A is ﬁxed for all signal realizations. It is randomly generated according to the procedure described in section III and it is characterized by the fraction of rows/columns with 𝑑 ones, denoted respectively as R(d) and L(d), the check and variable node degree proﬁles of the associated graph.\nThe proposed system employs a sequential non-adaptive sampling procedure, i.e., the measurements are drawn one by one following the order of the rows of matrix A and after each new measurement is drawn a new attempt is made to recover the sequence x. This procedure is followed until all the components in x are identiﬁed. Note that the number of measurements required to reconstruct vector x may be different for each signal realization.\nThe binary and sparsity features of A are exploited to retrieve the signal vector x with a low-complexity decoding algorithm based on the description of the system by a graph. The decoding attempts are done using the algorithm described in section IV-B.\nThe measurement matrix A can be represented by a graph having three sets of nodes: signal nodes (elements of vector x), measurement nodes (elements of vector y) and check nodes connected to 𝑑 signal and one measurement nodes as indicated by the binary measurement matrix A. Hereafter we refer to the signal nodes as variable nodes.\nThe graph is fundamentally characterized by the degree distribution of the check nodes R(𝑑) and that of the variable nodes L(𝑑). We design the distribution L(𝑑) to be regular, i.e. all the variable nodes are connected to the same number/degree 𝑁 of measurement nodes; the distribution R(𝑑) is optimized in section V, where a proﬁle R(𝑑) having at most two consecutive degrees is adopted.\nRather than generating matrix A as a random realization of the ensemble of matrices having degrees proﬁles R(𝑑) and L(𝑑), this paper proposes to introduce some structure in the construction of matrix A. The purpose is to guarantee that all components of vector x contribute to a measurement before any of them is sensed again. This structure is beneﬁcial in the sequential sampling procedure: it ensures that all the components of vector x are sensed equally early, avoiding the possibility that one non-zero sample may not be sensed until late and, therefore, the sequential sampling may require a large number of samples to stop.\nHence, we propose to construct a matrix A by sub-blocks. Let us assume, for the sake of simplicity that all rows have the same degree 𝑑. Then, 𝑁 = 𝑚𝑑/𝑛 sub-matrices A 1 ...A 𝑁 of dimension 𝑛/𝑑 × 𝑛 are randomly drawn from the ensemble of all matrices of row degree 𝑑 and column degree 1. Afterwards, these sub-matrices are stacked to build matrix A as A =\nThe design of sub-matrices A 1 ...A 𝑁 is done such that cycles of length four are avoided whenever possible. However, unfortunately, when the signal to be sensed is very sparse these cycles are unavoidable if one wants to keep the number of measurements low (and close to the theoretical limit) and the vector length 𝑛 is ﬁnite. Indeed, if the number of measurements is in the order of 𝑘 then the check node degree must be at least 𝑛/𝑘 in order to guarantee that all signal coefﬁcients are sensed once. Hence the check node degree grows as the signal becomes sparse, matrix A becomes denser and length-four cycles become unavoidable for ﬁnite length 𝑛. This behavior explains the relevance of the enhancement of the decoding algorithm proposed in the next section.\nIn this section we present the proposed reconstruction (decoding) algorithm, which is an enhancement of the veri- ﬁcation algorithm proposed in [11]. This algorithm performs message passing over the bipartite graph representation of the measurement matrix A.\nThe following notation is used hereafter: we denote as 𝜙 and ℐ a row vector of A and the set of non-null components of 𝜙, respectively. Hence, the cardinality of ℐ is ∣ℐ∣ = 𝑑, the check node degree.\nFirst, we present the application of the veriﬁcation algorithm (VA) in [11] to reconstruct the vector x in the set-up of section II. This algorithm was identiﬁed by [11] to be a different implementation of the sudocode decoding algorithm [10], which has 𝑂 (𝑘 log(𝑘) log(𝑛)) computational complexity as opposed to [11] whose computational complexity is 𝑂(𝑛). It must be noted that the algorithm presented in section IV-B can be implemented as in [10] or in [11].\nWhen applied in the CS framework, VA exploits that (i) there is no noise, (ii) the probability density function of the components of vector x is a continuous function with a mass concentration at zero, 𝑝(𝑥 = 0) = 1 − 𝑘/𝑛, and (iii) the graph has no cycles of length four. Due to these facts the following statements hold:\n∙ S1 . If a measurement 𝑦 is zero, all the variable nodes indexed by ℐ are zero.\n∙ S2 . When the graph does not exhibit cycles of length four and two measurements 𝑦 𝑗 and 𝑦 𝑣 are identical ( 𝑦 𝑗 = 𝑦 𝑣 ), that pair of measurements share a single variable node, ∣ℐ 𝑗 ∩ ℐ 𝑣 ∣ = 1. Therefore, it follows that the common component in the intersection set must be equal to 𝑦 𝑗 and the remaining variable nodes in ℐ 𝑗 and ℐ 𝑣 must be equal to zero.\nKeeping the above statements in mind, the algorithm works as follows. The check (variable) nodes exchange with the variable (check) nodes messages having the following form: {state,x} where state=\u2019v\u2019 indicates that we know for sure that the value of the associated variable node is x (i.e. the variable node is veriﬁed) whereas state=\u2019?\u2019 informs that x is solely an estimate of the variable node value (i.e. the variable\nnode is still not veriﬁed). A check node is said to verify a neighboring variable node whenever the ﬁrst can infer with inﬁnite reliability the value of the latter using statements S1 and S2. Then, the veriﬁed variable node propagates its state to the other check nodes in its neighborhood so that these check nodes can remove the contribution of the veriﬁed variable node from their respective measurement and try to infer, in the next round, the values of the remaining variable nodes connected to them.\nThe iterative process starts with the variable nodes sending a non-veriﬁed message {\u2019?\u2019,−}, with dummy value −, to all the attached check nodes. On its turn, the measurement nodes simply send to their respective check nodes a message containing the measurement value, {\u2019v\u2019, 𝑦}, and do not update their message during the decoding process.\nThen, the check nodes are activated and send a message computed as indicated by Algorithm 1, which is now explained brieﬂy, to the variable nodes in their neighborhood. First of all, each check node removes the contributions of the incoming veriﬁed messages from its measurement 𝑦 to obtain 𝑦 \u2032 . Next, it updates all the edges connected to variable nodes that remain not veriﬁed (indexed in the set ℐ ? ), according to rules C1, C2 and C3. Rule C1 exploits statement S1. Rule C2 deals with the trivial case in which there is a single non-veriﬁed variable node connected to the check node. Rule C3 deals with the remaining cases: whenever the check node cannot infer the value of a variable node it propagates the non-veriﬁed message with the updated measurement, (\u2019?\u2019, 𝑦 \u2032 ).\nAfter the check nodes update, the variable nodes are acti- vated and proceed as indicated in Algorithm 2. Rule V1 deals with veriﬁed variable nodes. In this case, due to the absence of noise, each veriﬁed variable node informs to the check nodes in its neighborhood that it is veriﬁed by propagating a veriﬁed message along with its value. Rule V2 exploits statement S2. In this case, the variable node receives more than one non-veriﬁed message with equal estimate 𝑦 \u2032 so it can infer its value. Hence, the variable node switches its state to veriﬁed and informs of this update to all the check nodes in its neighborhood by sending them the message {\u2019v\u2019,𝑦 \u2032 }. Finally, in case of rule V3, the variable node just sends a non-veriﬁed message {\u2019?\u2019,−} to all the neighboring check nodes.\nThis paper proposes a modiﬁcation of the veriﬁcation algo- rithm in section IV-A that improves its performance for very sparse sources while preserving its 𝑂(𝑛) computational com- plexity and requiring only local operations. This enhancement is similar in spirit to the one proposed in [13] for decoding of LDPC codes in BEC channels. More speciﬁcally, the improve- ment consists in extending rule V2 in Algorithm 2 to deal with graphs having cycles of length four, i.e., ∣ℐ 𝑗 ∩ ℐ 𝑣 ∣ ≥ 1. In the presence of such cycles, if two measurements 𝑦 𝑗 and 𝑦 𝑣 take the same value ( 𝑦 𝑗 = 𝑦 𝑣 ), we know that the variable nodes that are not in the intersection set ℐ 𝑗 ∩ ℐ 𝑣 are equal to zero but we cannot determine the individual value of each variable node that are in the intersection; in fact, we can only afﬁrm\nfor all the veriﬁed messages from the variable nodes in ℐ\nSubtract their values from 𝑦 and remove their indexes from ℐ to compute 𝑦 \u2032 and ℐ ? .\nfor all variable nodes in ℐ ? apply rules C1-C3: C1 If 𝑦 \u2032 = 0, propagate to all the vari-\nC2 If ∣ℐ ? ∣ = 1, propagate to the variable nodes in ℐ ? : {\u2019v\u2019, 𝑦 \u2032 }.\nC3 Otherwise, propagate to all the vari- able nodes indexed by ℐ ? : {\u2019?\u2019, 𝑦 \u2032 }.\nendfor endfor\nfor each non-veriﬁed variable node apply rules V1-V3: V1 If the variable node receives a veriﬁed mes-\nsage, then propagate this message through the remaining edges {\u2019v\u2019, 𝑦 \u2032 }.\nV2 If the variable node receives at least two non-veriﬁed messages with the same esti- mate 𝑦 \u2032 , then propagate {\u2019v\u2019, 𝑦 \u2032 } to all the nodes connected to them.\nthat variable nodes that are in the intersection must sum up to 𝑦 𝑗 . In that case, it is important that the variable nodes in the intersection inform the check nodes of this coincidence so that the variable nodes that do not belong to the intersection set can be veriﬁed as zero.\nTo implement the above mechanism, we have modiﬁed the original message passing algorithm including a new message named coincidence. This message is generated at the variable nodes that detect coincident measurements and is sent to the check nodes detected to be in the coincidence state. Thanks to this, the check nodes know that (i) there is at least another check node with a measurement equal to its measurement and the cardinality of the intersection (the number of received coincident messages) and (ii) one or more of the nodes in coincident state sum up to the measurement. In other words, if a check node receives only a coincidence message it means that there is no cycle of length four so it can verify the variable node that sent the coincidence message with the value of the measurement and the remaining nodes with zero (applying the original rule V2). Otherwise, if the check node receives more than one coincidence message, it sends a non-veriﬁed message to the variable nodes that sent coincidence messages and propagates veriﬁcation messages {\u2019v\u2019,𝑥 = 0} to the remaining ones. Brieﬂy, these changes are summarized as follows:\n∙ New variable node rule V2: If a variable node receives at least two non-veriﬁed messages with the same estimate,\nit sends a coincidence message to these check nodes, and transmits {\u2019?\u2019,−} to the remaining nodes in its neighborhood.\n∙ Check node rule C4: If a check node receives at least a coincidence message, it veriﬁes as zero the variable nodes that have not sent coincidence messages. Additionally, if the check node receives only a coincidence message, it veriﬁes the corresponding variable node using its own measurement. Otherwise, it propagates {\u2019?\u2019,𝑦 \u2032 } to the variable nodes that sent coincidence messages.\nThis section analyzes the inﬂuence of the sparsity ratio on the performance of a VA-based decoding scheme and applies the results to obtain a criterion to design R(𝑑) for the veriﬁcation algorithm. Even though it is out of the scope of the current work, it must be noted that the performance of the EVA can be further improved in graphs with length-four cycles by changing the criteria to design of the R(𝑑).\nLet us consider the case 𝑛 → ∞ and call 𝑝 the non- vanishing probability of a signal component being 0, 𝑝 = 1 − 𝑘/𝑛. For simplicity we assume that all the 𝜙 vectors have degree 𝑑. Then, when all the ones in 𝜙 overlap with null components of x, ∣ℐ ∩ 𝒮∣ = 0, the measurement 𝑦 = 𝜙 𝑇 x is equal to zero, using statement S1. This happens with probability:\nand tells us the probability of recovering 𝑑 zeros of vector x from a single measurement. Obtaining a vector 𝜙 which overlaps only with one element of the support set of x, i.e. ∣ℐ ∩ 𝒮∣ = 1, happens with probability\nHowever, we also generate measurements with contribution of more than one non-null component of vector x such that ∣ℐ ∩ 𝒮∣ > 1, with a probability\nAccording to equations (1)-(3), the probability of verifying null and non-null components of x depends critically on 𝑑 and 𝑝. Hence, the veriﬁcation process can be sped up by selecting a degree R(𝑑) that balances properly the amount of vectors generated according to equations (1) and (2). With this purpose, we propose to employ the check node degree distribution R(𝑑) that maximizes the average number of variable nodes connected to check nodes so as ∣ℐ ∩ 𝒮∣ ≤ 1, as follows\nwhere 𝑝(∣ℐ ∩ 𝒮∣ ≤ 1) = 𝑝(∣ℐ ∩ 𝒮∣ = 0) + 𝑝(∣ℐ ∩ 𝒮∣ = 1). Note that 𝑝(∣ℐ ∩ 𝒮∣ ≤ 1) is the fraction of check nodes with\n∣ℐ∩𝒮∣ ≤ 1 for asymptotic 𝑚. Then it follows that 𝑑 𝑝(∣ℐ∩𝒮∣ ≤ 1) is the average number of edges per check node that satisfy ∣ℐ ∩ 𝒮∣ ≤ 1 for a given 𝑝 and 𝑑.\nIt is straightforward to show that for a ﬁxed 𝑝 and 𝑑 ∈ 4 + the relaxed function 𝑑 𝑝(∣ℐ ∩𝒮∣ ≤ 1) has a single maximum at ¯\n𝑑. It can be shown that whenever we ﬁx the number of non-null coefﬁcients of R(𝑑) to two and assuming that 𝑛/𝑘 >> 1, the optimum choice of the two non-null degrees are the closest ones around ¯ 𝑑. In this case\nunique solution R(𝑑) that can be inferred from ¯ 𝑑 as follows 1 R(⌊ ¯ 𝑑⌋) = ⌈ ¯ 𝑑⌉ − ¯ 𝑑, R(⌈ ¯ 𝑑⌉) = ¯ 𝑑 − ⌊ ¯ 𝑑⌋ \t (5)\nNote that (6) only requires the knowledge of the sparsity ratio of x, 𝑝 = 1 − 𝑘/𝑛. It is worth mentioning that ¯ 𝑑 is close to the check node degrees that were found experimentally to be optimum in [10, ﬁgure 2] and that ¯ 𝑑 increases as 𝑛/𝑘 increases.\nThe idea behind equation (4) is that by verifying as much variable nodes per measurement as possible the phase tran- sition 2 zone will be reached as soon as possible (in terms of samples). The EVA algorithm shows a kind of avalanche effect when it reaches its phase transition zone: once it reaches this zone the addition of a single sample may enable the veriﬁcation of a large fraction of variable nodes. This happens because rule C2 in Algorithm 1 is activated when the average number of non-veriﬁed variable nodes connected to check nodes is close to 1. When this happens the avalanche effect is triggered; before that, variable nodes can only be veriﬁed in the graph when statements S1 and S2 in section IV-A or modiﬁed S2 in section IV-B happen. Hence, by maximizing the fraction of variable nodes connected to check nodes with ∣ℐ ∩ 𝒮∣ ≤ 1 we try to reduce as fast as possible the number of non-veriﬁed variable nodes while keeping low the number of generated measurements.\nTable I compares the joint performance of the proposed matrix construction method, the proposed check node degree design and the proposed message passing algorithm with the performance of the sudocodes [10]. In this table, 𝑚 represents\nthe maximum number of samples required for perfect recon- struction obtained after 10 5 Monte Carlo simulations. Note that the number of samples required by the proposed scheme is 2 to 10 times smaller that the one required by sudocodes. It must be noted that the proposed method also outperforms that one in [11, ﬁgure 6] where the adopted LM2-MB algorithm is the VA algorithm in presented in section IV-A. In this case for 𝑛 = 10000 the method in [11] requires 𝑚 > 500 measurements to reconstruct a sequence with 𝑘 = 100; in our case, 𝑚 = 375 sufﬁces to reconstruct almost any sequence with 𝑘 = 100 (see table I).\nFigure 1 shows a phase transition diagram as a function of the number of samples 𝑚, sparsity 𝑘 and block length 𝑛. The horizontal axis corresponds to the sampling ratio, 𝛿 = 𝑚/𝑛. The vertical axis represents the ratio 𝜌 = 𝑘/𝑚. The curve labeled as LP-PT is phase transition for 𝑙 1 reconstruction [7]. The curves represent the performance of the veriﬁcation (VA) and enhanced veriﬁcation (EVA) algorithms for different 𝑝 𝑒 = 𝑝(x ∕= ˆx) when the proposed matrix construction is employed. The same measurement matrix was employed to run all the simulations (VA and EVA) for a ﬁxed 𝑛/𝑘 ratio. The plots depict the performance of the veriﬁcation algorithm with binary matrices (VA). These matrices were obtained with the structure presented in section III and with R(𝑑) selected for 𝑝 = 1−𝑘/𝑛 as indicated by equations (6) and (5). At least 10 5 Monte Carlo simulations were run per point.\nThe plots obtained with VA and EVA show a dual behavior: both have the same performance as long as it is possible to generate graphs without cycles of length 4 (for 𝛿 < 0.05 in this case, i.e. 𝑛/𝑘 > 100). Once the measurement matrix becomes dense, i.e. for large 𝑛/𝑘 ratios, the behavior of the algorithms changes: VA performance decreases dramatically whereas EVA performance falls slowly. These results can be compared with the 𝑙 1 theoretical limit for asymptotic 𝑛. Figure 1 shows that VA can outperform the 𝑙 1 in an approximate range between 𝛿 ∈ (0.05, 0.25) and EVA between 𝛿 ∈ (0, 0.25), both\nemploying measurement matrices constructed with the method proposed in this paper. Notice that, the latter range coincides with 𝑛/𝑘 ≥ 100 for 𝑛 = 16000.\nThis paper has proposed a graph-based CS scheme that outperforms sudocodes irrespective of the sparsity ratio and also outperforms VA in very sparse scenarios when the measurement matrix is binary. The performance enhancement for sparsity ratios where a length-4 cycle-free graph can be constructed is obtained employing the proposed matrix structure and check node degree distribution, specially aimed to exploit the features of the VA algorithm. For larger sparsity ratios, an enhanced decoding algorithm has been introduced that allows to deal with cycles of length-4. The combination of the proposed enhancements over existing methods in the literature allow to obtain a low complexity CS scheme with good performance that outperforms the 𝑙 1 theoretical limits in the noiseless setting.\nFurther work must be done to improve the performance of this scheme in the noiseless setting, ﬁrst by ﬁnding new rules to enhance the performance of the reconstruction algorithm and ﬁnding new methods to design binary matrices to exploit both the enhancement of the EVA and these new rules."},"refs":[{"authors":[{"name":"L. Donoho"}],"title":{"text":"Compressed sensing"}},{"authors":[{"name":"J. Candes"},{"name":"T. Tao"}],"title":{"text":"Near-optimal signal recovery from random projections: Universal encoding strategies?"}},{"authors":[{"name":"R. Baraniuk"},{"name":"M. Davenport"},{"name":"R. DeVore"},{"name":"M. Wakin"}],"title":{"text":"A simple proof of the restricted isometry property for random matrices"}},{"authors":[],"title":{"text":"A negative result concerning explicit matrices with the restricted isometry property"}},{"authors":[{"name":"T. Blumensath"},{"name":"E. Davies"}],"title":{"text":"Stagewise weak gradient pursuits"}},{"authors":[{"name":"R. Calderbank"},{"name":"S. Howard"},{"name":"S. Jafarpour"}],"title":{"text":"Construction of a large class of deterministic sensing matrices that satisfy a statistical isometry property"}},{"authors":[{"name":"L. Donoho"}],"title":{"text":"Message- passing algorithms for compressed sensing"}},{"authors":[{"name":"W. Yihong"},{"name":"S. Verdu"}],"title":{"text":"Renyi information dimension: Fundamental limits of almost lossless analog compression"}},{"authors":[{"name":"P. Schniter"}],"title":{"text":"Turbo reconstruction of structured sparse signals"}},{"authors":[{"name":"S. Sarvotham"},{"name":"D. Baron"},{"name":"G. Baraniuk"}],"title":{"text":"Sudocodes - fast measurement and reconstruction of sparse signals"}},{"authors":[{"name":"H. D. Pﬁster"}],"title":{"text":"On the iterative decoding of high-rate LDPC codes with applications in compressed sensing"}},{"authors":[{"name":"D. Pﬁster"}],"title":{"text":"Analysis of veriﬁcation-based decoding on the q -ary symmetric channel for large q"}},{"authors":[{"name":"M. Olmos"},{"name":"J. Murillo-Fuentes"},{"name":"F. Perez-Cruz"}],"title":{"text":"Tree-structure expectation propagation for decoding LDPC codes over binary erasure channels"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566497.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T9.4","endtime":"12:50","authors":"Francisco Ramirez-Javega, Meritxell Lamarca, Javier Villares","date":"1341405000000","papertitle":"Binary Graphs and Message Passing Strategies for Compressed Sensing in the Noiseless Setting","starttime":"12:30","session":"S10.T9: Compressive Sensing and Algorithms","room":"Stratton West Lounge (201)","paperid":"1569566497"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
