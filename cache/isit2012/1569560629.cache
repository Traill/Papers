{"id":"1569560629","paper":{"title":{"text":"Estimation of the Entropy on the Basis of its Polynomial Representation"},"authors":[{"name":"Martin Vinck ∗"},{"name":"Francesco P. Battaglia ∗"},{"name":"Vladimir B. Balakirsky \u2020"},{"name":"A. J. Han Vinck \u2020"},{"name":"Cyriel Pennartz ∗"}],"abstr":{"text":"Abstract\u2014An algorithm for estimating the entropy, which is based on the representation of the entropy function as the sum of two polynomial terms, called the polynomial approximation function and the remainder, is proposed. We construct an accurate and unbiased estimate of the value of the polynomial approximation function and use the known Bayesian approach to estimate the remainder. The combined estimator essentially reduces the bias of the constructed estimate as compared to the known estimators. Simulation results that conﬁrm the claim are presented."},"body":{"text":"We consider the estimation of the entropy of the probability distributions for discrete memoryless sources when a limited amount of observed data is available. This mathematical problem is important for many applications. In particular, it received a great attention in neurophysiology where the performance of networks of neurons is evaluated by infor- mation theoretical quantities [1]. Plugging in the empirical frequencies into the entropy function yields a direct estimate of entropy, which is strongly negatively biased by sample size. This negative estimator bias can translate into a strong positive bias in mutual information. This is a serious issue for ﬁelds such as neuroscience, where obtaining large amounts of data is virtually impossible, because of technical limitations in maintaining stable monitoring of neural signals for long times, or because the brain remains in the same state only for short periods.\nSearching for an entropy estimator with minimum bias or distortion leads to large variance and asymptotical efﬁciency issues, and there is a general trade-off between the variance and bias of entropy estimators [2]. A particularly promising framework to estimate entropy is the Bayesian one [3], [4]. Wolpert and Wolf [4] developed a Bayesian estimator based on a uniform prior distribution of probabilities. Nemenman et al. [3], [5] pointed out that this uniform prior distribution of probabilities corresponds to a peaked and informative prior on the entropy with an expected value that is relatively close to maximum entropy, leading to large errors when the actual value of the entropy is small. Therefore, the Bayesian Nemenman-Shafee-Bialek (NSB) estimator uses a nearly ﬂat\nprior distribution over the values of the entropy constructed as a mixture of symmetric Dirichlet distributions [3]. The NSB estimator exhibits rapid convergence to the entropy and has good performance in terms of bias and robustness in compar- ison to other available estimators [1], [3], although systematic comparisons between available estimators are lacking.\nIn the present correspondence, we show that further im- provements can be made on the NSB estimator in terms of estimator bias. The paper is organized as follows. We use the Taylor power series for the logarithm function to decompose the entropy into two terms, called the polynomial approxi- mation term and the remainder. An accurate and unbiased estimate for the polynomial approximation term, which is of interest by itself, is presented. The remainder is estimated us- ing the known Bayesian approaches. The combined estimator has essentially better performance than the known estimators. This claim is also conﬁrmed by simulation results.\nSuppose that there is a discrete memoryless source which generates one of M values (states or symbols) from the set R = {r 1 , . . . , r M } speciﬁed by the vector of probabilities p ≡ (p 1 , . . . , p M ). The (Shannon) entropy function [6] is deﬁned as H(p) ≡ − M m=1 p m ln(p m ) (hereafter, we assume that 0 ln 0 = 0). The received vector of outcomes of n indepen- dent observations is deﬁned as x ≡ (x 1 , . . . , x n ) ∈ R n . Let N ≡ (n 1 , . . . , n M ) with the m-th component deﬁned as the number of symbols r m in the vector x. The \u2018plugin\u2019 entropy estimator is deﬁned as ˆ H plugin (n) = − M m=1 n m n ln n m n\nand it is well-known that it has a strict non\u2013positive bias: As f (p m ) ≡ −p m ln(p m ) is a concave function of p m , the inequality E{− n m n ln( n m n )} ≤ − E{ n m n } ln(E{ n m n }) stems from Jensen\u2019s inequality.\nThe problem under consideration is to provide an estimate of H(p) based on the observation n, ˆ H(n), that has both low bias | E{ ˆ H(n)} − H(p)| and low mean absolute error E{| ˆ H(n) − H(p)|}. We want bias and mean absolute error to be small for all vectors of probabilities p (equivalently, for all entropies H(p) ∈ [0, log(M )]).\n1 j\nSuch a polynomial representation follows from the (n − 1)- th order Taylor expansion of − ln(p m ) around p m = 1, − ln(p m ) ≈ n−1 k=1 (1−p m ) k k . The polynomial approximation of f (p m ) ≡ −p m ln(p m ) is then deﬁned as g(p m ) ≡\n(−p m ) k−j , and we can represent the approximation function g(p m ) as\nThe polynomial representation T (p) is a meaningful func- tion in itself that shares various properties with the entropy function. (i) Like entropy, T (p) is a non-negative function. (ii) Like the entropy function, the function T (p) is strictly concave. (iii) Both H(p) and T (p) attain maximum values when all probabilities are equal, and any change towards equalization of the probabilities increases both of them. (iv) The function T (p) is a monotonically increasing function of M when all p m \u2019s are equal to each other. (v) The inequality T (p) ≤ H(p) holds. (vi) As n → ∞, T (p) → H(p). (vii) The function T (p) outputs larger values for the joint distribution of multiple independent random variables than for the marginal distributions of the individual random variables. However, while the entropy of the joint probability distribution is the sum of the entropies of the marginal probability distributions of the independent random variables [6], this property does not hold for T (p) for ﬁnite n. This is the main difference between the entropy function and the polynomial approxi- mation function T (p), and justiﬁes the use of the entropy function as a measure of uncertainty about a random variable for applications where the property of additivity is important.\nThe problem is formulated as ﬁnding an unbiased and asymptotically convergent estimator of T (p) with controlled variance. We will show that as far as estimation of T (p) is concerned, the \u2018bias problem\u2019 can be solved completely, and does not require the speciﬁcation of a prior distribution on the entropy.\nWe ﬁrst consider the problem of deriving an unbiased esti- mate of the sum S(p, k) = M m=1 p k m for all k ∈ {1, . . . , n}. Let\nn m ≥ k ⇒ c(n m , n, k) = n m !(n − k)! n!(n m − k)!\nn k\nn k\nBy the linearity of the expectation, it then follows that the unbiased estimator of T (p) is given by\n, the expression for the polyno- mial estimator then simpliﬁes to\nˆ T (n) = ψ(n) − 1 n\nIt follows that ˆ T (n) is an asymptotically consistent estimator of H(p), since ψ(n m ) → ln(np m ) and ψ(n) → ln(n) as n → ∞. The asymptotic expansion series of ψ(x) is given as ψ(x) = ln(x) − 1 2x + ∞ j=1 ζ(1−2j) x 2j with ζ the Riemann Zeta function. It follows that our estimator is closely related to the classic Miller-Madow estimator [7], which is known to have a faster convergence rate than the plugin estimator, and is deﬁned as\nwhere we assume 0 0 ≡ 0. Our estimator is less biased than the Miller-Madow and plugin estimator. Since the inequality\n− ψ(n m ) holds, it follows that ˆ T (n) ≥ ˆ M (n) ≥ ˆ H plugin (n). As E{ ˆ T (n)} ≤ H(p), the claim follows.\nLet us address the problem of constructing a Bayesian estimator of the remainder function R(p). We ﬁrst design such an estimator using a symmetric Dirichlet prior over the vectors p, which has a disadvantage that it imposes an informative prior on H(p) [3] (see Section VI). The symmetric Dirichlet distribution has a probability density function deﬁned as D β (p) = Γ(β) M /Γ(M β) M m=1 p β−1 m . If β → 0, then it is concentrated on probability vectors that specify small values of the entropy. If β = 1, then all p ∈ ∆ M ≡ {(p 1 , . . . , p M ) ∈ R M | ( M m=1 p m ) = 1, p m ≥ 0 for all m} have a probability density of 1, meaning that the prior on p is uniform.\nWe deﬁne the estimator of the entropy with Bayesian estimation of the remainder according to a Dirichlet prior as\nwhere ˆ T (n) is deﬁned in Eq. 8 and ˆ R β is the Bayesian estimator of the remainder based on a symmetric Dirichlet prior on p with concentration parameter β, deﬁned as\n(n m + β) (n + βM )\nThese expressions are obtained using the following con- siderations. By Bayes\u2019 theorem, the posterior probability of the vector p given observation of n and prior P (p) is given as P (p|n) = P (n|p)P (p)/P (n) where P is a probability density function on the probability distribution of the source. The Bayesian estimator of the function of the probability distribution of the source, Q(p), is then deﬁned as ˆ Q(n) ≡\nQ(p)P (p|n)dp. Wolpert and Wolf [4] have shown that the Bayesian estimator ˆ Q(n) with P (p) = D β (p) is found by computing the ratio of integrals [4]\nNote that M m=1 p β−1 m ∝ D β (p) where the beta normalization has been omitted because it is eliminated by division in eq. 15, and M m=1 p n m m is the multinomial probability of observing n, where the multinomial normalization coefﬁcient has been omitted again. Wolpert and Wolf [4] have shown that\nUsing the extensive derivations from Section IV in [4] based on the Laplace transformation of p n m m (which hold similarly for p n m +β+1 m \t ), the expression for the Bayesian estimator of the entropy ˆ H β (n) (eq. 14) given a symmetric Dirichlet prior is obtained.\nUsing the linearity property of integration, eq. 17, and the equality Γ(x + y) = Γ(x)(x + y − 1)!/(x − 1)! for any (x, y),\nΓ(β + n m ) Γ(βM + n + k)\nwhere δ l.m is the Kronecker delta, and the n m + β terms in eq. 17 were replaced by n m + β + kδ l,m . The expression for the Bayesian estimator ˆ T β (n) in eq. 13 then follows.\nAs discussed in the introduction, Nemenman et al. devel- oped a nearly ﬂat prior, as a mixture of Dirichlet distributions, on the entropy and a Bayesian (the NSB) entropy estimate based on that [3]. Here, we will use the same prior as developed by Nemenman et al. [3] to provide an estimator of the remainder R(p). The uniform prior on the entropy has the following justiﬁcation: When estimating the entropy without any a priori knowledge about the probability distribution of p or H(p), we can either choose to use a uniform prior on H(p) [3], or on p [4]. A uniform prior on p imposes an informative prior on H(p), and vice versa. While the uniform prior on p is a sensible choice for computing the posterior estimate P (p | n), it dominates the estimation of H(p), such that relatively small errors in estimating p are traded against relatively large errors in estimating H(p).\nOur polynomial-Bayesian estimator of the entropy Υ ¯ β (n) then becomes\nwhere ˆ R β (n) is deﬁned in eq 12, and where the posterior probability density of β given observation of n is proportional to\nwhere E{H(p); β} is the expected value of the entropy for a Dirichlet distribution,\nThe Bayesian estimator ˆ R ¯ β is based on a nearly ﬂat prior distribution (the \u2018NSB\u2019 distribution) on the entropy that is obtained as mixture of Dirichlet distributions P β (p) [3] as\nThe rationale of the Dirichlet mixture is that the distribution of the entropy H(p) is very peaked around E{H(p); β} if p is Dirichlet-distributed with concentration parameter β [3]. Since the integral in eq. 23 runs effectively over d E{H(p);β} dβ \t dβ =\nE{H(p); β}, it follows that the prior distribution ¯ D(p) of p translates into a nearly ﬂat prior distribution of the entropy H(p) for the interval [0, ln(M )] [3]. However, since there is some spread of H(p) around E{H(p); β}, the ¯ D(p) prior does not translate into a completely ﬂat prior distribution of the entropy.\nNemenman et al. [3] showed that the expression for the Bayesian estimator of the entropy given this prior reduces to the one-dimensional integral\nUsing the linearity property of integration, we substitute ˆ H β (n) for ˆ R β (n) and the deﬁnition eq. 20 is obtained.\nWe performed an exact (by computing probability of all n) evaluation of the bias and mean absolute error of: (i) The plugin estimator ˆ H plugin (n). (ii) The classic Miller-Madow estimator [7] (eq. 10) (iii) The Bayesian ˆ H ¯ β (n) estimator (eq. 24) [3]. (iv) Our newly developed polynomial ˆ T (n) (eq. 8) estimator and our polynomial-Bayesian estimator Υ ¯ β (n) (eq. 19). For a given number of symbols M ∈ {16, 225}, we varied the concentration parameter β of a symmetric Dirichlet distribution such that we covered the expected entropies in the interval [1/10000, ln(M ) − 1/1000] with a step-size of 1/10. Note that the number of trials in neuroscience experiments is typically small and often does not exceed tens of trials per stimulus. For a particular symmetric Dirichlet distribution with concentration parameter β and expected entropy E{H(p); β} (eq. 22), we then drew a vector of probabilities p from that Dirichlet distribution. The entropy of the drawn vector of prob- abilities p, H(p), does not exactly coincide with E{H(p); β}, but lies relatively close to it, since the distribution of H(p) is highly peaked if p is Dirichlet-distributed [3].\nOur polynomial-Bayesian Υ ¯ β (n) estimator, exhibited a sig- niﬁcant reduction in bias in comparison to the other entropy estimators, having the lowest overall bias of all estimators, for all number of symbols M tested (Figure 1) (M = 5 and 81 gave similar results, data not presented). Furthermore, it had the smallest bias across the interval [0, ln(M )] for all estimators tested (Figure 2). The average reduction in bias in comparison to the NSB estimator was particularly strong when M was relatively small. This is a particularly useful property for applications where many symbols have p m = 0, and M is over-estimated. Similarly, the reduction in bias of the Υ ¯ β (n) estimator relative to the NSB estimator was particularly pronounced when the entropy was close to 0, but was also present when the entropy was close to ln(M ). This shows that, for this particular setting, the Υ ¯ β (n) estimator is, in terms of bias, more robust than the NSB estimator, since it exhibited the smallest maximum bias across probability\nvectors of p (minimax principle). However, the reduction in bias was accompanied by a balanced increase in variance such that the mean absolute errors of the NSB and Υ ¯ β (n) estimator were close to each other.\nWe have proposed a new algorithm for estimating the entropy, which is based on the representation of the entropy function as the sum of two polynomial terms, called the polynomial approximation function and the remainder. We have shown that an accurate and unbiased estimate of the value of the polynomial approximation function exists that does not depend on the choice of any particular prior on the probability distribution of the source. In addition, we have used the known Bayesian approach with a nearly ﬂat prior on the entropy [3] to estimate the remainder. Our simulations show that the combined estimator essentially reduces the bias of the constructed estimate as compared to the known estimators. An advantage of our estimator relative to the NSB estimator is that part of the entropy function is estimated without the dependence on any particular prior, since the nearly uniform prior developed by [3] as a mixture of Dirichlet distributions is not the only possible uniform prior on the entropy, and is uniform only by approximation."},"refs":[{"authors":[{"name":"S. Panzeri"},{"name":"R. Senatore"},{"name":"M. A. Montemurro"},{"name":"R. S. Petersen"}],"title":{"text":"Correct- ing for the sampling bias problem in spike train information measures"}},{"authors":[{"name":"L. Paninski"}],"title":{"text":"Estimation of entropy and mutual information"}},{"authors":[{"name":"I. Nemenman"},{"name":"W. Bialek"},{"name":"R. de Ruyter van Steveninck"}],"title":{"text":"Entropy and information in neural spike trains: Progress on the sampling problem"}},{"authors":[{"name":"D. H. Wolpert"},{"name":"D. R. Wolf"}],"title":{"text":"Estimating functions of probability distributions from a ﬁnite set of samples"}},{"authors":[{"name":"I. Nemenman"},{"name":"F. Shafee"},{"name":"W. Bialek"}],"title":{"text":"Entropy and inference, revis- ited"}},{"authors":[{"name":"C. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[{"name":"G. Miller"}],"title":{"text":"Note on the bias on information estimates Information Theory in Psychology"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569560629.pdf"},"links":[{"id":"1569564613","weight":5},{"id":"1569565907","weight":5},{"id":"1569565633","weight":5},{"id":"1569567029","weight":5},{"id":"1569565415","weight":5},{"id":"1569557275","weight":5},{"id":"1569561221","weight":5},{"id":"1569566823","weight":5},{"id":"1569566137","weight":5},{"id":"1569564281","weight":5},{"id":"1569565805","weight":5},{"id":"1569563725","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S6.T8.2","endtime":"12:10","authors":"Martin Vinck, Francesco Battaglia, Vladimir Balakirsky, Han Vinck, Cyriel Pennartz","date":"1341316200000","papertitle":"Estimation of the Entropy on the Basis of its Polynomial Representation","starttime":"11:50","session":"S6.T8: Probability and Estimation","room":"Stratton (491)","paperid":"1569560629"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
