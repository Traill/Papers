{"id":"1569564969","paper":{"title":{"text":"Broadcast Capacity Regions with Three Receivers and Message Cognition"},"authors":[{"name":"Tobias J. Oechtering"},{"name":"Mich`ele Wigger"},{"name":"Roy Timo"}],"abstr":{"text":"Abstract\u2014We consider the capacity region of a three receiver broadcast channel with some message cognition at two receivers. The problem generalizes the bi-directional broadcast channel to include a third receiver, a common message, and (partial) message cognition. We characterize the capacity region for several classes of less noisy, more capable, and deterministic broadcast channels."},"body":{"text":"A canonical cooperative-communications problem is the bi- directional broadcast channel (BC) [1], which is a special case of the almost lossless joint source-channel coding problem in [2]. In this paper we generalize the bi-directional BC to the problem in Fig. 1, i.e., to include a third receiver and a common message. Unlike the bi-directional BC [1], our setup of Fig. 1 is not a special case of [2], but rather closely related to the lossy joint source-channel coding problem in [3].\nWe will also extend the setup in Fig. 1 to a setup where Receivers 1 and 2 have only partial cognition of each other\u2019s messages. The capacity region for the two-user BC with degraded message sets and partial message cognition was ﬁrst studied in [4].\nOur goal is to determine the capacity region of the setup in Fig. 1 as well as the capacity region of the extended setup with partial message cognition.\nIn Fig. 1, the Transmitter wishes to send three messages, M 0 , M 1 , and M 2 , to the receivers. Receiver 1 requires the private message M 1 and the common message M 0 ; Receiver 2 requires the private message M 2 and M 0 ; and Receiver 3 requires only M 0 . The messages are modelled as independent random variables, where each M k is uniformly distributed over the set M k {1, 2, . . . , 2 nR k }, for k ∈ {0, 1, 2}. Here R k denotes the transmission rate of message M k , and n denotes the blocklength.\nThe BC is discrete and memoryless. Denoting by X the channel input alphabet and by Y k the channel output alphabet at Receiver k, the channel input X n \t (X 1 , . . . , X n ) takes value in X n and Receiver k\u2019s outputs Y n k \t (Y k,1 , . . . , Y k,n ) take value in Y n k , for k ∈ {1, 2, 3}. We consider a memoryless BC so that the conditional distribution of (Y n 1 , Y n 2 , Y n 3 ) given X n is deﬁned by P Y 1 ,Y 2 ,Y 3 |X (y 1 , y 2 , y 3 |x).\nPaper Outline: In Section II, we consider the setup in Fig. 1, where Receiver 1 (resp. 2) is fully cognizant of Message M 2 (resp. M 1 ). In Section III, we consider the extended setup with\npartial message cognition; i.e., Receiver 1 (resp. 2) knows only part of the message M 2 (resp. M 1 ).\nThroughout this section we assume full cognition at Re- ceivers 1 and 2, i.e., that Receiver 1 is cognizant of the entire message M 2 and Receiver 2 of the entire message M 1 .\nA code consists of four maps: An encoder at the Transmitter, f : M 0 × M 1 × M 2 → X n\ng 1 : Y n 1 × M 2 → M 0 × M 1 g 2 : Y n 2 × M 1 → M 0 × M 2\nThe Transmitter sends X n = f (M 0 , M 1 , M 2 ); Receiver 1 de- codes ( ˆ M 0,1 , ˆ M 1 ) = g 1 (Y n 1 , M 2 ); Receiver 2 decodes ( ˆ M 0,2 ,\nˆ M 2 ) = g 2 (Y n 2 , M 1 ); and Receiver 3 decodes ˆ M 0,3 = g 3 (Y n 3 ). The average joint probability of error is\nThe rates (R 0 , R 1 , R 2 ) are said to be achievable if for each > 0 there exists a code (f, g 1 , g 2 , g 3 ) with P e ≤ for some\nsufﬁciently large blocklength n. The capacity region C is the closure of the set of all achievable rates.\nWe ﬁrst give an inner bound for C. This bound is achieved using a combination of superposition coding, rate-splitting, and bi-directional coding. Roughly, the superposition cloud- centers carry the common message M 0 and the satellites simultaneously carry the bi-directional messages (M 1 , M 2 ). Rate-splitting is used to transfer rate from the satellites to the cloud-centers.\nLet R ∗ in denote the set of rate tuples (R 0 , R 1 , R 2 ) satisfying R 0 ≤ I(U ; Y 3 ) \t (1a)\nR 0 + R 1 ≤ min I(X; Y 1 ), I(U ; Y 3 ) + I(X; Y 1 |U ) (1b) R 0 + R 2 ≤ min I(X; Y 2 ), I(U ; Y 3 ) + I(X; Y 2 |U ) , (1c)\nfor some (U, X) with U − − X − −(Y 1 , Y 2 , Y 3 ). Proposition 1: C ⊇ R ∗ in , and R ∗ in is convex.\nProof: The proposition is a corollary of Theorem 2, which is given later in Section III. See Remark 3.\nRemark 1: The capacity region in [1, Thm. 2.5] can be recovered from Proposition 1 by setting U to be constant.\nThe next theorem gives ﬁve non-trivial settings for which the inclusion in Proposition 1 is an equality. We ﬁrst need two deﬁnitions from [5, p. 121]. A channel output Y i is said to be less noisy than another output Y j (abbreviated Y i\nY j ) if I(U ; Y i ) ≥ I(U ; Y j ) holds for every auxiliary random variable U with U − − X − − (Y i , Y j ). A channel output Y i is said to be more capable than another output Y j if I(X; Y i ) ≥ I(X; Y j ) holds for every X.\n(i) If Y 1 Y 3 and Y 2 Y 3 , then C is equal to the set of rate tuples (R 0 , R 1 , R 2 ) satisfying\nR 1 ≤ I(X; Y 1 |U ) R 2 ≤ I(X; Y 2 |U ) R 0 ≤ I(U ; Y 3 )\n(ii) If Y 3 is more capable than Y 1 and Y 2 , then C is equal to the set of all rate tuples (R 0 , R 1 , R 2 ) satisfying\nR 0 + R 1 ≤ I(X; Y 1 ) R 0 + R 2 ≤ I(X; Y 2 )\n(iii) If Y 3 \t Y 1 , then C is equal to the set of rate tuples (R 0 , R 1 , R 2 ) satisfying\nR 0 + R 1 ≤ I(X; Y 1 ) R 0 + R 2 ≤ I(X; Y 2 )\nR 0 + R 2 ≤ I(U ; Y 3 ) + I(X; Y 2 |U ) R 0 ≤ I(U ; Y 3 )\n(iv) If the marginal conditional distributions p Y 1 |X and p Y 2 |X are the same, then C is equal to the set of rate tuples (R 0 , R 1 , R 2 ) satisfying\nR 0 + R 1 ≤ I(X; Y 1 ) R 0 + R 2 ≤ I(X; Y 2 )\nR 0 + R 1 ≤ I(U ; Y 3 ) + I(X; Y 1 |U ) R 0 + R 2 ≤ I(U ; Y 3 ) + I(X; Y 2 |U )\n(v) If Y 3 is a deterministic function of X, then C is equal to the set of all rate tuples (R 0 , R 1 , R 2 ) satisfying\nR 0 + R 1 ≤ I(X; Y 1 ) R 0 + R 2 ≤ I(X; Y 2 )\nProof: The proof for (i), (iii), and (iv) are omitted due to space constraints. A sketch of the proof for case (v) can be found in Section IV. The direct part for (ii) follows by setting U = X in (1) and using the more capable condition. The converse to (ii) is trivial.\nA natural generalization of the setup in Fig. 1 is to vary the quantity of side information at Receivers 1 and 2, as it was done for the two-receiver BC setup in [4]. Speciﬁcally, suppose that the bi-directional messages take the form\nRemark 2: The partial-cognition setup includes the general two-receiver BC [5, Sect. 8] as a special case. Hence, we do not expect to completely characterise C part .\nThe next theorem is proved using a combination of super- position coding, rate-splitting, and bi-directional coding. Let R (1) in,part denote the set of all rates (R 0 , R 1,c , R 1,p , R 2,c , R 2,p ) satisfying\nR 0 ≤ I(U ; Y 3 ) \t (7a) R 1,p ≤ I(X; Y 1 |V ) \t (7b)\nR 0 + R 2 ≤ min{I(V ; Y 2 ), I(U ; Y 3 ) + I(V ; Y 2 |U )} (7c) R 0 + R 1 + R 2,p ≤ min{I(X; Y 1 ),\nfor some (U, V, X) with U − −V − −X − −(Y 1 , Y 2 , Y 3 ). Let R (2) in,part denote the set of all rates (R 0 , R 1,c , R 1,p , R 2,c , R 2,p ) satisfying (7) with indices 1 and 2 interchanged. Let\nRemark 3: The inner bound of Proposition 1 follows di- rectly from Theorem 2 by setting V = X.\nProof of Theorem 2: We now sketch the coding theorem. Code Construction: Split the messages M 1,c , M 2,c , M 2,p as\nwith rates R (k) 1,c , R (k) 2,c , and R (k) 2,p , k ∈ {1, 2}. Construct two new messages M (1) ⊕ and M (2) ⊕ as follows\nWe use a three-layer superposition coding scheme. The cloud-center encodes M 0 , M (1) ⊕ , M (1) 2,p , the ﬁrst satellite encodes M (2) ⊕ , M (2) 2,p , and the top-most satellite encodes M 1,p . For the random code construction we use the law P U to generate the cloud centers, the conditional law P V |U for the ﬁrst satellites, and the conditional law P X |V for the top-most satellites.\nDecoding: Receiver 3 decodes the cloud center, Receiver 2 decodes the cloud center and the ﬁrst satellite, and Receiver 1 decodes the cloud center and both satellites. Arbitrary small probability of error is achieved if\nR 0 + R (1) 1,c + R (1) 2,p ≤ I(U ; Y 3 ) \t (9a) R 0 + R (1) 2,c + R (1) 2,p ≤ I(U ; Y 3 ) \t (9b)\nR 0 + R 2 ≤ I(V ; Y 2 ) \t (9c) R (2) 2,c + R (2) 2,p ≤ I(V ; Y 2 |U ) \t (9d)\nR 0 + R 1 + R 2,p ≤ I(X; Y 1 ) \t (9e) R (2) 1,c + R 1,p + R (2) 2,p ≤ I(X; Y 1 |U ) \t (9f)\nApplying the Fourier-Motzkin elimination algorithm results in the rate constraints in (7).\nRemark 4: For the region deﬁned in (7), it can be shown following [5, Appendix C] that it sufﬁces to consider auxiliary random variables (U, V ) ∈ U ×V with cardinality |U| ≤ |X |+ 4 and |V| ≤ (|X | + 4)(|X | + 1). Tighter constraints can be obtained for some special cases.\nTheorem 3: C part = R in,part in each of the following settings. (i) If Y 1 \t Y 2 \t Y 3 , then C part is the set of rates\nR 0 ≤ I(U ; Y 3 ) \t (10a) R 1,p ≤ I(X; Y 1 |V ) \t (10b)\nR 2 ≤ I(V ; Y 2 |U ) \t (10c) R 1 + R 2,p ≤ I(X; Y 1 |U ) \t (10d)\nfor some (U, V, X) with U − −V − −X − −(Y 1 , Y 2 , Y 3 ). (ii) If Y 1 \t Y 3 \t Y 2 , then C part is the set of rates\nR 0 ≤ I(U ; Y 2 ) \t (11a) R 1,p ≤ I(X; Y 1 |V ) \t (11b)\nR 2 ≤ I(V ; Y 2 |U ) \t (11c) R 1 + R 2,p ≤ I(X; Y 1 |U ) \t (11d)\n(iii) If Y 3 \t Y 1 \t Y 2 , then C part is the set of rates (R 0 , R 1,c , R 1,p , R 2,c , R 2,p ) satisfying\nR 1,p ≤ I(X; Y 1 |U ) \t (12a) R 0 + R 2 ≤ I(U ; Y 2 ) \t (12b)\nfor some (U, X) with U − −X − −(Y 1 , Y 2 , Y 3 ). Proof: See Section IV.\nWhen Y 1 Y 3 Y 2 or Y 3 Y 1 Y 2 , the capacity region depends on the channel law P Y 3 |X to Receiver 3 only through the fact that it must satisfy the less-noisy conditions.\nWe observe that when Y 3 Y 1 Y 2 , a two-layer superpo- sition coding scheme sufﬁces to achieve capacity. Moreover, in this case, the result remains valid also when Y 3 is more capable than Y 1 , but not less noisy.\nProof: The converse follows by noting that our converse in Section IV-B for case (iii) only requires that Y 1 \t Y 2 , and thus remains valid in this slightly weaker setup. The achievability follows by modifying our scheme achieving R (1) in,part so that Receiver 3 also decodes the two satellites, in addition to the cloud center.\nIn the usual way, Theorem 3 can be adapted to the Gaussian BC, where Y k = X + Z k with Z k ∼ N 0, σ 2 k . For Gaussian BCs the capacity region takes on a particularly simple form. This can be proved with the entropy-power inequality and the maximal entropy property for a ﬁxed second moment.\nCorollary 3.2: Depending on the variances σ 2 1 , σ 2 2 , σ 2 3 > 0, the capacity C part for Gaussian channels is given as follows.\n(i) If σ 2 3 ≥ σ 2 2 ≥ σ 2 1 , then C part is the set of all rates (R 0 , R 1,c , R 1,p , R 2,c , R 2,p ) satisfying\n(ii) If σ 2 2 ≥ σ 2 3 ≥ σ 2 1 , then C part is the set of rates (R 0 , R 1,c , R 1,p , R 2,c , R 2,p ) satisfying\n(iii) If σ 2 2 ≥ σ 2 1 ≥ σ 2 3 , then C part is the set of rates (R 0 , R 1,c , R 1,p , R 2,c , R 2,p ) satisfying\nRemark 5: From the above capacity expressions we notice the following. In the above setups, when R 1,c = 0, i.e., Re- ceiver 2 does not have any knowledge about Message M 1 , then providing M 2,c (even when M 2,c = M 2 ) to Receiver 1 does not increase capacity. In fact, providing M 2,c to Receiver 1 only increases the capacity when R 1,c is above a certain threshold that depends on the channel parameters. In contrast, providing M 1,c to Receiver 2 is always beneﬁcial.\nWe have Y 3 = φ(X) for some deterministic φ : X → U. Recall Proposition 1 and (1). Choose U = Y 3 = φ(X), so that\nThe ﬁrst term in each min is larger because Y 3 − −X − −(Y 1 , Y 2 ), and so we have R 0 ≤ H(Y 3 ), R 0 + R 1 ≤ I(X; Y 1 ) and R 0 + R 2 ≤ I(X; Y 2 ). The converse is obvious.\nWe only present the essential parts of the proof. Standard arguments ﬁnalize the converses. In what follows, inequali- ties (a) are justiﬁed by Fano\u2019s inequality, equalities (b) by Csisz´ar\u2019s sum identity, and inequalities (c) by [6, Lemma 1]. (i) Y 1 Y 2 Y 3 .\nwith U i = (M 0 , Y i −1 2 ). In (c) we can apply [6, Lemma 1] because Y 2 Y 3 and (Y n 2 , Y n 3 ) − −X n − −M 0 .\nwith V i = (M 0 , M 1,c , M 2 , Y i −1 2 ) ≡ (U i , M 1,c , M 2 ) which satisﬁes U i − −V i − −X i − −(Y 1,i , Y 2,i , Y 3,i ).\nwhere in (c) we can apply [6, Lemma 1] because Y 1 Y 2 and (Y n 1 , Y n 2 ) − −X n − −(M 0 , M 1,c , M 2 ). Finally,\nwhere in (c) and (d) we used that Y 1 Y 2 and the Markov chains (Y 1,i , Y 2,i ) − −X i − −(M 0 , M 1 , M 2 , Y i −1 1 , Y n 2,i+1 ) and (Y n 1 , Y n 2 ) − −X n − −M 0 .\nY 3 implies I(U ; Y 3 ) ≤ min{I(U ; Y 1 ), I(U ; Y 2 )} for any U − −V − −X − −(Y 1 , Y 2 , Y 3 ). (ii) Y 1 Y 3 Y 2 .\nwith U i = (M 0 , Y i −1 2 ). In the same way as before, we can prove bounds (16), (17), and (18) with V i = (U i , M 1,c , M 2 ), which satisﬁes U i − −V i − −X i − −(Y 1,i , Y 2,i , Y 3,i ).\nwhich hold because Y 1 \t Y 3 \t Y 2 implies I(U ; Y 1 ) ≥ I(U ; Y 3 ) ≥ I(U ; Y 2 ) for any U − −V − −X − −(Y 1 , Y 2 , Y 3 ).\nConverse: For any achievable rate tuple we have n(R 0 +R 2 ) − n n\nwith U i = (M 0 , M 1,c , M 2 , Y i −1 1 ). Here, (e) and (c) use that Y 1 Y 2 and that (Y 1,i , Y 2,i ) − −X i − −(M 0 , M 1,c , M 2 , Y i −1 2 )\nwhere in (f ) we used Y 1 \t Y 2 and the Markov chain (Y 1,i , Y 2,i ) − −X i − −(M 0 , M 1 , M 2,c , Y i −1 1 , Y n 2,i+1 ).\nDirect part: Follows from (7) by setting V = U and from I(U ; Y 2 )≤ I(U ; Y 3 ) + I(U ; Y 2 |U ) = I(U ; Y 3 ) I(X; Y 1 )≤ I(U ; Y 3 ) + I(X; Y 1 |U ),\nwhich hold because Y 3 \t Y 1 \t Y 2 implies I(U ; Y 3 ) ≥ max{I(U ; Y 1 ), I(U ; Y 2 )} for any U − −X − −(Y 1 , Y 2 , Y 3 )."},"refs":[{"authors":[{"name":"T. J. Oechtering"},{"name":"C. Schnurr"},{"name":"I. Bjelakovi´c"},{"name":"H. Boche"}],"title":{"text":"Broadcast capacity region of two-phase bidirectional relaying"}},{"authors":[{"name":"E. Tuncel"}],"title":{"text":"Slepian-Wolf coding over broadcast channels"}},{"authors":[{"name":"J. Nayak"},{"name":"E. Tuncel"},{"name":"D. G¨und¨uz"}],"title":{"text":"Wyner-Ziv coding over broadcast channels: Digital schemes"}},{"authors":[{"name":"G. Kramer"},{"name":"S. Shamai"}],"title":{"text":"Capacity for classes of broadcast channels with receiver side information"}},{"authors":[{"name":"A. El Gama"},{"name":"Y.-H. Ki"}],"title":{"text":"Network information theory"}},{"authors":[{"name":"C. Nair"},{"name":"Z. V. Wang"}],"title":{"text":"The capacity region of the three receiver less noisy broadcast channel"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564969.pdf"},"links":[{"id":"1569566381","weight":18},{"id":"1569566485","weight":18},{"id":"1569566725","weight":12},{"id":"1569566385","weight":6},{"id":"1569566683","weight":6},{"id":"1569566227","weight":6},{"id":"1569566597","weight":6},{"id":"1569566943","weight":6},{"id":"1569566081","weight":12},{"id":"1569565931","weight":25},{"id":"1569565461","weight":6},{"id":"1569564245","weight":6},{"id":"1569565837","weight":6},{"id":"1569566119","weight":6},{"id":"1569564233","weight":6},{"id":"1569566319","weight":6},{"id":"1569566941","weight":6},{"id":"1569558459","weight":6},{"id":"1569565291","weight":12},{"id":"1569564203","weight":43},{"id":"1569566751","weight":6},{"id":"1569566467","weight":6},{"id":"1569565771","weight":6},{"id":"1569566157","weight":6},{"id":"1569566843","weight":6},{"id":"1569565455","weight":12},{"id":"1569566709","weight":12},{"id":"1569565953","weight":18},{"id":"1569564189","weight":25},{"id":"1569566985","weight":6},{"id":"1569566063","weight":6},{"id":"1569555999","weight":6},{"id":"1569565213","weight":6},{"id":"1569565841","weight":6},{"id":"1569565833","weight":18},{"id":"1569564611","weight":6},{"id":"1569566325","weight":6},{"id":"1569567015","weight":6},{"id":"1569562285","weight":6},{"id":"1569553519","weight":6},{"id":"1569566231","weight":6},{"id":"1569554971","weight":6},{"id":"1569566473","weight":6},{"id":"1569564333","weight":6},{"id":"1569566629","weight":12},{"id":"1569565033","weight":6},{"id":"1569565633","weight":12},{"id":"1569555879","weight":37},{"id":"1569558509","weight":6},{"id":"1569565095","weight":6},{"id":"1569566043","weight":6},{"id":"1569561245","weight":6},{"id":"1569566505","weight":6},{"id":"1569565527","weight":6},{"id":"1569565363","weight":6},{"id":"1569566695","weight":6},{"id":"1569566655","weight":12},{"id":"1569566673","weight":6},{"id":"1569566233","weight":6},{"id":"1569566667","weight":12},{"id":"1569564097","weight":6},{"id":"1569566407","weight":6},{"id":"1569566501","weight":6},{"id":"1569566481","weight":6},{"id":"1569563395","weight":12},{"id":"1569555367","weight":6},{"id":"1569566383","weight":6},{"id":"1569566805","weight":6},{"id":"1569566929","weight":6},{"id":"1569565665","weight":6},{"id":"1569565611","weight":6},{"id":"1569566983","weight":6},{"id":"1569566097","weight":6},{"id":"1569566479","weight":6},{"id":"1569565397","weight":6},{"id":"1569566129","weight":6},{"id":"1569565919","weight":12},{"id":"1569565181","weight":6},{"id":"1569566711","weight":6},{"id":"1569565661","weight":6},{"id":"1569564131","weight":12},{"id":"1569565511","weight":6},{"id":"1569561221","weight":6},{"id":"1569566823","weight":12},{"id":"1569566137","weight":18},{"id":"1569565375","weight":12},{"id":"1569566755","weight":6},{"id":"1569565293","weight":6},{"id":"1569566641","weight":12},{"id":"1569559035","weight":6},{"id":"1569556759","weight":18},{"id":"1569566619","weight":12},{"id":"1569566817","weight":6},{"id":"1569566389","weight":6},{"id":"1569566435","weight":6},{"id":"1569566299","weight":18},{"id":"1569564281","weight":6},{"id":"1569565769","weight":6},{"id":"1569557851","weight":6},{"id":"1569565537","weight":25},{"id":"1569566847","weight":6},{"id":"1569550425","weight":6},{"id":"1569564257","weight":12},{"id":"1569564141","weight":6},{"id":"1569566987","weight":6},{"id":"1569564509","weight":6},{"id":"1569564419","weight":18},{"id":"1569564807","weight":6},{"id":"1569566609","weight":6},{"id":"1569566113","weight":6},{"id":"1569565315","weight":6}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T2.3","endtime":"15:40","authors":"Tobias J. Oechtering, Michele A Wigger, Roy Timo","date":"1341242400000","papertitle":"Broadcast Capacity Regions with Three Receivers and Message Cognition","starttime":"15:20","session":"S3.T2: Three-Receiver Broadcast Channels","room":"Kresge Auditorium (109)","paperid":"1569564969"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
