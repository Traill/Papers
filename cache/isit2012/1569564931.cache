{"id":"1569564931","paper":{"title":{"text":"Successive Reﬁnement with Cribbing Decoders and its Channel Coding Duals"},"authors":[{"name":"Himanshu Asnani"},{"name":"Haim Permuter"},{"name":"Tsachy Weissman"}],"abstr":{"text":"Abstract\u2014We study cooperation in multi terminal source coding models involving successive reﬁnement. Speciﬁcally, we study the case of a single encoder and two decoders, where the encoder provides a common description to both the decoders and a private description to only one of the decoders. The decoders cooperate via cribbing, i.e., the decoder with access only to the common description is allowed to observe, in addition, a deterministic function of the reconstruction symbols produced by the other. We characterize the fundamental performance limits in the respective settings of non-causal, strictly-causal and causal cribbing. We use a new coding scheme, referred to as Forward Encoding and Block Markov Decoding, which is a variant of one recently used by Cuff and Zhao for coordination via implicit communication. Finally, we use the insight gained to introduce and solve some dual channel coding scenarios involving Multiple Access Channels with cribbing."},"body":{"text":"Cooperation can dramatically boost the performance of a network. The literature abounds with models for cooperation, when communication between nodes of a network is over a noisy channel. In multiple access channels, the setting of cribbing was introduced in [1], where one encoder obtains the channel input symbols of the other encoder (referred to as \u201ccrib\u201d) and uses it for coding over the noisy multiple access channel (MAC). This was further generalized to deterministic function cribbing (where an encoder obtains a deterministic function of the channel input symbols of another encoder) in [2]. Cooperation can also be modeled as information exchange among the transmitters and receivers via rate limited links, generally referred to as conferencing in the literature. Such a model was introduced in the context of the MAC in [3]. Cooperation has also been modeled via conferencing/cribbing in other settings of the MAC and cognitive interference chan- nels (cf. [4] for detailed references). In multi terminal source coding, cooperation between decoders is generally modeled as a rate limited link such as in the cascade source coding or the triangular source coding problems.\nThe contribution of this paper is to introduce new models of cooperation in multi terminal source coding, inspired by the cribbing of [1] and by the implicit communication model of [5]. Speciﬁcally, we consider cooperation between decoders in a successive reﬁnement setting (introduced in [6]). In successive reﬁnement, a single encoder describes a common rate to both the decoders and a private rate to only one of the decoders. We generalize this model to accommodate\ncooperation among the decoders via cribbing. The main setting considered in this paper is shown in Fig. 1. A single encoder\ndescribes a common message T 0 to both decoders and a reﬁned message T 1 to only Decoder 1. Decoder 2 \u201ccribs\u201d (in the spirit of [1]) a deterministic function g of the reconstruction symbols of Decoder 1, non-causally, strictly-causally, or causally. Note a trivial g function corresponds to the original successive reﬁnement setting characterized in [6]. The goal is to ﬁnd the optimal encoding and decoding strategy and to characterize the optimal encoding rate region which is deﬁned as the set of achievable rate tuples (R 0 , R 1 ) such that the distortion constraints are satisﬁed at both the decoders. In [5], authors considered the problem of characterizing the coordination region in our setting of Fig. 1, for a speciﬁc g such that g( ˆ X 1 ) = ˆ X 1 and a speciﬁc rate tuple (R 0 , R 1 ) = (0, ∞). We use a new source coding scheme which we refer to as Forward Encoding and Block Markov Decoding scheme, and show that it achieves the optimal rate region for strictly causal and causal cribbing. It draws on the achievability ideas (for causal coordination) introduced in [5]. This scheme operates in blocks, where in the current block, the encoder encodes for the source sequence of the future block, (hence the name Forward Encoding ) and the decoders rely on the decodability in the previous block to decode in the current block (hence the name Block Markov Decoding).\nOur models are motivated by various scenarios of practical interest. One such scenario may arise in the context of video coding, as considered in [7]. Consider two consecutive frames in a video ﬁle, denoted by Frame 1 and Frame 2, respectively. The video encoder starts by encoding Frame 1, and then it encodes the difference between Frame 1 and Frame 2. Decoder 1 represents decoding of Frame 1, while Decoder 2 uses the\nknowledge of decoded Frame 1 (via cribbing) to estimate the next frame, Frame 2.\nOur problem setting is equally natural for capturing non- cooperation as it is for capturing cooperation, by requiring the relevant distortions to be bounded from below rather than above (which, in turn, can be converted to our standard form of an upper bound on the distortion by changing the sign of the distortion criterion). For instance, Decoder 1, can represent an end-user with reﬁned information (common and private rate) about a secret document, the source in our problem, while Decoder 2 has a crude information about the document (via the common rate). Decoder 1 is required to publicly announce an approximate version of the document, but due to privacy issues would like to remain somewhat cryptic about the source (as measured in terms of distortion with respect to the source) while also helping (via conferencing or cribbing) Decoder 2 to better estimate the source. For example, Decoder 1 can represent a Government agency required by law to publicly reveal features of the data, while there are agents who make use of this publicly announced information, along with crude information about the source that they too, not only the government, are allowed to access, to obtain a good estimate of the classiﬁed information (the source). The task of the encoder would be to maximize distortion at these third-party agents.\nThe contribution of this paper is two-fold. First, we in- troduce new models of decoder cooperation in source cod- ing problems such as successive reﬁnement, where decoders cooperate via cribbing, and we characterize the fundamental limits on performance for these problems using new classes of schemes for the achievability part. Second, we leverage the insights gained from these problems to introduce and solve a new class of channel coding scenarios that are dual to the source coding ones. Speciﬁcally, we consider the MAC with cribbing and a common message, where there are two encoders, one has access to its own private message, there is a common message between the two encoders, and the encoders cooperate via cribbing, non-causally, strictly causally or causally.\nThe paper is organized as follows. Section II gives a formal description of the problem and the main results. Section III presents the main ideas of achievability (due to space con- straints all the theorems and their proofs, including converses, are omitted and deferred to [4]). Some numerical examples are presented in Section IV. Channel coding duals are considered in Section V. The paper is concluded in Section VI.\nIn this section we formally deﬁne the problem con- sidered in this paper (cf. Fig. 1). The source sequence X i ∈ X , i = 1, 2, ... is a discrete random variable drawn i.i.d. ∼ P X . Let ˆ X 1 and ˆ X 2 denote the reconstruction alphabets, and d i : X × ˆ X i → [0, ∞), for i = 1, 2, are single letter distortion measures. Distortion between sequences is deﬁned in the usual way, i.e., d i (x n , ˆ x n i ) = 1 n n i=1 d i (x j , ˆ x i,j ), for i = 1, 2.\nDeﬁnition 1. A (2 nR 0 , 2 nR 1 , n) rate-distortion code consists of the following,\nd = n, d = i − 1 and d = i respectively stand for non-causal, strictly-causal and causal cribbing.\nDeﬁnition 2. A rate-distortion tuple (R 0 , R 1 , D 1 , D 2 ) is called achievable if ∀ > 0, ∃ n and (2 nR 0 , 2 nR 1 , n) rate- distortion code such that (expected) distortion for decoders are bounded as, E d i (X n i , ˆ X n i ) ≤ D i + , i = 1, 2.\nDeﬁnition 3. The rate-distortion region R(D 1 , D 2 ) is de- ﬁned as the closure of all achievable rate-distortion tuples (R 0 , R 1 , D 1 , D 2 ).\nTheorem 1 (Successive Reﬁnement with Cribbing Decoders). The main results in the setting of successive reﬁnement with non-causal, strictly-causal and causal cribbing decoders (Fig. 1) are given by the regions presented in the Table I.\nNote that in all the rate regions in the table, we use the notation {a} + for max(a, 0), and we omit the distortion condition E[d i (X i , ˆ X i ] ≤ D i , i = 1, 2 for the sake of brevity.\nIn this section we provide high level description of the achievability techniques for non-causal and strictly-causal cribbing that are used in the paper. The details are omitted and deferred to [4]. Due to space constraints, we explain achievability ideas for the case when cribbing is perfect, that is, g( ˆ X 1 ) = ˆ X 1 .\nTo understand the basic intuition, let us ﬁrst consider a sim- pliﬁed setup where R 0 = 0, that is, only Decoder 1 has access\nto the description of the source, and Decoder 2 obtains the reconstruction symbols of Decoder 1 (\u201ccrib\u201d). The intuition is to reveal a lossy description of source to the Decoder 2 through the \u201ccrib\u201d. So we ﬁrst generate 2 nI(X; ˆ X 2 ) ˆ X n 2 codewords, and index them as 2 nI(X; ˆ X 2 ) bins. In each bin, we generate a superimposed codebook of 2 nI(X; ˆ X 1 | ˆ X 2 ) ˆ X n 1 codewords. Thus total rate of R 1 = I(X; ˆ X 2 ) + I(X; ˆ X 1 | ˆ X 2 ) = I(X; ˆ X 1 , ˆ X 2 )\nis needed to describe ˆ X n 1 to Decoder 1. Decoder 2, knows ˆ X n 1 via crib, it then tries to infer the unique bin index\nwhich was sent, as then it would infer ˆ X n 2 . The only issue is ˆ X n 1 codeword, known via cribbing, should not lie in two different bins. Now we upper bound the probability of occurrence of such event by 2 n(I(X; ˆ X 1 , ˆ X 2 )−H( ˆ X 1 )) , as there are overall 2 nI(X; ˆ X 1 , ˆ X 2 ) ˆ X n 1 codewords, and the probability that a particular ˆ X n 1 lies in two bins is 2 −nH( ˆ X 1 ) . This event has a vanishing probability if, I(X; ˆ X 1 , ˆ X 2 ) ≤ H( ˆ X 1 ). Thus the rate region is, R 1 ≥ I(X; ˆ X 1 , ˆ X 2 ) such that the constraint I(X; ˆ X 1 , ˆ X 2 ) ≤ H( ˆ X 1 ) and distortion constraints are satisﬁed.\nThe general coding scheme when R 0 > 0 is depicted in Fig. 2 and has a \u201cdoubly-binned\u201d structure. Non-zero R 0 will help reduce R 1 by providing an extra dimension of binning. We ﬁrst generate 2 nI(X; ˆ X 2 ) ˆ X n 2 codewords, the indices of which are the rows (or horizontal bins), and then in each row, we generate 2 nI(X; ˆ X 1 | ˆ X 2 ) ˆ X n 1 codewords. For each row, these ˆ X n 1 codewords are then binned uniformly into 2 nR 0 vertical bins, which are the columns of our \u201cdoubly-binned\u201d structure. Thus each bin is \u201cdoubly-indexed\u201d (row and column index) and has a uniform number of 2 n(I(X; ˆ X 1 | ˆ X 2 )−R 0 ) ˆ X n 1 codewords (as in Fig. 2). Note that this extra or independent dimension of vertical binning was not there when R 0 = 0. Intuition is that column indexing with common rate R 0 is independent or orthogonal to the row indexing, and hence it helps to reduce the private rate R 1 . The column or vertical\nbin index is described to both the decoders via common rate R 0 and thus R 1 reduces to I(X; ˆ X 1 , ˆ X 2 ) − R 0 to describe\nˆ X n 1 to Decoder 1. Again here, from the knowledge of crib, ˆ X n 1 and the column index, Decoder 2, infers the unique row\nB. Strictly-Causal Cribbing - \u201cForward Encoding\u201d and \u201cBlock Markov Decoding\u201d\nTo give an overview of the coding scheme, here also consider the case when common rate R 0 = 0. Thus the source description is available only to the Decoder 1, while Decoder 2 has access to the reconstruction symbols of Decoder 1, only strictly-causally. Hence in principle we cannot deploy a scheme to operate just in one block as it was the case for non-causal cribbing. We need to use a scheme to operate in multiple (large number) of blocks, and use an encoding procedure that makes sure, ˆ X n 1 codeword of previous block carries information about the source sequence of current block. This is because, then due to strictly causal cribbing, in current block Decoder 2 will know all the reconstruction symbols of Decoder 1 in previous block, which will describe to it information about the source sequence in the current block. This is the main idea and is operated as follows : in each block, ﬁrst we generate 2 nI(X;U ) U n codewords, and for each U n codeword, we generate 2 nI(X;U ) bins and in each bin 2 nI(X; ˆ X 1 |U ) ˆ X n 1 codewords are generated. So in each block, U n is jointly typical with source sequence in current block and bin index describes the U n sequence jointly typical with source sequence of the future block. This bin index carries information about the source in future block. Hence we address encoding as \u201cForward Encoding\u201d. Decoding is \u201cBlock Markov Decoding\u201d, as it assumes both the decoders have decoded U n sequence of previous block currently. The bin index and index of ˆ X n 1 codewords is described as R 1 which hence equals, I(X; U ) + I(X; ˆ X 1 |U ) = I(X; ˆ X 1 , U ). Due to cribbing, Decoder 2 knows the ˆ X n 1 of the previous block and aims to ﬁnd the bin index in which it lies. And as we argued in previous subsection, this is possible if I(X; ˆ X 1 , U ) ≤ H( ˆ X 1 |U ).\nThe general scheme when R 0 > 0 is depicted in Fig. 3, where structure of codebook in a single block is shown. The additional step which we do in the explanation above (for R 0 = 0) is to bin in an extra dimension, i.e., with respect to each U n sequence we generate a \u201cdoubly-binned\u201d codebook (as in achievability of non-causal cribbing, cf. Fig. 2). The row index encodes U n sequences of the future block, ˆ X n 1 codewords for each row are uniformly binned into 2 nR 0 columns. The column index is the common description R 0 , to both decoders, so R 1 reduces to I(X; ˆ X 1 , U ) − R 0 , and the decodability of Decoder 2 requires the condition I(X; ˆ X 1 , U )−R 0 ≤ H( ˆ X 1 |U ). Note that here we deliberately described the achievability with auxiliary r.v., although the rate region in the theorem will be obtained by simply plugging U = ˆ X 2 . The introduction of auxiliary r.v. makes the scheme to easily carry over to the causal cribbing case.\nWe provide an example illustrating the rate regions of non- causal and strictly causal cribbing. Along with them the region without cribbing is also compared (this region can be obtained by substituting constant function g in our regions) which equals,\nR 0 + R 1 ≥ I(X; ˆ X 1 , ˆ X 2 ) \t (2) R 0 ≥ I(X; ˆ X 2 ), \t (3)\nsuch that distortion constraints are satisﬁed. We plot for a speciﬁc example (cf. setting\nin Fig. 1 with perfect cribbing) with a bernoulli source X ∼ Bern(0.5), binary reconstruction alphabets and ham- ming distortion. We consider a particular distortion tuple (D 1 , D 2 ). Due to symmetry of the source, it is easy to argue that, P ˆ X\n(ˆ x 1 , ˆ x 2 |x), where x stands for complement of x. Thus all the expressions can be written in terms of variables p 1 = P ˆ X\n(1, 1|0), p 4 = 1−p 1 −p 2 −p 3 . However it is also easy to see that the distortion constraints are satisﬁed with equality, otherwise one can reduce the rate region slightly and still be under distortion constraint. The distortion constraints thus yield, E[d(X, ˆ X 1 )] = p 4 + p 3 = D 1 and E[d(X, ˆ X 2 )] = p 2 + p 4 = D 2 which implies, p 2 = 1 − D 1 − p 1 , p 3 = 1 − D 2 − p 1 , p 4 = p 1 + D 1 + D 2 − 1. Thus the equivalent probability distribution space over which the closure of rate regions is evaluated (such that distortion is satisﬁed) is equivalent to, P = {p 1 ∈ [1 − D 1 − D 2 , min{1 − D 1 , 1 − D 2 , 2 − D 1 − D 2 }], p 2 = 1 − D 1 − p 1 , p 3 = 1 − D 2 − p 1 , p 4 = p 1 +D 1 +D 2 −1}. The various entropy and mutual information expressions appearing in the rate regions can then be expressed in terms of the only variable of optimization, that is p 1 . Fig. 4 shows the rate regions for (D 1 , D 2 ) = (0.05, 0.1). Note that the region for no cribbing is smaller than that of strictly causal cribbing which is smaller than that of non- causal cribbing, as expected. We can also analytically compute the expressions of corner points A,B,C,D in Fig. 4. Points A = (0, 1 − h 2 (D 1 )), B = (1 − h 2 (D 1 ) − h 2 (D 2 ), 0),\nC = (1 − h 2 (D 2 ), 0) and D = (1 − h 2 (D 1 ), 0), where h 2 (α) = −α log α − (1 − α) log(1 − α), ∀ α ∈ [0, 1]. For explanation of this analytical evaluation, refer to [4].\nIn this section we show duality between the cribbing de- coders in successive reﬁnement problem and the cribbing en- coders in the MAC problem with a common message. We con- sider here the problem of MAC with cribbing encoders, where there is one private message m 1 ∈ {1, 2, ..., 2 nR 1 } known to Encoder 1 and one common message m 0 ∈ {1, 2, ..., 2 nR 0 } known to both encoders that need to be sent to the decoder as shown in Fig. 5 . We assume that Encoder 2 cribs the signal from Encoder 1, namely, Encoder 2 observes a deterministic function of the output of Encoder 1. To make the duality clearer and sharper, we consider coordination problems in source coding [8] and for channel coding we consider a new kind of problems which we refer to as channel coding with restricted code distribution. In the (weak) coordination problem [8], the goal is to generate a joint typical distribution of the sources and the reconstruction (or actions) rather than a distortion constraint between the source and its reconstruction. Similarly, we deﬁne a channel coding problem where the code is restricted to a speciﬁc type.\nHence we restrict the code to have a distribution P (x 1 , x 2 ), and deﬁne three regions R nc , R sc and R c , which correspond\n \nR 1 ≤ I(Y ; X 1 |U, Z 1 ) +H(Z 1 |U )\n(6) where the union is over joint distributions that preserve the\nTheorem 2 (MAC with common message and cribbing en- coders). The capacity regions of MAC with common message, restricted code distribution P (x 1 , x 2 ) and non-causal, strictly- causal and causal cribbing that is depicted in Fig. 5 are R nc (P ), R sc (P ) and R c (P ), respectively.\nThe duality principles between source coding and channel coding with cribbing appear in Table II. From ﬁrst glimpse at the regions of MAC with common message and of successive reﬁnement, they do not look dual. However, the corner points of the regions are dual according to the principles presented in Table II and as seen in Fig. 5. Applying the dual rules X 1 ↔ ˆ X 1 , X 2 ↔ ˆ X 2 , Y ↔ X, and ≥↔≤, we obtain duality between the corner points of the capacity region of MAC with common message and the rate region of the successive reﬁnement.\nIn this paper, we introduced new models of cooperation in multi terminal source coding. The setting of successive reﬁne- ment with single encoder and two decoders was generalized to\nincorporate cooperation between the users via cribbing. A new scheme,\u201cForward Encoding\u201d and \u201cBlock Markov Decoding\u201d scheme was used to derive the rate regions for strictly- causal and causal cribbing. Certain numerical examples are presented and show how cooperation via cribbing can boost the rate region. Finally we introduce dual channel coding problems, and establish duality between successive reﬁnement with cribbing decoders and communication over the MAC with common message and cribbing encoders.\nThe authors would like to acknowledge Paul Cuff for very helpful discussions that inspired their work."},"refs":[{"authors":[{"name":"F. Willems"},{"name":"E. van der Meulen"}],"title":{"text":"The discrete memoryless multiple- access channel with cribbing encoders"}},{"authors":[{"name":"H. H. Permuter"},{"name":"H. Asnani"}],"title":{"text":"Multiple access channel with partial and controlled cribbing encoders"}},{"authors":[{"name":"F. Willems"}],"title":{"text":"The discrete memoryless multiple access channel with partially cooperating encoders (corresp.)"}},{"authors":[{"name":"H. Asnani"},{"name":"H. H. Permuter"},{"name":"T. Weissman"}],"title":{"text":"Successive reﬁnement with decoder cooperation and its channel coding duals"}},{"authors":[{"name":"P. Cuff"},{"name":"L. Zhao"}],"title":{"text":"Coordination using implicit communication"}},{"authors":[{"name":"W. Equitz"},{"name":"T. Cover"}],"title":{"text":"Successive reﬁnement of information"}},{"authors":[{"name":"A. Aaron"},{"name":"D. Varodayan"},{"name":"B. Girod"}],"title":{"text":"Wyner-ziv residual coding of video"}},{"authors":[{"name":"P. Cuff"},{"name":"H. H. Permuter"},{"name":"T. M. Cover"}],"title":{"text":"Coordination capacity"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564931.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S8.T4.2","endtime":"17:20","authors":"Himanshu Asnani, Haim H Permuter, Tsachy Weissman","date":"1341334800000","papertitle":"Successive Refinement with Cribbing Decoders and its Channel Coding Duals","starttime":"17:00","session":"S8.T4: Multiple Description Coding","room":"Stratton 20 Chimneys (306)","paperid":"1569564931"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
