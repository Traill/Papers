{"id":"1569566415","paper":{"title":{"text":"On Cooperation in Multi-Terminal Computation and Rate Distortion"},"authors":[{"name":"Milad Seﬁdgaran"},{"name":"Aslan Tchamkerten"}],"abstr":{"text":"Abstract\u2014A receiver wants to compute a function of two corre- lated sources separately observed by two transmitters. One of the transmitters is allowed to cooperate with the other transmitter by sending it some data before both transmitters convey information to the receiver. Assuming noiseless communication, what is the minimum number of bits that needs to be communicated by each transmitter to the receiver for a given number of cooperation bits?\nIn this paper, ﬁrst a general inner bound to the above three dimensional rate region is provided and shown to be tight in a number of interesting settings: the function is partially invertible, full cooperation, one-round point-to-point communication, two- round point-to-point communication, and cascade.\nSecond, the related Kaspi-Berger rate distortion problem is investigated where the receiver now wants to recover the sources within some distortion. By using ideas developed for establishing the above inner bound, a new rate distortion inner bound is proposed. This bound always includes the time sharing of Kaspi- Berger\u2019s inner bounds and inclusion is strict in certain cases."},"body":{"text":"There has been a growing interest in communication prob- lems where instead of recovering sources of information, the receiver wants to compute a function of them. Related infor- mation theoretic works addressed a number of speciﬁc settings including one and two-round point-to-point communication [7], cascade network [1], [10], and multiple-access network [5], [6], [8].\nIn this paper, we investigate the role of cooperation in computation and consider the noiseless network conﬁguration depicted in Fig. 1 which includes all the above settings as special cases. Two sources, X and Y , are separately observed by two transmitters, and a receiver wants to compute a function f (X, Y ) of the sources. Transmitter-X ﬁrst sends some information to transmitter-Y (cooperation phase), then both transmitters send information to the receiver. The problem is to ﬁnd the minimum number of transmitted bits so that the receiver can compute f (X, Y ) reliably.\nWe ﬁrst provide a general inner bound for the above function computation problem. This bound is tight for the case of unlimited cooperation, i.e., when transmitter-Y knows X, and for the class of functions that are partially invertible, i.e., when X or Y is a function of f (X, Y ). In addition, the bound is also tight for the above mentioned speciﬁc settings for which\nwe recover the results of [7], [1], [10], and [8] (assuming no side information at the receiver).\nIf instead of recovering f (X, Y ) exactly, the receiver wants to recover X and Y within some prescribed distortions, we obtain the rate distortion problem considered by Kaspi and Berger [3].\nBuilding on ideas used to establish the above inner bound, we derive a new inner bound for the Kaspi-Berger rate distortion problem which always includes, and sometimes strictly, the time sharing of Kaspi-Berger\u2019s two inner bounds [3, Theorem 5.1] and [3, Theorem 5.4].\nThe paper is organized as follows. In Section II we formally state the problem and provide some background material and deﬁnitions. In Section III, we present our results, and in Section IV we provide proof sketches for certain results.\nLet X , Y, and F be ﬁnite sets, and f : X × Y → F . Let {(x i , y i )} ∞ i=1 be independent instances of random variables (X, Y ) taking values over X × Y and distributed according to p(x, y).\nDeﬁnition 1 (Code). An (n, R 0 , R X , R Y ) code consists of three encoding functions\nDeﬁnition 2 (Rate Region). A rate pair (R 0 , R X , R Y ) is achievable if, for any > 0 and all n large enough, there exists an (n, R 0 , R X , R Y ) code whose error probability is no larger than ε. The rate region is the closure of the set of achievable (R 0 , R X , R Y ).\nThe problem we consider in this paper is to characterize the rate region for given f and p(x, y).\nConditional characteristic graphs play a key role in coding for computing [11], [4], [8]. Below we introduce a deﬁnition of conditional characteristic graph tailored for the problem at hand.\nNotation. Given two random variables X and W , where X ranges over X and W over subsets of X , 1 we write X ∈ W whenever P (X ∈ W ) = 1.\nRecall that an independent set of a graph G is a subset of vertices no two of which are connected. The set of independent sets of G is denoted by Γ(G).\nDeﬁnition 3 (Conditional Characteristic Graph). Let T be an arbitrary discrete random variable taking on values in some set T . Given (X, T, Y ) ∼ p(x, t, y) and f (X, Y ), G t X|Y , t ∈ T , is deﬁned as the conditional characteristic graph of X given Y for T = t. Speciﬁcally, G t X|Y is the graph whose vertex set is X and such that x i and x j are connected if for some y ∈ Y\ni. p(x i , t, y) · p(x j , t, y) > 0, ii. f (x i , y) = f (x j , y).\nLet V be such that X ∈ V ∈ Γ(G T X|Y ) and such that (V, X, T, Y ) ∼ p(v, x, t, y). G t Y |V , t ∈ T is deﬁned as the conditional characteristic graph of Y given V for T = t. Speciﬁcally, G t Y |V is the graph whose vertex set is Y and such that y i and y j are connected if for some (v, x i , x j ), v ∈ V, x i , x j ∈ X\ni. p(v, x i , t, y i ) · p(v, x j , t, y j ) > 0, ii. f (x i , y i ) = f (x j , y j ).\nWhen T is constant, we write the above conditional charac- teristic graphs as G X|Y and G Y |V , respectively.\nNote that, given (X, T, U, Y ) ∼ p(x, t, u, y) and f (X, Y ), G T X|U,Y and G T U,Y |V , for some V such that X ∈ V ∈ Γ(G T X|U,Y ), can be deﬁned in the same way as above by deﬁning the function ˜ f (x, u, y) = f (x, y).\nDeﬁnition 4 (Conditional Graph Entropy [7]). Given (X, Y ) ∼ p(x, y), the conditional graph entropy is deﬁned as\nGiven a ﬁnite set S, we use M(S) to denote the collection of all multisets of S. 2\nWe ﬁrst provide a general inner bound to the rate region (see Deﬁnition 2).\nTheorem 1 (Inner bound-Computation). (R 0 , R X , R Y ) is achievable whenever\nThe rate region of Theorem 1 turns out to be tight in a number of interesting cases which we now list.\nThe ﬁrst case holds when the function is partially invertible with respect to X, i.e., when X is a function of f (X, Y ).\nTheorem 2 (Partially invertible function). The inner bound is tight when f (X, Y ) is partially invertible with respect to X. In this case, the rate region reduces to\nIn each of the following three cases, one of the links is rate unlimited.\nWhen there is full cooperation between transmitters, i.e., when transmitter-Y has full access to source X, the setting is captured by the condition R 0 ≥ H(X|Y ) and is depicted in Fig. 2(a).\nWhen condition R X ≥ H(X) holds, the situation reduces to the two-round communication setting depicted in Fig. 2(b). The receiver, having access to X, ﬁrst conveys information to transmitter-Y , which then replies.\nTheorem 4 (Two-round point-to-point communication). The inner bound is tight when\nR 0 ≥ I(X; U |Y ), R X ≥ H(X),\nThe rate region in the above case R X ≥ H(X) was pre- viously established in [7, Theorem 3]. However, the range of the auxiliary random variable W was left unspeciﬁed, except for the condition that U, W, X should determine f (X, Y ). By contrast, Theorem 4 speciﬁes W to range over independent sets of a suitable graph.\nWhen R Y is unlimited, i.e., when the receiver looks over the shoulder of transmitter-Y , the setting is captured by condition R Y ≥ R 0 + H(Y ) and reduces to point-to-point communication as depicted in Fig. 2(c) with the transmitter\nobserving X and the receiver observing Y . The rate region for this case was established in [7, Theorem 1].\nTheorem 5 (One-round point-to-point communication). The inner bound is tight when\nFinally, when R X = 0 there is no direct link between transmitter-X and the receiver, and the situation reduces to the cascade setting depicted in Fig. 2(d). The rate region for this case was established in [1, Theorem 3.1] (see also [10, Theorem 2]).\nR 0 ≥ H G X|Y (X|Y ), R Y ≥ H(f (X, Y )) .\nTheorem 1, gives an inner bound to the rate-distortion problem with zero distortion. It turns out that this inner bound is in general larger than the rate region obtained by Kaspi and Berger in [3, Theorem 5.1] for zero distortion. The reason for this lies in Kaspi and Berger achievable scheme upon which their inner bound relies. In fact, for any distortion their scheme implicitly allows the receiver to perfectly decode whatever is transmitted from transmitter-X to transmitter- Y . By contrast, we do not impose this constraint in the achievability scheme that yields Theorem 1. More generally, by relaxing this constraint it is possible to achieve a rate region that is in general larger than the rate region derived by time sharing of Kaspi-Berger\u2019s two inner bounds, [3, Theorems 5.1, 5.4].\nTheorem 7 (Inner bound-rate distortion). (R 0 , R X , R Y ) is achievable whenever\nand if there exist functions g 1 (V, T, W ) and g 2 (V, T, W ) such that\nTo show that the the Kaspi-Berger general inner bound [3, Theorem 5.1] is included into the rate region deﬁned by Theorem 7, it sufﬁces to set T = U in Theorem 7.\nIn the speciﬁc case of full cooperation, [3, Theorem 5.4] provides an inner bound which, in this case, corresponds to Theorem 7. To see this it sufﬁces to set U = X and V to be a constant in Theorem 7.\nSo, from the above statements, it can be concluded that Theorem 7 includes the rate region derived by time sharing of two schemes of Kaspi-Berger, [3, Theorems 5.1, 5.4]. Within the following example we show that in some cases, this inclusion is strict.\nLet X = (X 1 , X 2 ), with X 1 independent of X 2 , be independent of Y , where X 1 and Y are distributed uniformly over {1, 2, 3} and X 2 = Bern(p), p ≤ 1 2 . Deﬁne the binary function f (X 1 , Y ) to be 1 whenever X 1 = Y and 0 otherwise. Let the distortion function to be hamming distance and the distortion criteria to be as\nFirst, we claim that for any value of R 0 , in the achievable scheme [3, Theorem 5.1], we have\nThis is true because in their scheme, whatever X-transmitter sends to Y -transmitter will be retransmitted to the receiver; so, the sum rate constraint is at least as big as the point-to- point problem where a transmitter has X, and a receiver who has Y wants to recover f (X 1 , Y ) and X 2 with distortions 0 and d, respectively. For the point-to-point case, due to the independence of X 1 and X 2 , as well as (X 1 , X 2 ) and Y , the minimum number of bits is\nwhere R 0 (f (X 1 , Y )) is minimum number of bits for recover- ing f (X 1 , Y ) with zero distortion, which due to [7, Theorem 2] equals to\nand R d (X 2 ) is minimum number of bits for recovering X 2 with distortion d, which is equal to\nSecond, in the scheme [3, Theorem 5.4] for R 0 = H(X|Y ) the minimum sum rate is\nSo, (1) and (4) concludes that for any time sharing of two achievable schemes [3, Theorems 5.1, 5.4] we have\nfor some 0 ≤ α ≤ R 0 H(X|Y ) , where the right hand side of above equation becomes minimized by choosing α = R 0 H(X|Y ) .\nOn the other hand, by letting U = X 1 , T = Constant, V = Bern( p−d 1−2×d ), 3 and W = f (X 1 , Y ), Theorem 7 gives that for R 0 = H(X 1 |Y ) the sum rate\nis achievable. Comparing (5) and (6) for R 0 = H(X 1 |Y ) and noting that\n3 ) = H(f (X 1 , Y )) < H(X 1 ) = log 2 (3) H(X 1 |Y ) < H(X|Y ) = H(X 1 |Y ) + H b (p)\nconcludes that for this example Theorem 7 strictly includes the time sharing of two schemes [3, Theorem 5.1] and [3, Theorem 5.4].\nProof of Theorem 1: Our coding procedure consists of two phases. In the ﬁrst phase transmitter-X sends (T (X), U (T (X))) using Slepian-Wolf coding [9] to transmitter-Y and in the second phase, transmitter- X and transmitter-Y send (T (X), V (X, T (X))) and (T (X), W (Y, T (X), U (T (X)))), respectively using Slepian- Wolf coding to the receiver where U (T (X)), V (X, T (X)) and W (Y, T (X), U (T (X))) are chosen conditioned on T (X).\nPick T , U , V and W as in the theorem. These random variables together with X, Y are distributed according to some p(v, x, t, u, y, w).\nFor t ∈ T , v ∈ Γ(G T X|U,Y ) and w ∈ Γ(G T U,Y |V ), deﬁne ˜ f (v, t, w) to be equal to f (x, y) for all x ∈ v and (u, y) ∈ w such that p(x, t, u, y) > 0. Further, for t = (t 1 , . . . , t n ), v = (v 1 , . . . , v n ) and w = (w 1 , . . . , w n ) let\ni ∈ {1, 2, . . . , 2 nI(X;T ) }, i.i.d. according to the marginal distribution p(t).\nj ∈ {1, 2, . . . , 2 nI(X;U |T ) }, i.i.d. according to the marginal distribution p(u|t), and randomly bin each sequence (t (i) , u (j) (t (i) )) uniformly into 2 nR 0 bins. Similarly, generate\nrespectively, i.i.d. according to p(v|t) and p(w|t), and ran- domly bin each sequence (t (i) , v (k) (t (i) )) and (t (i) , w (l) (t (i) )) uniformly into 2 nR X and 2 nR Y bins, respectively. Reveal the bin assignment φ 0 to the both encoders and the bin assignments φ X and φ Y to the encoders and decoder.\nEncoding: First phase: Transmitter-X ﬁnds a sequence (t, u(t)) that is jointly robust typical with x, and sends the index of the bin that contains this sequence, i.e., φ 0 (t, u(t)) to transmitter-Y .\nSecond phase: Transmitter-X ﬁnds a unique v(t) that is jointly robust typical with (x, t), and sends the index of the bin that contains (t, v(t)), i.e., φ X (t, v(t)) to the receiver.\nThe transmitter upon receiving the index q 0 , ﬁrst ﬁnds a unique (ˇ t, ˇ u(ˇ t)) such that (ˇ t, ˇ u(ˇ t), y) becomes jointly robust typical and φ 0 (ˇ t, ˇ u(ˇ t)) = q 0 , otherwise it declares an errors. Then, it ﬁnds a unique w(ˇ t) that is jointly robust typical with ( ˇ u(ˇ t), y), and sends the index of the bin that contains (ˇ t, w(ˇ t)), i.e., φ Y (ˇ t, w(ˇ t)) to the receiver.\nIf a transmitter doesn\u2019t ﬁnd such an index it declares an errors, and if there are more than one indices, the transmitter selects one of them randomly and uniformly.\nDecoding: Given the index pair (q X , q Y ), declare ˜ f (ˆ t, ˆ v(ˆ t), ˆ w(ˆ t)) if there exists a unique jointly robust typical\n(ˆ t, ˆ v, ˆ w) such that φ X (ˆ t, ˆ v(ˆ t)) = q X and φ Y (ˆ t, ˆ w(ˆ t)) = q Y , and such that ˜ f (ˆ t, ˆ v(ˆ t), ˆ w(ˆ t)) is deﬁned. Otherwise declare an error.\nProbability of Error: In each of two phases there are two types of error.\nIn ﬁrst phase, the ﬁrst type of error occurs when no (t, u(t)) is jointly robust typical with x. The probability of this errors is shown to be negligible in [7] due to the number of generated codewords t and u(t).\nThe second type of error occurs if (ˇ t, ˇ u( ˇ t)) = (t, u(t)). Due to the Markov chain\nit can be shown that the probability of this error goes to zero when\nIn second phase, the ﬁrst type of error occurs when no v(t), respectively w(ˇ t), is jointly robust typical with (x, t), respectively with ( ˇ u(ˇ t), y). The probability of each of these two errors is shown to be negligible in [7] due to the number of generated codewords v(t) and w(t). Hence, the probability of the ﬁrst type of error is negligible.\nThe second type of error refers to the Slepian-Wolf coding procedure. By symmetry of the encoding and decoding pro- cedures, the probability of error of the Slepian-Wolf coding procedure, averaged over sources outcomes, over t\u2019s, v(t)\u2019s and w(t)\u2019s, and over the binning assignments, is the same as the average error probability conditioned on the transmit- ters selecting T (1) , U (1) (T (1) ) ,V (1) (T (1) ) and W (1) (T (1) ). Note that whenever\nthere is no error, i.e., ˜ f (V (1) , T (1) , W (1) ) = f (X, Y ) by deﬁnition of robust typicality and by the deﬁnitions of T , V\nand W . Similarly to [3], one can show this probability of error goes to zero when other inequalities of the Theorem hold. Due to the lack of space, we just compute the probability that\nin all three coordinates. The number of triples that satisfy (8) is roughly\nThe probability that each triple that satisfy (8) has the same bin numbers (q X , q Y ) is roughly\nand the probability that the triple become jointly robust typical is roughly\nSo, from (9), (10) and (11), the probability of event (8) goes to zero, when n is large enough and R X + R Y is larger than\nI(X; T ) + I(V ; X|T ) + I(U, Y ; W |T ) − I(V ; W |T ) = I(X, Y ; V, T, W ) + I(U ; W |V, X, T, Y )"},"refs":[{"authors":[{"name":"P. Cuf"},{"name":"A. El Gamal"}],"title":{"text":"Han-I Su, and  Cascade multiterminal source coding"}},{"authors":[{"name":"A. Giridha"},{"name":"R. Kumar"}],"title":{"text":"P"}},{"authors":[{"name":"A. Kasp"},{"name":"T. Berger"}],"title":{"text":"Rate-distortion for correlated sources with partially separated encoders"}},{"authors":[{"name":"J. K¨orner"}],"title":{"text":"Coding of an information source having ambiguous alphabet and the entropy of graphs"}},{"authors":[{"name":"J. K¨orne"},{"name":"K. Marton"}],"title":{"text":"How to encode the modulo-two sum of binary sources (corresp"}},{"authors":[{"name":"B. Naze"},{"name":"M. Gastpar"}],"title":{"text":"Computation over multiple-access channels"}},{"authors":[{"name":"A. Orlitsk"},{"name":"J. R. Roche"}],"title":{"text":"Coding for computing"}},{"authors":[{"name":"M. Seﬁdgara"},{"name":"A. Tchamkerten"}],"title":{"text":"Computing a function of correlated sources"}},{"authors":[{"name":"D. Slepia"},{"name":"J. Wolf"}],"title":{"text":"Noiseless coding of correlated information sources"}},{"authors":[{"name":"K. Viswanathan"}],"title":{"text":"Information-theoretic analysis of function computation on streams"}},{"authors":[{"name":"H. Witsenhausen"}],"title":{"text":"The zero-error side information problem and chro- matic numbers (corresp"}},{"authors":[{"name":"H. Yamamoto"}],"title":{"text":"Correction to \u2018wyner-ziv theory for a general function of the correlated sources\u2019"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566415.pdf"},"links":[{"id":"1569566381","weight":13},{"id":"1569566485","weight":4},{"id":"1569566725","weight":9},{"id":"1569565067","weight":4},{"id":"1569565691","weight":4},{"id":"1569566875","weight":4},{"id":"1569566981","weight":4},{"id":"1569566683","weight":4},{"id":"1569566697","weight":4},{"id":"1569566597","weight":4},{"id":"1569566943","weight":9},{"id":"1569566591","weight":4},{"id":"1569566571","weight":9},{"id":"1569552245","weight":4},{"id":"1569564481","weight":18},{"id":"1569566081","weight":4},{"id":"1569565931","weight":13},{"id":"1569565547","weight":9},{"id":"1569565461","weight":4},{"id":"1569564227","weight":4},{"id":"1569564233","weight":4},{"id":"1569563411","weight":9},{"id":"1569559541","weight":4},{"id":"1569564203","weight":4},{"id":"1569556713","weight":4},{"id":"1569565859","weight":13},{"id":"1569565347","weight":4},{"id":"1569565455","weight":4},{"id":"1569566709","weight":9},{"id":"1569564989","weight":4},{"id":"1569566523","weight":4},{"id":"1569551763","weight":4},{"id":"1569564189","weight":4},{"id":"1569566985","weight":4},{"id":"1569564647","weight":4},{"id":"1569566095","weight":9},{"id":"1569565907","weight":4},{"id":"1569566239","weight":4},{"id":"1569563981","weight":4},{"id":"1569566063","weight":4},{"id":"1569566643","weight":4},{"id":"1569566511","weight":4},{"id":"1569566531","weight":9},{"id":"1569567665","weight":9},{"id":"1569561143","weight":9},{"id":"1569565833","weight":9},{"id":"1569565667","weight":4},{"id":"1569567015","weight":4},{"id":"1569566851","weight":4},{"id":"1569553909","weight":9},{"id":"1569559111","weight":4},{"id":"1569566939","weight":4},{"id":"1569553537","weight":4},{"id":"1569552251","weight":4},{"id":"1569553519","weight":4},{"id":"1569566231","weight":9},{"id":"1569554881","weight":4},{"id":"1569566209","weight":4},{"id":"1569565655","weight":4},{"id":"1569565151","weight":4},{"id":"1569565033","weight":4},{"id":"1569566357","weight":4},{"id":"1569565055","weight":4},{"id":"1569565633","weight":4},{"id":"1569555879","weight":4},{"id":"1569565219","weight":9},{"id":"1569566553","weight":4},{"id":"1569566043","weight":4},{"id":"1569565029","weight":4},{"id":"1569565357","weight":4},{"id":"1569566191","weight":4},{"id":"1569565527","weight":4},{"id":"1569566603","weight":9},{"id":"1569565363","weight":4},{"id":"1569566051","weight":4},{"id":"1569566673","weight":9},{"id":"1569565441","weight":4},{"id":"1569566233","weight":4},{"id":"1569560997","weight":4},{"id":"1569566501","weight":4},{"id":"1569560503","weight":9},{"id":"1569565463","weight":4},{"id":"1569565439","weight":4},{"id":"1569563395","weight":9},{"id":"1569565415","weight":4},{"id":"1569565571","weight":9},{"id":"1569566831","weight":4},{"id":"1569565397","weight":4},{"id":"1569565765","weight":4},{"id":"1569565435","weight":4},{"id":"1569557275","weight":4},{"id":"1569566129","weight":4},{"id":"1569565919","weight":9},{"id":"1569566711","weight":9},{"id":"1569565661","weight":4},{"id":"1569565319","weight":9},{"id":"1569566267","weight":4},{"id":"1569564919","weight":4},{"id":"1569565511","weight":4},{"id":"1569566691","weight":4},{"id":"1569566651","weight":4},{"id":"1569566823","weight":9},{"id":"1569566137","weight":4},{"id":"1569565375","weight":9},{"id":"1569566639","weight":4},{"id":"1569566755","weight":4},{"id":"1569566813","weight":4},{"id":"1569566641","weight":4},{"id":"1569564247","weight":4},{"id":"1569564437","weight":4},{"id":"1569551905","weight":4},{"id":"1569564787","weight":4},{"id":"1569566487","weight":4},{"id":"1569556759","weight":13},{"id":"1569566619","weight":4},{"id":"1569561185","weight":4},{"id":"1569558779","weight":9},{"id":"1569565669","weight":4},{"id":"1569565233","weight":4},{"id":"1569566817","weight":9},{"id":"1569564923","weight":4},{"id":"1569566299","weight":9},{"id":"1569564769","weight":4},{"id":"1569565805","weight":9},{"id":"1569566577","weight":4},{"id":"1569557851","weight":9},{"id":"1569565537","weight":13},{"id":"1569567013","weight":9},{"id":"1569560459","weight":4},{"id":"1569550425","weight":13},{"id":"1569565889","weight":4},{"id":"1569563725","weight":4},{"id":"1569564505","weight":4},{"id":"1569565165","weight":4},{"id":"1569566375","weight":4},{"id":"1569566555","weight":4},{"id":"1569564141","weight":18},{"id":"1569566973","weight":4},{"id":"1569551751","weight":9},{"id":"1569565139","weight":9},{"id":"1569564419","weight":9},{"id":"1569566113","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T1.3","endtime":"10:50","authors":"Milad Sefidgaran, Aslan Tchamkerten","date":"1341311400000","papertitle":"On Cooperation in Multi-Terminal Computation and Rate Distortion","starttime":"10:30","session":"S5.T1: Multiterminal Source Coding","room":"Kresge Rehearsal B (030)","paperid":"1569566415"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
