{"id":"1569565185","paper":{"title":{"text":"How Fast Can Dense Codes Achieve the Min-Cut Capacity of Line Networks?"},"authors":[{"name":"Anoosheh Heidarzadeh"},{"name":"Amir H. Banihashemi"}],"abstr":{"text":"Abstract\u2014In this paper, we study the coding delay and the average coding delay of random linear network codes (dense codes) over line networks with deterministic regular and Poisson transmission schedules. We consider both lossless networks and networks with Bernoulli losses. The upper bounds derived in this paper, which are in some cases more general, and in some other cases tighter, than the existing bounds, provide a more clear picture of the speed of convergence of dense codes to the min-cut capacity of line networks."},"body":{"text":"Random linear network codes (dense codes) achieve the capacity over various network scenarios, in particular, unicast over line networks. Lun et al. [1] showed that dense codes achieve the capacity of networks with transmission and loss schedules speciﬁed by stochastic processes with bounded average rate. They however did not discuss the speed of convergence of such codes to the capacity.\nThe speed of convergence of dense codes to the capacity of networks with arbitrary deterministic transmission schedules was studied in [2] and [3]. It is not, however, straightforward to apply the results to the networks with probabilistic schedules.\nThe coding delay or the average coding delay is often used to measure the speed of convergence of a code to the capacity of a network. The coding delay of a code over a network with a given schedule of transmissions and losses, referred to as trafﬁc , is the minimum time that the code takes to transmit all the message vectors from the source to the sink over the network. The average coding delay of a code over a network with respect to a class of trafﬁcs is the average of the coding delays of the code with respect to all the trafﬁcs. 1\nPakzad et al. [4] studied the average coding delay of dense codes over the networks with deterministic regular transmissions and Bernoulli losses, where the special case of two identical links in tandem was considered. The analysis however did not provide any insight about how the coding delay (which is random with respect to both the codes and the trafﬁcs) can deviate from the average coding delay (which is random with respect to the codes but not the trafﬁcs).\nMore recently, Dikaliotis et al. [5] studied both the average coding delay and the coding delay over networks similar to those in [4], under the assumption that all the packets are innovative. 2 This is not however a valid assumption in practice, where the ﬁeld size is ﬁnite and can be as small as two.\nIn this paper, we study the coding delay and the average coding delay of dense codes over the ﬁeld of size two (F 2 ). The analysis however can be generalized to ﬁnite ﬁelds of larger size. We consider both lossless networks and networks with Bernoulli losses. We also study both deterministic regular and Poisson transmission schedules.\n\u2022 For networks with deterministic regular transmissions and Bernoulli losses, we derive upper bounds on the average coding delay of dense codes (as a function of the message size, the network length and the trafﬁc parameters) tighter than what were presented in [4], [5] in the asymptotic regime as the message size goes to inﬁnity.\n\u2022 We show that, for such networks, the coding delay may have a large deviation from the average coding delay in both cases of identical and non-identical links. For non- identical links, our upper bound on such a deviation is smaller than what was previously shown in [5]. It is worth noting that, for identical links, upper bounding such a deviation has been an open problem (see [5]).\n\u2022 We generalize the results to the networks with Poisson transmissions for both lossless networks and networks with Bernoulli losses.\nThe proofs are omitted due to the lack of space, and can be found in [6].\nWe consider a line network of length L, where the L + 1 nodes {v i } 0≤i≤L are connected in tandem. The underlying problem is unicast: The source node v 0 is given a message of k vectors from a vector space over F 2 , and the sink node v L demands to have all the message vectors.\nEach node transmits a (coded) packet at each transmission opportunity in discrete-time where the number of transmis- sions per transmission opportunity is one. The points in time at which the transmissions occur over each link follow a stochas- tic point process. The processes specifying the transmissions over different links are considered to be independent.\nEach packet transmission is either successful or fails. In the latter case, the packet is erased. We consider two scenarios: (i) lossless, where all packet transmissions are successful, and (ii) lossy, where all packet transmissions are subject to independent erasures over the same link or different links. The trafﬁc over a link is fully described by the processes describing the schedule of transmissions and by the loss model.\nThe links are assumed to be delay-free, i.e., the arrival time of a successful packet at a receiving node is the same as the departure time of the packet from the transmitting node.\nThe goal in this paper is to upper bound the coding delay and the average coding delay of dense codes over networks with two types of transmission schedules and two types of loss models speciﬁed below. 3\nThe transmission schedules are described by (i) a determin- istic process where at each time unit there is a transmission opportunity at each node (such a schedule is referred to as deterministic regular ), or (ii) a Poisson process with parameter λ i : 0 < λ i < 1, over the i th link, where λ i is the average number of transmission opportunities per time unit.\nThe loss models are described by (i) a deterministic process where each packet transmission is successful (such a model is referred to as lossless), or (ii) a Bernoulli process with parameter p i : 0 < p i < 1, over the i th link, where p i is the average number of successes per transmission opportunity.\nIn a dense coding scheme, the source node, at each trans- mission opportunity, transmits a packet by randomly linearly combining the message vectors, and each non source non- sink (interior) node transmits a packet by randomly linearly combining its previously received packets. The vector of coefﬁcients of the linear combination associated with a packet is called the local encoding vector of the packet, and the vector of the coefﬁcients representing the mapping between the message vectors and a coded packet is called the global encoding vector of the packet. The global encoding vector of each packet is assumed to be included in the packet header. The sink node can recover all the message vectors as long as it receives an innovative collection of packets of the size equal to the number of message vectors at the source node.\nThe entries of the global encoding vectors of a collection of packets are independent and uniformly distributed (i.u.d.) Bernoulli random variables as long as the local encoding vectors of the packets are linearly independent. Such packets (with linearly independent local encoding vectors), called dense , are of main importance in our analysis.\nThe ﬁrst step is to lower bound the size of a maximal collection of dense packets at the sink node until a certain decoding time. We, next, lower bound the probability that the underlying collection includes a sufﬁcient number of packets with linearly independent global encoding vectors.\nLet Q be a matrix over F 2 . A maximal collection of rows in Q with i.u.d. entries is called dense. The matrix Q is called a dense matrix if all its rows form a dense collection. We refer to the number of rows in a dense collection of rows in Q as the density of Q, denoted by D(Q), and refer to each row in such a collection as a dense row.\nLet O i (I i ) be the set of the packets transmitted (received) by the i th node and let D i be the set of the dense packets at the i th node. Let r and d be the size of O i and D i , respectively.\nThe global encoding vectors of the received packets at a node form the rows of the decoding matrix at that node. Let Q i+1 and Q i be the decoding matrices at the (i + 1) th and i th nodes, respectively, and T i be a matrix over F 2 such that Q i+1 = T i Q i . The rows of T i are the local encoding vectors of the packets transmitted by the i th node, i.e., (T i ) n,j = λ n,j , ∀n ∈ O i and ∀j ∈ I i , where λ n is the local encoding vector of the n th packet. Let Q i be Q i restricted to its dense rows, i.e., Q i is dense and has d rows (D(Q i ) = d). We can write Q i+1 = T i Q i , where T i , the transfer matrix at the i th node, is a matrix over F 2 with d columns: (T i ) n,j =\nλ n, γ ,j , ∀n ∈ O i , ∀j ∈ D i and {γ ,j } are in F 2 satisfying j∈D i γ ,j λ j,k = λ ,k , ∀k ∈ I i .\nThe n th row of T i indicates the dense packets at the i th node which contribute to the n th packet sent by the i th node, and the j th column of T i indicates the packets sent by the i th node to which the j th dense packet contributes. Let T (n) i row (T (j) i col ) be the set of labels (indices) of i.u.d. entries in the n th row (j th column) of T i . Thus, |T (n) i\n| ≥ max{n − r + d, 0} (in particular, the ﬁrst max{n − r + d, 0} entries of the n th row are i.u.d.). Similarly, |T (j) i\n| ≥ d − j + 1 (in particular, the last d − j + 1 entries of the j th column are i.u.d.).\nLet rank(T ) denote the rank of a matrix T over F 2 . The following result is then useful to lower bound the density of the decoding matrix Q i+1 in terms of rank(T i ). 4\nLemma 1: Let Q be a dense matrix over F 2 , and T be a matrix over F 2 , where the number of rows in Q and the number of columns in T are equal. If rank(T ) ≥ γ, then D(T Q) ≥ γ.\nThe rank of a matrix T similar to that of the transfer matrix T speciﬁed earlier can be lower bounded as follows.\nLemma 2: Let T be an n × d (d ≤ n) matrix over F 2 such that for any 1 ≤ j ≤ d, at least d − j + 1 entries of its j th column are i.u.d.. For every integer 0 ≤ γ ≤ d − 1,\nLet (0, N T ] be the period of time over which the transmis- sions occur. The decoding matrix at the ﬁrst internal node (v 1 ) is dense and its density is equal to the number of packets at the node until time N T , i.e., D(Q 1 ) = N T . The density of the decoding matrix at the other non-source nodes is bounded from below as follows by applying the preceding lemmas.\nBy combining the result of Lemma 3 with D(Q 1 ) = N T , we can derive the following result.\nLemma 4: Suppose that a dense code is applied over a line network of L links with deterministic regular lossless trafﬁcs until time N T . Then, the inequality\nNow, we lower bound the probability that the collection of dense packets at the sink node includes an innovative sub- collection of size k. This itself lower bounds the probability that a dense code succeeds.\nLemma 5: Let M be an n × k (k ≤ n) dense matrix over F 2 . For every 0 < < 1,\nThe following result upper bounds the coding delay by putting together the results of Lemmas 4 and 5.\nTheorem 1: The coding delay of a dense code over a line network of L links with deterministic regular lossless trafﬁcs is larger than\nIn this case, the Bernoulli parameters {p i } 1≤i≤L are all the same, and equal to p. Similar to the analysis of the previous case, in the case of the deterministic regular trafﬁc with Bernoulli losses, we need to track the number of dense packets through the network.\nThe density of the decoding matrix at the receiving node of a link depends on the density of the decoding matrix and the rank of the transfer matrix at the transmitting node of the link. The rank of a matrix is a function of its structure, and the structure of the transfer matrix at a node depends on the number of dense packet arrivals at the node and the number of packet departures from the node before or after any given time. Such parameters depend on the transmission schedule and the loss model of the link, and are therefore random variables. It is however not straightforward to ﬁnd the distribution of such random variables. We rather adopt a probabilistic technique to lower bound the rank of the transfer matrices as follows.\nWe split the time interval (0, N T ] into a number of disjoint subintervals (partitions) of the same length. The arrivals in the ﬁrst j partitions occur before the departures in the (j + 1) th partition. Thus the number of arrivals before a given point in time within the (j + 1) th partition is bounded from below by the sum of the number of arrivals in the ﬁrst j partitions. Such a method of counting is however suboptimal since there might be some extra arrivals in the (j + 1) th partition before some points in time within the same partition. To control the impact of suboptimality, the length of the partitions thus needs to be chosen with some care. 5\nLet w be the number of partitions of the interval (0, N T ]. Let I ij be the j th partition pertaining to the i th link for all i\nand j. We start off with lower bounding the number of packets in I ij . Let ϕ ij be the number of packets in I ij . The length of the partition I ij is N T /w. Thus, ϕ ij is a binomial random variable with the expected value ϕ . = pN T /w.\nHereafter, for the ease of exposition, let us denote x/2 by ˙ x, for every x ∈ R. By applying the Chernoff bound, one can show that the inequality\nfails w.p. b.a.b. ˙, so long as γ ∗ is chosen such that r is an integer, and γ ∗ goes to 0 as N T goes to inﬁnity, where\nWe focus on the set of all packets over the i th link in the active partitions: I ij is \u2018active\u2019 if i ≤ j ≤ w − L + i. Such a partition is active in the sense that (i) there exists some other partition over the upper link so that all its packets arrive before the departure of all the packets in the underlying active partition, and (ii) there exists some other partition over the lower link so that all its packets depart after the arrival of all the packets in the underlying active partition.\nLet w T denote the total number of active partitions. It is easy to see that w T = L(w − L + 1). We select r packets in each active partition and ignore the rest. This method of selection fails if the number of packets in some active partition is less than r. Clearly, the failure occurs w.p. b.a.b. w T ˙.\nWe shall lower bound the number of dense packets in active partitions. Before explaining the lower bounding technique, let us ﬁrst state two lemmas which will be useful to lower bound the rank of the transfer matrix at each node (depending on whether the number of dense packet arrivals at the node in a partition is larger or smaller than the number of packet departures from the node in the same partition).\nFor given integers w, r and {r j } 1≤j≤w (0 ≤ r j ≤ r), let T i,j be deﬁned as follows: T i,j is an r × r j dense matrix over F 2 , if 1 ≤ j ≤ i ≤ w; or an arbitrary r × r j matrix over F 2 , otherwise. Let T = [T i,j ] 1≤i,j≤w , and n . = 1≤j≤w r j .\nLemma 6: Let T be deﬁned as above. For every integer 0 ≤ γ ≤ n − 1,\nwhere r max = max j r j , r min = min j r j , and u = (n − γ)/r min .\nFor given integers w, r and {r j } 1≤j≤w (r ≤ r j ), let T i,j be deﬁned as follows: T i,j is an r × r j dense matrix over F 2 , if 1 ≤ j ≤ i ≤ w; or an arbitrary r × r j matrix over F 2 , otherwise. Let T = [T i,j ] 1≤i,j≤w , and n . = wr.\nLemma 7: Let T be deﬁned as above. For every integer 0 ≤ γ ≤ n − 1,\nFor every 1 < i ≤ L, and 1 ≤ j ≤ w −L+1, the number of dense packets in the ﬁrst j active partitions over the i th link can be lower bounded as follows: For every 1 ≤ l ≤ j, suppose that the number of dense packets in the ﬁrst l active partitions\nover the (i − 1) th link is already lower bounded. Let T be the transfer matrix at the i th node, restricted to the successful packet transmissions within the ﬁrst j active partitions over the i th link (the number of such packets in each partition is already lower bounded). Then, it can be shown that T includes a sub-matrix T with a structure similar to that in Lemma 6 or the one in Lemma 7. 6 By applying the proper lemma, the rank of the transfer matrix at the i th node, and consequently, by applying Lemma 1, the number of dense packets in the ﬁrst j active partitions over the i th link can be lower bounded.\nNote that, because of its recursive nature, the above algo- rithm lower bounds the number of dense packets in the ﬁrst j active partitions over the i th link as a function of the number of dense packets in the active partitions pertaining to the ﬁrst link. Further, the packets over the ﬁrst link are all dense (by the deﬁnition of the dense packets), and hence by using the recursion, the following results can be derived.\nLet D(Q j i ) be the number of dense packets in the ﬁrst j active partitions over the i th link. Clearly, D(Q j 1 ) ≥ rj, ∀j : 1 ≤ j ≤ w − L + 1 (since r packets are selected in each partition). For any other values of i and j, D(Q j i ) is lower bounded as follows.\nLemma 9: For every 1 < i ≤ L, and 1 < j ≤ w − L + 1, D(Q j i ) ≥ rj − L ij\nfails w.p. b.a.b. ˙, so long as log(w T / ) = o(r), where L ij = j(1+o(1))(log(ij/ )+1)+log((j(1+o(1))+1)/ )+log(ij)+ 1, and the o(1) term is (log(ij/ ) + 1)/r.\nThe result of Lemma 9 lower bounds the number of dense packets at the sink node as follows.\n− (w T /L) log(w T / ˙) − (w T /Lϕ) log 2 (w T / ) − −(w T /Lϕ) log(w T / ) − log(w T / ) −\nLet n T be equal to the right-hand side of the inequality (2). Thus, Q L fails to include an n T × k dense sub-matrix w.p. b.a.b. . By applying Lemma 5, the probability of {rank(Q L ) < k} is b.a.b. , so long as k ≤ n T − log(1/ ). We replace with ˙ everywhere. Then, a dense code fails to transmit k message vectors w.p. b.a.b. , so long as k ≤ n T − log(1/ ) − 1.\nIn the asymptotic setting as N T goes to inﬁnity, n T can be written as pN T −(1+o(1))(pN T L/w+ pN T w log(wL/ )+ w log(wL/ )). We rewrite the last inequality as k ≤ pN T − (1 + o(1))(pN T L/w + pN T w log(wL/ ) + w log(wL/ )) − log(1/ ) − 1. Let k max be the largest integer k satisfying this\ninequality. Thus, k max ∼ pN T , as n T ∼ pN T and log(1/ ˙) = o(n T ). The following result can be shown by replacing N T with k/p in the right-hand side of the latter inequality.\nTheorem 2: The coding delay of a dense code over a line network of L identical links with regular trafﬁcs and Bernoulli losses with parameter p is larger than\nw.p. b.a.b. , where w ∼ kL 2 / log(kL/ ) 1 3 , and the o(1) term goes to 0 as k goes to inﬁnity. 7\nIt is worth noting that Theorem 1 is not a special case of Theorem 2 with p = 1. In fact, Theorem 1 provides a tighter bound compared to the result of Theorem 2 with p = 1.\nWe now study the average coding delay of dense codes with respect to the trafﬁcs with deterministic regular transmissions and Bernoulli losses. It should be clear that, in this case, the deviation of the number of packets per partition should not be taken into account. Thus, by replacing r with ϕ in Lemmas 8 and 9, and redeﬁning w as pN T L/log(pN T L/ ), we have the following result. 8\nTheorem 3: The average coding delay of a dense code over a network similar to Theorem 2 is larger than\n1 p\nk + (1 + o(1)) kL w\nw.p. b.a.b. , where w ∼ (kL/ log(kL/ )) 1 2 . B. Non-Identical Links\nThe preceding results regarding the identical links imme- diately serve as upper bounds for the case of non-identical links with arbitrary parameters {p i } 1≤i≤L , by replacing p with min 1≤i≤L p i . The results however might not be very tight, e.g., for the case where, for some 1 ≤ i ≤ L, p i is much larger than p. Thus the actual values of parameters {p i } need to be taken into consideration to derive tighter bounds. In particular, for every 1 ≤ i < L, depending on whether the i th or the (i + 1) th link has a larger parameter, Lemma 6 or 7 is useful to lower bound the rank of the transfer matrix at the i th node. The rest of the analysis remains the same.\nIn the following, we present the main results (without proof) for a special case of non-identical links with \u201cunequal\u201d parameters {p i }, where no two parameters are equal.\nTheorem 4: Consider a sequence of unequal parameters {p i } 1≤i≤L . The coding delay of a dense code over a line network of L links with deterministic regular trafﬁcs and Bernoulli losses with parameters {p i } is larger than\n1 p\nk + (1 + o(1)) kL w\nw.p. b.a.b. , where w ∼ γ e kL 2 / log(kL/ ) 1 3 , p . = min 1≤i≤L p i , γ e . = min 1<i≤L γ e i , and γ e i . = |p i − p i−1 |.\nTheorem 5: The average coding delay of a dense code over a network similar to Theorem 4 is larger than\nkL w\nw.p. b.a.b. , where w ∼ γ e k/ (f (k) log(kL/ )), and f (k) goes to inﬁnity, as k goes to inﬁnity, such that f (k) = o(γ e k/ log(kL/ )).\nIn the following, we discuss how to generalize the preceding results to the case of identical links with Poisson transmis- sions. The generalization of the results to the case of non- identical links should be straightforward and hence omitted.\nIn the case of the lossless Poisson trafﬁc with parameter λ, the number of packets in each partition of length N T /w is a Poisson random variable with the expected value λN T /w. By applying the Chernoff bound to the Poisson random variable (see [7, Theorem A.1.15]), the main results in Section IV are applicable to this network scenario, where p is replaced by λ.\nIn the case of Bernoulli losses over a Poisson trafﬁc with parameters p and λ, respectively, it can be shown that the points in time at which the arrivals/departures occur follow a Poisson process with parameter λp, and hence the number of packets in each partition has a Poisson distribution with the expected value λpN T /w. Thus the main results in Section IV apply by replacing p with λp.\nThe upper bounds on the coding delay and the average coding delay, derived in this paper, are valid for any arbitrary choice of . However, in the following, to compare our results with those of [4] and [5], we focus on the case where goes to 0 polynomially fast, as k goes to inﬁnity. For such a choice of , the upper bounds on the coding delay and the average coding delay hold w.p. 1, as k goes to inﬁnity.\nIn [4], the average coding delay of dense codes over the networks of length 2 with deterministic regular transmissions and Bernoulli losses with equal parameters (p) is shown to be upper bounded by 1 p (k + O(\nk log k)). The result of Theo- rem 3 indicates that the average coding delay of dense codes over the networks of length L with similar trafﬁcs as above (i.e., identical links with equal parameters) 9 is upper bounded by 1 p (k + (1 + o(1))( kL log(kL))). This is consistent with the result of [4], although the bound presented here provides more details (about the O(.) term in the former bound).\nThe result of Theorem 2 suggests that the coding delay of dense codes over network scenarios as above is upper bounded by 1 p (k + (1 + o(1))(k 2 L log(kL)) 1 3 ). One should note that there has been no result on the coding delay of dense codes over identical links in the existing literature. In fact, this was posed as an open problem in [5]. It is also noteworthy that unlike the analysis of [5], our analysis does not rely on the existence of a single worst link, and hence is applicable to the special case of identical links.\nIn [5], the average coding delay of dense codes over the networks of length L with deterministic regular transmissions and Bernoulli losses with parameters {p i } was upper bounded\n, where p = min i p i is the unique minimum and ν = arg min i p i . This result was derived under the unre- alistic assumption that all the coded packets are innovative.\nRelated to this result, Theorem 3 or Theorem 5 indicates that the average coding delay of dense codes over line networks with trafﬁcs as above, but with arbitrary or unequal parameters, is upper bounded by 1 p (k + (1 + o(1))( kL log(kL))), or\n(k + (1 + o(1))(f (k)L log(kL))), respectively, where f (k) goes to inﬁnity sufﬁciently slow, as k goes to inﬁnity. 10 It is important to note that both Theorems 3 and 5 do not have the limiting assumption of the result of [5] regarding the innovation of all the packets. The bounds of Theorems 3 and 5 are larger than that of [5], which is expected, since the former, unlike the latter, are derived based on the realistic assumption of operating over a ﬁnite ﬁeld, which has the consequence that not all the coded packets are innovative.\nThe results of Theorems 2 and 4 indicate that for both trafﬁcs with arbitrary or unequal parameters, the coding de- lay is upper bounded by 1 p (k + (1 + o(1))(k 2 L log(kL)) 1 3 ). This is while, in [5], the coding delay is upper bounded by 1 p (k + O(k 3 4 )). This bound is looser than the bound in Theorem 2, or the one in Theorem 4, although it is derived under the same limiting assumption as the one used in [5] for the average coding delay. Such an assumption makes the bound appear smaller than what it would be at the absence of the assumption. This demonstrates the strength of the bounding technique used in this work.\nBy combining Theorems 2 and 3, or Theorems 4 and 5, it can be seen that the coding delay might be much larger than the average coding delay. This highlights the fact that the analysis of the average coding delay does not provide a complete picture of the speed of convergence of dense codes to the capacity of line networks."},"refs":[{"authors":[{"name":"D. Lun"},{"name":"M. M´edard"},{"name":"R. Koetter"},{"name":"M. Effros"}],"title":{"text":"On Coding for Reliable Communication over Packet Networks"}},{"authors":[{"name":"P. Maymounkov"},{"name":"N. Harvey"},{"name":"D. Lun"}],"title":{"text":"Methods for Efﬁcient Network Coding"}},{"authors":[{"name":"A. Heidarzadeh"},{"name":"A. Banihashemi"}],"title":{"text":"Network Codes with Overlapping Chunks over Line Networks: A Case for Linear-Time Codes"}},{"authors":[{"name":"P. Pakzad"},{"name":"C. Fragouli"},{"name":"A. Shokrollahi"}],"title":{"text":"Coding Schemes for Line Networks"}},{"authors":[{"name":"T. Dikaliotis"},{"name":"A. Dimakis"},{"name":"T. Ho"},{"name":"M. Effros"}],"title":{"text":"On the Delay of Network Coding over Line Networks"}},{"authors":[{"name":"A. Heidarzadeh"},{"name":"A. Banihashemi"}],"title":{"text":"How Fast Can Dense Codes Achieve the Min-Cut Capacity of Line Networks?"}},{"authors":[{"name":"N. Alo"},{"name":"J. Spence"}],"title":{"text":"The Probabilistic Method"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565185.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S14.T1.3","endtime":"17:40","authors":"Anoosheh Heidarzadeh, Amir Banihashemi","date":"1341508800000","papertitle":"How Fast Can Dense Codes Achieve the Min-Cut Capacity of Line Networks?","starttime":"17:20","session":"S14.T1: Network Erasure Correction","room":"Kresge Rehearsal B (030)","paperid":"1569565185"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
