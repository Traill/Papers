{"id":"1569565439","paper":{"title":{"text":"Pointwise lossy source coding theorem for sources with memory"},"authors":[{"name":"Barlas O˘guz"},{"name":"Venkat Anantharam"}],"abstr":{"text":"Abstract\u2014We investigate the minimum pointwise redundancy of variable length lossy source codes operating at ﬁxed distor- tion for sources with memory. The redundancy is deﬁned by l n (X n 1 ) − nR(D), where l n (X n 1 ) is the code length at block size n and R(D) is the rate distortion function. We restrict ourselves to the case where R(D) can be calculated, namely the cases where the Shannon lower bound to R(D) holds with equality. In this case, for balanced distortion measures, we provide a pointwise lower bound to the code length sequence in terms of the entropy density process. We show that the minimum coding variance with distortion is lower bounded by the minimum lossless coding variance, and is non-zero unless the entropy density is deterministic. We also examine lossy coding in the presence of long range dependence, showing the existence of information sources for which long range dependence persists under any codec operating at the Shannon lower bound with ﬁxed distortion."},"body":{"text":"Let (X n ) be a discrete, stationary, ergodic source taking values in a ﬁnite set K. For each n consider a mapping φ n : K n → ˆ K n where the output alphabet ˆ K is also ﬁnite. We consider balanced distortion measures d n (x n 1 ; y n 1 ) =\nDeﬁnition I.1. d(x; y) is said to be a balanced distortion measure whenever the set of possible values d(·; y) takes is identical for each y ∈ ˆ K.\nWe are concerned with the problem of ﬁnding minimum length mappings φ n with the property d n (X n 1 ; φ n (X n 1 )) ≤ D for each n. Let ψ n : ˆ K n → {0, 1} ∗ , which maps φ n (X n 1 ) to a variable length binary string. Let l n (X n 1 ) be the length of ψ n (φ n (X n 1 )) (i.e. the description length at block size n). We allow any mapping that constitutes a valid code, i.e. any invertible mapping ψ n . It is well known that the average behavior of l n (X n 1 ) is bounded by the rate distortion function.\nGoing beyond average behavior, one might be interested in the more reﬁned problem of how close the code lengths can get to the rate distortion function. This problem is referred to as the redundancy problem of lossy source coding. The average redundancy of code lengths E[l n (X n 1 )] − nR(D) has been studied in the works of [4], [5]. There, the minimum average redundancy has been shown quite generally to be O(log n).\nHere we are concerned with the pointwise redundancy l n (X n 1 ) − nR(D). This problem was considered in the work [6], where it was proved that:\nTheorem I.4. ([6], theorem 6(ii) ) For any sequence {c n } of positive constants with 2 −c n < ∞,\nand ˜ Q n is a probability measure that minimizes E[− log Q n (B(X n 1 , D))] under all probability measures on ˆ K n .\nUnfortunately, very little can be said about the measures ˜ Q n , except when (X n ) is i.i.d, in which case − log ˜ Q n (B(X n 1 , D)) = − log Q ∗ n (B(X n 1 , D)) − O(log n) a.s. where Q ∗ is a product distribution. We aim to produce a more workable lower bound to l n (X n 1 ) − nR(D).\nTheorem I.5. Let ν = E[− log P (X 1 |X 0 −∞ )] be the entropy rate of (X n ). Then\nHere R l (D) is the Shannon lower bound to the rate- distortion function, to be deﬁned in the next section. The advantage of this bound over I.4 is that the quantity − log P (X n 1 |X 0 −∞ ) can be written as a running sum of a stationary random process as\nThe random process ρ n = − log P (X n |X n−1 −∞ ) is referred to as the entropy density process. Consequently, the asymptotic (sec- ond order) behavior of l n (X n 1 ) can generally be inferred from\nlimit theorems on ρ i , as the stationary ergodic process (ρ n ) typically inherits the mixing properties of the source (X n ). The caveat is that the RHS of I.5 has mean nR l (D)−O(log n), meaning that if the Shannon lower bound is not tight, the bound is of little interest.\nTo put this restriction in context, we point out that in the literature of rate-distortion theory for sources with memory, complete results are rare even in ﬁrst order discussions (i.e. calculation of R(D)). In fact, rate distortion functions can be calculated exactly only in a few special cases (ﬁnite state Markov chains with strictly positive transition matrices [7], Gaussian autoregressive processes [8] ) and even for those, only for a small range of low distortion. These examples have the property that the Shannon bound to the rate-distortion function is tight. For all other processes with memory, one needs to work with bounds on the rate-distortion function, which is useless for second order discussions.\nThe rest of the paper is structured as follows. In the next section we deﬁne the Shannon lower bound to the rate distor- tion function for balanced distortion measures, and discuss the conditions under which it is tight. In section III, we present the proof of our main theorem I.5. Then we discuss applications of this theorem to fast mixing sources in section IV. A one sided central limit theorem for such sources is given, as well as a discussion of minimum coding variance. In section V, we extend the discussion long range dependent sources.\nThe Shannon lower bound (SLB) to the rate-distortion function is deﬁned as follows. (see e.g. [9], problem 10.6.) Deﬁnition II.1 (Shannon lower bound).\nWhere the min and max are over joint distributions P (X n 1 , Y n 1 ). (a) follows because the distortion is balanced. The last equality follows because H(X n 1 ) is maximized by a product distribution on X n 1 , and by the concavity of entropy.\nLet x, y ∈ K where K is an additive group. If the distortion can be written as d(x; y) = d(x − y) for some function d : K → R, then d is referred to as a difference distortion measure. For difference distortion measures, the case in which the SLB is tight is characterized by the following theorem.\nTheorem II.3. (Theorem 4.3.1 in [10]) R l (D) = R(D) iff the source r.v. X admits the following characterization.\nAlthough the theorem is stated for difference distortion mea- sures, the proof generalizes to balanced distortion measures without alteration (also see [7] for a partial discussion). To state the general version, let Φ y , y ∈ ˆ K, be the permutation function with d(x; y) = d(Φ y (x); 0), ∀x ∈ K, for a balanced distortion d. Then we have\nTheorem II.4. R l (D) = R(D) iff the source r.v. X admits the following characterization.\nImmediate examples of sources which admit such a char- acterization are explicit constructions where an underlying process is observed through a memoryless, time invariant channel (e.g. hidden Markov models). There also exist more surprising examples however, for instance some ﬁnite state Markov chains [7] and autoregressive processes [8]. While the Shannon lower bound is known to be asymptotically tight for small distortions quite generally [11], it is in general a difﬁcult question as to when such a decomposition exists.\nOnce the mapping φ n has been chosen, the following lemma provides a pointwise lower bound on the code length process. Lemma III.1 (Barron\u2019s Lemma [12]). For any sequence {c(n)} of positive constants with 2 −c(n) < ∞ we have\nP (X n 1 |φ n (X n 1 )) \t (3) = − log P (X n 1 ) + log P (X n 1 |φ n (X n 1 )). (4)\nTheorem III.2. Let φ n be a series of codes operating at ﬁxed distortion level D n ≤ D, ∀n for some balanced distortion measure d. Then\nl n (X n 1 ) ≥ − log P (X n 1 )+log P (X n 1 |φ n (X n 1 ))−O(log n), ev. a.s. (5)\nDeﬁne S(y n 1 ) = {x n 1 : d n (x n 1 ; y n 1 ) ≤ D}. For balanced distortion measures, |S| n := |S(y n 1 )| does not depend on y n 1 . We will argue that:\n|φ n (X n 1 )) . \t (8) For any pair of random variables X n 1 , Y n 1 with X n 1 ∈ S(Y n 1 ) we have\nP (x n 1 , y n 1 ) P (x n 1 |y n 1 )\nP (x n 1 |y n 1 ) P (x n 1 |y n 1 )\nwhere the inequality is due to the fact that only those x n 1 with P (x n 1 |y n 1 ) > 0 contribute to the inner sum.\nApplying the Borel-Cantelli lemma with e.g. c n = 2 log n, we get the desired result.\n. Lemma III.3 combined with equation (5) gives\n(9) Lemma III.4.\nProof: Since d is balanced, notice that d n (x n 1 ; y n 1 ) only depends on the \u2018type\u2019 of Φ y n 1 (x n 1 ). (Recall that Φ is the permutation with the property d(x; y) = d(Φ y (x); 0).) By well known arguments resulting from the combinatorics of types (see e.g. chapter 2 of [13]), we know\nwhere X has the distribution that maximizes H(X) subject to Ed(X; 0) ≤ D. Taking logarithms, we get\nThe result follows by the deﬁnitions of R ∗ n (D) and R l (D). Combining lemma III.4 with eq. 9 concludes the proof.\nPicking c n = 2 log n and invoking the Borel-Cantelli lemma completes the proof.\nDeﬁne the function ρ n = − log P (X n |X n−1 −∞ ). Theorem I.5 can be re-written as\nThis allows us to bound the limiting behavior of the code length sequence by applying well known limit theorems to the stationary sequence ρ n . For instance, when (X n ) are i.i.d., it follows that (ρ n ) are also i.i.d.. It can easily be shown that the variance of ρ n = − log P (X n |X n−1 −∞ ) is bounded:\nTherefore (ρ n ) satisﬁes a central limit theorem with limiting variance var[ρ 0 ]. It follows that for memoryless, ﬁnite state sources (X n ):\nCorollary IV.2. There exists a sequence of random variables (z n ) s.t.\nWhen (X n ) are not i.i.d, but sufﬁciently fast mixing, one would expect that the same holds for the sequence (ρ n ). In general, suppose that the sequence\nAn easy corollary to theorem I.5 for the above cases is the following one sided central limit theorem.\nCorollary IV.3. There exists a sequence of random variables (z n ) s.t.\nWe refer to lim inf 1 n E[(l n (X n 1 ) − nR l (D)) 2 ] as the coding variance. Then σ 2 is a lower bound on the minimum coding variance. In the memoryless case, this can easily be calculated as var(− log P (X 0 )). In general, for sources that meet the Shannon lower bound, and for which the sum in (14) is absolutely summable, we conclude that the minimum coding variance is strictly positive unless ρ n is equal to a deterministic constant. This conﬁrms the conjecture raised in [6] in a more general setting.\nWhat is perhaps more interesting is that minimum coding variance for lossy coding that meets the Shannon lower bound admits a lower bound that is independent of the distortion level D and is equal to the minimum lossless coding variance. 1 This is surprising, because it implies that the minimum coding variance can be discontinuous at distortion level D max := inf D {R(D) = 0}. Consider an information source for which the Shannon lower bound holds with equality for the entire range of distortions 0 ≤ D ≤ D max . The i.i.d. X n ∼ Bernoulli(p) process with Hamming distortion measure is one such source. It is easy to show that:\nLemma IV.4. For D = D max + , there exists an achievable coding scheme with\nProof: Without loss of generality, let p ≤ 1 2 . Note that D max = p. We code as follows. If n i=1 X i < n(p + ), we map to all zeros. This is within distortion D max + . Otherwise, we transmit the exact string X n 1 . We use a 1 bit ﬂag to indicate which event happens. Since we know\nthe error event stops happening eventually almost surely by Borel-Cantelli, thus proving the lemma.\nThis shows that the minimum coding variance is 0 when D = D max + for any , while it is strictly non-zero when D = D max .\nThe results in the previous section imply that for sufﬁciently fast mixing information sources, the optimal pointwise redun- dancy in the code length process is bounded below by an order √\nn random process. In this section, we investigate the case when the memory in the source decays much more slowly.\nA stationary random process (X n ) with E[X 2 n ] < ∞ is said to be long range dependent (LRD) if\nThe degree of long range dependence is measured by the Hurst index H ( 1 2 ≤ H ≤ 1).\nn 2h−1 \t < ∞ . Equivalently, we can write\nAssume that the entropy density (ρ n ) is LRD with Hurst index 1 2 ≤ H ≤ 1. From theorem I.5, we conclude that the process l n (X n 1 ) − R l (D) is lower bounded by the partial sums of a zero-mean LRD process with Hurst index H and therefore the pointwise redundancy in code length is lower bounded by a process that is at least of order n H . The result is true for any coding algorithm with ﬁxed distortion that has average code length equal to the Shannon lower bound. In other words, long range dependence is an information theoretic quantity that persists under any coding scheme. This result was ﬁrst suggested in [16] in the context of lossless coding of an LRD renewal process. The extension to the lossy case is important, because in practice, long range dependence is observed in the context of coding with distortion (e.g. at the output of a variable bit-rate video coder [18], [19], [20]).\nTherefore efforts to mitigate long range dependence using clever coding might be futile, at least in the constant distortion case. To maintain a less bursty rate, one might try to use codes with variable quality, in which case we conjecture that the distortion function will likely end up being LRD.\nThis entire discussion hinges on the fact that there exists information sources for which the entropy density (ρ n ) is LRD, and for which the Shannon lower bound is tight. Below we construct an example process with these properties, demonstrating that the above discussion is not vacuous.\nAn example of a concrete information source which has (ρ n ) LRD was presented in [16]. There it is proved that if (X n ) is a stationary discrete time LRD renewal process with Hurst index H, then ρ n = − log P (X n |X n−1 −∞ ) is also LRD with identical Hurst index H. Here we demonstrate an\ninformation source such that (ρ n ) is LRD with Hurst index H for which the Shannon lower bound is tight for some strictly non-zero distortion D > 0.\nLet X 1 (n) ∈ {0, 1} be an LRD renewal process with Hurst index H. Let X 2 (n) ∈ {0, 1} be an i.i.d. Bernoulli(p) process. Let X 1 be independent of X 2 . Deﬁne\nwith d(x; y) = 1 − δ(x = y) for x, y ∈ {0, 1} 2 . Note that we are able to write\nfor the appropriate addition operation deﬁned on {0, 1} 2 . Since d is a difference distortion measure, by theorem II.3, we conclude that the Shannon lower bound holds for this source for a strictly non-zero distortion level D.\n= − log P (X 1 (n)|(X 1 ) n−1 −∞ ) − log P (X 2 (n)) := ρ 1 (n) + ρ 2 (n),\nwhich is LRD with Hurst index H by virtue of (ρ 1 ) having this property.\nWhile this is clearly a contrived example, there exist more general and practical information sources for which theorem I.5 provides a meaningful bound. These include for example LRD semi-Markov chains observed through a memoryless channel. Such sources can be constructed from LRD Markov chains using the tools provided in [17]. We will discuss these constructions in a more extensive document.\nWe considered the pointwise minimum redundancy problem for lossy coding for codes operating at ﬁxed distortion. In contrast to the average redundancy framework, where the redundancy is O(log n), the pointwise redundancy is typically of order\nn for sufﬁciently fast mixing sources, and of order n H for sources with long range dependent entropy density with Hurst index H.\nThe results were enabled by a new pointwise lower bound to the code length for any code that operates with ﬁxed distortion. The lower bound directly relates the code length to the entropy density process − log P (X n |X n−1 −∞ ). The usefulness of the bound is restricted to the case where R(D) can be calculated, namely the cases where the Shannon lower bound to R(D) holds with equality.\nAlthough we work in the restricted case of the Shannon lower bound, our results are still considerable generalizations over previous work that only considered memoryless sources. We provide the ﬁrst practical pointwise bound that holds for a class of information sources with memory. For this class, we were able to show that the minimum coding variance with distortion is lower bounded by the minimum lossless coding variance, and is non-zero unless the entropy density\nis deterministic. We also examined lossy coding in the pres- ence of long range dependence, and showed the existence of information sources for which long range dependence persists under any codec operating at the Shannon lower bound with ﬁxed distortion. Therefore efforts to mitigate long range dependence using clever coding might be futile, at least in the constant distortion case.\nResearch support from the ARO MURI grant W911NF-08- 1-0233, \u201cTools for the Analysis and Design of Complex Multi- Scale Networks\u201d, from the NSF grant CNS-0910702, from the NSF Science & Technology Center grant CCF-0939370, \u201cSci- ence of Information\u201d, from Marvell Semiconductor Inc., and from the U.C. Discovery program is gratefully acknowledged."},"refs":[{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[],"title":{"text":"Coding theorems for a discrete source with a ﬁdelity criterion"}},{"authors":[{"name":"J. Kieffer"}],"title":{"text":"Sample converses in source coding theory"}},{"authors":[{"name":"Z. Zhang"},{"name":"E. Yang"},{"name":"V. Wei"}],"title":{"text":"The redundancy of source coding with a ﬁdelity criterion. 1. Known statistics"}},{"authors":[{"name":"E. Yang"},{"name":"Z. Zhang"}],"title":{"text":"On the redundancy of lossy source coding with abstract alphabets"}},{"authors":[{"name":"I. Kontoyiannis"}],"title":{"text":"Pointwise redundancy in lossy data compression and universal lossy data compression"}},{"authors":[{"name":"R. Gray"}],"title":{"text":"Rate distortion functions for ﬁnite-state ﬁnite-alphabet Markov sources"}},{"authors":[],"title":{"text":"Information rates of autoregressive processes"}},{"authors":[{"name":"T. Cove"},{"name":"J. Thoma"},{"name":"J. Wiley et al"}],"title":{"text":"Elements of information theory"}},{"authors":[{"name":"T. Berge"}],"title":{"text":"Rate-Distortion Theory"}},{"authors":[{"name":"T. Linder"},{"name":"R. Zamir"}],"title":{"text":"On the asymptotic tightness of the Shannon lower bound"}},{"authors":[{"name":"I. Kontoyiannis"}],"title":{"text":"Second-order noiseless source coding theorems"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orne"}],"title":{"text":"Information theory: Coding theorems for dis- crete memoryless systems "}},{"authors":[{"name":"P. Algoet"},{"name":"T. Cover"}],"title":{"text":"A sandwich proof of the Shannon-McMillan- Breiman theorem"}},{"authors":[{"name":"W. Philip"},{"name":"W. Stou"}],"title":{"text":"Almost sure invariance principles for partial sums of weakly dependent random variables "}},{"authors":[{"name":"B. O˘guz"},{"name":"V. Anantharam"}],"title":{"text":"Compressing a long range dependent renewal process"}},{"authors":[],"title":{"text":"Hurst index of functions of long range dependent Markov chains"}},{"authors":[{"name":"J. Beran"},{"name":"R. Sherman"},{"name":"M. Taqqu"},{"name":"W. Willinger"}],"title":{"text":"Long-range dependence in variable-bit-rate video trafﬁc"}},{"authors":[{"name":"O. Rose"}],"title":{"text":"Statistical properties of MPEG video trafﬁc and their impact on trafﬁc modeling in ATM systems"}},{"authors":[{"name":"F. Fitzek"},{"name":"M. Reisslein"}],"title":{"text":"Mpeg4 and h. 263 video traces for network performance"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565439.pdf"},"links":[{"id":"1569566527","weight":3},{"id":"1569566485","weight":3},{"id":"1569565663","weight":3},{"id":"1569564635","weight":3},{"id":"1569565067","weight":3},{"id":"1569565691","weight":3},{"id":"1569559617","weight":10},{"id":"1569566981","weight":6},{"id":"1569566683","weight":3},{"id":"1569566697","weight":3},{"id":"1569565551","weight":3},{"id":"1569566943","weight":3},{"id":"1569566591","weight":3},{"id":"1569566571","weight":6},{"id":"1569552245","weight":13},{"id":"1569566415","weight":3},{"id":"1569566469","weight":3},{"id":"1569566081","weight":10},{"id":"1569565355","weight":10},{"id":"1569564469","weight":3},{"id":"1569565931","weight":6},{"id":"1569551535","weight":3},{"id":"1569565461","weight":6},{"id":"1569564245","weight":3},{"id":"1569565837","weight":3},{"id":"1569566119","weight":6},{"id":"1569563411","weight":3},{"id":"1569565123","weight":6},{"id":"1569566941","weight":3},{"id":"1569565291","weight":3},{"id":"1569564203","weight":3},{"id":"1569556713","weight":3},{"id":"1569562685","weight":6},{"id":"1569566467","weight":6},{"id":"1569566999","weight":3},{"id":"1569566843","weight":3},{"id":"1569558483","weight":6},{"id":"1569556091","weight":3},{"id":"1569565347","weight":10},{"id":"1569565455","weight":3},{"id":"1569566795","weight":3},{"id":"1569551763","weight":3},{"id":"1569565953","weight":3},{"id":"1569564189","weight":3},{"id":"1569566865","weight":3},{"id":"1569566095","weight":10},{"id":"1569566167","weight":3},{"id":"1569563981","weight":3},{"id":"1569559565","weight":3},{"id":"1569565213","weight":3},{"id":"1569566643","weight":10},{"id":"1569566531","weight":6},{"id":"1569567665","weight":10},{"id":"1569561143","weight":6},{"id":"1569564611","weight":3},{"id":"1569561795","weight":3},{"id":"1569566437","weight":3},{"id":"1569566851","weight":3},{"id":"1569559111","weight":3},{"id":"1569553537","weight":3},{"id":"1569565915","weight":3},{"id":"1569552251","weight":6},{"id":"1569553519","weight":6},{"id":"1569566231","weight":3},{"id":"1569554881","weight":3},{"id":"1569565559","weight":3},{"id":"1569566909","weight":3},{"id":"1569565151","weight":6},{"id":"1569566913","weight":3},{"id":"1569566809","weight":6},{"id":"1569566447","weight":3},{"id":"1569565055","weight":3},{"id":"1569565633","weight":6},{"id":"1569555879","weight":3},{"id":"1569565219","weight":3},{"id":"1569566003","weight":3},{"id":"1569565095","weight":3},{"id":"1569566553","weight":3},{"id":"1569565029","weight":3},{"id":"1569565357","weight":6},{"id":"1569566191","weight":3},{"id":"1569566603","weight":10},{"id":"1569566695","weight":3},{"id":"1569566673","weight":6},{"id":"1569566297","weight":6},{"id":"1569560997","weight":6},{"id":"1569566501","weight":3},{"id":"1569560503","weight":3},{"id":"1569564339","weight":3},{"id":"1569562551","weight":6},{"id":"1569563395","weight":3},{"id":"1569566901","weight":6},{"id":"1569551347","weight":6},{"id":"1569565571","weight":6},{"id":"1569564411","weight":3},{"id":"1569565665","weight":3},{"id":"1569566983","weight":3},{"id":"1569566873","weight":6},{"id":"1569565765","weight":3},{"id":"1569565435","weight":3},{"id":"1569566129","weight":10},{"id":"1569565919","weight":6},{"id":"1569566711","weight":3},{"id":"1569565661","weight":3},{"id":"1569566267","weight":3},{"id":"1569566035","weight":3},{"id":"1569566253","weight":3},{"id":"1569566547","weight":3},{"id":"1569566595","weight":6},{"id":"1569565375","weight":3},{"id":"1569566813","weight":3},{"id":"1569566771","weight":3},{"id":"1569566641","weight":3},{"id":"1569563975","weight":3},{"id":"1569566487","weight":3},{"id":"1569556759","weight":6},{"id":"1569566619","weight":3},{"id":"1569565271","weight":6},{"id":"1569561185","weight":3},{"id":"1569566397","weight":3},{"id":"1569558779","weight":6},{"id":"1569566817","weight":10},{"id":"1569566435","weight":3},{"id":"1569564923","weight":3},{"id":"1569565367","weight":3},{"id":"1569564281","weight":3},{"id":"1569565805","weight":10},{"id":"1569563919","weight":3},{"id":"1569566577","weight":6},{"id":"1569557851","weight":6},{"id":"1569565389","weight":6},{"id":"1569565537","weight":10},{"id":"1569564961","weight":6},{"id":"1569564253","weight":3},{"id":"1569565853","weight":3},{"id":"1569565889","weight":6},{"id":"1569564505","weight":3},{"id":"1569565165","weight":3},{"id":"1569565635","weight":3},{"id":"1569565113","weight":3},{"id":"1569566375","weight":10},{"id":"1569564257","weight":6},{"id":"1569566555","weight":6},{"id":"1569564931","weight":3},{"id":"1569564141","weight":6},{"id":"1569566973","weight":3},{"id":"1569566449","weight":6},{"id":"1569565031","weight":3},{"id":"1569564755","weight":10},{"id":"1569564509","weight":3},{"id":"1569566839","weight":3},{"id":"1569551751","weight":3},{"id":"1569564419","weight":6},{"id":"1569566067","weight":3},{"id":"1569566113","weight":6},{"id":"1569566443","weight":6},{"id":"1569566417","weight":6},{"id":"1569560581","weight":6}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T1.3","endtime":"15:40","authors":"Barlas Oğuz, Venkat Anantharam","date":"1341242400000","papertitle":"Pointwise lossy source coding theorem for sources with memory","starttime":"15:20","session":"S3.T1: Lossy Source Coding","room":"Kresge Rehearsal B (030)","paperid":"1569565439"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
