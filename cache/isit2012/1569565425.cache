{"id":"1569565425","paper":{"title":{"text":"Beyond Worst-Case Reconstruction in Deterministic Compressed Sensing"},"authors":[{"name":"Sina Jafarpour"},{"name":"Member"},{"name":"IEEE"},{"name":"Marco F. Duarte"},{"name":"Member"},{"name":"IEEE"},{"name":"Robert Calderbank"},{"name":"Fellow"},{"name":"IEEE"}],"abstr":{"text":"Abstract\u2014The role of random measurement in compressive sensing is analogous to the role of random codes in coding theory. In coding theory, decoders that can correct beyond the minimum distance of a code allow random codes to achieve the Shannon limit. In compressed sensing, the counterpart of minimum distance is the spark of the measurement matrix, i.e., the size of the smallest set of linearly dependent columns. This paper constructs a family of measurement matrices where the columns are formed by exponentiating codewords from a classical binary error-correcting code of block length M . The columns can be partitioned into mutually unbiased bases, and the spark of the corresponding measurement matrix is shown to be O(\nM ) by identifying a conﬁguration of columns that plays a role similar to that of the Dirac comb in classical Fourier analysis. Further, an explicit basis for the null space of these measurement matrices is given in terms of indicator functions of binary self-dual codes. Reliable reconstruction of k-sparse inputs is shown for k of order M/ log(M ) which is best possible and far beyond the worst case lower bound provided by the spark."},"body":{"text":"The central goal of compressed sensing (CS) is to capture a signal using very few measurements. In most work to date, this broader objective is exempliﬁed by the important special case in which the measurement data constitute a vector f = Φα+e where Φ is an M ×N matrix called the sensing matrix, α ∈ R N is a k-sparse signal, and e ∈ R M is the additive noise. There are two distinct CS frameworks with different objectives.\nWorst-case CS [1], [2]: In the worst-case CS framework, the goal is to recover every k-sparse vector α from the corresponding measurement vector f . It is known that certain probabilistic processes generate sensing matrices that support worst-case CS [3]. However, the random sensing framework suffers from storage and computation limitations. As a result, there has been a signiﬁcant amount of research on designing alternative deterministic matrices for worst-case CS framework over the last few years [4]. Most such constructions rely on the coherence between the columns of the matrix. When the coherence follows the Welch Bound µ = O 1 √ M , the Gerschgorin Circle Theorem guarantees reconstruction of any k-sparse signal with k = O(\nAverage-case CS [4]\u2013[6]: In many practical applications, including wireless communications and radar, it is not nec-\nessary to reconstruct every sparse vector [7]. The goal of average-case CS is to recover most (in contrast to all) k-sparse vectors. Here, the sparse vector is often modeled to have a uniformly random support and random sign for the k non- zero entries. The average-case CS framework relies on the coherence and the spectral norm of the deterministic sensing matrix. The ideal case is when coherence follows the Welch bound [8], and different measurements are orthogonal. Then, as long as k = O (M/log N ), with high probability a k- sparse vector has a unique sparse representation, and can be efﬁciently recovered from the compressive measurements [6].\nIn this paper, we construct an explicit basis for the null space of a large family of deterministic sensing matrices designed for the average-case CS framework (see [5] and the references therein) using the indicator vectors of binary self- dual codes. Characterizing the null space of these matrices makes it possible to investigate and analyze the geometric properties of these matrices more precisely.\nMore speciﬁcally, we introduce a family of deterministic sensing matrices called the extended Delsarte-Goethals frames (EDGFs) that hold the following three properties simultane- ously: (i) as long as k ≤ c 1\nM it is possible to recover every k-sparse vector α from the measurement vector Φα; (ii) there exists a k-sparse vector α with k = c 2\nM such that no reconstruction algorithm can uniquely recover α from Φα; and (iii) it is possible to recover most k-sparse vectors α from the measurement vector Φα as long as k ≤ c 3 M log N (which can be much larger than c 2\nM ), where c 1 , c 2 , and c 3 are ﬁxed constants. The EDGFs meet the coherence-based lower bound on worst-case reconstruction and the order-optimal upper bound on average-case reconstruction.\nThe rest of the paper is organized as follows. Section II reviews Delsarte-Goethals frames (DGFs). Section III explains the properties of the self-dual codes used in this paper. Sections IV and V characterize the null space of the DGFs and the EDGFs. The experiments are provided in Section VI. Section VII concludes the paper.\nA vector is k-sparse if it has at most k non-zero entries. The support of a k-sparse vector indicates the positions of its non-zero entries. Let Φ be an M ×N matrix. Then Φ is a tight- frame with redundancy ρ if ΦΦ \u2020 = ρI N ×N , where Φ \u2020 denotes the conjugate transpose of Φ. The spark of the measurement matrix Φ, denoted as spark(Φ), is the size of the smallest set\nof linearly dependent columns of Φ. Let ϕ i denote the i th column of Φ. The coherence between the columns of Φ is deﬁned as the maximum inner product between two distinct columns of Φ:\nIn this paper, we simplify the analysis by focusing on the noiseless CS problem, and note that it is straightforward to generalize the analysis to include noise. The following two theorems relate the maximum sparsity level k to the parameters of the sensing matrix Φ, so that it is possible to efﬁciently recover all (respectively most) k-sparse vectors α from Φα.\nTheorem 2.1 (Worst-case CS [9]): Let Φ be a an M × N sensing matrix with worst-case coherence µ Φ . Then as long as k = O(µ −1 Φ ), it is possible to efﬁciently recover every k-sparse vector α from the measurement vector Φα. In contrast, when k ≥ spark(Φ) 2 , there exist multiple k-sparse vectors that are mapped to the same measurement vector, rendering recovery impossible.\nTheorem 2.2 (Average-case CS [6], [10]): Let Φ be a an M × N tight-frame with redundancy ρ = N/M and with worst-case coherence µ Φ . If k = O min µ −2 Φ log N , M log N , then with probability 1 − 1/N , it is possible to efﬁciently recover a k-sparse vector α with uniformly random support and uniformly random sign from the measurement vector Φα.\nThroughout this paper, m denotes an integer and M = 2 m . Given a vector v with binary entries, we let v(x) denote the entry of v indexed by x. The inner product of two binary vectors u, v is denoted by u v.\nThe Delsarte-Goethals frames (DGFs) are a class of CS matrices that have been recently introduced by Calderbank, Howard, and Jafarpour [5]. Speciﬁcally, let m be an odd positive integer, and r to be an integer smaller than m−1 2 . Next, let DG(m, r) denote the Delsarte-Goethals set of bi- nary symmetric matrices, as described in [11]. Then, given M = 2 m and N = M r+2 , the M × N DGF Φ is constructed from DG(m, r) in the following way. Index the rows of Φ by binary vectors x ∈ F m 2 and index the columns of Φ by pairs (P, b), where P ranges over all 2 (r+1)m binary symmetric matrices DG(m, r) and b ranges over all members of F m 2 . The entries of Φ are given by\n−1. It is easy to see from this description that (i) DGFs are unions of orthonormal bases and (ii) each DGF can be represented as\nwhere R = N −M M = M r+1 − 1, H is the Hadamard matrix, and each D i is a diagonal matrix whose (x, x) entry is\ni x P i x where P i is a binary symmetric matrix from the Delsarte-Goethals set DG(m, r). A DGF is a tight-frame with redundancy M r+1 and worst-case coherence µ Φ = 2 r √ M [5]. As a result, it follows from Theorems 2.1 and 2.2 that as long as k = O(\nM /2 r ), it is possible to recover every k- sparse vector α from Φα using the 1 -minimization method.; moreover, even if k = O \t M 2 2r log N it is still possible to recover most k-sparse vectors α from Φα using the same techniques.\nWe start by deﬁning a binary self-dual code and explaining some of its properties.\nDeﬁnition 3.1 (Binary Self-Dual Code): A binary code C is self-dual if\nLet C be a self-dual code of length m, and let b be a binary m-tuple vector in the ﬁnite ﬁeld F m 2 . Throughout the paper, by b ⊕ C we mean {b ⊕ c : c ∈ C}.\nDeﬁnition 3.2: Let C be a binary self-dual code of length m. The binary vector v of length M = 2 m with entries v(x) = 1 if x ∈ C and v(x) = 0 if x / ∈ C is called the indicator of C.\nA direct calculation, captured in the following lemma, shows that the indicator of a self-dual code can be viewed as the binary counterpart of the Dirac comb in Fourier analysis.\nLemma 3.1: Let C be a binary self-dual linear code of length m, and let v ∈ {0, 1} M be the indicator of C. Let H be the M × M Hadamard matrix. Then Hv = |C|v.\nNext, we use the vector v to construct a sparse vector in the null space of the matrix Φ 0 = I, 1 √ M H .\nTheorem 3.1: Let Φ 0 = I, 1 √ M H be an M × 2M matrix generated from concatenating the identity matrix and the normalized Hadamard matrix. Let C be a binary linear self- dual code with indicator v. Deﬁne v 2 . = [−v, v] . Then\nI. Φ 0 is a tight frame with redundancy 2. II. v 2 is a 2\nM -sparse vector in the null space of Φ 0 . Therefore spark(Φ 0 ) ≤ 2\nI. Φ 0 is a union of two orthonormal bases, therefore ΦΦ \u2020 = 2I M ×M .\nII. Every self-dual code of length m has dimension m 2 , and hence exactly\n√ M different codewords. Therefore, v is √\nnon-zero entries. Moreover, it follows from Lemma 3.1 that Hv =\nM Iv or equivalently I, 1 √ M H [−v, v] = 0.\nCorollary 3.1: Let D be an M × M diagonal matrix, and let Φ 1 = 1 √ M [DH, H] be an M × 2M matrix generated from concatenating the modulated Hadamard (DH) matrix and the Hadamard (H) matrix. Deﬁne v N . = − 1 √ M H D −1 v ; v . Then v N is in the null space of Φ 1 .\nProof: Theorem 3.1 guarantees that 1 √ M H v − I v = 0. On the other hand, since the Hadamard matrix has orthogonal binary-valued rows, we have H −1 = 1 M H, and therefore\nM of the second M entries of v N are non- zero. Now we analyze the ﬁrst M entries of v N . Let ∆ denote the diagonal of D −1 , let ξ . = D −1 v, and denote η . = Hξ. Here we focus on a special but important case where there exists an m × m binary symmetric matrix P such that ∆(x) = i −x Px for every x ∈ F m 2 . This is the case for a large class of CS matrices, including the DGFs.\nNote that the calculation y Py +2x y is now over Z 4 and not Z 2 . Let η(x) denote the complex conjugate of η(x). Observe that η(x) is zero if and only if η(x) is zero. Therefore, it is sufﬁcient to analyze η(x).\nTheorem 3.2: Let P be an m × m binary symmetric matrix, and let E be the null space of P. Deﬁne\nLet d P denote the diagonal of P and assume that d P ∈ C. This assumption is easily satisﬁed if we only consider zero-diagonal matrices in the construction of the DGFs. Deﬁne\nThen η(x) = 0 for at most 2 t values of x, where t = m − dim(C 0 ).\nThe inner sum is zero if (d P + Pz) / ∈ C. Otherwise the inner sum is |C|, and we have\n= z 1 Pz 1 + 2x z 1 + z 2 Pz 2 + 2x z 2 + 2z 1 Pz 2 \t (5) = z 1 Pz 1 + 2x z 1 + z 2 Pz 2 + 2x z 2 (mod 4),\nwhere the second equality follows from the fact that both Pz 1 and z 2 are codewords of C. If this linear map is the zero map on x, then |η(x)| 2 = |C||C 0 |, and otherwise η(x) = 0. As a result, η(x) vanishes for all but 2 t values of x, where t = m − dim(C 0 ).\nIn this section, we construct a basis for the null space of matrices of the form of Equation (1). To do this, we ﬁrst provide an orthogonal basis for the null space of the matrix Φ 0 . = I, 1 √ M H, .\nLet a be a binary m-dimensional vector. The time-shift matrix A a is a circulant matrix so that every row x of A a has only one 1 at the corresponding column indexed at x ⊕ a and zeros elsewhere. Similarly, the frequency-shift matrix B a is a diagonal matrix with diagonal entries (−1) xa , where the m-dimensional binary vector x ranges over all M = 2 m rows. A direct calculation reveals that HA a = B a H, and HB a = A a H.\nLemma 4.1: Let Φ 0 and v be as above, and let a and b be any two binary m-dimensional offsets. Then the vector w 2 = [A a B b v; −B a A b v] is in the null space of Φ 0 .\nThen W is an orthogonal basis forming the null space of Φ 0 . Proof: Since v is the indicator of a self dual code, it\nM offsets a that produce distinct shifts of v. The reason is that since C is linear, if c is any codeword of C, then both A a v and A a⊕c v provide the same vector whose non-zero entries correspond to indices of the form z ⊕ a where z ranges over all codewords. That is A a v = A a⊕c v. Similarly, since C is self-dual, for every pair of codewords z and c we have (−1) b z = (−1) (b⊕c) z . This implies that there are\nM distinct choices b for the frequency shift of the vector v.\nNow we show that these M vectors are orthogonal. To see this, let (a, b) and (a , b ) be two distinct pairs of time and frequency shift offsets. Let ξ 1 = B a A b v and ξ 2 = B a A b v. Then, it is sufﬁcient to show that if ξ 1 and ξ 2 are distinct, then ξ 1 ξ 2 = 0.\nSince v is the indicator vector of a linear self-dual code, then if Supp(ξ 1 ) ∩ Supp(ξ 2 ) contains some element y, then\nthe set y ⊕ C is also in the intersection of the two supports. This set has\n√ M elements, and since both supports also have √\nM elements, we must have Supp(ξ 1 ) = Supp(ξ 2 ). As a result, if A b v = A b v then Supp(ξ 1 ) ∩ Supp(ξ 2 ) = Ø, and therefore B a A b v and B a A b v are orthogonal.\nOn the other hand, if A b v = A b v, then ξ 1 and ξ 2 are two distinct Walsh tones restricted to the set b ⊕ C. Now let ˜ c be an element in C such that ˜ c (a ⊕ a ) = 1. Then\nTherefore, we must have x:x⊕b∈C (−1) x (a⊕a ) = 0, which proves that ξ 1 and ξ 2 are orthogonal. A similar argument can be used to show that if ξ 1 = A a B b v and ξ 2 = A a B b v, then, if ξ 1 and ξ 2 are distinct, then ξ 1 ξ 2 = 0.\nAs a result, we have M distinct linearly independent vectors of the form w 2 = [A a B b v; −B a A b v] which are all in the null space of Φ 0 . On the other hand, since Φ 0 is M × 2M , its null space has dimension M , and therefore the M null space vectors above form a basis for the null space of Φ 0 .\nNext we show that the same argument can be used to form an orthogonal basis for the null space of matrices of form Φ 1 = 1 √ M [DH, H], where D is a diagonal matrix with diagonal entries i xPx , x ∈ F m 2 .\nTheorem 4.1: Let Φ 1 = 1 √ M [DH, H], where D = [i xPx ], and where P is a zero-diagonal binary symmetric matrix 1 . Let C be a self-dual code such that for any codeword c, Pc is also a codeword in C. Let v denote the indicator vector of the codewords of C. Let\nThen W 2 forms an orthonormal basis for the null space of the matrix Φ 1 ; moreover, every element of W 2 is 2\nM -sparse. Proof: Let w be any vector in W 2. We have\nM -sparse. First, note that since v is the indicator of a self-dual code, and the operators A b and B a do not change the sparsity level, B a A b v is\nM -sparse. Hence, we need to show that ω . = HD −1 A a B b v is also\nSince C is self-dual and y and P y are both codewords in C, the mapping y → 2(b + x + aP ) y + y P y is a linear map from C to Z 4 . Theorem 3.2 now guarantees that w(x) is non-zero only for\nRemark 4.1: So far we have shown how to generate a basis for the null space of a M × 2M matrix of the form [DH, H]. This construction can be generalized to matrices of form [D R H, · · · , D 1 H, H] in a straightforward manner by ﬁrst zero- padding each null space vector, so that it becomes (R + 1)M - dimensional, and then collecting all these RM vectors. For instance, a basis for the null space of a M × 3M matrix [D 2 H, D 1 H, H] has the form\n0 V 2 V 1 0\nwhere [V 1 ; V 1 ] is a basis for the null space of [D 1 H, H], and [V 2 ; V 2 ] is a basis for the null space of [D 2 H, H].\nIn this section, we use the results of the previous sections and design an M × N sensing matrix with M \t N for which there exists constants c 1 , c 2 , and c 3 , such that (i) it is possible to uniquely recover every k-sparse vectors as long as k ≤ c 1\nM ; (ii) there exists a k-sparse vector α with k = c 2\nM such that no reconstruction algorithm is able to uniquely recover it from the measurement vector f = Φα; and (iii) it is possible to uniquely recover most k-sparse vectors as long as k ≤ c 3 M/ log N .\nThe extended Delsarte-Goethals frame (EDGF) is con- structed by concatenating the M × M identity matrix to a DGF.\nTheorem 5.1: Let r be a constant integer, and let m > 2r + 1 be an integer. Then:\nI. The EDGF has M = 2 m rows, and N = M r+2 + M columns\nII. The EDGF is a tight frame with redundancy M r+1 + 1, and coherence 2 r √ M .\nProof: The proof of Part I follows from the construction of the EDGF. To prove Part II. observe that the EDGF is a union of orthonormal bases, and therefore it is a tight-frame with redundancy N M . The coherence bound follows from the fact that the inner product between two distinct columns of a DGF is bounded by 2 r √ M , and that the inner product between a column of a DGF, and a column of the identity matrix is bounded by 1 √ M .\nSince the matrix Φ 0 = [I, 1 √ M H] is a submatrix of any EDGF, it follows from Theorem 3.1 that the spark of any\n√ M . As a result, there can be two distinct √\nM -sparse vectors mapped to the same low-dimemsional measurement vector. On the other hand, Theorem 2.1 states that it is possible to uniquely recover every k-sparse vector with sparsity k ≤ c 1\nM . In contrast, it follows from Theo- rem 2.2 that if α is a k-sparse vector with uniformly random support and random sign, then as long as k ≤ c 3 M/ log N (which can be much larger than c 1\nM ), α is uniquely recoverable from f = Φα with overwhelming probability.\nThe experiments of this section compare the performance of the Basis Pursuit algorithm [6] in recovering sparse vectors with uniformly random supports versus recovering sparse vectors supported on a rank-deﬁcient submatrix obtained using a self-dual code. We provide the comparisons for the DGFs (Fig. 1), as well as for the EDGFs (Fig. 2).\n[H, D 1 H], where D 1 is a diagonal matrix with d D 1 (x) = i x Px , and P is the 8 × 8 zero-diagonal binary symmetric matrix obtained from applying the Gray map [12] to a 7 × 7 binary DG(7, 0) matrix. We also used the self-dual Hamming code C of length 8 in order to ﬁnd a rank-deﬁcient submatrix of Φ.\nA simple calculation reveals that only 4 codewords are in C 0 (deﬁned by eq. (3)). Therefore, dim(C 0 ) = 2, and Theorem 3.2 predicts that HD −1 1 v must have 256/4 = 64 non-zero entries. A direct calculation reveals that HD −1 1 v indeed has 64 non- zero entries. Therefore, the null space of 1 √ M [H, D 1 H] has a 80-sparse element. That is, there exists, a 256 × 80 submatrix of Φ that is rank-deﬁcient. As illustrated in Fig. 1, the Basis Pursuit Algorithm fails to recover some k-sparse vectors with sparsity level k > 40 which are supported on this rank- deﬁcient submatrix. However, it is still possible to recover most k-sparse vectors with uniformly random support over the 512 columns, even for sparsity level k = 80. Fig. 2 shows the result of the same experiment using an EDGF.\nWe have determined a natural basis for the null space of an extended Delsarte-Goethals frame and shown that this null space contains a vector that is 2\nM -sparse. We have demonstrated that this family of measurement matrices meets the lower bound of k = O(\nM ) on worst-case CS as well as the order optimal upper bound of k = O(M/ log(N ))."},"refs":[{"authors":[{"name":"E. Cand`e"},{"name":"J. Romber"},{"name":"T. Tao"}],"title":{"text":"Stable signal recovery from incomplete and inaccurate measurements"}},{"authors":[{"name":"D. Donoho"}],"title":{"text":"Compressed Sensing"}},{"authors":[{"name":"R. Baraniu"},{"name":"M. Davenpor"},{"name":"R. DeVor"},{"name":"M. Wakin"}],"title":{"text":"A simple proof of the restricted isometry property for random matrices"}},{"authors":[{"name":"W. Bajw"},{"name":"R. Calderban"},{"name":"S. Jafarpour"}],"title":{"text":"Model Selection: Two fundamental measures of coherence and their algorithmic signiﬁcance"}},{"authors":[{"name":"R. Calderban"},{"name":"S. Howar"},{"name":"S. Jafarpour"}],"title":{"text":"Construction of a large class of matrices satisfying a statistical isometry property"}},{"authors":[{"name":"E. Cand`e"},{"name":"J. Plan"}],"title":{"text":"Near-ideal model selection by 1 minimization"}},{"authors":[{"name":"L. Applebau"},{"name":"S. Howar"},{"name":"S. Searl"},{"name":"R. Calderbank"}],"title":{"text":"Chirp sensing codes: Deterministic compressed sensing measurements for fast recovery"}},{"authors":[{"name":"W. Bajw"},{"name":"J. Haup"},{"name":"G. Ra"},{"name":"S. Wrigh"},{"name":"R. Nowak"}],"title":{"text":"Toeplitz- structured compressed sensing matrices"}},{"authors":[{"name":"D. L. Donoh"},{"name":"M. Elad"}],"title":{"text":"Optimally sparse representation in general (nonorthogonal) dictionaries via 1 minimization"}},{"authors":[{"name":"J. Tropp"}],"title":{"text":"On The Conditioning of Random Subdictionaries"}},{"authors":[{"name":"R. Calderban"},{"name":"S. Jafarpour"},{"name":"O. International Conference on Sequence"}],"title":{"text":"Reed-Muller sensing matrices and the LASS their Applications (SETA), pp"}},{"authors":[{"name":"R. Calderban"},{"name":"J. Camero"},{"name":"M. Kanto"},{"name":"J. Seidel"}],"title":{"text":"A"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565425.pdf"},"links":[{"id":"1569566321","weight":6},{"id":"1569565227","weight":9},{"id":"1569567005","weight":6},{"id":"1569564469","weight":3},{"id":"1569565123","weight":3},{"id":"1569566157","weight":3},{"id":"1569566497","weight":12},{"id":"1569566895","weight":6},{"id":"1569567009","weight":3},{"id":"1569564337","weight":12},{"id":"1569566167","weight":12},{"id":"1569566753","weight":6},{"id":"1569567015","weight":3},{"id":"1569554689","weight":3},{"id":"1569565559","weight":9},{"id":"1569566913","weight":6},{"id":"1569565929","weight":3},{"id":"1569566141","weight":6},{"id":"1569566115","weight":3},{"id":"1569555787","weight":3},{"id":"1569566245","weight":3},{"id":"1569565885","weight":3},{"id":"1569566929","weight":6},{"id":"1569557715","weight":6},{"id":"1569566983","weight":3},{"id":"1569566873","weight":12},{"id":"1569565353","weight":6},{"id":"1569552025","weight":6},{"id":"1569566715","weight":3},{"id":"1569566755","weight":6},{"id":"1569565529","weight":6},{"id":"1569566001","weight":3},{"id":"1569567691","weight":3},{"id":"1569562367","weight":3},{"id":"1569565997","weight":3},{"id":"1569566635","weight":3},{"id":"1569566611","weight":6},{"id":"1569566125","weight":12},{"id":"1569566825","weight":3},{"id":"1569566443","weight":3},{"id":"1569565515","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T9.1","endtime":"11:50","authors":"Sina Jafarpour, Marco F Duarte, Robert Calderbank","date":"1341401400000","papertitle":"Beyond Worst-Case Reconstruction in Deterministic Compressed Sensing","starttime":"11:30","session":"S10.T9: Compressive Sensing and Algorithms","room":"Stratton West Lounge (201)","paperid":"1569565425"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
