{"id":"1569565123","paper":{"title":{"text":"Sumset Inequalities for Differential Entropy and Mutual Information"},"authors":[{"name":"Ioannis Kontoyiannis"},{"name":"Mokshay Madiman"}],"abstr":{"text":"Abstract\u2014The Pl ¨unnecke-Ruzsa sumset theory gives bounds connecting the cardinality of the sumset A + B deﬁned as {a + b ; a ∈ A, b ∈ B} with the cardinalities of the original sets A, B. For example, the sum-difference bound states that, |A+B| |A| |B| ≤ |A−B| 3 , where A−B = {a−b ; a ∈ A, b ∈ B}.\nInterpreting the differential entropy h(X) as (the logarithm of) the size of the effective support of X, the main results here are a series of natural information-theoretic analogs for these bounds. For example, the sum-difference bound becomes the new inequality, h(X + Y ) + h(X) + h(Y ) ≤ 3h(X − Y ), for independent X, Y . Our results include differential-entropy versions of Ruzsa\u2019s triangle inequality, the Pl ¨unnecke-Ruzsa inequality, and the Balog-Szemer´edi-Gowers lemma. Versions of most of these results for the discrete entropy H(X) were recently proved by Tao, relying heavily on a strong, functional form of the submodularity property of H(X). Since differential entropy is not functionally submodular, in the continuous case many of the corresponding discrete proofs fail, in several cases requiring substantially new proof strategies. The basic property that naturally replaces functional submodularity is the data processing property of mutual information."},"body":{"text":"The ﬁeld of additive combinatorics [14] provides tools that allow us to count the number of occurrences of particular additive structures in speciﬁc subsets of a discrete group. The prototypical example is the study of the existence of arithmetic progressions within speciﬁc sets of integers \u2013 as opposed to the multiplicative structure that underlies prime factorization and much of classical combinatorics and number theory. There have been several major developments and a lot of high- proﬁle mathematical activity in connection with additive com- binatorics in recent years, perhaps the most famous example being the celebrated Green-Tao theorem on the existence of arbitrarily long arithmetic progressions within the set of prime numbers.\nThe Pl¨unnecke-Ruzsa sumset theory offers an important collection of tools, consisting primarily of sumset inequalities [14]. The sumset A+B of two discrete sets A and B is deﬁned as, A + B = {a + b : a ∈ A, b ∈ B}, and a sumset inequality is an inequality connecting the cardinality |A + B| of A + B\nwith the cardinalities |A|, |B| of A and B, respectively. For example, we have the obvious bounds,\nas well as much more subtle results, like the Ruzsa triangle inequality [10],\nall of which hold for arbitrary subsets A, B, C of the integers or any other discrete abelian group, and where A−B = {a−b : a ∈ A, b ∈ B}.\nRecall that Shannon\u2019s asymptotic equipartition property (AEP) [2] says that the entropy H(X) of a discrete ran- dom variable X can be thought of as the logarithm of the effective cardinality of the alphabet of X. This suggests a correspondence between bounds for the cardinalities of sumsets like, e.g., |A + B|, and corresponding bounds for the entropy of sums of discrete random variables, e.g., H(X +Y ). First identiﬁed by Ruzsa [11], this connection has also been explored in the last few years in different directions by, among others, Tao and Vu [15], Lapidoth and Pete [6], Madiman and Kontoyiannis [8], and Madiman, Marcus and Tetali [9].\nMost recently (and most extensively), this connection was developed by Tao in [13]. The main idea is to replace sets by (independent, discrete) random variables, and then replace the log-cardinality, log |A|, of each set A by the (discrete, Shannon) entropy of the corresponding random variable. Thus, for independent discrete random variables X, Y, Z, the simple bounds (1) become [2], respectively,\nH(X − Z) + H(Y ) ≤ H(X − Y ) + H(Y − Z) and H(X + Y ) + H(X) + H(Y ) ≤ 3H(X − Y ).\nFollowing recent work reported in [7][8], our main mo- tivation is to examine the extent to which this analogy can be carried further: According to the AEP [2], the differential entropy h(X) of a continuous random variable X can be thought of as the logarithm of the \u201csize of the effective support\u201d of X. In this work we state and prove natural \u201cdifferential entropy analogs\u201d of various sumset bounds, many of which were proved for the discrete Shannon entropy in [4][15][7][11][13][9].\nThe main technical ingredient in the proofs of many of the corresponding discrete bounds was a strong, functional form of the submodularity property of the discrete Shannon entropy; see Section II. The fact that differential entropy is not function- ally submodular was the source of the main difﬁculty as well as the excitement for the present development. Instead, the main technical ingredient in our proofs is the data processing property of mutual information. Indeed, most of our results can be equivalently stated in terms of mutual information instead of differential entropy. And since data processing is universal in that it holds regardless of the space in which the relevant random variables take values, these proofs offer alternative derivations for the discrete counterparts of these results.\nIn view of the fact that additive noise is one of the most common modeling assumptions in Shannon theory, it is natural to expect that the bounds developed here may have appli- cations in core information-theoretic problems. Preliminary connections in this direction can be found in the recent work [1][3][16].\nThroughout the paper, log denotes the natural logarithm log e , the discrete entropy is deﬁned in nats (in terms of log e ), and the alphabet of any discrete random variable X is assumed to be a (ﬁnite or countably inﬁnite) subset A of the real line or of an arbitrary discrete abelian group. Perhaps the simplest bound on the entropy H(X+Y ) of the sum of two independent random variables X, Y is,\nH(X) + H(Y ) = H(X, Y ) = H(Y, X + Y ) = H(X + Y ) + H(Y |X + Y )\nand similarly with the roles of X and Y interchanged. The ﬁrst and third equalities follow from the chain rule and independence, the second equality follows from the \u201cdata processing\u201d property that H(F (Z)) = H(Z) if F is a one- to-one function, and the inequality follows from the fact that conditioning reduces entropy.\nA similar argument using the nonnegativity of conditional entropy [2],\n= H(X + Y ) + H(Y |X + Y ) ≥ H(X + Y ),\nOur starting point is the recent work of Tao [13], where a series of sumset bounds are established for H(X), beginning with the elementary inequalities (4) and (5). The arguments in [13] are largely based on the following important observation [13][9]:\nLemma 2.1 (Functional submodularity of discrete entropy): If X 0 = F (X 1 ) = G(X 2 ) and X 12 = R(X 1 , X 2 ), then:\nProof. By data processing for mutual information and en- tropy, H(X 1 ) + H(X 2 ) − H(X 12 ) ≥ H(X 1 ) + H(X 2 ) − H(X 1 , X 2 ) = I(X 1 ; X 2 ) ≥ I(X 0 ; X 0 ) = H(X 0 ).\nOur main goal in this work is to examine the extent to which the bounds in [13] and in earlier work extend to the continuous case. The differential entropy h(X) of a continuous random variable (or random vector) X is deﬁned in nats, and in order to avoid uninteresting technicalities, we assume throughout that the differential entropies in the statements of all our results exist and are ﬁnite.\nThe ﬁrst important difference between H(X) and h(X) is that the differential entropy of function of X is typically different from that of X itself, even for linear functions [2]: For any continuous random vector X and any nonsingular matrix T , h(T X) = h(X) + log |det(T )|, which is different from h(X) unless T has determinant equal to ±1.\nThe upper bound in (5) also fails in general for independent continuous X, Y : Take, e.g., X, Y to be independent Gaus- sians, one with variance σ 2 > 2πe and the other with variance 1/σ 2 . And the functional submodularity Lemma 2.1 similarly fails for differential entropy. For example, taking X 1 = X 2 an arbitrary continuous random variable with ﬁnite entropy, F (x) = G(x) = x and R(x, x ) = ax for some a > 1, the obvious differential-entropy analog of Lemma 2.1 yields log a ≤ 0.\nOn the other hand, the simple lower bound in (5) does generalize,\nas can be easily seen using standard properties of the entropy and mutual information.\nOur overall development will be largely based on the idea that the use of functional submodularity can be avoided by reducing the inequalities of interest to data-processing inequalities for appropriately deﬁned mutual informations. This reduction is sometimes straightforward, but sometimes far from obvious.\nUnless explicitly stated otherwise, all random variables are assumed to be continuous (i.e., taking real values with an absolutely continuous density with respect to Lebesgue measure), and the differential entropy of any random variable or random vector appearing in the statement of any of our results is assumed to exist and be ﬁnite.\nA. Ruzsa distance and the doubling and difference constants In analogy with the corresponding deﬁnition for discrete\nrandom variables [13], we deﬁne the Ruzsa distance between any two continuous random variables X and Y as,\nwhere X ∼ X and Y ∼ Y are independent. It is obvious that dist R is symmetric, and it is nonnegative because of the lower bound in (6). Our ﬁrst result states that it also satisﬁes the triangle inequality:\nTheorem 3.1 (Ruzsa triangle inequality): If X, Y, Z are in- dependent, then:\nThe proof of the discrete version of this result in [13] is based on the discrete entropy analog of the bound,\nwhich is proved using functional submodularity. Although in general it fails for differential entropy, we may try to adapt the proof of Lemma 2.1 itself in this particular setting. But the obvious modiﬁcation of the discrete proof in the continuous case also fails; the analog of the ﬁrst inequality in the proof of Lemma 2.1, corresponding to H(X 12 ) ≤ H(X 1 , X 2 ), is,\nwhich is false, since the last term, h(X − Y, Y − Z, X, Z) = −∞. Nevertheless, the actual inequality (7) does hold true.\nLemma 3.2: The inequality (7) holds true for any three independent random variables X, Y, Z, and it is equivalent to the following data processing inequality:\nThe proof of Theorem 3.1 based on Lemma 3.2 and the proof of the lemma itself are both given in [8].\nThe proof of Lemma 3.4 is given in [7][8]. Combining the last two lemmas, yields:\nTheorem 3.5 (Doubling-difference inequality): If X 1 , X 2 are independent and identically distributed (i.i.d.), then:\n1 2\n1 2\nIf we deﬁne the doubling constant and the difference con- stant of a random variable X as,\nσ[X] = exp{h(X + X ) − h(X)} and \t δ[X] = exp{h(X − X ) − h(X)},\nrespectively, where X is an independent copy of X, then Theorem 1 says that:\nCorollary 3.6: For any random variable X, 1\n2 dist R (X, X) ≤ dist R (X, −X) ≤ 2dist R (X, X), equivalently,\nNote. As mentioned on pp. 64-65 of [14], the analog of the above upper bound, σ[X] ≤ δ[X] 2 , in additive combinatorics is established via an application of the Pl¨unnecke-Ruzsa in- equalities. It is interesting to note that the entropy version of this result (both in the discrete and continuous case) can be deduced directly from elementary arguments. Perhaps this is less surprising in view of the fact that strong versions of the Pl¨unnecke-Ruzsa inequality can also be established by elementary methods in the entropy setting. See Section III-B and the discussion in [12].\nThe proofs of Theorem 3.5 and Corollary 3.6 are given in [8].\nWe now come to the ﬁrst result whose proof in the contin- uous case is necessarily signiﬁcantly different than its discrete counterpart. It is also the only major result here for which we give a complete proof. For the proofs of all other results, see [5].\nTheorem 3.7 (Sum-difference inequality): For any two in- dependent random variables X, Y :\nThe equivalence of (9) and (8) follows simply from the deﬁnition of the Ruzsa distance. Before giving the proof, we state and prove the following simple version of the theorem in terms of mutual information:\nFor any pair of independent random variables X, Y , and all 0 ≤ α ≤ 1:\nProof. Applying (8) with X, Y in place of X , Y , and subtracting h(X) from both sides, yields,\nRepeating the same argument, this time subtracting h(Y ) instead of h(X) from both sides, gives,\nMultiplying (10) by α, (11) by (1 − α), and adding the two inequalities gives the stated result.\nThe main result (8) of Theorem 3.7 is a simple consequence of the following proposition.\nProposition 3.9: Suppose X, Y are independent, let Z = X − Y , and let (X 1 , Y 1 ) and (X 2 , Y 2 ) be two conditionally independent versions of (X, Y ) given Z. If (X 3 , Y 3 ) ∼ (X, Y ) are independent of (X 1 , Y 1 , X 2 , Y 2 ), then:\nThe proof of the discrete analog of the bound (12) in [13] contains two important steps, both of which fail for differential entropy. First, functional submodularity is used to deduce the discrete version of,\nbut (13) is trivial because the ﬁrst term above is equal to −∞. Second, the following simple mutual information identity (implicit in [13]) fails: If Z = F (X) and X, X\nare conditionally independent versions of X given Z, then I(X; X ) = H(Z). Instead, for continuous random variables, Z and X are conditionally independent given X , and hence,\nProof. Expanding and using elementary properties, we have that h(Z, Y 1 , Y 2 ) + h(Z) − h(Y 1 ) − h(Y 2 ) equals,\n= h(Y 1 |Z) + h(Y 2 |Z) + 2h(Z) − h(Y 1 ) − h(Y 2 ) = h(Y 1 , Z) + h(Y 2 , Z) − h(Y 1 ) − h(Y 2 ) = 2h(Z) − I(Y 1 ; Z) − I(Y 2 ; Z)\nProof of Proposition 3.9. The most important step of the proof is the realization that the (trivial) result (13) needs to be replaced by the following:\nwhich, combined with Lemma 3.10, gives the required result. To establish (14) we ﬁrst note that, by construction, X 1 −\nwhere the last equality follows from the fact that the linear map, (z, y 1 , y 2 , y 3 , x 3 ) → (x 3 − y 2 , y 1 + z − y 3 , y 2 + z, y 1 , x 3 ), has determinant 1. Rearranging and using the independence of X 3 and Y 3 gives (14) and completes the proof.\nIn additive combinatorics, the Pl¨unnecke-Ruzsa inequality for iterated sumsets is a subtle result with an involved proof based on the theory of commutative directed graphs; see Chapter 6 of [14]. It is interesting that its entropy version can be proved as a simple consequence of the data processing bound in Lemma 3.4; see [5] for details.\nTheorem 3.11 (Pl¨unnecke-Ruzsa inequality): Suppose that the random variables X, Y 1 , Y 2 , . . . , Y n are independent, and that, for each i, Y i is only weakly dependent on (X + Y i ), in that I(X + Y i ; Y i ) ≤ log K i for ﬁnite constants K 1 , K 2 , . . . , K n . In other words,\nBy an application of the entropy Pl¨unnecke-Ruzsa inequality we can establish the following bound on iterated sums; see [5] for the proof.\nTheorem 3.12 (Iterated sum bound): Suppose that X and Y are independent random variables, let (X 0 , Y 0 ), (X 1 , Y 1 ), . . . , (X n , Y n ) be i.i.d. copies of (X, Y ), and write S i = X i + Y i for the sums of the pairs, i = 0, 1, . . . , n. Then:\nThe differential entropy version of the Balog-Szemer´edi- Gowers lemma stated next says that, if X, Y are weakly dependent and X + Y has small entropy, then there exist conditionally independent versions of X, Y that have almost the same entropy, and whose independent sum still has small entropy.\nTheorem 3.13: (Balog-Szemer´edi-Gowers lemma) Suppose that X, Y are weakly dependent in the sense that I(X; Y ) ≤ log K, i.e.,\nh(X) + 1 2\nLet X 1 , X 2 be conditionally independent versions of X given Y , and let Y be a conditionally independent version of Y , given X 2 and Y ; in other words, the sequence X 2 , Y, X 1 , Y\nh(X 2 |X 1 , Y ) ≥ h(X) − log K h(Y |X 1 , Y ) ≥ h(Y ) − log K\nFollowing the corresponding development in [13] for dis- crete random variables, ﬁrst we establish a weaker result in the following proposition.\nProposition 3.14: (Weak Balog-Szemer´edi-Gowers lemma) Under the assumptions of Theorem 3.13, we have:\nThe proofs of the last two results are both signiﬁcantly different from the proofs of the corresponding discrete versions in [13]. The main step, in each case, is the identiﬁcation of the \u201ccorrect\u201d data processing bound that needs to replace the use of functional submodularity. See [5] for details."},"refs":[{"authors":[{"name":"S. Cohe"},{"name":"R. Zamir"}],"title":{"text":"A"}},{"authors":[{"name":"M. Cove"},{"name":"A. Thomas"},{"name":"J. Wile"}],"title":{"text":"T"}},{"authors":[{"name":"H. Etki"},{"name":"E. Ordentlich"}],"title":{"text":"R"}},{"authors":[{"name":"A. Kaimanovic"},{"name":"M. Vershik"}],"title":{"text":"V"}},{"authors":[{"name":"I. Kontoyianni"},{"name":"M. Madiman"}],"title":{"text":"Sumset and inverse sumset inequal- ities for differential entropy and mutual information"}},{"authors":[{"name":"A. Lapidot"},{"name":"G. Pete"}],"title":{"text":"On the entropy of the sum and of the difference of two independent random variables"}},{"authors":[{"name":"M. Madiman"}],"title":{"text":"On the entropy of sums"}},{"authors":[{"name":"M. Madima"},{"name":"I. Kontoyiannis"}],"title":{"text":"The entropies of the sum and the difference of two IID random variables are not too different"}},{"authors":[{"name":"M. Madima"},{"name":"A. Marcu"},{"name":"P. Tetali"}],"title":{"text":"Entropy and set cardinality inequalities for partition-determined functions, with applications to sum- sets"}},{"authors":[{"name":"Z. Ruzsa"},{"name":"V. Chudnovsky D"},{"name":"V. Chudnovsk"},{"name":"B. Nathanso"}],"title":{"text":"I"}},{"authors":[{"name":"Z. Ruzsa"}],"title":{"text":"I"}},{"authors":[{"name":"T. Tao"}],"title":{"text":"An entropy Pl¨unnecke-Ruzsa inequality"}},{"authors":[{"name":"T. Tao"}],"title":{"text":"Sumset and inverse sumset theory for Shannon entropy"}},{"authors":[{"name":"T. Ta"},{"name":"V. Vu"}],"title":{"text":"Additive combinatorics "}},{"authors":[{"name":"T. Ta"},{"name":"V. Vu"}],"title":{"text":"Entropy methods"}},{"authors":[{"name":"Y. W"},{"name":"S. Shamai (Shitz"},{"name":"S. Verd´u"}],"title":{"text":"A general formula for the degrees of freedom of the interference channel"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565123.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T8.3","endtime":"15:40","authors":"Ioannis Kontoyiannis, Mokshay Madiman","date":"1341328800000","papertitle":"Sumset Inequalities for Differential Entropy and Mutual Information","starttime":"15:20","session":"S7.T8: Information Inequalities","room":"Stratton (491)","paperid":"1569565123"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
