{"id":"1569553537","paper":{"title":{"text":"Source Coding With Delayed Side Information"},"authors":[{"name":"Osvaldo Simeone"},{"name":"Haim H. Permuter"}],"abstr":{"text":"Abstract\u2014For memoryless sources, delayed side information at the decoder does not improve the rate-distortion function. However, this is not the case for more general sources with memory, as demonstrated by a number of works focusing on the special case of (delayed) feedforward. In this paper, a setting is studied in which the side information is delayed and the encoder is informed about the side information sequence. Assuming a hidden Markov model for the sources, at ﬁrst, a single-letter characterization is given for the set-up where the side information delay is arbitrary and known at the encoder, and the reconstruction at the destination is required to be (near) lossless. Then, with delay equal to zero or one source symbol, a single-letter characterization is given of the rate-distortion function for the case where side information may be delayed or not, unbeknownst to the encoder. Finally, an example for a binary source is provided."},"body":{"text":"Consider a sensor network in which a sensor measures a certain physical quantity Y i over time i = 1, 2, ...n. The aim of the sensor is communicating a processed version X n = (X 1 , ..., X n ) of the measured sequence Y n = (Y 1 , ..., Y n ) to a receiver. As an example, each element X i could be obtained by quantizing Y i , for i = 1, 2, ...n. To this end, the sensor communicates a message M of nR bits to the receiver, based on the observation of X n and Y n ( R is the message rate in bits per source symbol). The receiver is endowed with sensing capabilities and hence it can measure the physical quantity Y n as well. However, due to the fact that the receiver is located further away from the physical source, such measure may come with a delay of d symbols. In other words, when estimating X i , the receiver has available not only the message M received from the sensor, but also the sequence Y i−d = (Y 1 , ..., Y i−d ), so that the estimate Z i is a function of M and Y i−d . Delay d may or may not be known at the sensor 1 . The situation described above can be illustrated schematically as in Fig. 1 and in Fig. 2, where Fig. 1 models the case where the delay d is known to the sensor (i.e., the encoder), while 2 accounts for a setting where the side information at the decoder, unbeknownst to the encoder, may be delayed by d or not delayed.\nPrior work: If sequences X n and Y n are memoryless, from available results [2][3], it can be inferred that: (i) for zero delay, i.e., d = 0, the performance of the systems in Fig. 1-2 would remain unchanged even if the decoder(s) had access to non-causal side information, in which case the decision about Z ji , j = 1, 2, at each time i, could be based on the entire sequence Y n , rather than only Y i ; and (ii) for strictly positive delay d > 0, delayed side information does not improve performance. However, these conclusions do not generally hold if the sources have memory.\nFor sources with memory, a number of works have focused on the scenario of Fig. 1 where X i = Y i , which entails that the decoder observes sequence X n itself with a delay of d symbols. This setting is typically referred to as source coding with feedforward, as introduced in [5]. Reference [1] derives the rate-distortion function for this problem (i.e., Fig. 1 with X i = Y i ) for ergodic and stationary sources in terms of multi-letter mutual informations 2 . This function is explicitly evaluated for some special cases in [2][4] (see also [6]), while an algorithm for its numerical calculation is also proposed in [4]. The more general case of Fig. 1 with X i = Y i is studied in [7] assuming stationary and ergodic sources X n and Y n . The rate-distortion function is expressed in terms of multi-letter mutual informations, and no speciﬁc examples are provided for which the function is explicitly computable. Moreover, extensions of the characterization of achievable rate-distortion trade-offs to the setting of Fig. 2 for sources with memory has not, to the best of the authors\u2019 knowledge, been studied. We ﬁnally remark that for more complex networks than the ones studied here, strictly delayed side information may be useful\nalso in the presence of memoryless sources. This is illustrated in [9] for a multiple description problem with feedforward.\nContributions: In this work, we assume that the source Y n is a Markov chain, and X n is such that X i is obtained by pass- ing Y i through a memoryless channel q(x|y) for i = 1, ..., n, i.e., X n corresponds to a hidden Markov model. Note that the latter may model a symbol-by-symbol processing of source Y n as per the initial example. We derive a single-letter character- ization of the minimal rate (bits/source symbol) required for (near) lossless compression in the scenario of Fig. 1 for any de- lay d ≥ 0 (Sec. III). Achievability is based on a novel scheme that consists of simple multiplexing/demultiplexing operations along with standard entropy coding techniques. Furthermore, we derive a single-letter characterization of the minimal rate (bits/source symbol) required for lossy compression in the scenarios of Fig. 1 and Fig. 2 for delays d = 0 and d = 1 (Sec. IV). Finally, we study the speciﬁc example of a binary- alphabet source with Hamming distortion (Sec. V).\nNotation: For a, b integer with a ≥ b, we deﬁne [a, b] as the interval [ a, a + 1, ..., b] and x b a = (x a , ..., x b ); if instead a < b we set [ a, b] = ∅ and x b a = ∅. We will also write x b 1 for x b for simplicity of notation. Given a sequence x n = [x 1 , ..., x n ] and a set I = {i 1 , ..., i |I| } ⊆ [1, n], we deﬁne sequence x I as x I = [x i 1 , x i 2 , ..., x i |I| ] where i 1 ≤ ... ≤ i |I| .\nWe present the system model for the scenario of Fig. 2, as the scenarios of Fig. 1 follows as a special case. The random process Y i ∈ Y, i ∈ {..., −1, 0, 1, ...}, measured at the encoder, and, possibly with delay, at the decoders, is a stationary and ergodic Markov chain with transition probabil- ity Pr[ Y i = a|Y i−1 = b] = w 1 (a|b). We deﬁne the probability Pr[Y i = a] π(a) and also the k-step transition probability Pr[Y i = a i |Y i−k = b] w k (a|b), which are both indepen- dent of i by stationarity of Y i . We also set, for notational convenience, w 0 (a|b) = π(a). Sequence Y n = (Y 1 , ..., Y n ) is thus distributed as p(y n ) = π(y 1 ) n i=2 w 1 (y i |y i−1 ) for any integer n > 0. The random process X i ∈ X , i ∈ {..., −1, 0, 1, ...}, measured only at the encoder, is such that vector X n = (X 1 , ..., X n ) ∈ X n , for any integer n > 0, is\nIn other words, process X i ∈ X , i ∈ {..., −1, 0, 1, ...} cor- responds to a hidden Markov model with underlying Markov process given by Y n .\nAn ( d, n, R, D 1 , D 2 ) code, with delay d ≥ 0, is deﬁned by: (i) An encoder function\nf: ( X n × Y n ) → [1, 2 nR ], \t (2) which maps sequences X n and Y n into message M ∈ [1, 2 nR ]; (ii) a sequence of decoding functions for decoder 1\ng 1i : [1 , 2 nR ] × Y i−d → Z 1 , \t (3) for i ∈ [1, n], which, at each time i, map message M, or rate R [bits/source symbol], and the delayed side information Y i−d into the estimate Z 1i ∈ Z 1 ; (iii) a sequence of decoding function for decoder 2\nfor i ∈ [1, n], which, at each time i, map messages M and the non-delayed side information Y i into the estimate Z 2i ∈ Z 2 . Encoding/decoding functions (2)-(4) must satisfy the distortion constraints\n1 n\nwhere the distortion metrics d j (x, y, z j ): X × Y × Z j → [0, d max ] are such that 0 ≤ d j (x, y, z j ) ≤ d max < ∞ for all ( x, y, z) ∈ X × Y × Z j for j = 1, 2. Note that these constraints are fairly general in that they allow to impose not only requirements on the lossy reconstruction of X i or Y i (obtained by setting d j (x, y, z j ) independent of y or x, respectively), but also on some function of both X i and Y i (by setting d j (x, y, z j ) to be dependent on such function of ( x, y)).\nGiven a delay d ≥ 0, for a distortion pair (D 1 , D 2 ), we say that rate R is achievable if, for every > 0 and sufﬁciently large n, there exists a (d, n, R, D 1 + , D 2 + ) code. We refer to the inﬁmum of all achievable rates for a given distortion pair ( D 1 , D 2 ) and delay d as the rate-distortion function R d (D 1 , D 2 ). For the setting of Fig. 1, we similarly deﬁne the rate-distortion function R d (D 1 ).\nHere we consider the setting of Fig. 1 and we characterize the rate-distortion function R d (D 1 ) for any delay d ≥ 0 under the Hamming distortion metric (i.e., d 1 (x, y, z 1 ) = 1(x = z 1 ), where 1(a) = 1 if a is true and 1(a) = 0 otherwise) for D 1 = 0. In other words, we impose that the sequence\nX n be recovered with vanishingly small average symbol error probability as n → ∞ at the decoder. We refer to this scenario as (near) lossless. We have the following characterization of R d (0).\nProposition 1. For any delay d ≥ 0, the rate-distortion function for the set-up in Fig. 1 under Hamming distortion is given at D 1 = 0 by\nwhere the conditional entropy is calculated with respect to the distribution\np(y 1 , x 1 ) = π(y 1 )q(x 1 |y 1 ) for d = 0, \t (7) and p(y 1 , x 2 , ..., x d+1 ) = π(y 1 ) \t (8)\nThe proof of achievability is sketched below. Details can be found in [10], along with the proof of the converse.\nRemark 2. Proposition 1 provides a \u201csingle-letter\u201d characteri- zation of R d (0) for the setting of Fig. 1, since it only involves a ﬁnite number of variables. This contrasts with the general characterization for stationary ergodic processes of R d (D) (in the general lossy case D ≥ 0) given in [7], which is a \u201cmulti-letter\u201d expression, whose computation can generally only attempted numerically using approaches such as the ones proposed in [4]. Note that a multi-letter expression is also given in [2] to characterize R d (D) for negative delays d < 0. Remark 3. By setting d = 0 in (6) we obtain R 0 (0) = H(X 1 |Y 1 ). This result generalizes [2, Remark 3, p. 5227] from i.i.d. sources ( X n , Y n ) to the hidden Markov model (1) considered here. Note that, for d = 1, we instead obtain R 1 (0) = H(X 2 |Y 1 ). As another notable special case, if side information is absent, or equivalently if d → ∞, in accordance to well-known results, we obtain that R ∞ (0) equals the entropy rate H(X ).\nRemark 4. Is delayed side information useful (when known also at the encoder)? That this is generally the case follows from the inequality R d (0) = H(X d+1 |X d 2 , Y 1 ) ≤ R ∞ (0) = H(X ), since R ∞ (0) is the required rate without side infor- mation. However, the inequality above may not be strict, and thus side information may not be useful. This is the case for instance if X i is an i.i.d. process or in the setting of source coding with feedforward [5], [1], i.e., X i = Y i , with a Markov source X n . We will see below that the conclusion that feedforward is not useful for Markov sources need not hold for lossy compression (i.e., for D 1 > 0).\nProof: (Achievability) Here we propose a coding scheme that achieves rate (6). The basic idea is a non-trivial extension of the approach discussed in [2, Remark 3, p. 5227] and is\ndescribed as follows. A block diagram is shown in Fig. 3 for encoder (Fig. 3-(a)) and decoder (Fig. 3-(b)). We ﬁrst describe the encoder, which is illustrated in Fig. 3-(a). To encode sequences ( x n , y n ) ∈ (X n ×Y n ), we ﬁrst partition the interval [1 , n] into |X | d−1 |Y| subintervals, which we denote as I(˜x d−1 , ˜ y) ⊆ [1, n], for all ˜ x d−1 ∈ X d−1 and ˜ y ∈ Y. Every such subinterval I(˜x d−1 , ˜ y) is deﬁned as\nI(˜x d−1 , ˜ y) = {i: i ∈ [1, n] and y i−d = ˜ y, x i−1 i−d+1 = ˜ x d−1 }. (9)\nIn words, the subinterval I(˜x d−1 , ˜ y) contains all symbol indices i such that the corresponding delayed side information available at the decoder is y i−d = ˜ y and the previous d−1 sam- ples in x n are x i−1 i−d+1 = ˜ x d−1 . For the out-of-range indices i ∈ [−d + 1, 0], one can assume arbitrary values for x i ∈ X and y i ∈ Y, which are also shared with the decoder once and for all. Note that ˜x d−1 ∈X d−1 , ˜ y∈Y I(˜x d−1 , ˜ y) = [1, n]. Fig. 4 illustrates the deﬁnitions at hand for d = 2.\nAs a result of the partition described above, the en- coder \u201cdemultiplexes\u201d sequence x n into |X | d−1 |Y| sequences x I(˜x d−1 ,˜ y) , one for each tuple (˜ x d−1 , ˜ y) ∈ X d−1 ×Y. This demultiplexing operation, which is controlled by the previous values of source and side information, is performed in Fig. 3-(a) by the block labelled as \u201cDemux\u201d, and an example of its operation is shown in Fig. 4. By the ergodicity of process X i and Y i , for every > 0 and all sufﬁciently large n, the length of any sequence x I(˜x d−1 ,˜ y 1 ) is guaranteed to be less than np Y 1 X 2 ,...,X d (˜ y, ˜ x d−1 ) + symbols with abitrarily large probability.\nThe entropy encoder can be implemented in different ways, e.g., using typicality or Huffman coding. Here we consider a typicality-based encoder. Note that the entries X i of each sequence X I(˜x d−1 ,˜ y) are i.i.d. with distribution\np X d+1 |Y 1 X 2 ,...,X d ( ·|˜y, ˜x d−1 ), since conditioning on the event {y i−d = ˜ y, x i−1 i−d+1 = ˜ x d−1 } makes the random variables X i independent. Therefore, a rate in bits per source symbol of H(X d+1 |X d 2 = ˜ x d−1 , Y 1 = ˜ y) + is sufﬁcient for the entropy encoder to label all -typical sequences.\nWe now describe the decoder, which is illustrated in Fig. 3-(b). By undoing the multiplexing operation just described, the decoder, from the message M , can recover the individual sequences x I(˜x d−1 ,˜ y) through a simple demultiplexing opera- tion for all ˜ x d−1 ∈ X d−1 and ˜ x d−1 ∈ X d−1 . This operation is represented by block \u201cDemux\u201d in Fig. 3-(b). However, while the individual sequences x I(˜x d−1 ,˜ y) can be recovered through the discussed demultiplexing operation, this does not imply that the decoder is also able to reorder the symbols in the sequences so as to obtain the original sequence x n . However, note that at time i, the decoder knows Y i−d and the previously decoded X i−1 and can thus identify the subinterval I(˜x d−1 , ˜ y) to which the current symbol X i belongs. This symbol can be then immediately read as the next yet-to-be-read symbol from the corresponding sequence x I(˜x d−1 ,˜ y) . Note that for the ﬁrst d symbols, the decoder uses the values for x i and y i at the out- of-range indices i that were agreed upon with the encoder (see above). A more detailed description, including the analysis of the impact of errors, can be found in [10].\nIn this section, we consider the general problem of lossy compression for the set-up of Fig. 2, and we obtain an achievable rate R (a) d (D 1 , D 2 ) ≥ R d (D 1 , D 2 ) for all delays d ≥ 0 and prove that such rate equals the rate-distortion function, i.e., R (a) d (D 1 , D 2 ) = R d (D 1 , D 2 ), for d = 0 and d = 1.\nProposition 5. For any delay d ≥ 0 and distortion pair (D 1 , D 2 ) , the following rate is achievable for the setting of Fig. 2\n(10) = min I(Y ; Z 1 |Y d ) + I(X; Z 1 Z 2 |Y Y d ), (11)\nwith mutual informations evaluated with respect to the joint distribution\n(12) and where minimization is done over all conditional distribu- tions p(z 1 , z 2 |x, y, y d ) such that\nMoreover, rate (10)-(11) is the rate-distortion function, i.e., R (a) d (D 1 , D 2 ) = R d (D 1 , D 2 ) , for d = 0 and d = 1.\nRemark 6. Rate (10) can be easily interpreted in terms of achievability. To this end, we remark that variable Y d plays the role of the delayed side information Y i−d at decoder 1. The coding scheme achieving rate (10) operates in two successive phases. In the ﬁrst phase, the encoder encodes the reconstruction sequence Z n 1 for decoder 1. Since decoder 1 has available delayed side information, using a strategy similar to the one discussed in Sec. III-A, this operation requires I(XY ; Z 1 |Y d ) bits per source sample. Note that decoder 2 is able to recover Z n 1 as well, since decoder 2 has available side information Y i , and thus also the delayed side information Y i−d . In the second phase, the reconstruction sequence Z n 2 for decoder 2 is encoded. Given the side information available at decoder 2, this operation requires rate I(X; Z 2 |Y Y d Z 1 ), using again an approach similar to the one discussed in Sec. III-A. Details can be found in [10], along with the converse proof.\nRemark 7. For memoryless sources X n and Y n , by compar- ison with the results in [3], it can be concluded that delayed side information is not useful for memoryless sources. This conclusion generalizes the result of [2], which applies for the setting of Fig. 1 in the special case of feedforward (i.e., X i = Y i ).\nNote that, by setting D 2 = d max in R (a) d (D 1 , D 2 ), we obtain an achievable rate R (a) d (D 1 ) for the setting of Fig. 1 (see [10] for details).\nIn this section, we assume that Y i is a binary Markov chain with symmetric transition probabilities w 1 (1 |0) = w 1 (0 |1)\nε and we assume that X i = Y i ⊕ N i , with \u201c⊕\u201d being the modulo-2 sum and N i being i.i.d. binary variables, indepen- dent of Y n , with p N i (1) q, q ≤ 1/2. Note that we have the k-step transition probabilities w k (1 |0) = w k (0 |1) ε (k) , which can be obtained recursively as ε (1) = ε and ε (k) = 2ε (k−1) (1 − ε (k−1) ) for k ≥ 2. We adopt the Hamming distortion d 1 (x, z 1 ) = x ⊕ z 1 .\nWe start by showing in Fig. 5 the rate R d (0) obtained from Proposition 1 corresponding to zero distortion ( D 1 = 0) versus the delay d for different values of ε and for q = 0.1. For d = 0, we have R 0 (0) = H(X 1 |Y 1 ) = H b (q) = 0.589, irrespective of the value of ε, where we have deﬁned the binary entropy function H b (a) = −a log 2 a − (1 − a) log 2 (1 − a). Instead, for d increasingly large, the rate R d (0) tends to the entropy rate\nR ∞ (0) = H(X ). Note that a larger memory, i.e., a smaller ε, leads to smaller required rate R d (0) for all values of d.\nFig. 6 shows the rate R d (0) for ε = 0.1 versus q for different values of delay d. For reference, we also show the performance with no side information, i.e., R ∞ (0) = H(X ). For q = 1/2, the source X n is i.i.d. and delayed side information is useless in the sense that R d (0) = R ∞ (0) = H(X 1 ) = 1 (Remark 4). Moreover, for q = 0, we have X i = Y i , so that X i is a Markov chain and the problem becomes one of lossless source coding with feedforward. From Remark 4, we know that delayed side information is useless also in this case, as R d (0) = R ∞ (0) = H(X ) = H b (ε) = 0.469. 3 For intermediate values of q, side information is generally useful, unless the delay d is too large.\nFinally, we evaluate the achievable rate of Proposition 2 for a general non-zero distortion D 1 (see details in [10]), obtaining\nfor 0 ≤ D 1 ≤ min{ε (d) ∗ q, 1 − ε (d) ∗ q} and R (a) d (D 1 ) = 0 otherwise, where p ∗ q p(1 − q) + (1 − p)q. This result with q = 0 (i.e., with feedforward) and d = 1 recovers the calculation in [5, Example 2] (see also [4]). We remark that the rate-distortion function of a Markov source X n without feedforward, i.e., R ∞ (D 1 ), is equal to H b (ε) − H(D 1 ) only for D 1 smaller than a critical value, but is otherwise larger [8]. This demonstrates that feedforward, unlike in the lossless setting discussed above, can be useful in the lossy case for distortion levels D 1 sufﬁciently large.\nA general information-theoretic characterization of the trade-off between rate and distortion for the problem of compressing information sources in the presence of delayed side information can be generally given in terms of multi- letter expressions, as done in [7]. In this work, we have instead focused on a speciﬁc class of sources, which evolve\naccording to hidden Markov models, and derived single- letter characterizations of the rate-distortion trade-off. Such characterizations are established based on simple achievable scheme that are based on standard \u201coff-the-shelf\u201d compression techniques. Moreover, we have extended the analysis to a more general set-up in which side information may or may not be delayed.\nThe work of O. Simeone was supported in part by the U.S. National Science Foundation under Grant No. 0914899. H. H. Permuter was supported in part by the Marie Curie Reintegration fellowship."},"refs":[{"authors":[{"name":"R. Venkataramanan"},{"name":"S. S. Pradhan"}],"title":{"text":"Source coding with feed- forward: Rate-distortion theorems and error exponents for a general source"}},{"authors":[{"name":"T. Weissman"},{"name":"A. El Gamal"}],"title":{"text":"Source coding with limited-look-ahead side information at the decoder"}},{"authors":[{"name":"A. H. Kaspi"}],"title":{"text":"Rate-distortion function when side-information may be present at the decoder"}},{"authors":[{"name":"I. Naiss"},{"name":"H. Permuter"}],"title":{"text":"Computable bounds for rate distortion with feed-forward for stationary and ergodic sources"}},{"authors":[{"name":"T. Weissman"},{"name":"N. Merhav"}],"title":{"text":"On competitive prediction and its relation to rate-distortion theory"}},{"authors":[{"name":"R. Venkataramanan"},{"name":"S. S. Pradhan"}],"title":{"text":"On computing the feedback capacity of channels and the feed-forward rate-distortion function of sources"}},{"authors":[{"name":"R. Venkataramanan"},{"name":"S. S. Pradhan"}],"title":{"text":"Directed information for com- munication problems with side-information and feedback/feed-forward"}},{"authors":[{"name":"R. Gray"}],"title":{"text":"Information rates of autoregressive processes"}},{"authors":[{"name":"R. Venkataramanan"},{"name":"S. S. Pradhan"}],"title":{"text":"Achievable rates for multiple de- scriptions with feed-forward"}},{"authors":[{"name":"O. Simeone"},{"name":"H. H. Permuter"}],"title":{"text":"Source coding when the side infor- mation may be delayed"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569553537.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S1.T1.3","endtime":"10:50","authors":"Osvaldo Simeone, Haim H Permuter","date":"1341225000000","papertitle":"Source Coding With Delayed Side Information","starttime":"10:30","session":"S1.T1: Source Coding with Side Information","room":"Kresge Rehearsal B (030)","paperid":"1569553537"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
