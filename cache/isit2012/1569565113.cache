{"id":"1569565113","paper":{"title":{"text":"On Lossless Universal Compression of Distributed Identical Sources"},"authors":[{"name":"Ahmad Beirami"},{"name":"Faramarz Fekri"}],"abstr":{"text":"Abstract\u2014Slepian-Wolf theorem is a well-known framework that targets almost lossless compression of (two) data streams with symbol-by-symbol correlation between the outputs of (two) distributed sources. However, this paper considers a different scenario which does not ﬁt in the Slepian-Wolf framework. We consider two identical but spatially separated sources. We wish to study the universal compression of a sequence of length n from one of the sources provided that the decoder has access to (i.e., memorized) a sequence of length m from the other source. Such a scenario occurs, for example, in the universal compression of data from multiple mirrors of the same server. In this setup, the correlation does not arise from symbol-by-symbol dependency of two outputs from the two sources. Instead, the sequences are correlated through the information that they contain about the unknown source parameter. We show that the ﬁnite-length nature of the compression problem at hand requires considering a notion of almost lossless source coding, where coding incurs an error probability p e (n) that vanishes with sequence length n. We obtain a lower bound on the average minimax redundancy of almost lossless codes as a function of the sequence length n and the permissible error probability p e when the decoder has a memory of length m and the encoders do not communicate. Our results demonstrate that a strict performance loss is incurred when the two encoders do not communicate even when the decoder knows the unknown parameter vector (i.e., m → ∞)."},"body":{"text":"Many practical applications involve compression of data that are taken from multiple spatially separated sources. A key challenge in most of such applications is that the sources usu- ally cannot communicate with each other. Theoretical results by Slepian and Wolf demonstrate that if the data streams from two sources have symbol-by-symbol correlation, the sequences can be compressed to their joint entropy even when the two encoders do not communicate [1]. In other words, as in Fig. 1, assume that sources S 1 and S 2 wish to transmit the sequences y n and x n , respectively, to a node R. As the length n of the sequences increases, the decoding of x n at R with the help of y n can be performed using a code with the average length that asymptotically approaches the conditional entropy, (i.e., H(X n |Y n )) with asymptotically zero error probability. If the decoder did not choose to use y n in decoding, the encoder at S 2 would have to encode the sequence x n irrespective to y n with an average length that is lower bounded by H(X n ). Note that the conditional entropy H(X n |Y n ) may be signiﬁcantly smaller than the individual entropy H(X n ). After recent development of practical Slepian-Wolf (SW) coding schemes by Pradhan and Ramchandran [2], SW coding has drawn a great deal of attention as a promising technique for sensor networks [3] and distributed video coding [4].\nThe Slepian-Wolf theorem naturally suits applications where the (new) sequence x n from S 2 (in Fig. 1) can be viewed as a noisy version of the (previously seen) sequence y m that could possibly be exploited as side information to reduce the code length of x n . Data gathering from sensors that measure the same phenomenon is one example. However, in many scenarios, the compression of distributed sources cannot be modeled by the SW framework. As an example, consider the universal compression of data from the mirrors of the same server, where the sources are exact copies of each other. Hence, it is plausible to assume that the sources (S 1 and S 2 in Fig. 1) follow the same statistical model. On the other hand, the source model might be unknown requiring universal compression [5]\u2013[7]. The question is, assuming two identical sources S 1 and S 2 and having y m from S 1 at the decoder, what is the achievable universal compression performance on x n at S 2 provided that the encoders at S 1 and S 2 do not communicate.\nWe stress that the nature of this problems is fundamentally different from those addressed by the Slepian-Wolf (SW) theorem in [1]. Here, instead of symbol-by-symbol correlation between the sequences as in SW setup, the redundancy is due to the fact that when the source parameter is a priori unknown there is signiﬁcant overhead in the universal compression of ﬁnite-length sequences [7]\u2013[9]. Considering the example in Fig. 1 with two identical sources S 1 and S 2 , y m and x n would be independent given that the source model is known. However, when the source parameter is unknown, y m and x n are correlated with each other through the information they contain about the unknown source parameter. The question is whether or not this correlation can be potentially leveraged by the encoder of S 2 and the decoder at R in the decoding of x n using y m in order to reduce the code length of x n .\nIn this paper, we study the universal compression of dis- tributed identical sources. By identical we mean that the sources (S 1 and S 2 ) share the same unknown source parameter. By distributed we mean that the sources are spatially separated and the encoders do not communicate with each other. This problem can also be viewed as universal compression with training data that is only available to the decoder. It is known that forming a statistical model from a training data set would improve the performance of universal compression [10], [11]. In [9], [12], we theoretically derived the gain that is obtained\nin the universal compression of the new sequence x n from S 2 by memorizing (i.e., having access to) y m from S 1 at both the decoder (at R) and the encoder (at S 2 ). This corresponds to the reduced case of our problem where the sources S 1 and S 2 are either co-located (a single source) or allowed to communicate. For the reduced problem case, in [11], [13], we further extended the setup to a network with a single source and derived bounds on the network-wide gain where a small fraction of the intermediate nodes in the network are capable of memorization. However, as we demonstrate in the present paper, the extension to the multiple spatially separated sources, where the training data is only available to the decoder, is non-trivial and raises a new set of challenges that we aim to address. The rest of this paper is organized as follows. In Sec. II, we brieﬂy review the necessary background. In Sec. III, we describe the problem setup. In Sec. IV, we present our main results. In Sec. V, we provide discussion on the results. In Sec. VI, we present the technical analysis of the results. Finally Sec. VII concludes the paper.\nIn this section, we review the necessary background, nota- tions, and deﬁnitions followed by the formal problem setup. Following the notation in [12], let A be a ﬁnite alphabet. Let d be the number of the source parameters. Further, let θ = (θ 1 , ..., θ d ) denote the d-dimensional parameter vector as- sociated with the parametric source (that is a priori unknown). Let Θ d denote the space of d-dimensional parameter vectors. We denote μ θ as the probability measure that is deﬁned by the parameter vector θ. Let P d denote the family of sources that are described with the d-dimensional unknown parameter vector θ ∈ Θ d . We use the notation x n = (x 1 , ..., x n ) ∈ A n to present a sequence of length n from the alphabet A. We further denote X n as a random sequence of length n (that follows the probability distribution μ θ ). Let H n (θ) be the source entropy given θ, i.e., H n (θ) = E log 1 μ θ (X n ) . 1\nLet c n : A n → {0, 1} ∗ be an injective mapping from the set A n of the sequences of length n over A to the set {0, 1} ∗ of binary sequences. Next, we present the notions of strictly lossless and almost lossless source codes, which will be needed for the study of UC-DIS.\nDeﬁnition 1 The code c n (·) : A n → {0, 1} ∗ is called strictly lossless (also called zero-error) if there exists a reverse mapping d n (·) : {0, 1} ∗ → A n such that\nAll of the practical data compression schemes are examples of strictly lossless codes, namely, the arithmetic coding, Huffman, Lempel-Ziv, and Context-Tree-Weighting algorithms.\nOn the other hand, due to the distributed nature of the sources, we are concerned with the slightly weaker notion of almost lossless source coding in this paper.\nDeﬁnition 2 The code ˆc p e n (·) : A n → {0, 1} ∗ is called almost lossless with permissible error probability p e (n) = o(1), if\nthere exists a reverse mapping ˆ d p e n (·) : {0, 1} ∗ → A n such that\nwhere 1 e (x n ) denotes the error indicator function, i.e, 1 e (x n ) = 1 ˆ d p e n (ˆc p e n (x n )) = x n , 0 otherwise.\nThe almost lossless codes allow a non-zero error probability p e (n) for any n while they are almost surely asymptotically error free. Note that strictly lossless codes correspond to p e (n) = 0. The proofs of Shannon [14] for the existence of entropy achieving source codes are based on almost lossless random codes. Further, the proof of the SW theorem [1] also uses almost lossless codes. Further, all of the practical implementations of SW source coding are based on almost lossless codes (cf. [2], [3]). We stress that the nature of the almost lossless source coding is different from that incurred by the lossy source coding (i.e., the rate-distortion theory). In the rate-distortion theory, a code is designed to asymptotically achieve a given distortion level as the length of the sequence grows to inﬁnity. Therefore, since the almost lossless coding asymptotically achieves a zero-distortion, in fact, it coincides with the special case of zero-distortion in the rate-distortion curve.\nWe present the problem setup in the most basic scenario, shown in Fig. 1, consisting of two identical sources located in nodes S 1 and S 2 , and the destination node R. We let the information sources at S 1 and S 2 be parametric with an identical d-dimensional parameter vector that is unknown a priori to the encoder and the decoder. Let y m and x n denote two sequences with lengths m and n, respectively, that are generated by the unknown information source model. In the sequel, we describe the communication scenario for universal compression of distributed identical sources. We assume that S 1 has transmitted the sequence y m to R. Next, at some later time, S 2 wishes to send x n to R. We further assume that R is a memory unit and is capable of memorizing the sequence y m . We investigate the achievable saving in the compression of x n in the S 2 -R link when R has memorized the sequence y m . Note that S 2 does not have access to the sequence y m . If the node R did not have a memory unit, S 2 would have to apply an end-to-end universal compression to x n . However, the side information provided by y m at R about the source parameter can potentially result in a reduction in the amount of bits required to be transmitted in the S 2 -R link. Throughout the paper, we refer to this problem setup as Universal Compression of Distributed Identical Sources (UC-DIS).\nIn the study of coding strategies for UC-DIS, we compare the following cases for the compression of x n at S 2 . Note that we assume that y m is already universally compressed at S 1 and transmitted and decoded at R.\n\u2022 UComp (Universal compression), which only applies end-to-end lossless universal compression to x n at S 2 without regard to y m .\n\u2022 UCompM (Universal compression with memorization at both the encoder and the decoder), which assumes that\nthe encoder (at S 2 ) and the decoder (at R) have access to a common memory (i.e., sequence y m ), which is utilized in the lossless compression of x n at S 2 .\n\u2022 DUCompM (Distributed universal compression with memorization at the decoder), which assumes that de- coder (at R) has memorized (i.e., has access to) y m while the encoder (at S 2 ) only knows the length m of the side information but does not know the exact sequence y m . The encoder then applies an almost lossless code to x n that is decoded at R with permissible error probability p e using y m .\nNote that UComp does not beneﬁt from the memorization and is the conventional scheme. Further, UCompM is introduced as the benchmark for the purpose of evaluating the performance of DUCompM and is not practically useful since it requires the sequence y m from S 1 to be available at the encoder of S 2 .\nLet l n (x n ) denote the strictly lossless length of the code- word associated with the sequence x n . Further, let L n denote the space of strictly lossless universal length functions on a sequence of length n. Denote R n (l n , θ) as the expected redun- dancy of such strictly lossless codes on a sequence of length n for the parameter vector θ, i.e., R n (l n , θ) = El n (X n )−H n (θ). Further, denote ¯ R UComp (n) as the average minimax redundancy as given by\nR n (l n , θ). \t (1) In UCompM, let l n|m be the strictly lossless universal\nlength function with a memory sequence of length m. Denote L n|m as the space of such strictly lossless universal length functions. Let R n (l n|m , θ) be the expected redundancy of encoding a sequence of length n form the source μ θ using the length function l n|m . Further, let ¯ R UCompM (n, m) denote the corresponding average minimax redundancy, i.e.,\nIn DUCompM, let ˆ l p e n|m denote the almost lossless universal length function with a memorized sequence of length m that is only available to the decoder, where the permissible error prob- ability on decoding x n is p e . Further, denote ˆ L p e n|m as the space of such universal length functions. Denote R n (ˆl p e n|m , θ) as the expected redundancy of encoding a sequence x n of length n using the length function ˆ l p e n|m . Denote ¯ R p e DUCompM (n, m) as the expected minimax redundancy as given by\nNote that we denote ¯ R DUCompM (n, m) \t ¯ R 0 DUCompM (n, m) as the expected minimax redundancy of strictly lossless DUCompM coding strategy.\nIn this section, we provide results on the average minimax redundancy of the different coding strategies introduced in the previous section for the UC-DIS problem. Discussion on the implications of the results and the proof sketches are deferred to Sec. V and Sec. VI, respectively.\nIn the case of strictly lossless UComp, Clarke and Barron derived the expected minimax redundancy ¯ R UComp (n) for memoryless sources [15], which was later generalized by Atteson for Markov sources, as the following [16]:\nTheorem 1 The average minimax redundancy of strictly loss- less UComp coding strategy is given by\nn , where I n (θ) is the Fisher information matrix.\nIn the case of strictly lossless UCompM (i.e., when the two encoders can communicate), we obtain the average minimax redundancy in the following theorem.\nTheorem 2 The average minimax redundancy of strictly loss- less UCompM coding strategy is given by\n¯ R UCompM (n, m) = d 2 log 1 + n m\nIn the next proposition, we conﬁne ourselves to strictly lossless codes in the DUCompM strategy.\nProposition 3 The average minimax redundancy of strictly lossless DUCompM coding strategy is equal to that of UComp coding strategy. That is ¯ R DUCompM (n, m) = ¯ R UComp (n).\nFinally, in the case of almost lossless DUCompM, our main result is given in the following theorem.\nTheorem 4 The average minimax redundancy of almost loss- less DUCompM coding strategy is upper bounded by ¯ R p e\nwhere F(d, p e ) is the penalty term due to the encoders not communicating, which is given by\nIn this section, we provide some discussion on the signif- icance of the results for different UC-DIS coding strategies. Figures 2 and 3 demonstrate the redundancy rate for the three coding strategies, namely, UComp, UCompM, and DUCompM for memoryless sources and ﬁrst-order Markov sources with alphabet size k = 256, respectively. In the case of UComp, Theorem 1 deﬁnes the achievable average minimax redun- dancy for the compression of a sequence of length n encoded without regard to the previously seen sequence y m .\nAccording to Theorem 2, if the encoder and the decoder have access to a common memory y m , i.e., UCompM coding strategy, the average minimax redundancy could be much smaller than that of UComp depending on how large m is. In particular, when m → ∞ we have lim m→∞ ¯ R UCompM (n, m) = 0. 2 This corresponds to the case where the parameter vector\nis known to both the encoder and the decoder, and thus, the redundancy is zero similar to a perfect Shannon code. Hence, the fundamental limits are those of known source parameters and universality no longer imposes a compression overhead. This is also demonstrated in Figs. 2 and 3, where m has been chosen to be sufﬁciently large.\nProposition 3 demonstrates that if strictly lossless DUCompM coding strategy (i.e., p e = 0) is to be used for the compression of x n from S 2 , the memorization of y m from S 1 only at the decoder does not provide any compression beneﬁt, assuming that the two encoders at S 1 and S 2 do not communicate. In other words, the best that S 2 can do is to simply apply a traditional universal compression on x n .\nFinally, according to Theorem 4, unlike the asymptotic behavior of the Slepian-Wolf problem, the distributed nature in this problem incurs an extra redundancy on the compression. As can be seen in Fig. 2, the overhead can be signiﬁcant in the compression of memoryless sources. For example, when n = 512B, m = 32kB, and p e = 10 −6 , the redundancy rate is around 0.05, as compared with the almost zero redundancy rate of UCompM. On the other hand, as demonstrated in Fig. 3, when d is relatively larger, for medium length sequences even with extremely small error probability, DUCompM performs fairly close to UCompM. Further, DUCompM by far out- performs UComp in the compression of short to medium length sequences with reasonable permissible error probability, justifying usefulness of DUCompM in practice. If log 1 p\nd, the penalty term can be further simpliﬁed to be approximately equal to F(d, p e ) ≈ log 1 p e for the practical ranges of p e .\nWe prove that the RHS is both an upper bound and a lower bound for ¯ R UCompM (n, m). The upper bound is obtained using the KT-estimator [19] along with a proper Shannon code [14] and the proof follows the analysis of the redundancy of the KT-estimator. In the next lemma, we obtain the lower bound.\nLemma 1 The average minimax redundancy of UCompM is lower-bounded by\n¯ R UCompM (n, m) ≥ d 2 log 1 + n m\nProof: It can be shown that the minimax redundancy is equal to the capacity of the channel between the unknown parameter vector θ and the sequence x n given the sequence y m (cf. [8] and the references therein). Thus,\ndenotes the Jeffreys\u2019 prior, and ¯ R UComp (·) is given in Theorem 1. Further simpliﬁcation of (5) leads to the desired result in Lemma 1.\nSince the source is assumed to be from the family P d of d-dimensional parametric sources, in particular, it is also an ergodic source. Thus, any pair (x n , y m ) occurs with non- zero probability and the support set of (x n , y m ) is equal to A\nn × A m . Therefore, Proposition 3 trivially follows from the known results on strictly lossless compression (cf. [20] and the references therein).\nWe provide a constructive optimal coding strategy at the en- coder and obtain its achievable average minimax redundancy, which provides with an upper bound on the average minimax redundancy of the almost lossless DUCompM coding strategy.\nLet ˆ θ(x n ) (or ˆ θ(y m )) denote the Maximum Likelihood (ML) estimate for the unknown source parameter given that the sequence x n (or y m ) is observed, i.e., ˆ θ(x n )\narg max λ μ λ (x n ). Further, let ˆ θ X ˆ θ(x n ) and ˆθ Y ˆθ(y m ). As discussed earlier μ θ (x n ) is the probability distribution induced by the parameter vector θ on the sequence x n . It is straightforward to derive the pmf of the ML-estimate p(ˆ θ X |θ) from μ θ (x n ) by summing over all the sequences that corre- spond to the same ML-estimate. Note that ˆ θ X follows a dis- crete distribution only taking values on a ﬁnite set of (n + 1) d points in the space Θ d . For any λ, θ ∈ Θ d , let D n (μ λ ||μ θ ) be the KL-divergence, i.e., D n (μ λ ||μ θ ) E log μ θ (X n ) μ λ (X n ) .\nIt can be shown that expectations with respect to p(ˆ θ X |θ) can be performed using a continuous RV ˜ θ X (with uniformly vanishing error) whose distribution conditioned on θ is given by\np(˜ θ X |θ) = |I n (˜ θ X )| 1 2 n 2π d 2 exp(−D n (μ ˜ θ X ||μ θ )), (6) where n has to be large enough so that Stirling\u2019s approxima- tion can be applied. Further, it is straightforward to show that this distribution can be approximated using a Gaussian distri- bution with mean θ and inverse covariance matrix nI n (θ).\nNext, we will obtain an approximation for the distribution of ˆ θ X conditioned on ˆ θ Y .\nLemma 2 Let ˆ θ X and ˆ θ Y denote the ML-estimate parameter given observed sequences x n and y m , respectively. Further, let p(˜ θ X |ˆθ Y ) follow a Gaussian distribution with mean ˆ θ Y and inverse covariance matrix nm n+m I m (ˆ θ Y ). Then, all expectations with respect to p(ˆ θ X |ˆθ Y ) can be performed using p(˜θ X |ˆθ Y ) with uniformly vanishing error.\nNow, we are equipped to deﬁne S n (y m , p e ) as the set with smallest Lebesgue volume such that\nThe following lemma shows as to how S n (y m , p e ) is deter- mined.\nLemma 3 Let ˆ θ Y denote the ML-estimate for the unknown parameter vector given sequence y m is observed. Then, S n (y m , ) is given by\nThe next lemma determines the probability measure of the set S n (y m , p e ) under Jeffreys\u2019 prior.\nLemma 4 Assume that the parameter vector θ follows Jef- freys\u2019 prior. Then, the probability measure P S (p e ) of the set S n (y m , p e ) is given by\nNext, consider the following coding scheme. Let the space be partitioned into ellipsoids of the form S n (y m , p e ). Then, each sequence is encoded within its respective ellipsoid with- out regard to the rest of the parameter space. The decoder chooses the decoding ellipsoid using the ML estimate ˆ θ Y and the permissible decoding error probability p e . The probability measure covered by each ellipsoid is P S (p e ) is independent of ˆ θ Y , and provides with − log P S (p e ) reduction in the re- dundancy. Further, simpliﬁcation of P S (p e ) and the fact that δ d (p e ) ≈ d 2 log e + log 1 p e will lead to the desired result.\nIn this paper, we introduced and studied the problem of Universal Compression of Distributed Identical Sources (UC- DIS), which is a more favorable framework as compared to the Slepian-Wolf (SW) framework in several applications, such as the compression of data from mirrors of a data server. In UC-DIS, the correlation among outputs of the sources is due to the ﬁnite-length universal compression constraint, departing from the nature of the correlation in the SW framework. For UC-DIS, involving two identical sources, we introduced DUCompM coding strategy (compression using the side information at the decoder when the two encoders do not communicate) and obtained an upper bound on its average minimax redundancy. We demonstrated that for ﬁnite- length sequences with reasonable permissible error probability, DUCompM coding strategy by far outperforms traditional universal compression, and hence, justifying the usefulness of DUCompM coding strategy in practice."},"refs":[{"authors":[{"name":"D. Slepian"},{"name":"J. K. Wolf"}],"title":{"text":"Noiseless coding of correlated information sources"}},{"authors":[{"name":"S. Pradhan"},{"name":"K. Ramchandran"}],"title":{"text":"Distributed source coding using syndromes (DISCUS): design and construction"}},{"authors":[{"name":"M. Sartipi"},{"name":"F. Fekri"}],"title":{"text":"Distributed source coding using short to moderate length rate-compatible LDPC codes: the entire Slepian-Wolf rate region"}},{"authors":[{"name":"B. Girod"},{"name":"A. Aaron"},{"name":"S. Rane"},{"name":"D. Rebollo-Monedero"}],"title":{"text":"Distributed video coding"}},{"authors":[{"name":"M. Weinberger"},{"name":"J. Rissanen"},{"name":"M. Feder"}],"title":{"text":"A universal ﬁnite memory source"}},{"authors":[{"name":"J. Rissanen"}],"title":{"text":"Universal coding, information, prediction, and estimation"}},{"authors":[{"name":"A. Beirami"},{"name":"F. Fekri"}],"title":{"text":"Results on the redundancy of universal compression for ﬁnite-length sequences"}},{"authors":[{"name":"N. Merhav"},{"name":"M. Feder"}],"title":{"text":"A strong version of the redundancy-capacity theorem of universal coding"}},{"authors":[{"name":"A. Beirami"},{"name":"F. Fekri"}],"title":{"text":"Memory-assisted universal source coding"}},{"authors":[{"name":"G. Korodi"},{"name":"J. Rissanen"},{"name":"I. Tabus"}],"title":{"text":"Lossless data compression using optimal tree machines"}},{"authors":[{"name":"M. Sardari"},{"name":"A. Beirami"},{"name":"F. Fekri"}],"title":{"text":"Memory-assisted universal compression of network ﬂows"}},{"authors":[{"name":"A. Beirami"},{"name":"M. Sardari"},{"name":"F. Fekri"}],"title":{"text":"Results on the fundamental gain of memory-assisted universal source coding"}},{"authors":[{"name":"M. Sardari"},{"name":"A. Beirami"},{"name":"F. Fekri"}],"title":{"text":"On the network-wide gain of memory-assisted source coding"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A Mathematical Theory of Communication"}},{"authors":[{"name":"B. Clarke"},{"name":"A. Barron"}],"title":{"text":"Information-theoretic asymptotics of Bayes methods"}},{"authors":[{"name":"K. Atteson"}],"title":{"text":"The asymptotic redundancy of Bayes rules for Markov chains"}},{"authors":[{"name":"M. Drmota"},{"name":"W. Szpankowski"}],"title":{"text":"Precise minimax redundancy and regret"}},{"authors":[{"name":"W. Szpankowski"}],"title":{"text":"Asymptotic average redundancy of Huffman (and other) block codes "}},{"authors":[{"name":"R. E. Krichevsky"},{"name":"V. K. Troﬁmov"}],"title":{"text":"The performance of universal encoding"}},{"authors":[{"name":"N. Alon"},{"name":"A. Orlitsky"}],"title":{"text":"Source coding and graph entropies"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565113.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T1.1","endtime":"17:00","authors":"Ahmad Beirami, Faramarz Fekri","date":"1341247200000","papertitle":"On Lossless Universal Compression of Distributed Identical Sources","starttime":"16:40","session":"S4.T1: The Slepian-Wolf and CEO Problems","room":"Kresge Rehearsal B (030)","paperid":"1569565113"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
