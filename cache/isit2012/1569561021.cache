{"id":"1569561021","paper":{"title":{"text":"The Information Content in Sorting Algorithms"},"authors":[{"name":"Ludwig M. Busse"},{"name":"Morteza Haghir Chehreghani"},{"name":"Joachim M. Buhmann"}],"abstr":{"text":"Abstract\u2014Sorting algorithms like MergeSort or BubbleSort order items according to some criterion. Whereas the compu- tational complexities of the various sorting algorithms are well understood, their behavior with noisy input data or unreliable algorithm operations is less known.\nIn this work, we present an information-theoretic approach to quantifying the information content of algorithms. We exemplify the signiﬁcance of this approach by comparing different algo- rithms w.r.t to both informativeness and stability. For the ﬁrst time, the amount of order information that a sorting algorithm can extract in uncertain settings is measured quantitatively. Such measurements not only render a principled comparison of algorithms possible, but also guide the design and construction of algorithms that provide the maximum information.\nResults for ﬁve popular sorting algorithms are illustrated, giving new insights about the amount of ordering information achievable for them. For example, in noisy settings, BubbleSort can outperform MergeSort in the number of bits that can be effectively extracted per comparison made."},"body":{"text":"Keeping items in order is at the very heart of organising information. Computers spend a massive amount of their time sorting data.\nGeneral-purpose sorting routines have been well studied over the past 100 years. An in-depth overview on sorting algorithms is provided in [7]. There is a whole zoo of sorting algorithms available today. Sorting approaches can be schematically clas- siﬁed into sorting by insertion, exchanging, selection, and merging [7]. In this work, we focus on comparison-based sorting algorithms, where the sorting is produced by com- parisons between pairs of items i < j. Mathematically, the problem rephrases as: determine the total order on a set of n items via questions of the form \u201cis i < j?\u201d. Technically, the different algorithms then differ in the order that the pairwise comparisons are queried from the oracle, and in the way they use each comparison for generating the ﬁnal order.\nThe computational complexities of sorting algorithms have been well studied and typically lie between O(n 2 ) and O(n log n). On contrary, the behavior of sorting algorithms in uncertain settings are rarely tackled. In [6], a lower bound for the number of comparisons that any randomized algorithms needs to approximate any given ranking within an expected Spearman\u2019s footrule distance is shown. The robustness of sorting and tournament algorithms against faulty comparisons is analyzed in [5]. A lower bound on the query complexity with erroneous comparisons is shown in [8].\nBesides computational complexity, however, robustness can be a key concern as well in the analysis of algorithms. Two possible sources of uncertainties in sorting problems are: (i.) Comparing items might suffer from noise in the input data, i.e. queries i < j might return the wrong result (ii.) Algorithmic operations might be unreliable, here: the operation i < j is subject to possibly erroneous execution.\nIn Fig. 1, we observe the performance of ﬁve standard sorting algorithms in terms of both the number of compar- isons (efﬁciency) and the deviation from the correct ordering (robustness) . Items are to be ordered, but when the algorithms query the comparison i < j between items, the item\u2019s true ranks are perturbed by Gaussian noise (centered around the true rank with a given standard deviation; more details on the experimental setting later in this paper). The deviation from the true ordering is measured with the Spearman\u2019s rho distance (quadratic distance of ranks) [3]. As we can see, the sorting algorithms suffer differently from the noisy comparisons depending on how errors can affect the overall result. Since algorithms with computational complexity higher than the lower bound Ω(n log n) obviously make a number of redundant pairwise comparisons, one would in general expect a tradeoff between the number of comparisons and the accuracy of the result. Very efﬁcient O(n log n) algorithms might be more prone to errors because they exploit each comparison to a larger extent. However, we observe that some algorithms are Pareto-dominant over others: MergeSort, and also the berated BubbleSort.\nThis experimental observation raises suspicion that \u2013 in noisy conditions \u2013 the amount of attainable sorting information is different for different sorting algorithms. We now proceed in analyzing sorting algorithms with the goal of measuring their information content. That is, we wish to quantify how many bits a noise-exposed algorithm can effectively extract for the order relation. Thereby, we can consolidate the snapshot result in Fig. 1, and also select or build an algorithm that will operate with the maximum information gain possible. The information-theoretic approach that we present here is actually applicable to all kind of algorithms, and we demonstrate it for sorting algorithms. The approach generalizes Approximation Set Coding [2], which was initially proposed for model vali- dation in clustering [1].\nSection II introduces the theory of informativeness of algo- rithms, and in section III the application to sorting algorithms is presented. Results are given in section IV, before this paper ends with a conclusion (Section V).\nLet us begin with an example that is helpful for un- derstanding the main concept of the following work. The example is optimization under uncertainty. Assume there is data X ⊂ X given, for example measurements in a data space X . Such measurements can be d-dimensional vectors or relations X = (x ij ) ∈ B n×n (these will become the pairwise comparisons in sorting). Let h be a solution or hypothesis of an optimization problem. In sorting, h is a ranking encoding the order information of the items. The hypothesis class is the set of hypotheses H(X) = {h(X) : X ∈ X }. An optimization problem involves a cost function R : H × X → R ≥0 that assigns each hypothesis h(X) a real value R(h, X). h ⊥ (X) = arg min h R(h, X) denotes the min-cost solution, the hypothesis that empirically minimizes costs.\nThe key concept brought up in Approximation Set Coding (ASC) [1] is the notion of an approximation set. This set contains all solutions C γ (X) of the optimization problem that are γ-close in costs to h ⊥ (X):\nThese sets serve the purpose to stabilize optimization results. The input data of the problem, the measurements X, can be affected by perturbations due to random noise. For the same signal in X but a different noise instantiation, h ⊥ (X) might considerably differ. However, for sufﬁciently large γ, the approximation set C γ (X) is almost invariant under noise. ASC develops a criterion to minimize the approximation parameter γ under the constraint that C γ (X) is stable under noise perturbation. Shrinkage of the approximation set (governed by the parameter γ) is of advantage to gain information about the solution, but it should happen under the constraint of robustness. In summary, provided are both informative and robust solutions when choosing hypotheses guided by a cost function.\nIn the optimization setting described above, introducing approximation sets pursues the goal to ultimately determine the informativeness of cost functions exposed to noisy data. Indeed, the deﬁnition of sets C γ (X) by means of a cost function and a closeness-concept parametrized by γ constitutes a special case for a set-deﬁning mechanism. We generalize to mechanisms A, also called algorithms, i.e. A is a mapping into the powerset P(H) of the hypothesis class\nand we call the sets C A γ (X) the representer sets of algorithm A. Fig. 2 visualizes representer sets as characteristic output of an algorithmic procedure with input data X and parameter γ.\nWe now wish to quantify the amount of information \u2013 measured w.r.t. the hypothesis class H(X) \u2013 that A is able to extract from the possibly noisy data X.\nExample: Imagine the hypotheses {h(X)} being the set of all rankings (which order items). Given pairwise data in X, a procedure that selects just a few compatible rankings (and excludes many others; ultimately, just one ﬁnal ranking is selected) is more informative compared to a procedure that does not commit or dare to restricting the set of solutions to the same extent. At the same time, a high overlap of the repre- senter sets for two instances X (1) and X (2) ensures robustness against the noise actually present in the data. Mathematically, this rephrases as: How small can the representer sets be chosen (size adjustable by γ) to still ensure identiﬁability of C A γ (X) under variation of the data X?\nWith the representer sets of an algorithm A at hand, we now have the ingredient to compute the algorithm\u2019s capacity as the maximum information it can make use from the data X under noise effects. In analogy to Approximation Set Coding (ASC) [1], [2], we refer to a generic set-based coding and communication scenario and use the representer sets C A γ (X) in a ﬁctitious communication scenario to derive a criterion for the optimal information gain. There exists a conceptual analogy between learning and communication: For communication, one demands a high rate (a large amount of information transferred per channel use) together with a decoding rule that is stable under the perturbations of the messages by the noise in the channel.\nASC adopts the two sample set scenario, which is widely used in statistical learning theory [9] and is analogous to two-terminal systems in information theory [4]. Two datasets X (1) and X (2) are generated with the same underlying signal structure (e.g. pairwise comparisons as induced by the same ordering), but different noise realizations (e.g. some pairwise comparisons ﬂip randomly). This two-dataset situation cor- responds to a communication scenario where the code on both sides of the channel differs due to a noise process. As derived in [1], an asymptotically vanishing error probability is achievable for rates bounded by 1\nHere, |{σ}| denotes the cardinality of the hypothesis class and is the term accounting for the problem complexity. ∆C A γ\nis the overlap, how many γ-solutions of the ﬁrst dataset are also in the γ-representer set of the second dataset, i.e. ∆C A γ (X (1) , X (2) ) := |C A γ (X (1) ) ∩ C A γ (X (2) )|. For a ﬁxed γ,\na large overlap means that the algorithm\u2019s evaluation of the ﬁrst dataset generalizes to the second dataset, whereas a small or empty intersection indicates lack of generalization. The fraction |∆C A γ (X (1) , X (2) )|/|C A γ (X (1) )||C A γ (X (2) )| measures the stability of the solutions under noise ﬂuctuations. It is in [0, 1] and thereby controls the effective useable size of the hypothesis class. Normalizing with m ensures a bit rate per measurement made. ˆ I A γ is an estimate for the mutual infor- mation and, in analogy to information theory, the capacity is deﬁned as C A := max γ E[ ˆ I A γ ]. This information capability of an algorithm A is context-sensitive, because it is measured w.r.t. to the hypothesis class.\nNow, we study the use of the new framework to compute the capacity of sorting algorithms. Thereby, we can quantify how much information (in terms of bits, here: for ordering items) an algorithm can actually provide in the presence of noise, and then compare the algorithms against each other.\nPairwise comparison-based sorting algorithms sequentially query pairs of items i < j until they determine a complete order as their result. Sorting algorithms differ in the number and order in which they compare pairs of items. The lower bound on the number of necessary pairwise comparisons to sort n items is known to be Ω(n log n) [7]. The computa- tional complexity of sorting algorithms typically lies between O(n log n) and O(n 2 ). Hence, algorithms of the latter com- plexity class obviously do redundant comparisons. Confronted with noisy data, the question then is whether these algorithms can exploit the redundancy to their advantage when it comes to improving the accuracy of the sorting result .\nWe now give a viewpoint on sorting algorithms that will help us tackling them in a set-based fashion.\nIn fact, sorting technologies can be understood as mechanisms that operate on the symmetric group S n (the space of all permutations of n items=rankings/orderings). At the beginning of a sorting procedure, every element of S n (each ordering) is equally admissible. During the execution of the algorithm \u2013 in the light of the acquired pairwise comparisons \u2013 the set of possible orders is further reduced until the algorithms ﬁnally ﬁnd the one and only ordering that is compatible with the pairwise comparisons that were obtained. In other words, sorting algorithms aim at constructing a series of partial orders, reducing their cardinality so as to ultimately narrow down to a single element in S n which is the sorting result.\nThe relation to the representer sets is now immediate: The hypothesis class is the set of all permutations H = {π ∈ S n }, and the data are the pairwise comparisons x ij ∈ {0, 1}. Steps in an algorithm\u2019s execution path are related to the γ- parametrization. An algorithm that has just started, at time t = t Start , has not yet condensed any information (0 Bits): the representer set C A γ=t Start is the full hypothesis class with cardinality n!. Now, suppose we look at the algorithm at time t = t + 1 after it has established some order. The cardinality of the representer set has been reduced, thereby providing more information about the order of the items. Finally, at t = t End , the algorithm has narrowed down the hypothesis class to one ﬁxed ordering π ∗ ∈ H, yielding log 2 (n!) bits of information. Such a trajectory of a sorting algorithm is visualized in Fig. 3. In analogy with statistical physics, an algorithm at the beginning is at high temperature smoothing over the solution space and containing no information, then runs and cools down committing more and more to speciﬁc solutions (and thus exhibiting information).\nWith noise involved, an algorithm might end up with the wrong result (Fig. 4 left). In another run, the result might differ again; Fig. 4 (right) shows both the representer sets for the two samples and their overlap at step t. Their cardinalities can be used to compute the mutual information, whose maximum is the capacity of the algorithm.\nThe following sections detail the computation of representer sets for ﬁve standard algorithms.\na) SelectionSort: We begin with SelectionSort for didac- tical reasons, because of simplicity in understanding the role of the representer sets and the ease of their computation.\nBasically, as with all algorithms, we need to identify time points in the execution path of the algorithm where the algo- rithm has achieved \u201csome\u201d partial sorting. In SelectionSort, after one iteration of the outer loop, the ﬁrst position of the ordering is ﬁlled with an item (the \u201cselected\u201d item). The item in the ﬁrst position is ﬁxed from now on, and will never be moved again. On contrary, comparisons made in the inner\nloop are not necessarily binding for the ﬁnal ordering output. In total, the SelectionSort procedure decomposes into n time steps, after each of them the next position will be ﬁlled. Thus, a series of top-t orderings constitute the representer sets C SelectionSort t=1,...,n . Their cardinality can be straightforwardly computed as |C SelectionSort t \t | = (n − t)!. The overlap is 100% (|∆C SelectionSort t \t | = |C SelectionSort t \t |) as long as the ﬁrst t positions are equal in the two runs of the algorithm (resulting in I SelectionSort t ∗ \t = log 2 ( n! (n−t ∗ )! ) bits of information), otherwise the overlap immediately collapses to 0%. The largest t ∗ achievable on average by the algorithm will determine its capacity.\nb) InsertionSort: InsertionSort produces a series of n−1 transitive chains of lengths t = 2, . . . , n. The average last t ∗ before two samples of the algorithm diverge determines this algorithm\u2019s capacity by I InsertionSort t ∗ \t = log 2 ( n! n!/t ∗ ! ).\nc) MergeSort: Because MergeSort compares items from disjoint subsets organized as a balanced tree, it roughly splits the hypothesis class of orderings into two halfs after each of t = 1, . . . , log 2 (n!) pairwise comparisons. Intuitively, MergeSort has a strong tendency to respect all transitive consequences of the comparisons done. The overlap of noisy MergeSort trajectories immediately drops down to 0 after any inconsistent comparison. The capacity of MergeSort can be approximated via I MergeSort t ∗ \t = log 2 n! n!/2 t∗ . For a given noise model or assumption, this time point t ∗ should be amenable to analytical calculation.\nd) QuickSort (random pivot): In an intermediate parti- tioning step of QuickSort, assume a items have already been decided to be before the pivot element, b items have already been placed after the pivot element, and c is the number of items that still need to be placed. As always, the representer set is the set of linear extensions, and its cardinality is\n|C QuickSort t=1,... | = c k=0 c! k!(c−k)! (a + k)!(b + (c − k))! · f , where f is the factorial of all other divisions not yet partitioned by QuickSort (can be computed recursively). The capacity is then computed via I QuickSort t ∗ \t = log 2 n! |C\ne) BubbleSort: There is one peculiarity with noisy Bub- bleSort. The standard implementation of the algorithm con- tinues bubbling until no adjacent items have been exchanged in a complete pass over the ordering. Thus, with noisy comparisons, BubbleSort is not guaranteed to halt, rather it will continue resorting items. We decided to parametrize BubbleSort by the number of outer loop iterations (more on this in the results section).\nAfter one iteration, the largest item should have found its place on the last position, then the second largest on the second to last position and so on. Thus, BubbleSort works in the opposite direction as SelectionSort. The parameter of the number of outer iterations determines the number of steps, e.g. t = 1, . . . , n − 1. Since the BubbleSort dynamical system has the potential to \u201cforget\u201d wrong pairwise order information, the time point t ∗ can be choosen, at which the two trajectories were closest together (overlapped in the last t positions). There, the maximal capacity is I BubbleSort t ∗ \t = log 2 n! (n−t)! .\nResults are provided for a comparison of ﬁve standard sorting algorithms. n = 16 items have been sorted 2 . A true underlying ranking π ∈ S n is assumed. When the rank π(i) of item i is smaller than the rank π(j) of item j, the pairwise comparison between the items yields i < j. When the algo- rithms put a query for a pairwise comparison, the true ranks are infected with Gaussian noise, i.e. ˜ π(i) ∼ N (π(i), σ 2 ) is used for evaluating the comparison. The respective standard deviation is denoted in the plots.\nFig. 5 shows the algorithms\u2019 absolute capacity versus the noise present in the data. We observe consistency between the ranking of the algorithms as induced by their capacity and the initial experimental ﬁndings in Fig. 1. Algorithms closer to the Pareto-front receive higher capacities. In the second plot, the capacity is normalized on the actual number of comparisons made, yielding the effective number of bits extractable by an algorithm per comparison. As expected, MergeSort reaches a value of 1 Bit here. Interestingly, passing some noise threshold, BubbleSort turns out to dominate other sorting procedures.\nPlots in Fig. 6 visualize the capacity of different MergeSort- and BubbleSort-variants. For MergeSort, we implemented\nmultiple comparisons (3, 5 and 7) and grounded each decision on their majority vote. This parametrization of the algorithm lets expect a higher capacity despite increasing noise, and the results conﬁrm this. The same strategy for BubbleSort can be implemented by increasing the time amount for bubbling (we used n/2, n − 1, 2n and 3n outer loop iterations). Also here we recognize that the capacity tends toward an upper bound. Knowledge of this upper bound will enable the selection of the sorting algorithm optimal in a practical noise situation. The plots with the capacity normalized by the number of comparisons again attest on BubbleSort performing better in terms of information extraction than MergeSort when the noise passes some threshold. This coincides with the experimental observation that, with increasing noise, the MergeSort Pareto- front moves behind the front spanned by BubbleSort.\nAlgorithm design has been mostly concerned with resource issues like time and space requirements. In this work, we have presented a new analysis technique for algorithms: The trajectories of algorithm runs are tracked in the relevant solu- tion space, thereby counting the number of bits an algorithm can extract before the overlap (and thus stability) between runs vanishes. This information-theoretic approach sheds new light on the informativeness of algorithms in noisy settings, and thereby will enable a new dimension of analysis of algorithms. It fosters the development of new stable algorithms\nwhich have to cope with uncertainties, e.g. for new transistor technologies that are energy-saving but prone to unreliable operation execution.\nThe impact of the new analysis was demonstrated for sorting algorithms. The method also will allow for an analytic calcula- tion of the maximal information capacity of an algorithm and thus can guide to optimally robust algorithm design. Moreover, the approach presented in this article relates data-dependent algorithmic complexities and statistical complexities."},"refs":[{"authors":[{"name":"J. M. Buhman"}],"title":{"text":"Information theoretic model validation for clustering, ISIT\u201910"}},{"authors":[{"name":"J. M. Buhman"}],"title":{"text":"Context Sensitive Information: Model Validation by Information Theory , Pattern Recognition, Lecture Notes in Computer Science, Volume 6718, Springer, 2011"}},{"authors":[{"name":"D. E. Critchlo"}],"title":{"text":"Metric Methods for Analyzing Partially Ranked Data, Springer, 1985"}},{"authors":[{"name":"I. Csicza"},{"name":"J. Koerne"}],"title":{"text":"Information Theory: Coding theorems for discrete memoryless systems , Academic Press, New York, 1981"}},{"authors":[{"name":"W. Elmenreic"},{"name":"T. Ibouni"},{"name":"I. Fehervar"}],"title":{"text":"Robustness versus Performance in Sorting and Tournament Algorithms , Acta Polytechnica Hungarica, 2009"}},{"authors":[{"name":"J. Giese"},{"name":"E. Schubert"},{"name":"M. Stojakovi"}],"title":{"text":"Approximate Sorting, Lecture Notes in Computer Science 3887, 2006"}},{"authors":[{"name":"D. E. Knut"}],"title":{"text":"The Art of Computer Programming, VOLUME 3, Sorting and Searching , 2nd ed"}},{"authors":[{"name":"K. B. Lakshmana"},{"name":"B. Ravikuma"},{"name":"K. Ganesa"}],"title":{"text":"Coping with Erroneous Information While Sorting , IEEE Transactions on Computers, Volume 40 Issue 9, 1991"}},{"authors":[{"name":"V. N. Vapni"}],"title":{"text":"Estimation of Dependences Based on Empirical Data, Springer, 1982"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569561021.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S15.T9.2","endtime":"10:30","authors":"Ludwig Busse, Morteza Chehreghani, Joachim Buhmann","date":"1341569400000","papertitle":"The Information Content in Sorting Algorithms","starttime":"10:10","session":"S15.T9: Strings, Sorting, and Biology","room":"Stratton West Lounge (201)","paperid":"1569561021"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
