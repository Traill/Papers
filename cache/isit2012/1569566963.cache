{"id":"1569566963","paper":{"title":{"text":"Design of Error-free Perfect Secrecy System by Preﬁx Codes and Partition Codes"},"authors":[{"name":"Chinthani Uduwerelle"},{"name":"Siu-Wai Ho"},{"name":"Terence Chan"}],"abstr":{"text":"Abstract\u2014We investigate how to design an error-free and perfectly secure crypto-system. In particular, we are interested in the efﬁciency of an EPS system. A approach based on preﬁx codes is introduced. Also an optimum partition code is introduced where the key consumption is minimum for ﬁxed number of channel uses. Results obtained in this paper can also be applied to study the tradeoff between the key consumption and the number of channel uses needed to transmit the encrypted message."},"body":{"text":"A crypto-system is deﬁned by a triple of random variables (U, X, R) such that U is the input message, R is the random key (shared by a pair of legitimate transmitter and receiver) and X is the cipher-text. The encrypted message X will be sent across a public channel to the receiver. We assume exis- tence of an adversary who can eavesdrop the public channel. In many cases, it is often assumed that the random key R should be independent of the message U , or equivalently,\nWith this assumption, the secret key can be generated and be shared between the transmitter and the receiver even before the encoder knows what the message U will be. In this paper, we will always keep (1) as our assumption.\nThe condition (2) guarantees that any eavesdropper who learns X from wiretapping the public channel will obtain no information about the input message U .\nIn addition to the above two conditions we assume that the decoder (which has access to R and receives X) can reconstruct U with zero error. Hence, U is a function of the cipher-text X and the shared private key R. So\nDeﬁnition 1 (EPS System [1]). A crypto-system (U, X, R) is called an error-free and perfectly secure (EPS) system if (U, X, R) satisﬁes constraint (1)\u2013(3).\nOnce we have identiﬁed the properties of an EPS system next task is to investigate how to design an EPS system efﬁciently. Efﬁciency of an EPS system is measured by two pa- rameters: 1) Key consumption which is deﬁned by I(R; U X) (see [1] for details) and 2) the number of channel uses to transmit X which is deﬁned by H(X).\nThe approach in this paper is inspired by the work done in [2], where it aims to design an EPS system for sources with unknown statistics. In [2] two crypto-systems, (U, R, X) and (U , R , X ) are considered with probability distribu- tions P U RX (u, r, x) and P U R X (u, r, x) respectively. These (U, R, X) and (U , R , X ) satisfy the following criteria:\n(C1) The sample spaces of the pair of random variables {U, U } (also of the pair {R, R } and the pair {X, X }) are the same.\n(C2) The support of P U is a subset of the support of P U . In other words, if P U (u) = 0, then P U (u) = 0.\n(C3) P XR|U (x, r|u) = P X R |U (x, r|u) for all x, r, u. It means that the encoder in the two crypto-systems are essentially the same.\nThe authors showed [2] that if (C1)\u2013(C4) are satisﬁed, then (U, R, X) is also an EPS system. This result has a very interesting implication in the design methodology of an EPS system. Suppose we do not know what the true statistic of our source U is. In this case, we can only make our best guess (P U ) on the probability distribution of U . Let U be the \u201cimaginary source\u201d whose underlying distribution is P U . Now, suppose we design an EPS system (speciﬁed by the conditional probability distribution P X R |U ) for this imaginary source U . The authors thus showed [2] that if we use the so-designed EPS encoder for U , the resulting crypto- system is also an EPS system.\nIn addition to the above results, [2] also proved that if the crypto-system (U , R , X ) achieves the minimum expected key consumption (i.e., I(R ; U X ) = H(U ) or equivalent I(X ; R ) = 0) then the expected key consumption in the new system [2, Theorem 4] is given by\nThe formula (4) clearly illustrates the relation between the amount of key consumption and the error in estimating the source\u2019s distribution. In this paper we will study how these results can be further extended in designing efﬁcient EPS systems for a given source distribution P U .\nThe organisation of this paper is as follows. First, we will study whether crypto-systems constructed from preﬁx codes (further details will be given later) are efﬁcient or not. We will also consider the relationship between the expected key\nconsumption and the expected codeword length. Applications of Huffman and Shannon coding to crypto-system design will be studied as a special case. Finally we introduce an optimum partition code [1] for a ﬁxed number of channel uses which can achieve the minimum expected key consumption.\nIn order to construct an EPS system, the conditions (1)\u2013(3) must be satisﬁed. To facilitate the discussion, we consider a simple approach by using preﬁx code together with one-time pad. However, it is easy to see that we need to apply the one- time pad carefully in order to satisfy the security constraint.\nExample 1. Suppose Huffman code is used to compress a source U where P U = {0.5, 0.25, 0.125, 0.125}. Then one of the possible set of codewords from the Huffman code is {0, 10, 110, 111}. Consider one-time pad is directly applied to this code (i.e., by doing bitwise XOR between the codeword and a key with the same length). Then it will result in a variable length cipher-text X which can still give some information about the message U . Hence the security condition (2) is violated.\nTo avoid this problem, we can add some random padding bits to the end of the codewords such that all the codewords have the same length. A formal deﬁnition is given as follows.\nDeﬁnition 2 (Preﬁx code with padding bits). Consider a preﬁx code C and suppose U = u.\n\u2022 Step 2: Add random padding bits to C(u) so that a random variable V u is constructed where the padding bits are uniformly distributed and independent of other random variables. The amount of padding bits are chosen such that V u and the longest codeword in C have the same length.\n\u2022 Step 3: Let ϕ be the total number of possible values of V u over all u ∈ U . Choose R to be uniformly distributed over ϕ.\nFollowing Example 1, if U = 1 and P U (1) = 0.5, then \u20180\u2019 is chosen from C. Two random bits are added to this codeword so that V 1 = (000), (001), (010) or (011) with equal chance. Finally, X is generated by applying one-time pad to V 1 . Note that the key consumption is not simply H(R) = 3 bits. Since two extra secret bits are securely received by the decoder, the net key consumption in this case is 3 − 2 = 1 bit.\nHere we can easily verify that the system described in Deﬁnition 2 is an EPS system, i.e., (1)\u2013(3) are satisﬁed. The Following theorem gives the key consumption of this system for a source distribution P U .\nTheorem 1. Consider a crypto-system based on the preﬁx code C in Deﬁnition 2. Let {l u } be the set of codeword lengths in C and ν = u 2 −l u . If this crypto-system is applied to a\nν , for 1 ≤ u ≤ |U | \t (6) then\nProof: We can ﬁrst obtain (7) by substituting (6) in to (4). By re-expressing (7), we can get (5) in terms of expected codeword length.\nThe equation (7) tells that the minimum expected key consumption, i.e., I(R; U X) = H(U ) can be achieved if D(P U ||P U ) = 0. Therefore, if the source distribution is dyadic, Huffman code can achieve the minimum I(R; U X). This can be easily veriﬁed from (5) which also shows that the expected key consumption is not simply the expected codeword length because there is an extra term with ν. We will give some examples showing that ν plays a critical role here and Huffman code is not always the best to minimize the key consumption although it gives the minimum expected codeword length. Before that, we present two simple results about I(R; U X).\nProof: Since ν ≤ 1 for preﬁx codes [3], (8) follows from (5).\nCorollary 3. Following the setup in Theorem 1, if Shannon and Huffman coding is used, then\nProof: This follows from the properties of the Huffman and Shannon coding [3].\nSince I(R; U X) ≥ H(U ) [1], the Corollary 3 shows that the new expected key consumption is within one bit from H(U ). Furthermore, Shannon coding can sometimes give a smaller I(R; U X) comparing with Huffman coding as shown in the example in Table I. However, in some cases Huffman code is better than Shannon code as shown in Table II.\nTherefore, it is interesting to ﬁnd a code which can always minimize I(R; U X). In particular, we are interest to ﬁnd a code which can achieve the minimum, i.e., I(R; U X) = H(U ). Furthermore, the above discussion has not consid- ered H(X) yet. Recall that H(X) measures the number of channel uses to transmit X. It is also important to minimize H(X). From Deﬁnition 2, we can easily check that H(X) is equal to the longest codeword length. If we are interested in minimizing H(X) as well, Huffman code may not be a good choice because the longest codeword in Huffman code can be as much as 44% larger than the longest codeword in\nShannon code [4]. In the next section, we will propose a code called partition code which will always outperform the coding scheme described in Deﬁnition 2.\nThe initial ideas about partition code have been introduced in [1]. This code was used to achieve the minimum key consumption, i.e., I(R; U X) = H(U ). It is speciﬁed by a vector of integers Ψ as shown in the following deﬁnition.\nDeﬁnition 3. [Partition Code C(Ψ θ )] Assume that U is deﬁned on a ﬁnite alphabet with size M . Let Ψ θ = (ψ θ (1), ψ θ (2), . . . , ψ θ (M )) and let θ = M i=1 ψ θ (i). If U = i, A is uniformly picked from the set {1, 2, . . . , ψ θ (i)}. Let A = U −1 i=1 ψ θ (i) + A − 1, R be uniformly distributed on the set {0, 1, . . . , θ − 1} and X = (A + R) mod θ.\nNote that for θ = \t M i=1 ψ θ (i), the code C(Ψ θ ) gives H(X) = log θ. In this section, we will show the optimal partition code which can achieve the minimum I(R; U X) when θ (i.e., H(X)) is ﬁxed. As we will see, there is a tradeoff between I(R; U X) and H(X). Furthermore, partition code performs better than Shannon code and Huffman code for both aspects in key consumption and number of channel uses.\nDeﬁnition 4. [Algorithm to generate r θ ] Let r θ = (r θ (1), r θ (2), . . . , r θ (M )) be a vector of positive integers such that\nFor any given P U = {P U (1), P U (2), . . . , P U (M )} and θ ≥ M , r θ is deﬁned by the following iterative procedure:\n2) Iteration step: If j = θ, then go to Step 4. Otherwise, let\nr j (i) \t if i = , r j ( ) + 1 if i = ,\nFor any given θ, the optimal partition code C(Ψ θ ) can be obtained by letting Ψ θ = r θ from Deﬁnition 4 as shown in the following theorem.\nTheorem 4. For any given P U , the partition code C(r θ ) constructed from Deﬁnition 4 achieves the minimum I(R; U X) among all the partition codes achieving H(X) = log θ.\nProof: For any given θ, consider the partition code C(Ψ θ ). In order to prove this theorem, we need to construct some auxiliary random variables (U , R , X ). For 1 ≤ i ≤ M , let\nθ . \t (14) If the partition code C(Ψ θ ) is applied to U , it can be veriﬁed that R and X are both uniformly distributed on the set {0, 1, . . . , θ − 1} and I(X ; R ) = 0. Also, {U , R , X }\nsatisﬁes Deﬁnition 1 for the deﬁnition of an EPS system. Now, suppose the partition code C(Ψ θ ) is applied to U and we have a set of random variables (U, R, X). Note that {U, R, X} and {U , R , X } satisfy (C1)\u2013(C4). From (4), the expected key consumption for applying C(Ψ θ ) on P U is given by\nSince P U and θ are given, we only need to consider the last summation in (16) in order to ﬁnd the minimum I(R; U X). Note that the support of U may be less than θ. Therefore,\nwhere (20) follows from log x is an increasing function of x > 0. Deﬁne\nWe are going to show that for r θ obtained from Deﬁnition 4, Φ(r θ ) ≥ Φ(Ψ θ ) and hence the minimum I(R; U X) can be achieved by r θ due to (20).\nand this is the only vector satisfying the summation of elements equal to θ. Therefore, Φ(r θ ) ≥ Φ(Ψ θ ).\nIn the following, we are going to show that Φ(r θ+1 ) ≥ Φ(Ψ θ+1 ) for any Ψ θ+1 .\nLet w θ+1 = (w(1), . . . , w(M )) be a vector with θ + 1 = M i=1 w(i) and w θ+1 = r θ+1 . Now, we prove by contradiction with the assumption\nFollow the deﬁnition of in (13) and consider the problem into two cases:\nTherefore, Φ(w θ ) ≥ Φ(r θ ) + (Φ(w θ+1 ) − Φ(r θ+1 )) > Φ(r θ ), which contradicts (23). So the assumption (24) is incorrect and Φ(w θ+1 ) ≤ Φ(r θ+1 ).\nw(i) = θ + 1 = 1 + i r θ (i). Since w( ) and r θ ( ) are integers\n(32) where (30)\u2013(32) follows from (28), (13) and (29). This can be simpliﬁed to\nNow, we construct w (1) θ+1 by changing only w( ) and w(j) in w θ+1 . Let\nwhere the inequalities follow from (33) and (24), respec- tively. Therefore,\na) We can check case i) for w (1) θ+1 . If w (1) θ+1 ( ) ≥ r( )+ 1, then it leads to a contradiction as shown in case i). Therefore, the theorem is proved.\nb) If w (1) θ+1 belongs to case ii), we can generate a new vector w (2) θ+1 according to the above argument. After that, we go back to a) and check case i) again.\nc) Note that w (i) θ+1 = w (j) θ+1 for i = j. Also, the -th term in w (i) θ+1 is equal to w( ) + i. Therefore, the condition in case i) will be satisﬁed in ﬁnite steps and the theorem is proved.\nTherefore we have proved that (24) will lead to contradictions and hence r θ minimizes I(R; U X).\nRemark: The optimum vector may not be unique. For example, if P U = (0.4, 0.4, 0.2), then\nθ = 3, Ψ 3 = (1, 1, 1) \t (36) θ = 4, Ψ 1 4 = (2, 1, 1) \t (37)\nΨ 2 4 = (1, 2, 1) \t (38) θ = 5, Ψ 5 = (2, 2, 1). \t (39)\nIn this example, both C(Ψ 1 4 ) and C(Ψ 2 4 ) achieve the minimum I(R; U X) for θ = 4.\nIf we consider the complexity of this algorithm. In the initial step, it will calculate the values for all r θ (i) where 1 ≤ i ≤ M and all the values will be stored. For all further steps, it will calculate only the l-th term which is selected according to (13) and increased by 1 in the last step. So the complexity of this algorithm is linear in θ.\nthe optimum partition code from Deﬁnition 4 and we have plotted I(R; U X) verses H(X) in Fig. 1. The Huffman and Shannon coding together with padding are also shown in the same picture. We can see that the performance of partition code is better than Huffman or Shannon codes. This can be explained by Theorem 4 together with the following theorem. Theorem 5. The preﬁx code with padding scheme described in Deﬁnition 2 is a special case of partition code in Deﬁnition 3.\nProof: Let {l u } be the set of codeword lengths given by the preﬁx code C in Deﬁnition 2 and I max to be the length of the longest codeword. Take N as the total number\nof partitions. For 1 ≤ i ≤ M , let ψ N (i) = 2 {I max −l u } . The code in Deﬁnition 2 can be seen as Ψ N in Deﬁnition 3.\nWe can see a tradeoff between I(R; U X) and H(X) in Fig. 1. Also it is clear from the ﬁgure that there is a signiﬁcant reduction in H(X) if partition code is used instead of Huffman or Shannon coding for the same I(R; U X). Although we know how to ﬁnd the partition code which minimize I(R; U X) for any given H(X) = log θ, it is interesting to ﬁnd an analytical bound on I(R; U X) as well. Then we can see how fast I(R; U X) drops when we increase H(X). This will be done in the next section.\nSuppose the support of P U is {1, . . . , |U |} and P U (i) ≥ P U (j) if i < j. Consider a partition code C(Ψ N ) with\n \nwhere N is sufﬁciently large such that P U (i)N ≥ 1 for all 1 ≤ i ≤ |U |. Let\nWe can verify that if the partition code C(Ψ N ) is used to encode P U , then I(X ; R ) = 0. Therefore, if the partition code C(Ψ N ) is used to encode P U , we can apply (4) to show that\nFrom (40) it is easy to verify that P U is majorized by P U [5]. Therefore by applying [6, Theorem 3]\nI(R; U X) ≤ H(U ) + (H(U ) − H(U )) \t (43) = H(U ). \t (44)\nBy the interesting property in (44), we have the following theorem which shows the upper bound on I(R; U X) in terms of H(X) for the optimal code.\nTheorem 6. For any given integer N ≥ |U | + 1, if H(X) = log N , then the partition code achieving minimum I(R; U X) satisﬁes\n|U | N\n|U | N\nwhere h(x) = x log 1 x + (1 − x) log 1 1−x for 0 < x < 1 and h(0) = h(1) = 0.\nProof: Consider the partition code deﬁned in (40) and the induced P U given in (41). Then the variational distance\nSince we know the variational distance between P U and P U , we can ﬁnd a bound on H(U ). Note that V (P U , P U ) ≤\n|U |+1 because N ≥ |U | + 1. Then we can apply [7, Theorem 6] to show that\nN log |U |, (51) so the partition code achieving the minimum I(R; U X) and H(X) = log N can give a smaller I(R; U X) and hence (45) is justiﬁed.\nThe bound (45) can be considered as an inner bound for the optimal tradeoff between the expected key consumption and the number of channel uses. If N is large, the term |U | N log |U | is dominated in (45). In this case, I(R; U X) − H(U ) is decaying in 1 N . Since H(X) = log N , I(R; U X) − H(U ) is decaying exponentially fast in H(X).\nWe studied the relationship between the expected codeword length of the encrypted message and the expected key con- sumption when a preﬁx code is encrypted after padding some random bits at the end of the codewords. The expected key consumption of EPS systems obtained by this approach is determined. Besides this approach, we have proposed a better code which achieves the minimum expected key consumption among all partition codes when the number of channel uses is ﬁxed. Comparing to the approach based on Huffman and Shannon coding, the number of channel uses is signiﬁcantly reduced in our code for the same amount of key consumption."},"refs":[{"authors":[{"name":"S.-W. Ho"},{"name":"T. Chan"},{"name":"C. Uduwerelle"}],"title":{"text":"Error-free Perfect Secrecy Systems"}},{"authors":[{"name":"C. Uduwerelle"},{"name":"S.-W. Ho"},{"name":"T. Chan"}],"title":{"text":"Design of an Error-free Perfect Secrecy System for unknown inputs"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, New York: Wiley-Interscience, Second Ed"}},{"authors":[{"name":"Y. Abu-Mostafa"},{"name":"R. McEliece"}],"title":{"text":"Maximal codeword lengths in huffman codes"}},{"authors":[{"name":"A. W. Marshal"},{"name":"I. Olki"}],"title":{"text":"Inequalities: Theory of Majorization and Its Applications, Academic Press, New York, 1979"}},{"authors":[{"name":"S.-W. Ho"},{"name":"S. Verd´u"}],"title":{"text":"On the Interplay Between Conditional Entropy and Error Probability."}},{"authors":[{"name":"S.-W. Ho"},{"name":"R. Yeung"}],"title":{"text":"The interplay Between Entropy and Variational Distance."}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566963.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T6.3","endtime":"10:50","authors":"Chinthani Uduwerelle, Siu-Wai Ho, Terence H. Chan","date":"1341397800000","papertitle":"Design of Error-free Perfect Secrecy System by Prefix Codes and Partition Codes","starttime":"10:30","session":"S9.T6: Synchrony and Perfect Secrecy","room":"Kresge Rehearsal A (033)","paperid":"1569566963"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
