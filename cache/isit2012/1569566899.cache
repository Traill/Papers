{"id":"1569566899","paper":{"title":{"text":"Extrinsic Jensen\u2013Shannon Divergence with Application in Active Hypothesis Testing"},"authors":[{"name":"Mohammad Naghshvar"},{"name":"Tara Javidi"}],"abstr":{"text":"Abstract\u2014Consider a decision maker who is responsible to dynamically collect observations so as to enhance his information in a speedy manner about an underlying phenomena of interest while accounting for the penalty of wrong declarations.\nIn this paper, Extrinsic Jensen\u2013Shannon (EJS) divergence is introduced as a measure of information. Using EJS as an information utility, a heuristic policy for selecting actions is proposed. Via numerical and asymptotic optimality analysis, the performance of the proposed policy, hence the applicability of the EJS divergence in the context of the active hypothesis testing is investigated."},"body":{"text":"This paper considers the problem of active M \u2013ary hy- pothesis testing. A Bayesian decision maker is responsible to enhance his information about the true hypothesis in a speedy manner while accounting for the penalty of wrong declarations. In contrast to the classical hypothesis testing problem [1], our decision maker can choose one of K available actions and hence exert some control over the collected sample\u2019s \u201cinformation content.\u201d The special cases of active hypothesis testing naturally arise in a broad spectrum of applications in cognition, medical diagnosis, communication, and sensor management (see [2] and references therein).\nTo achieve the best performance, it is essential that the decision maker, in each step, selects an action that provides the \u201chighest amount of information.\u201d This raises the question as what measure of information is appropriate in this context. In [3], posing the problem as a POMDP, the optimal notion of information utility was shown to be nothing but the optimal value function of the corresponding dynamic programming. In this fashion, the problem of active sequential hypothesis testing reduces to a sequence of one\u2013shot problems, in each of which an optimal action is the one that maximizes the optimal information utility. Although this result provides a general and structural characterization of the optimal policy (Markov and deterministic), in the absence of a closed\u2013form for the optimal value function, it is of little use in identifying achievable schemes.\nIn [3], inspired by Burnashev\u2019s coding scheme [4], we proposed a two\u2013phase randomized policy for selecting actions as follows. In the ﬁrst phase, the decision maker iteratively\nreﬁnes his belief about the true hypothesis; while the second phase serves to verify the correctness of the output of phase one. As a measure of performance, we used the notion of asymptotic optimality due to Chernoff [5] when the penalty of wrong declaration, l, increases (l → ∞). It was shown that the proposed policy is asymptotically optimal in many special cases of active hypothesis testing such as active binary hypothesis testing ( M = 2), noisy dynamic search, and channel coding with ﬁnite messages (zero rate) in the presence of perfect feedback. Later, in [6], we showed that the asymptotic optimality of the proposed policy remains valid beyond the above special cases so long as the number of hypotheses M is ﬁxed (also proved independently in [7]). In [2], we tackled the asymptotic optimality in M as well and proposed yet another two\u2013phase randomized policy and proved its asymptotic optimality as l, M → ∞ for channel coding with perfect feedback (non\u2013zero rate).\nWhat remains unsatisfying about the above achievability schemes is that they all rely on randomization and a two\u2013 phase operation exhibiting a disconnect to our earlier structural characterization of the optimal policy [3] as sequential and deterministic (Markov) in nature. In this paper, we take a ﬁrst step toward constructing deterministic one\u2013phase policies for the problem of active hypothesis testing. Inspired by the popularity of Jensen\u2013Shannon divergence among practitioners as a measure of information content of samples [8], [9], we introduce two heuristic policies. We start with a policy that at any given time maximizes Jensen\u2013Shannon divergence; but to address the shortcomings of this approach, we introduce Extrinsic Jensen\u2013Shannon (EJS) divergence as an alternative measure 1 maximizing which gives rise to our second policy. Via numerical and asymptotic analysis, the performance of the proposed policies and the relevance of the EJS divergence in the context of the active hypothesis testing are investigated.\nThe remainder of this paper is organized as follows. In Section II, we formulate the problem of active hypothesis testing and propose various heuristics for selecting actions. Section III provides the main results of the paper. Finally, we conclude the paper and discuss future work in Section IV.\nNotation: Let [x] + = max{x, 0}. A random variable is denoted by an upper case letter (e.g. X) and its realiza-\ntion is denoted by a lower case letter (e.g. x). Similarly, a random column vector and its realization are denoted by bold face symbols (e.g. X and x). For any set S, |S| de- notes the cardinality of S. The entropy function on a vector ρ = [ρ 1 , ρ 2 , . . . , ρ M ] ∈ [0, 1] M is deﬁned as H(ρ) =\nρ i log(1/ρ i ), with the convention that 0 log 1 0 = 0. The Kullback\u2013Leibler (KL) divergence between two probability density functions q(·) and q (·) is denoted by D(q||q ) where D(q||q ) = q(z) log q (z) q (z) dz. Finally, let N (m, σ 2 ) denote a normal distribution with mean m and variance σ 2 .\nIn Subsection II-A, we formulate the problem of active hypothesis testing, referred to as Problem (P) hereafter. In Subsection II-B, heuristic policies for selecting actions are proposed.\nProblem (P): Let Ω = {1, 2, . . . , M }. Let H i , i ∈ Ω, denote M hypotheses of interest among which only one holds true. Let θ be the random variable that takes the value θ = i on the event that H i is true for i ∈ Ω. We consider a Bayesian scenario with prior belief ρ = [ρ 1 , ρ 2 , . . . , ρ M ], i.e., initially P ({θ = i}) = ρ i for all i ∈ Ω. A is the set of all sensing actions and is assumed to be ﬁnite with |A| = K < ∞. Z is the observation space. For all a ∈ A, the observation kernel q a i (·) (on Z) is the probability density function for observation Z when action a has been taken and H i is true. We assume that {q a i (·)} i,a are known. Let l denote the penalty for a wrong declaration. Let τ be the stopping time at which the decision maker retires. The objective is to ﬁnd a sequence of sensing actions A(0), A(1), . . . , A(τ − 1), a stopping time τ , and a declaration rule d : A τ −1 ×Z τ −1 → Ω that collectively minimize the total cost:\nwhere the expectation is taken with respect to the initial belief as well as the distribution of observation sequence.\nProblem (P) is a partially observable Markov decision problem (POMDP) where the state is static and observations are noisy. This problem is equivalent to an MDP whose information state at time t is the belief vector ρ(t) = [ρ 1 (t), . . . , ρ M (t)] where ρ i (t) = P ({θ = i}|ρ, A t −1 , Z t −1 ). Accordingly, the information state space is deﬁned as P(Θ) = {ρ ∈ [0, 1] M : M i =1 ρ i = 1} where Θ is the σ\u2013algebra generated by random variable θ.\nDeﬁnition 1. Let d i , i ∈ Ω, represent an action under which the decision maker retires and declares H i as the true hypothesis. Markov stationary deterministic policy 2 is a mapping from P(Θ) to A ∪ {d 1 , . . . , d M } based on which sensing actions A(t), t = 0, 1, . . . , τ − 1 and stopping time τ\nare selected (the choice of any of the retire-declare actions marks the stopping time τ ).\nIn this paper, we consider two heuristic Markov policies based on the following two principles. The ﬁrst principle follows from rewriting (1) as\nand then noting that when l(1 − ρ i (τ )) ≤ 1, the further reduction in l(1 − ρ i (τ )) is not worth taking one more sensing action and hence increasing τ by 1. The intuition behind the second principle is that, by choosing an appropriate measure of information, the problem of active sequential hypothesis testing can be reduced to a sequence of one\u2013shot problems in each of which an optimal sensing action is selected determin- istically so as to provide the highest amount of information. In particular, we ﬁrst consider the following notion of divergence as a measure of information:\nDeﬁnition 2. The Jensen\u2013Shannon (JS) divergence among probability density functions q 1 , q 2 , . . . , q M with respect to ρ = [ρ 1 , ρ 2 , . . . , ρ M ] is deﬁned as\nRemark 1. JS divergence was ﬁrst introduced in [12]. This di- vergence measure, which is nothing but the mutual information between distributions ρ and q ρ , i.e. I(ρ; q ρ ), has been applied in many areas such as information theory, image processing, genome comparison, etc.\nNote that as the belief about one of the hypotheses, say ρ i , approaches 1, D(q i ||q ρ ) approaches to D(q i ||q i ) = 0; and consequently, independently of the observation kernels q 1 , q 2 , . . . , q M , JS(ρ; q 1 , q 2 , . . . , q M ) approaches 0. To ad- dress this insensitivity to the choice of the observation kernels, we propose the following modiﬁcation.\nDeﬁnition 3. The Extrinsic Jensen\u2013Shannon (EJS) divergence among probability density functions q 1 , q 2 , . . . , q M with re- spect to ρ = [ρ 1 , ρ 2 , . . . , ρ M ] is deﬁned as\nWe are now ready to introduce our heuristic policies. 1) Policy π JS is deﬁned as follows:\nd i \t if l(1 − ρ i ) ≤ 1, i ∈ Ω arg max\nd i \t if l(1 − ρ i ) ≤ 1, i ∈ Ω arg max\nRemark 2. Note that as belief about one of the hypotheses, say ρ i , becomes large, π EJS selects action a such that D(q a i || k =i ρ k 1−ρ i q a k ) is maximized, i.e. it selects an action that distinguishes H i from the collection of alternate hypothe- ses the most; in contrast to π JS maximization of the mutual information. As we will see in Section III-C, these different philosophies result in signiﬁcant performance difference.\nSubsection III-A provides the results from [3] on the cor- responding dynamic programming (DP) equation for Prob- lem (P) and characterizes an optimal policy. We then use this formation to connect our heuristic policies to the MDP approach and further provide numerical and analytical results regarding the performance of the proposed policies.\nAs mentioned in Section II-B, Problem (P) is equivalent to an MDP with state space P(Θ) whose information state at time t is the belief vector ρ(t). In one sensing step, the evolution of the belief vector follows Bayes\u2019 rule and is given by Φ a , a measurable function from P(Θ) × Z to P(Θ) for all a ∈ A:\nDeﬁnition 4. A Markov stationary deterministic policy that minimizes (1) is referred to as an optimal policy and is denoted by π ∗ .\nWe deﬁne operator T a , a ∈ A, such that for any measurable function g : P(Θ) → R,\nFact 1 (Consequence of Theorems 1, 4 in [11]). Let V ∗ : P(Θ) → R + be a functional solving the following ﬁxed point equation:\nThen V ∗ (ρ), referred to as the optimal value function, is equal to the minimum cost in Problem (P) when the initial belief is ρ.\nAs a corollary to Fact 1, we can characterize an optimal policy π ∗ from (5). To do so, we ﬁrst introduce a notion of information utility .\nDeﬁnition 5. Associated with a functional V : P(Θ) → R + , the information utility of action a at information state ρ is deﬁned as IU(a, ρ, V ) := V (ρ) − (T a V )(ρ).\nTogether with (5), this results in the following optimal deterministic policy:\nd i \t if V ∗ (ρ) = l(1 − ρ i ), i ∈ Ω arg max\nFinding optimal policy π ∗ for Problem (P) requires knowl- edge of V ∗ whose closed\u2013form, in general, is not known. Next we connect the above dynamic programming characterization to the construction of π JS and π EJS .\nProposition 1 below shows that the proposed JS and EJS divergences are nothing but the information utility associated with the following two candidate functionals:\nProposition 1. The information utilities associated with the entropy function H(·) and the average likelihood function U (·) are respectively given by\nIU (a, ρ, H) = JS(ρ; q a 1 , . . . , q a M ), \t (6) IU (a, ρ, U ) = EJS(ρ; q a 1 , . . . , q a M ). \t (7)\nProof: The proof follows a simple algebraic manipulation and is omitted in the interest of brevity.\nSimilarly, Proposition 1 shows that policies π JS and π EJS are nothing but Markov deterministic policies which maximize the information utility associated with Shannon entropy and average log-likelihood. In the next subsection we will discuss the analytic performance of these policies.\nIn this section, we ﬁrst ﬁnd an upper bound for the expected total cost of π EJS under the following technical assumptions.\nAssumption 1. For any two hypotheses i and j, i = j, there exists an action a, a ∈ A, such that D(q a i ||q a j ) > 0.\nAssumption 1 ensures the possibility of discrimination be- tween any two hypotheses. Assumption 2 implies that no two hypotheses are fully distinguishable using a single observation sample.\n1) Performance Bounds: To continue with our analysis, we need the following notation. Let Λ(A) denote the collection of all probability distributions on elements of A, i.e., Λ(A) = {λ ∈ [0, 1] |A| : a ∈A λ a = 1}.\nFact 2 (Proposition 2 in [2]). Under Assumption 1 and for arbitrary δ ∈ [0, 0.5], there exists K independent of M and l such that\n2) Asymptotic Optimality: The upper and lower bounds provided by Proposition 2 and Fact 2 can be applied to establish the asymptotic optimality of π EJS as deﬁned below. Deﬁnition 6. Let V π (ρ) denote the the expected total cost of policy π when the initial belief is ρ. Policy π is referred to as asymptotically optimal in l (and M ) if for all ρ ∈ P(Θ),\nand π EJS is asymptotically optimal in l and M if the following condition holds as well,\nThere are many important special cases of the active hy- pothesis testing for which conditions (8) and (9) hold. Among these cases are active binary hypothesis testing (M=2) and noisy dynamic search under which (8) holds establishing the asymptotic optimality of π EJS in l (see [2] for details). In [2], we have addressed the relation between the problems of chan- nel coding with perfect feedback and active hypothesis testing and in [13], we have shown that π EJS achieves asymptotic optimality in l and M providing the only known deterministic one\u2013phase policy that achieves the optimal error exponent of any discrete memoryless channel (DMC) in the presence of the perfect feedback.\nIn the next subsection, the performance of policies π ∗ , π JS , and π EJS will be compared numerically.\nConsider the problem of sequentially searching for one and only object of interest in an image with M segments. Let A be the set of all allowable combinations of segments that the player can select to observe in one time slot. Fig. 1 illustrates an instance of the problem where M = 4 and A = {1, 2, 3, 4, (1, 2), (3, 4), (1, 3), (2, 4)}. The player collects visual samples affected by the background noise and is respon- sible to ﬁnd the object quickly and accurately.\nObservation kernels {q a i (·)} i,a are assumed to be of the following form:\nIn other words, observation samples are distributed as f obj if the object is in the segment(s) that the player is observing; otherwise samples have the probability density function f noise .\nIn our performance comparisons, we also consider policy π M L below whose asymptotic optimality was proved in [3].\nd i \t if l(1 − ρ i ) ≤ 1, i ∈ Ω arg max\nNext the performance of policies π ∗ , π JS , π EJS , and π M L are compared numerically for the example of Fig. 1 with f obj = N (1, 1) and f noise = N (0, 0.5). Fig. 2 plots the expected total cost of the candidate policies under uniform initial information state ρ = [1/4, 1/4, 1/4, 1/4]. Fig. 2 conﬁrms the signiﬁcant improvement of π EJS over all other heuristics.\nIn this paper, we considered the problem of active sequential hypothesis testing. Using Extrinsic Jensen\u2013Shannon (EJS) divergence as an information utility, a heuristic policy, referred to as π EJS , was proposed. Via numerical and asymptotic analysis, the performance of π EJS was investigated. It was shown that π EJS is the only known deterministic one\u2013phase policy that achieves asymptotic optimality in many practically relevant problems such as noisy dynamic search and channel coding with perfect feedback. We conjecture that the asymp- totic optimality of π EJS remains valid beyond the special cases discussed in this paper. Proving this conjecture requires improving the proposed lower and upper bounds and is part of the current research.\nThe authors would like to thank M. Raginsky and Y. Polyan- skiy for helpful discussions and V. Anantharam for pointing out the connection to anthropic correction. Authors are in- debted to S. Verd´u for suggesting the word extrinsic.\nLet ˜ ρ = 1 − 1 log M and ˜ τ , ˜ τ i , τ , τ i , i ∈ Ω, be Markov stopping times deﬁned as follows:\nρ j (n) ≥ ˜ ρ}, ˜ τ i = min{n : ρ i (n) ≥ ˜ ρ}, τ = min{n : max\nFrom (2), the total cost under policy π EJS satisﬁes V π EJ S (ρ) ≤ E[τ] + 1 = E[˜τ] + E[τ − ˜τ] + 1\nwhere Z i = {Z ∞ : ρ i (˜ τ ) < ˜ ρ} and (a) follows from the fact that τ ≤ τ i , i ∈ Ω.\nWhat remains is to ﬁnd upper bounds for the terms in the right-hand side of (10).\nLet F n be the history of actions and observations up to time n, i.e. F n = σ{ρ, A(0), Z(0), . . . , A(n − 1), Z(n − 1)}. Moreover, let deﬁne the sequence {ζ n }, n = 0, 1, . . ., as follows\nNote that under policy π EJS sensing actions are selected in a way to maximize the EJS divergence and we obtain,\nE [ζ n +1 − ζ n |F n ] = E [U (ρ(n)) − U (ρ(n + 1))|F n ] = max\nTherefore, the sequence { ζ n D ∗ − n}, n = 0, 1, . . . forms a submartingale with respect to a ﬁltration {F n } and by Doob\u2019s stopping theorem,\nNext we ﬁnd an upper bound for the second term in the right\u2013hand side of (10). Note that if ρ i (n) ≥ ˜ ρ, then\nConstructing a proper submartingale and using Doob\u2019s stopping theorem, we obtain"},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566899.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T8.1","endtime":"11:50","authors":"Mohammad Naghshvar, Tara Javidi","date":"1341487800000","papertitle":"Extrinsic Jensen-Shannon Divergence with Application in Active Hypothesis Testing","starttime":"11:30","session":"S12.T8: Hypothesis Testing","room":"Stratton (491)","paperid":"1569566899"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
