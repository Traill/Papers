{"id":"1569564613","paper":{"title":{"text":"Jar Decoding: LDPC Coding Theorems for Binary Input Memoryless Channels"},"authors":[{"name":"En-Hui Yang"},{"name":"Jin Meng"}],"abstr":{"text":"Abstract\u2014Recently, a new decoding rule called jar decoding was proposed, under which the decoder ﬁrst forms a set of suitable size, called a jar, consisting of sequences from the channel input alphabet considered to be closely related to y n , and then takes any codeword from the jar as the estimate of the transmitted codeword. In this paper, we show that under jar decoding, the analysis of low density parity check (LDPC) codes is much easier compared to maximum a posteriori (MAP) or maximum likelihood (ML) and Belief Propagation (BP) decoding, and new general LDPC coding theorems can be established. Speciﬁcally, it is proved that LDPC codes can approach the mutual information, with diminishing bit error probability, of any binary input memoryless channel with uniform input distribution when the average variable node degree is large. Moreover, simulation shows an interesting connection between jar decoding and BP decoding, i.e., BP decoding can be regarded as one of many ways to pick up a codeword from the jar for LDPC codes when it succeeds in outputting a codeword.\nIndex Terms\u2014Belief propagation decoding, channel capacity, jar decoding, low-density parity-check codes, maximum a pos- teriori (MAP) decoding, non-asymptotic coding theorems, non- asymptotic equipartition properties."},"body":{"text":"So far, the theoretic performance analysis of low density parity check (LDPC) codes has been carried out in two distinct approaches. Originating from Gallager\u2019s milestone paper [1], one approach is to exploit the best possible performance of LDPC codes with respect to word error probability under maximum a posteriori (MAP), maximum likelihood (ML) or minimum distance (MD) decoding. The other approach, as demonstrated in Urbanke and Richardson\u2019s award-winning paper [2], is to investigate the performance of LDPC codes with respect to bit error probability under belief propagation (BP) decoding.\nBoth approaches have their strengths and weaknesses. Un- der ML decoding (which is codebook-centric as it involves the optimization of likelihood of codewords over the en- tire codebook), coding theorems on LDPC ensembles can be derived rigorously for memoryless binary-input output- symmetric (MBIOS) channels [1], [3] [4]. Based on Hamming weight distributions (or spectrums) of LDPC ensembles, the analysis in this direction, for which symmetry of channel is essential, is concerned mainly with word error probability and is asymptotic. On the other hand, density evolution [2]\n[5] is a powerful tool to analyze LDPC codes under BP decoding (which is output-centric, as the iterative decoding procedure starts with a channel output and ends upon ﬁnding a codeword); it provides a good approximation of performance (concerning bit error probability) of LDPC codes and also facilitates the optimization of degree distributions. In general, however, rigorous analysis in this approach is computationally prohibitive, and its result is hard to comprehend without any approximation. In addition, most of analysis is also asymp- totic.\nRecently, jar decoding was proposed as an alternative de- coding rule [6]. Under jar decoding, the decoder ﬁrst forms a set of suitable size, called a jar, consisting of sequences from the channel input alphabet considered to be closely related to the channel output, and then takes any codeword from the jar as the estimate of the transmitted codeword. In contrast with MAP and ML decoding, jar decoding is channel output sequence centric. It is ﬂexible in the sense that it can handle both the word error probability and bit error probability; it is also powerful in the sense that new general coding theorems (with respect to channel statistics) can be established. Extracting the portion related to LDPC codes from its full version [6], this paper aims to demonstrate the capability of jar decoding to handle bit error probability. It is shown that compared to ML or BP decoding, jar decoding makes LDPC code performance analysis much easier; under jar decoding, general LDPC coding theorems can be established. Speciﬁ- cally, consider any binary input memoryless channel (BIMC) X → Y where X is assumed to be uniformly distributed throughout the paper. Then given any bit error probability, non- asymptotic bounds on the achievable rate of LDPC codes are derived, which show that for any valid variable and check node degree distribution L(z) and R(z), LDPC ensembles with variable and check node degree distributions L(z k ) and R(z k ) (where each degree in L(z) and R(z) is multiplied by k) can approach I(X; Y ), with diminishing bit error probability, as k increases. As a by-product, it is also proved that for any ﬁxed variable node degree distribution, the optimal check node degree distribution of a LDPC code under jar decoding is right concentrated. Last but not the least, as jar decoding and BP decoding shares an interesting similarity\u2014both are output sequence centric\u2014simulation further reveals that BP decoding can be considered as one of many ways to pick up a codeword from the jar for LDPC codes when it succeeds in outputting a codeword.\nThe rest of the paper is organized as follows. Section II reviews the concept of jar decoding. LDPC coding theorems\nunder jar decoding are established in Section III. Simulation results are reported in Section IV to conﬁrm that for LDPC codes, BP decoding can be viewed as one of many ways to pick up a codeword from the jar when it succeeds in outputting a codeword. Finally, conclusions are drawn in Section V.\nConsider a BIMC {p(y|x) : x ∈ X , y ∈ Y}, where X = {0, 1} is the channel input alphabet, and Y is the channel output alphabet, which is arbitrary and could be discrete or continuous. As such, for any x ∈ X , p(y|x) is a probability mass function (pmf) over Y if Y is discrete, and a probability density function (pdf) over Y if Y is the real line.\nFor any set S, let S n denote the set of all sequences of length n drawn from S.\nDeﬁnition 1 (Jar Decoding). Given any channel with input alphabet X and output alphabet Y, which may not be neces- sarily memoryless, any code of block length n for the channel, and any channel output sequence y n ∈ Y n , jar decoding ﬁrst forms a set of suitable size (called a jar and denoted by J (y n )) consisting of sequences x n ∈ X n considered to be closely related to y n through the channel, and then picks any codeword (if any) from the jar J (y n ) as the estimate of the transmitted codeword.\nExample 1 (Hamming Jar) : Consider the binary symmetric channel (BSC) with cross-over probability 0 < p < 0.5. No matter what the code of block length n used over the BSC is, the jar J (y n ) for each y n ∈ {0, 1} n can be formed as\nJ (y n ) = x n ∈ X n : 1 n\nwhere wt(z n ) denotes the Hamming weight of z n , i.e., the number of nonzero entries in z n , and δ is a real number. For the obvious reasons, the jar deﬁned in (2.1) will be referred to as a Hamming jar. The size of J (y n ) is upper bounded by e nH(p+δ) whenever p + δ < 0.5.\nExample 2 (BIMC Jar) : Consider now an arbitrary BIMC, where Y is the BIMC output in response to the uniform input random variable X. For any x n ∈ X n and y n ∈ Y n , let\nis a pmf over X n . In this case, the jar J (y n ) for y n can be formed as\nJ (y n ) = x n ∈ X n : − 1 n\n(2.2) where δ is a real number. Once again, one can verify that\nIn this section, the capacity of LDPC codes under jar decoding with bit error probability for BSC and BIMC will be analyzed, and certain interesting results will be reported (including mutual information-approaching LDPC codes and optimality of check node concentration degree distribution).\nTo facilitate the following discussion, we use the standard notion of tanner graph [7] of linear codes and the normalized variable and check degree distributions from a node perspec- tive of a parity check matrix H m×n (and its tanner graph) [5] denoted by L(z) = L i=1 L i z l i and R(z) = R j=1 R j z r j , where L i and R j represent the percentages of variable and check nodes with degrees l i and r j respectively ∗ .\nGiven m, n, and (normalized) variable and check degree distributions L(z) and R(z) satisfying nL (1) = mR (1), let\nn , \t (3.1) and H m,n,L(z),R(z) denote the collection of all m × n parity check matrices with normalized variable and check degree distributions L(z) and R(z). Then an LDPC code of designed rate (1 − m/n) ln 2 (in nats) is said to be randomly generated from the ensemble C m,n,L(z),R(z) with degree distributions L(z) and R(z) if its parity check matrix H m×n is uni- formly picked from H m,n,L(z),R(z) . Denote the designed rate (1 − m/n) ln 2 as R(C m,n,L(z),R(z) ). The encoding procedure of C m,n,L(z),R(z) is assumed to be systematic so that the original information bits are visible in each codeword.\nTo establish our LDPC coding theorems, the probability Pr {H m×n x n = 0 m } is investigated ﬁrst, which depends on the support set of x n , i.e., the positions of non-zero elements in x n . Let κ(x n ) represent the support set of x n , and we write κ(x n ) simply as κ whenever x n is generic or can be determined from context. Let H κ m×|κ| be the matrix con- sisting of those columns of H m×n with indices in κ. The degree polynomial of κ, denoted by L κ (z), is deﬁned by L κ (z) ∆ = L i=1 L κ i z l i where L κ i n is the number of columns with degree l i within H κ m×|κ| . And deﬁne ¯ l κ ∆ = L i=1 L κ i l i . Then the following lemma is proved in [6] (a similar result has been proved in [8]); its proof is omitted here due to the page limitation.\nLemma 1. Let L(z) and R(z) be normalized variable and check node degree distributions from a node perspective with minimum variable node degree l 1 ≥ 1. Let g(τ, k) ∆ =(1+τ ) k + (1−τ ) k for any τ and k. Suppose H m×n ( m ≤ n) is uniformly picked from ensemble H m,n,L(z),R(z) . Then for any x n = 0 with its support set κ,\n1 n\n¯ l ¯ r\n(3.2) in which τ is the solution to\n¯ l ¯ r\nfor ξ ∈ ¯ l − ¯ l ¯ r R j=1 R j π(r j ), ¯ l with the convention that e −∞ = 0, and where for any integer r\nBy investigating the properties of P (¯ l, R(z), ξ), it can be further shown [6] that\nWe ﬁrst establish our non-asymptotic LDPC coding result for the BSC with capacity C BSC . By assuming the encoding procedure to be systematic, the original information bits are visible in the transmitted codeword X n , and we can measure the bit error probability by\nwhere ˆ X n denotes the estimated codeword by jar decod- ing, and the expectation is with respect to the transmitted random codeword, the BSC, and the random LDPC code C m,n,L(z),R(z) itself. Selecting δ = ln n n in the Hamming jar (2.1), we then have the following theorem.\nTheorem 1. For any variable and check node degree distri- butions L(z) and R(z), and for any block length n,\nB(x n , ) ∆ = z n : 1 n\nfor any x n ∈ X n and 0 ≤ < 1. Let X n be the transmitted codeword, and Y n the output of the BSC in response to X n , i.e.,\nwhere 1) is due to Hoeffding\u2019s inequality. On the other hand, we have\nPr{∃x n ∈ J (Y n ) ∩ B(X n , ), X n ∈ J (Y n ), H m×n x n = 0 m } = Pr {∃x n ∈ J (X n + W n ) ∩ B(X n , ),\nX n ∈ J (X n + W n ), H m×n (x n − X n ) = 0 m } = Pr {∃x n , x n − X n ∈ J (W n ) ∩ B(0 n , ),\n0 n ∈ J (W n ), H m×n (x n − X n ) = 0 m } = Pr{∃z n ∈ J (W n ) ∩ B(0 n , ),\n0 n ∈ J (W n ), H m×n z n = 0 m } =\nwhere the inequality 2) is due to (3.5), z n ∈ B(0 n , ), and the fact that w n ∈ J (0 n ) and z n ∈ J (w n ) imply\nwhenever (3.9) holds. Then (3.8) is proved by combining (3.11), (3.12) and (3.13). This completes the proof of The- orem 1.\nwhere tightening (3.11) kills and Pr{H m×n z n = 0 m } is bounded by Lemma 1. Some symmetry can be exploited to evaluate this bound in future research.\nNow let us extend Theorem 1 to an arbitrary BIMC {p(y|x) : x ∈ X , y ∈ Y}. Towards this, we modify C m,n,L(z),R(z) in the following way: H m×n and S m are uniformly picked from H m,n,L(z),R(z) and X m respectively, and the codebook consists of\nIn other words, we randomly choose a coset code of H m×n for use over the BIMC. To present our LDPC coding theorem for the BIMC, let X and Y be the uniform random variable and corresponding channel output respectively, deﬁne\nspecify J (Y n ) as the BIMC jar with δ = σ H (X|Y ) 4 ln n n , and further deﬁne\nwhere −x is the complement of x, i.e. the module-2 addition of x and 1. Then we have the following theorem.\nTheorem 2. For any variable and check node degree distribu- tions L(z) and R(z), any block length n, and any ∈ (0, 0.5),\nn \t (3.16) and\nRemark. To show the existence of satisfying (3.16), it can be veriﬁed that γ n (X|Y ) > 0, by observing that\nSimilarly, for the BEC with erasure probability p, a tighter upper bound can be obtained as follows:\nFrom Theorems 1 and 2, it can be clearly seen that there is a constant gap between the rate of LDPC and I(X; Y ):\nSeveral interesting results arise from the study of this gap with respect to the degree distributions L(z) and R(z). First of all, let us consider the optimal R(z) which can minimize (3.19) given L(z).\nTheorem 3. Given the variable node degree distribution L(z) and the rate of code R = 1− m n , the optimal R(z) is the check node concentrated distribution, i.e.\n¯ r = n m\nThe next result shows that LDPC codes can achieve asymp- totically I(X; Y ) of any BIMC with diminishing bit error probability when large degrees are used.\nTheorem 4. Given any variable and check node degree distributions L(z) and R(z),\n+ ln n n\n(3.23) Remark 1. In Theorem 4, k is not related to n, and can remain a constant as n approaches inﬁnity. Therefore, the parity check matrix of the code can be always sparse, although large k is needed for the rate of the code to approach I(X; Y ).\nThe proofs of Theorem 2, 3 and 4 are in [6] and omitted due to the length of this paper. Also, the proof of Theorem 4 is similar to that of Theorem 4 in [8].\nIn this section, we demonstrate, by simulation, that for LDPC codes, BP decoding can be regarded as one of many ways to pick up a codeword from a jar when it succeeds in outputting a codeword.\nIn our simulation, we ﬁrst selected a LDPC code with block length n = 8000, coding rate 1 − m n = 0.5 (in bits), variable node degree distribution\nand check node concentration degree distribution R(z), and then randomly chose its coset code\nfor use over our testing channel. Let X n denote the transmitted codeword and Y n denote the channel output.\nAt the decoder, the standard BP decoding algorithm was used. Simply speaking, messages are passed and modiﬁed in certain manner between check and variable nodes in the tanner graph, and eventually the decoding output is the hard decision on each variable node with channel statistics ln Pr{X i =0|Y i } Pr{X\nand messages passed to it. The algorithm kept running until it either found a codeword ˆ x n i.e. H m×n ˆ x n = S m or the upper bound on the number iteration (N ) was reached (in our simulation N = 100).\nThe ﬁrst testing channel we selected is the BSC with crossover probability 0.09 and capacity 0.564 (in bits). BP\ndecoding was run for 1000 blocks. In our simulation, we observed that BP decoding always failed whenever\n1 n\nand sometimes succeeded and sometimes failed when 1\nThe second testing channel we selected is the binary input additive white Gaussian channel with variance of noise σ = 0.875 and channel capacity 0.575 (in bits). The codeword was modulated to {−1, +1}. In our simulation, we observed that BP decoding always failed whenever\nand sometimes succeeded and sometimes failed when − 1 n ln p(X n |Y n ) < 0.322 .\nBoth simulations conﬁrm that BP decoding can be regarded as one of many ways to pick up a codeword from a jar when it succeeds in outputting a codeword.\nIn this paper, LDPC codes are investigated under jar de- coding. In comparison with old decoding rules (MAP or ML and BP decoding), jar decoding really makes LDPC coding analysis simpler and easier. Speciﬁcally, we have shown that LDPC codes can achieve, with diminishing bit error prob- ability, the mutual information of any BIMC with uniform input distribution as their average node degrees increase, and given any variable node degree distribution, the check node concentration distribution is optimal under jar decoding. More- over, simulation conﬁrms the interesting connection between BP decoding and jar decoding, i.e. from the perspective of jar decoding, BP decoding is one of many ways to pick up a codeword in the jar when it succeeds in outputting a codeword. Therefore, with jar decoding, we believe that there is ample room to design effective codes and jar decoding algorithms."},"refs":[{"authors":[{"name":"R. G. Gallage"}],"title":{"text":"Low-Density Parity-Check Codes"}},{"authors":[{"name":"T. J. Richardson"},{"name":"R. L. Urbanke"}],"title":{"text":"The capacity of low-density parity- check codes under message-passing decoding"}},{"authors":[{"name":"G. Miller"},{"name":"D. Burshtein"}],"title":{"text":"Bounds on the maximum-likelihood decod- ing error probability of low-density parity-check codes"}},{"authors":[{"name":"I. Sason"},{"name":"R. Urbanke"}],"title":{"text":"Parity-check density versus performance of binary linear block codes over memoryless symmetric channels"}},{"authors":[{"name":"T. Richardso"},{"name":"R. Urbank"}],"title":{"text":"Modern Coding Theory"}},{"authors":[{"name":"E.-H. Yang"},{"name":"J. Meng"}],"title":{"text":"Jar decoding: Basic concepts and non- asymptotic capacity achieving coding theorems for channels with discrete inputs"}},{"authors":[{"name":"R. M. Tanner"}],"title":{"text":"A recursive approach to low complexity codes"}},{"authors":[{"name":"J. Meng"},{"name":"E.-H. Yang"}],"title":{"text":"Interactive encoding and decoding based on bi- nary ldpc codes with syndrome accumulation"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564613.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T5.1","endtime":"11:50","authors":"En-hui Yang, Jin Meng","date":"1341574200000","papertitle":"Jar Decoding: LDPC Coding Theorems for Binary Input Memoryless Channels","starttime":"11:30","session":"S16.T5: Decoding Techniques for LDPC Codes","room":"Kresge Little Theatre (035)","paperid":"1569564613"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
