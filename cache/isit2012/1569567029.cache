{"id":"1569567029","paper":{"title":{"text":"Information Theory for DNA Sequencing: Part I: A Basic model"},"authors":[{"name":"Abolfazl Motahari"},{"name":"Guy Bresler"},{"name":"David Tse"}],"abstr":{"text":"Abstract\u2014DNA sequencing is the basic workhorse of modern day biology and medicine. Shotgun sequenc- ing is the dominant technique used: many randomly located short fragments called reads are extracted from the DNA sequence, and these reads are assembled to reconstruct the original sequence. By drawing an analogy between the DNA sequencing problem and the classic communication problem, we deﬁne an in- formation theoretic notion of sequencing capacity. This is the maximum number of DNA base pairs that can be resolved reliably per read, and provides a fundamental limit to the performance that can be achieved by any assembly algorithm. We compute the sequencing capacity explicitly for a simple statistical model of the DNA sequence and the read process."},"body":{"text":"DNA sequencing is the basic workhorse of modern day biology and medicine. Since the sequencing of the Human Reference Genome ten years ago, there has been an explosive advance in sequencing technology, resulting in several orders of magnitude increase in throughput and decrease in cost. This advance allows the generation of a massive amount of data, enabling the exploration of a diverse set of questions in biology and medicine that were beyond reach even several years ago. These ques- tions include discovering genetic variations across diﬀerent humans (such as single-nucleotide polymorphisms SNPs), identifying genes aﬀected by mutation in cancer tissue genomes, sequencing an individual\u2019s genome for diagnosis (personal genomics), and understanding DNA regulation in diﬀerent body tissues.\nShotgun sequencing is the dominant method currently used to sequence long strands of DNA, including entire genomes. The basic shotgun DNA sequencing set-up is shown in Figure 1. Starting with a DNA molecule, the goal is to obtain the sequence of nucleotides (A, G, C or T ) comprising it. (For humans, the DNA sequence has about 3×10 9 nucleotides, or base pairs.) The sequencing machine extracts a large number of reads from the DNA; each read is a randomly located fragment of the DNA sequence, of lengths of the order of 100-1000 base pairs, depending on the sequencing technology. The number of reads can be of the order of 10\u2019s to 100\u2019s of millions. The DNA assembly problem is to reconstruct the DNA sequence from the many reads.\nWhen the human genome was sequenced in 2001, there was only one sequencing technology, the Sanger platform. Since 2005, there has been a proliferation of \u201cnext gener- ation\u201d platforms, including Roche/454, Life Technologies SOLiD, Illumina Hi-Seq 2000 and Paciﬁc Biosciences RS. Compared to the Sanger platform, these technologies can provide massively parallel sequencing, producing far more reads per instrument run and at a lower cost. Each of these technologies generates reads of diﬀerent lengths and with diﬀerent noise proﬁles. At the same time, there has been a proliferation of a large number of assembly algorithms, many tailored to speciﬁc sequencing technologies. (A re- cent article [1] surveys no less than 13 such algorithms.)\nThe design of these algorithms is based primarily on computational considerations. The goal is to design eﬃ- cient algorithms that can scale well with the large amount of sequencing data. Current algorithms are often tailored to particular machines and are designed based on heuris- tics and domain knowledge regarding the speciﬁc DNA being sequenced; this makes it diﬃcult even to compare diﬀerent algorithms, not to mention to deﬁne what it means by an \u201coptimal\u201d assembly algorithm for a given sequencing problem.\nAn alternative to the computational view is the statisti- cal view. In this view, the genome sequence is regarded as a random string to be estimated based on the read data. The basic question is: what is the minimum number of reads needed to reconstruct the DNA sequence with a given reliability? This minimum number can be used as a benchmark to compare diﬀerent algorithms, and an optimal algorithm is one that achieves this minimum number. It can also provide an algorithm-independent basis for comparing diﬀerent sequencing technologies and for designing new technologies.\nThis statistical view falls in the realm of DNA sequenc- ing theory. A well-known lower bound on the number of reads needed can be obtained by a coverage analysis, an approach pioneered by Lander and Waterman [2]. This lower bound is the number of reads such that with a desired probability the randomly located reads cover the entire genome sequence. While this is clearly a lower bound on the minimum number of reads needed, it is in general not tight: only requiring the reads to cover the entire genome sequence does not guarantee that consecutive reads can actually be stitched back together to recover the entire sequence. The ability to do that depends on other factors such as the statistical characteristics of the DNA sequence and also the noise proﬁle in the read process. Thus, characterizing the minimum number of reads required for reconstruction is an open question in general.\nIn this paper, we provide a new formulation and ob- tain new results to this question by drawing an analogy between the DNA sequencing problem and the classical communication problem. The basic communication prob- lem is that of encoding an information source at one point for transmission through a noisy channel to be decoded at another point (Figure 2(a)). The problem is to design a \u201dgood\u201d encoder and a \u201dgood\u201d decoder. The DNA sequencing problem can be cast as a restricted case of the communication problem (Figure 2(b)). The DNA sequence of base pairs s 1 , s 2 , . . . , s G is the sequence of information source symbols. The physical sequencing process is the channel which generates from the DNA sequence a set of reads r 1 , r 2 , . . . r N . Each read is viewed as a channel output. The assembly algorithm is the decoder which tries to reconstruct the original genome sequence from the sequence of reads. The problem is to design a \u201dgood\u201d assembly algorithm. Note that the only diﬀerence with the general communication problem is that there is no explicit encoder to optimize and the DNA sequence is sent directly onto the channel.\nCommunication is an age-old ﬁeld and in its early days, communication system designs were ad hoc and tailored for speciﬁc sources and speciﬁc channels. In 1948, Claude Shannon changed all this by introducing information theory as a uniﬁed framework to study communication problems [3]. He made several key contributions. First, he explicitly modeled the source and the channel as random processes. Second, he showed that for a wide class of\nsources and channels, there is a maximum rate of ﬂow of information, measured by the number of information source symbols per channel output, which can be conveyed reliably through the channel. Third, he showed how this maximum reliable rate can be explicitly computed in terms of the statistics of the source and the statistics of the channel.\nThe main goal of the present paper is to initiate a similar program for the DNA sequencing problem. Shannon\u2019s main result assumes that one can optimize the encoder and the decoder. For the DNA sequencing problem, the encoder is ﬁxed and only the decoder (the assembly al- gorithm) can be optimized. Nevertheless, we show in this paper that one can also deﬁne a maximum reliable rate of ﬂow of information for the DNA sequencing problem. We call this quantity the sequencing capacity C. It gives the maximum number of DNA base pairs that can be resolved per read, by any assembly algorithm, without regard to computational limitations. Equivalently, the minimum number of reads required to reconstruct a DNA sequence of length G base pairs is G/C.\nThe sequencing capacity C depends on both the statis- tics of the DNA sequence as well as the speciﬁc physical sequencing process. To make our ideas concrete, we com- pute C explicitly for a very simple model in this paper:\n1) the DNA sequence is modeled as an i.i.d. random process of length G with each symbol taking values according to a probability distribution p on the alphabet {A, G, C, T }.\n2) each read is of length L symbols and begins at a uniformly distributed location on the DNA sequence and the locations are independent from one read to another.\nFor this model, it turns out that the sequencing capacity C depends on the read length L through a normalized parameter ¯ L := L ln G as follows:\nThe result is summarized in Figure 3. Here H 2 (p) is the Renyi entropy of order-2, deﬁned to be\nSince each read reveals L DNA base pairs, a naive thought would be that the sequencing capacity C is simply L base pairs/read. However, this is not correct since the location of each read is unknown to the decoder. The larger the length G of the DNA sequence, the more uncertainty there is about the location of each read, and the less in- formation each read provides. The result says that L/ ln G is the eﬀective amount of information provided by a read, provided that this number is larger than a threshold. If\nC( ¯ L) = ¯ L R\nL/ ln G is below the threshold, reconstruction is impossible no matter how many reads are provided to the assembly algorithm. The threshold value 2/H 2 (p) depends on the statistics of the source.\nThe condition ¯ L > 2/H 2 (p) can be interpreted as the condition for no duplication of length L subsequences in the length G DNA sequence. Arratia et al [4] showed that this is a necessary and suﬃcient condition for reconstruc- tion of the i.i.d. DNA sequence if all length L subsequences of the DNA sequence are given as reads. This arises in a setup called sequencing by hybridization. What our result says is that, for shotgun sequencing where the reads are randomly sampled, if in addition to this no-duplication condition, it also holds that the sequencing rate G/N is less than ¯ L, then reconstruction is possible. We will see that this second condition is precisely the coverage condition of Lander-Waterman. Hence, what our result says is that no-duplication and coverage are suﬃcient for reconstruction.\nLi [5] has also posed the question of minimum number of reads for the i.i.d. equiprobable DNA sequence model. He showed that if L > 4 log 2 G, then the number of reads needed is O(G log 2 G/L). Specializing our result to this case, our capacity result shows that reconstruction is possible if and only if L > log 2 G and the number of reads is G ln G/L. Not only our result is necessary and suﬃcient, we have a much weaker condition on the read length L and we get the right pre-constant on the number of reads needed, not only how it scales with G and L.\nDue to a lack of space, the proofs of the results can be found in the full paper [6]. Also, various extensions of the basic result are explored in the full paper, including the case when the reads are noisy.\nA brief remark on notation is in order. Sets (and probabilistic events) are denoted by calligraphic type, e.g. A, B, E, vectors by boldface, e.g. s, x, y, and random vari- ables by capital letters such as S, X, Y . Random vectors\nare denoted by capital boldface, such as S, X, Y. The exception to these rules, for the sake of consistency with the literature, are the (non-random) parameters G, N, and L, and the constants R and C.\nII. Problem Formulation and Main Result A. Formulation\nA DNA sequence S = S 1 S 2 . . . S G is a long sequence of nucleotides, or bases, with each base S i ∈ {A, C, T, G}. For notational convenience we instead denote the bases by numerals, i.e. S i ∈ {1, 2, 3, 4}. As discussed in the introduction, in this paper each base S i is selected in- dependently and identically according to the probability distribution P(S i = j) = p j , with p = (p 1 , p 2 , p 3 , p 4 ). We assume that the DNA sequence is circular, i.e., S i = S j if i = j mod G; this simpliﬁes the exposition, and all results apply with minor modiﬁcation to the non-circular case as well.\nThe objective of DNA sequencing is to reconstruct the whole sequence based on N reads drawn randomly from the sequence. A read is a substring of length L from the DNA sequence. The set of reads is denoted by R = {R 1 , R 2 , . . . , R N }. The starting location of read i is T i , so R i = S[T i , T i + L − 1]. The set of starting locations of the reads is denoted T = {T 1 , T 2 , . . . , T N }. In this paper each read starting location T 1 , . . . , T N is selected uniformly and independently from the DNA sequence.\nAn assembly algorithm is a map taking a set of N reads R = {R 1 , . . . , R N } and returning an estimated sequence ˆ S = ˆ S (R). We require perfect reconstruction, which presumes that the algorithm φ makes an error if ˆ S = S. We let P denote the probability model for the (random) DNA sequence S and the sample locations T , and E := {ˆ S = S} the error event. A question of central interest is: what is the minimum number of reads N such that the reconstruction error probability is less than a given target ǫ, and what is the optimal assembly algorithm that can achieve such performance? Unfortunately, this is in general a diﬃcult question to answer.\nTaking a cue from information theory, we instead ask an easier asymptotic question: what is the largest information rate R := G/N achievable such that P(E) → 0 as N, G → ∞, and which algorithm achieves the optimal rate asymptotically? More precisely, a rate R base pair/read is said to be achievable if there exists an assembly algorithm such that P(E) → 0 as N, G → ∞ with G/N ﬁxed to be R. The capacity C is deﬁned as the supremum of all achievable rates. Given a probability model for S and the sample locations T , we would like to compute C.\nThis general deﬁnition of capacity is in the limit of large G, the length of the DNA sequence. However, we were not explicit about how the read length L scales. For the i.i.d. noiseless model it turns out, for reasons that will become clear, that the natural scaling is to also let L → ∞ but ﬁxing the ratio ¯ L = L/ ln G deﬁned earlier.\nTheorem 1 ﬁnds the capacity for the i.i.d. noiseless sequencing model. The proof of this Theorem is sketched in the next two subsections, with the details relegated to [6]. Section II-B explains why the expression (1) is a natural upper bound to the capacity, while section II-C gives a simple assembly algorithm which can achieve the upper bound.\nIn this section we derive an upper bound on the capacity for the i.i.d. sequence model; such a bound holds for any algorithm, even one possessing unbounded computational power. The bound is made up of two necessary conditions. First, reconstruction of the DNA sequence is clearly impos- sible if it is not covered by the reads. Second, reconstruc- tion is not possible if there are excessively long duplicated portions of the genome. Loosely, such duplications (which arise due to the random nature of the DNA source) create confusion if they are longer than the read length.\na) Coverage.: In order to reconstruct the DNA se- quence it is necessary to observe each of the nucleotides, i.e. the reads must cover the sequence. Worse than the missing nucleotides, a gap in coverage also creates ambi- guity in the order of the contiguous pieces. The paper of Lander and Waterman [2] studied the coverage problem in the context of DNA sequencing.\nLemma 2 (Coverage-limited). Suppose the information rate is R = G N ≥ ¯ L. Then the sequence is not covered by the reads with probability 1 − o(1). On the other hand, if R < ¯ L, the sequence is covered by the reads with probability 1 − o(1).\nThe ﬁrst statement of the lemma implies that C( ¯ L) ≤ ¯ L. A standard coupon collector-style argument proves this\nlemma. A back-of-the-evelope justiﬁcation, which will be useful in the sequel, is as follows. To a very good approx- imation, the starting locations of the reads are given ac- cording to a Poisson process with rate λ = N/G, and thus each spacing has an exponential(λ) distribution. Hence, the probability that there is a gap between two successive reads is approximately e − λL . Hence, the expected number of gaps is approximately N e − Lλ . This quantity is bounded away from zero if R ≥ ¯ L, and approaches zero otherwise.\nNote that coverage depends on the read locations but is independent of the DNA sequence itself; the next condition depends only on the sequence.\nb) Duplication: The random nature of the DNA se- quence gives rise to a variety of patterns. The key obser- vation in [7] is that there exist two patterns in the DNA sequence precluding reconstruction from an arbitrary set of reads of length L. In other words, reconstruction is not possible even if the L-spectrum, i.e. the set of all substrings of length L appearing in the DNA sequence, is given. The ﬁrst pattern is the three way duplication of a substring of length L − 1. The second pattern is two interleaved pairs of duplications of length L − 1, see Figure 4. Arratia et al.\n[4] carried out a thorough analysis of randomly occurring duplications for the same i.i.d. sequence model as ours, and showed that the second pattern of two iterleaved duplications is the typical event for reconstruction to fail. A consequence of Theorem 7 in [4] is the following lemma, see also [8].\n, then a random DNA sequence contains two pairs of interleaved duplications with probability 1 − o(1) and hence C( ¯ L) = 0.\nFollowing Arratia et al. [4], the ﬁrst-order back-of-the- envelope calculation to understand this result is how many duplications of length L are expected to arise. If there are two pairs of interleaved duplications, then there must be some duplications in the ﬁrst place; and conversely, if there are several pairs of duplications, it is plausible that with a reasonably high probability they will be interleaved. Denoting by S L i the length-L subsequence starting at position i, we have\n(3) Now, the probability that two speciﬁc physically disjoint length-L subsequences are identical is:\nwhere H 2 (p) = − log i p 2 i is the R´enyi entropy of order two. Ignoring the GL terms in (3) where S L i and S L j overlap, we get the lower bound:\n(4) This number approaches inﬁnity if ¯ L = L/ ln G is less than 2/H 2 (p), suggesting that under this condition, the probability of having two pairs of interleaved duplications is very high. Moreover, as is proved in [6] that the contri- bution of the terms in (3) due to physically overlapping subsequences is not large, and so the lower bound in (4) is essentially tight. This suggests that ¯ L = 2/H 2 (p) is in fact the threshold for existence of interleaved duplications.\nNote that for any ﬁxed read length L, the probability of such a duplication event will approach 1 as the DNA length G → ∞. This means that if we had deﬁned capacity for a ﬁxed read length L, then for any value of L, the capacity would have been zero. Thus, to get a meaningful capacity result, one must scale L with G, and Lemma 3 suggests that letting L and G grow while ﬁxing ¯ L\nis the correct scaling. This is further validated by the achievability result in the next section. C. Optimal Algorithm\nA simple greedy algorithm (perhaps surprisingly) turns out to be optimal, achieving the capacity described in Theorem 1. Essentially, the greedy algorithm merges the reads repeatedly into contigs 1 repeatedly and greedily based on an overlap score between any two strings. The algorithm is given as follows.\nIn this paper the overlap score W (S 1 , S 2 ) is deﬁned as the maximum length suﬃx of S 1 identical to a preﬁx of S 2 .\nTheorem 4 (Achievable rate). The greedy algorithm with overlap score W can achieve any rate R < ¯ L if ¯ L > 2/H 2 (p), thus achieving capacity.\nBasically, this result says that if the reads cover the DNA sequence and there are no duplications of length L, then the greedy algorithm can reconstruct the DNA sequence. Let us give a back-of-the-envelope calculation to understand this result. The detailed proof will be given in [6].\nSince the greedy algorithm merges reads according to overlap score, we may think of the algorithm as working in stages, starting with an overlap score of L down to an overlap score of 0. At stage ℓ, the merging is between contigs with overlap score ℓ. The key is to ﬁnd the typical stage at which errors in merging ﬁrst occurs. Assuming no errors have occurred in stages L, L − 1, . . . , ℓ + 1. Consider the situation in stage ℓ. The algorithm has already merged the reads into a number of contigs. The boundary between two neighboring contigs is where the overlap between the neighboring reads is less than or equal to ℓ; if it were larger than ℓ, the two contigs would have been merged already. Hence, the expected number of contigs at stage ℓ is the expected number of pairs of successive reads with spacing greater than L − ℓ. Again invoking the Poisson approximation, this is roughly equal to\nTwo contigs will be merged in error in stage ℓ if the length ℓ suﬃx of one contig equals the length ℓ preﬁx of another contig. Assuming these substrings are physically disjoint, the probability of this event is e − ℓH 2 (p) . Hence, the expected number of pairs of contigs for which this confusion event happens is approximately\nThis number is largest either when ℓ = L or ℓ = 0. This suggests that, typically, errors occurs in stage L or stage 0 of the algorithm. Errors occur at stage L if there are duplications of length L substrings in the DNA sequence. Errors occur at stage 0 if there are still leftover unmerged contigs. The no-duplication condition ensures that the probability of the former event is small. The coverage condition ensures that the probability of the latter event is small. Thus, the two necessary conditions are also suﬃcient for reconstruction.\nIt should be noted that the greedy algorithm has also been shown to be optimal for the shortest common su- perstring problem under certain probabilistic settings [9]. The shortest common superstring problem is the problem of ﬁnding the shortest string containing a set of given strings. However, in their model, the given strings are all independently distributed and does not from a single \u201dmother\u201d sequence. In contrast, in our model, even though the original DNA sequence is assumed to be i.i.d., the reads will be highly correlated, since many of the reads will be physically overlapping. In fact, it follows from [9] that, given N reads and the read length L scales like ln N , the length of the shortest common superstring scales like N ln N . On the other hand, in our model, the length of the reconstructed sequence would be proportional to N . Hence, the length of the shortest common superstring is much longer, a consequence of the fact that the reads are all independent and therefore much harder to merge. So the two problems are totally diﬀerent, although coinciden- tally the greedy algorithm is optimal for both problems.\nAcknowledgements: This work is supported by the NSERC of Canada and by the NSF Center for Science of Information under grant agreement CCF-0939370."},"refs":[{"authors":[{"name":"J. Miller"},{"name":"S. Koren"},{"name":"G. Sutton"}],"title":{"text":"Assembly algorithms for next-generation sequencing data"}},{"authors":[{"name":"E. S. Lander"},{"name":"M. S. Waterman"}],"title":{"text":"Genomic mapping by ﬁnger- printing random clones: A mathematical analysis"}},{"authors":[{"name":"C. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[{"name":"R. Arratia"},{"name":"D. Martin"},{"name":"G. Reinert"},{"name":"M. S. Waterman"}],"title":{"text":"Poisson process approximation for sequence repeats, and sequencing by hybridization"}},{"authors":[{"name":"M. Li"}],"title":{"text":"Towards a DNA sequencing theory (learning a string)"}},{"authors":[{"name":"S. Motahari"},{"name":"G. Bresler"},{"name":"D. Tse"}],"title":{"text":"Information theory of DNA sequencing"}},{"authors":[{"name":"E. Ukkonen"}],"title":{"text":"Approximate string matching with q-grams and maximal matches"}},{"authors":[{"name":"M. Dyer"},{"name":"A. Frieze"},{"name":"S. Suen"}],"title":{"text":"The probability of unique solu- tions of sequencing by hybridization"}},{"authors":[{"name":"A. Frieze"},{"name":"W. Szpankowski"}],"title":{"text":"Greedy algorithms for the shortest common superstring that are asymptotically optimal"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569567029.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S15.T9.1","endtime":"10:10","authors":"Abolfazl Motahari, Guy Bresler, David Tse","date":"1341568200000","papertitle":"Information Theory for DNA Sequencing: Part I: A Basic Model","starttime":"09:50","session":"S15.T9: Strings, Sorting, and Biology","room":"Stratton West Lounge (201)","paperid":"1569567029"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
