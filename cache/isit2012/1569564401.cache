{"id":"1569564401","paper":{"title":{"text":"On Optimal Two Sample Homogeneity Tests for Finite Alphabets"},"authors":[{"name":"Jayakrishnan Unnikrishnan"}],"abstr":{"text":"Abstract\u2014Suppose we are given two independent strings of data from a known ﬁnite alphabet. We are interested in testing the null hypothesis that both the strings were drawn from the same distribution, assuming that the samples within each string are mutually independent. Among statisticians, the most popular solution for such a homogeneity test is the two sample chi-square test, primarily due to its ease of implementation and the fact that the limiting null hypothesis distribution of the associated test statistic is known and easy to compute. Although tests that are asymptotically optimal in error probability have been proposed in the information theory literature, such optimality results are not well-known and such tests are rarely used in practice. In this paper we seek to bridge the gap between theory and practice. We study two different optimal tests proposed by Shayevitz [1] and Gutman [2]. We ﬁrst obtain a simpliﬁed structure of Shayevitz\u2019s test and then obtain limiting distributions of the test statistics used in both the tests. These results provide guidelines for choosing thresholds that guarantee an approximate false alarm constraint for ﬁnite length observation sequences, thus making these tests easy to use in practice. The approximation accuracies are demonstrated using simulations. We argue that such homogeneity tests with provable optimality properties could potentially be better choices than the chi-square test in practice."},"body":{"text":"Suppose we are given two independent strings of data x m := (x 1 , x 2 , . . . , x m ) and y n := (y 1 , y 2 , . . . , y n ) drawn from the same known ﬁnite alphabet Z := {z 1 , z 2 , . . . , z N }. We are interested in testing homogeneity, i.e., whether or not both these strings are drawn i.i.d. from the same distribution in P(Z), the collection of all multinomial probability distri- butions on Z. In other words this hypothesis testing problem fundamentally aims to identify whether or not the two collec- tions of samples are drawn from the same population. This is a fundamental problem in statistics with various practical applications [3].\nThis problem can be interpreted as a binary hypothesis testing problem with a composite null hypothesis representing the situation where both strings are drawn from identical distributions and a composite alternate hypothesis where both the strings are drawn from distinct distributions. Thus one can deﬁne two different probabilities of error, viz., the probability of false alarm under the null hypothesis and the probability of missed detection under the alternate hypothesis. A reasonable approach to solve this problem is to identify some testing procedure that optimizes the trade-off between these two quantities. Although it is intractable to solve this problem\nexactly, two different results satisfying two different notions of asymptotic optimality are known in the information theory literature [1] [2]. However, the most commonly used solution for this homogeneity testing problem is the two sample chi- square test [4] originally proposed by Pearson [5]. In this paper we ﬁrst obtain a simpliﬁed structure for the test proposed in [1]. We then proceed to identify the limiting behavior of the test statistics used in the optimal tests of [1] and [2]. Such limiting results can be used to approximate the thresholds for these tests for a target false alarm probability, thus providing a practical alternative to the popular two sample chi-square test which does not have any known optimality properties.\nFor any probability mass function π ∈ P(Z) we use π(z) to denote the probability of symbol z ∈ Z. We sometimes also use the notation π to denote the vector of probabilities (π(z 1 ), π(z 2 ), . . . , π(z N )) and π, f to denote the inner prod- uct N i =1 π (z i )f (z i ) for any function f deﬁned on Z. For two distributions π, ν ∈ P(Z) the Kullback-Leibler divergence between π and ν is given by\nWe use Γ x m to denote the empirical distribution of x m and Γ y n to denote the empirical distribution of y n . We use P π 1 ,π 2 to denote the probability measure when the ﬁrst string is drawn from distribution π 1 and the second string is drawn from distribution π 2 . When both strings are drawn from the same distribution µ we use P µ for the probability measure. We use N (a, B) to denote a Gaussian random vector with mean a and covariance matrix B and χ 2 d to denote a chi-square random variable with d degrees of freedom.\nIn Section II we describe the mathematical problem state- ment and describe the known results. We provide a simpliﬁed version of a known optimal test [1] in Section III. We then present our new results on the weak convergence of the various test statistics in Section IV. We discuss how these results can be used for selecting test thresholds in Section V and conclude in Section VI.\nSuppose we are given two independent strings of data x m := (x 1 , x 2 , . . . , x m ) and y n := (y 1 , y 2 , . . . , y n ) drawn from the same known ﬁnite alphabet Z. We think of x m as the ﬁrst m observations from an i.i.d. sequence x and y n as the ﬁrst n observations from another i.i.d. sequence y. We are interested in testing whether or not both these sequences are drawn from the same distribution in P(Z). For each m, n ∈ Z let φ m,n : Z m × Z n → {0, 1} represent a test on the ﬁrst m observations from x and ﬁrst n observations from y. The test outcome φ m,n (x m , y n ) = 0 represents a decision in favor of the null hypothesis that both x and y are drawn from the same distribution and the outcome φ m,n (x m , y n ) = 1 represents a decision in favor of the alternate hypothesis that x and y are drawn from different distributions. For any distribution µ ∈ P(Z) of the observations under the null hypothesis, the probability of false alarm is given by\nSimilarly for any distinct distributions π 1 , π 2 ∈ P(Z) the probability of missed detection is given by\nIn the classical Neyman-Pearson formulation of hypothesis testing one seeks to minimize the probability of missed de- tection subject to an upper bound on the probability of false alarm. In our problem since we do not know the values of µ, π 1 or π 2 , it is not possible to solve this problem exactly. Instead, we have to use an asymptotic version. For this purpose we consider the limit as m, n → ∞. We further assume that m scales linearly in n as m = λn for some λ ≥ 1. In this setting we use φ n (x, y) to denote the test outcome φ λn,n (x λn , y n ). We deﬁne two kinds of error exponents. The false alarm error-exponent and the missed-detection exponent are deﬁned respectively as\nTwo versions of asymptotically optimal tests are known in literature.\nShayevitz [1] studied this problem in the context of a two-sensor network. The null hypothesis corresponds to the scenario in which both sensors observe noise and the alternate hypothesis corresponds to the scenario in which some phe- nomenon is present which leads to both sensors making ob- servations from distinct distributions. One of the contributions of [1] is a solution to the following optimization problem:\nsup φ E M D (φ|π 1 , π 2 ) s.t. lim\nGutman [2] studied this problem in the context of multihy- pothesis testing with training sequences. He used the following optimality criterion\ns.t. E F A (φ n |µ) ≥ η for all µ ∈ P(Z) \t (3) and showed that the following sequence of likelihood ratio tests solves problem (3):\nwhere ˜ δ n = η + O( log n n ). Interestingly, both the optimal sequences of tests of (2) and (4) do not depend on the true values of π 1 and π 2 .\nAlthough these optimal solutions are known, the test usually used by statisticians is the two sample chi-square test. The chi- square distance between two distributions is deﬁned as\nwhere ˆ δ n is chosen to approximately meet the false alarm constraint based on the weak convergence of the test statistic.\nThe main reason for popularity of the chi-square test is the fact that the test statistic is easy to compute and that the limiting behavior of the test statistic is known, which makes it possible to set an approximate threshold for a target false alarm probability, or to compute the p-value of the test statistic [3]. We observe that in both the optimal tests (2) and (4), the guarantees on the false alarm probability (p F A ) hold only in the asymptotic sense as the sequence length goes to inﬁnity. In (2) we are guaranteed that p F A will eventually go to zero and in (4) we are guaranteed that p F A decays as exp(−nη). However, in practice one has access to only a ﬁnite number of data points and is interested in guaranteeing a constant upper bound on the false alarm probability. For this, we need to be able to choose the thresholds of these tests to meet a given target false alarm level for a given sequence length. In the following sections we derive weak-convergence results for the test statistics used in (2) and (4) and demonstrate that these results give good approximations to the actual false alarm probabilities in these tests. We ﬁrst obtain a simpliﬁed form of the test of (2).\nThe test statistic used in the test φ A of (2) can be simpliﬁed a great deal via the following lemma.\nLemma III.1. For any distributions µ 1 , µ 2 ∈ P(Z), the inﬁmum in\nis achieved at a point ν ∗ that satisﬁes D (µ 1 ν ∗ ) = D(µ 2 ν ∗ ). Furthermore, ν ∗ can be expressed in the form ν ∗ = αµ 1 + (1 − α)µ 2 for some 0 ≤ α ≤ 1.\nProof: \t Deﬁne \t the \t function f 12 (ν) \t := max{D(µ 1 ν ), D(µ 2 ν )}. Since f 12 (u) is ﬁnite when u is the uniform distribution on Z we see that the value of the optimization problem in (6) is ﬁnite. It is also easy to see that without loss of optimality we can restrict the inﬁmum to P 12 (Z) := {ν ∈ P(Z) : supp(ν) ⊆ supp(µ 1 ) ∪ supp(µ 2 )}. This is because for any ν ∈ P(Z) its restriction ν 12 onto supp (µ 1 ) ∪ supp(µ 2 ) satisﬁes D(µ 1 ν 12 ) ≤ D(µ 1 ν ) and D (µ 2 ν 12 ) ≤ D(µ 2 ν ). Now P 12 (Z) is a compact set, the function f 12 (.) is bounded below by 0 on P 12 (Z), and moreover the function f 12 (.) is continuous in the relative interior of the set P 12 (Z). Thus the inﬁmum in inf ν ∈P 12 (Z) f 12 (ν) is achieved since the optimal value is ﬁnite by the argument above.\nNow (6) can be equivalently written as a convex problem: min τ,ν τ\nLet ˆ ν represent the optimizer of this problem. Considering the ﬁrst order condition for optimality in a Lagrange-relaxed version of this problem it follows that there exists scalars ℓ 1 , ℓ 2 , and κ such that\nwhich implies that the optimizer ˆ ν can be expressed as an afﬁne combination of µ 1 and µ 2 . Now by the deﬁnition of f 12 (.) it further follows that ˆ ν can be expressed as a convex combination of µ 1 and µ 2 .\nFrom the above lemma it follows that the test (2) can equivalently be written as\nφ A n (x, y) = I {D(Γ x λn α n Γ x λn + (1 − α n )Γ y n ) ≥ δ n } (7) where α n ∈ [0, 1] satisﬁes\nD (Γ x λn α n Γ x λn +(1−α n )Γ y n ) = D(Γ y n α n Γ x λn +(1−α n )Γ y n ). Furthermore, it is obvious that given Γ x λn and Γ y n the value of α n can be easily computed by binary search since the function g (α) := D(Γ x λn α Γ x λn + (1 − α)Γ y n ) − D(Γ y n α Γ x λn + (1 − α )Γ y n ) is a monotonically decreasing function of α, and α n can be approximated by the value of α at which the function g (.) is approximately zero. Now let Z n := D(Γ x λn α n Γ x λn + (1−α n )Γ y n ). Thus the test of (2) is just a threshold test on Z n . Although the test (2) looks complicated, the discussion above implies that the test statistic is in fact quite easy to compute.\nIn the classical Neyman-Pearson hypothesis testing prob- lem, one chooses the threshold that guarantees some bound on the false alarm probability of the test. Although it is not tractable to obtain an exact evaluation of the false alarm\nprobability as a function of the threshold, we will now show that in the asymptotic regime, it is possible to obtain weak- convergence results on the test statistics that can be used to approximate the false alarm probability. All our results are based on the following basic lemma.\nLemma IV.1. Suppose we are given a string x of observations of length λn and another independent string y of length n both drawn i.i.d. from the same distribution µ ∈ P(Z) such that µ has full support over Z. Let Γ x λn denote the empirical distri- bution of the observations in x and Γ y n denote the empirical distribution of the observations in y. Let h : P(Z)×P(Z) → R be a continuous real-valued function whose gradient and Hessian are continuous in the neighborhood of (µ, µ). If the directional derivative satisﬁes ∇h(µ, µ) T (ν 1 − µ, ν 2 − µ) = 0 for all ν 1 , ν 2 ∈ P(Z), then\nwhere M = ∇ 2 h (µ, µ) and W λ and W are independent ran- dom vectors distributed as W λ ∼ N (0, Σ λ ) and W ∼ N (0, Σ) with Σ = diag(µ) − µµ T .\nProof: Let G n,x := n 1 2 (Γ x λn − µ) and G n,y := n 1 2 (Γ y n − µ ). We know that Γ x λn and Γ y n can be written as empirical averages of i.i.d. vectors. Hence, they satisfy the central limit theorem which says that,\nwhere the distributions of W and W λ are as deﬁned in the statement of the lemma. Considering a second-order Taylor\u2019s expansion and using the condition on the directional derivative, we have, for n large enough,\nwhere ˜ Γ x n = γΓ x λn + (1 − γ)µ and ˜ Γ y n = γΓ y n + (1 − γ)µ for some γ = γ(n) ∈ [0, 1]. We also know by the strong law of large numbers that Γ x λn and Γ y n and hence ˜ Γ x n and ˜ Γ y n converge to µ almost surely. By the continuity of the Hessian, we have\nBy applying the vector-version of Slutsky\u2019s theorem [7], together with (8), (9) and (10), we conclude that\nAs an immediate consequence of the above lemma we have the following result:\nLemma IV.2. Suppose we are given a string x of observations of length λn and another independent string y of length n both drawn i.i.d. from the same distribution µ ∈ P(Z) such that µ has full support over Z. Let Y n 1 := D(Γ x λn 1 2 (Γ x λn + Γ y n )) and Y n 2 := D(Γ y n 1 2 (Γ x λn + Γ y n )). Then the following results hold:\nProof: To prove (11) we apply Lemma IV.1 to the function h (π, ν) := D(π 1 2 (π + ν)). It is easily veriﬁed that the gradient and Hessian satisfy the necessary regularity conditions. Computing the Hessian at (µ, µ) we obtain\ndiag 1 4µ \t −diag 1 4µ −diag 1 4µ \t diag 1 4µ\nwhere diag 1 4µ denotes a diagonal matrix with the i-th diagonal entry given by 1 4µ\n. Applying the conclusion of Lemma IV.1 we obtain\nEquivalently we can write 8nλ 1+λ Y n 1 \t d. −−−−→ n →∞ W T diag 1 µ W. It can be shown using the result of [8, Lemma III.7] that W T diag 1 µ W has a χ 2 N −1 distribution thus proving (11).\nSimilarly for proving (12) we apply Lemma IV.1 to the function h (π, ν) := λD(π 1 2 (π + ν)) + D(π 1 2 (π + ν)). Computing the Hessian at (µ, µ) we see that the new Hessian is just (1 + λ) times M . Thus the result of (12) by a similar argument as before.\nNow if we apply Lemma IV.1 to the function h (π, ν) := D (π 1 2 (π + ν)) − D(ν 1 2 (π + ν)), we see that the Hessian at (µ, µ) vanishes. Hence (13) follows.\nWe are now ready to obtain the weak convergence behavior of the test statistic Z n used in the test of (7).\nProposition IV.3. Assume that the data strings x m and y n are drawn i.i.d. according to some ﬁxed distribution µ ∈ P(Z) such that µ has full support on Z. Further assume that m grows linearly in n as m = λn. Let α n and Z n be as before with Z n = D(Γ x λn α n Γ x λn + (1 − α n )Γ y n ). Then if m grows linearly in n as m = λn, we have\nProof: Let Y n 1 := D(Γ x λn 1 2 (Γ x λn + Γ y n )) and Y n 2 := D (Γ y n 1 2 (Γ x λn + Γ y n )). From Lemma III.1 we have\nNow if W n := min{Y n 1 , Y n 2 }, then we have |W n − Y n 1 | ≤ |Y n 1 − Y n 2 |. Hence by (13) we have n(W n − Y n 1 ) d. −−−−→\nCombining with (11) we get 8nλ 1+λ W n d. −−−−→ n →∞ χ 2 N −1 . By a similar argument it also follows that V n := max{Y n 1 , Y n 2 } satisﬁes 8nλ 1+λ V n d. −−−−→\nχ 2 N −1 . Thus by (15) we see that nZ n is sandwiched between two random quantities having the same weak convergence behavior. Thus nZ n should also have the same weak convergence limit.\nWe now consider the test of (4) proposed in [2]. The test statistic in this test can be expressed as:\nIn the following theorem we characterize the limiting behavior of this test statistic.\nProposition IV.4. Assume that the data strings x m and y n are drawn i.i.d. according to some ﬁxed distribution µ ∈ P(Z) such that µ has full support on Z. Let Y n := λD(Γ x λn 1 2 (Γ x λn + Γ y n )) + D(Γ y n 1 2 (Γ x λn + Γ y n )). Then we have\nSimilarly, we can also identify the limiting behavior of the chi-square test statistic used in (5) via the results of Lemma IV.1. Although this result is well known in statistics literature, we provide a simple proof for completeness.\nProposition IV.5. Assume that the data strings x m and y n are drawn i.i.d. according to some ﬁxed distribution µ ∈ P(Z) such that µ has full support on Z. Let X n := χ 2 (Γ x λn , Γ y n ). Then we have\nProof: We apply Lemma IV.1 to the function f (π, ν) = χ 2 (π, ν). It is easily veriﬁed that the gradient and Hessian satisfy the necessary regularity conditions. Computing the Hessian at (µ, µ) we obtain\ndiag 2 µ \t −diag 2 µ −diag 2 µ \t diag 2 µ\nFollowing the same steps as in the proof of (11) in Lemma IV.2, the conclusion follows.\nWe observe from Propositions IV.3, IV.4 and IV.5 that the limiting distribution of the test statistics of all the three tests φ A of (7), φ B of (4) and φ C of (5) under the null hypothesis depend only on the support size of the true distribution µ and not on the speciﬁc value of µ. In the following section we discuss how these weak convergence results can be used to select the test thresholds for a target false alarm probability.\nThe weak convergence behavior of the test statistics in the three tests we have considered can be used to approximately choose the test threshold for a target false alarm probability. For example in the chi-square test φ C of (5) if under the null\nhypothesis the observations are drawn from some distribution µ ∈ P(Z) with full support, then the test statistic X n satisﬁes\nwhere F (x) denotes the cdf of a chi-square random variable with N − 1 degrees of freedom. This relation can be used to approximate the threshold to be used in (5) for a target false alarm probability, by approximating the true probability with the limiting probability. Similarly, the thresholds for the optimal tests φ A of (7) and φ B of (4) can be chosen using the weak convergence of their respective test statistics.\nIn order to estimate the accuracy of the approximation obtained from the weak convergence, we simulated the three tests using a uniform distribution over an alphabet of size 8 for µ. In Figure 1 we have plotted the false alarm probabilities of the three tests as a function of the sequence length n obtained by simulations. In the same ﬁgure we also have a plot of the approximate value of the false alarm probability computed using the weak convergence approximation suggested in the previous paragraph. Clearly, that the error predictions obtained via the weak-convergence approximations are quite accurate for values of n greater than 45.\nWe have studied the homogeneity testing problem for multinomial distributions. Although optimal results have been proposed for this problem in information theory literature, such results are not well-known among statisticians and such tests are rarely used in practice. In this paper, we have simpliﬁed the structure of one of these tests and also identiﬁed the limiting behavior of the test statistics used in both the tests. These results can be used to approximate the thresholds for these tests. Such homogeneity tests with provable optimality properties could potentially be better choices than the chi- square test in practice.\nIn terms of future work it would be of interest to identify the optimal tests for this problem in the setting in which the\nalphabet size is allowed to increase with the sample size. Such a setup is relevant in problems involving data from continuous alphabet distributions which could be ﬁrst quantized and then tested. The literature (see, e.g., [9] and references therein) on the simpler single sample goodness-of-ﬁt problem could be a good starting point for such an investigation. A different direction for future work is to extend these results on two- sample tests to more general k-sample tests and to evaluate the asymptotic efﬁciency of these tests.\nThe author thanks Dayu Huang for useful discussions. This research was supported by ERC Advanced Investigators Grant: Sparse Sampling: Theory, Algorithms and Applications SPARSAM no 247006."},"refs":[{"authors":[{"name":"O. Shayevitz"}],"title":{"text":"On R´enyi measures and hypothesis testing"}},{"authors":[{"name":"M. Gutman"}],"title":{"text":"Asymptotically optimal classiﬁcation for multiple tests with empirically observed statistics"}},{"authors":[{"name":"E. L. Lehman"},{"name":"J. P. Roman"}],"title":{"text":"Testing statistical hypotheses, 3rd ed"}},{"authors":[{"name":"P. Greenwoo"},{"name":"M. Nikuli"}],"title":{"text":"A guide to chi-squared testing, ser"}},{"authors":[{"name":"K. Pearson"}],"title":{"text":"On the probability that two independent distributions of frequency are really samples from the same population"}},{"authors":[{"name":"I. Csisz´ar"},{"name":"P. C. Shields"}],"title":{"text":"Information theory and statistics: A tutorial"}},{"authors":[{"name":"P. Billingsle"}],"title":{"text":"Convergence of Probability Measures"}},{"authors":[{"name":"J. Unnikrishnan"},{"name":"D. Huang"},{"name":"S. Meyn"},{"name":"A. Surana"},{"name":"V. Veeravalli"}],"title":{"text":"Universal and composite hypothesis testing via mismatched divergence"}},{"authors":[{"name":"P. Harremoes"},{"name":"I. Vajda"}],"title":{"text":"On the Bahadur-Efﬁcient Testing of Uni- formity by Means of the Entropy"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564401.pdf"},"links":[{"id":"1569566725","weight":5},{"id":"1569567049","weight":5},{"id":"1569559259","weight":5},{"id":"1569566597","weight":5},{"id":"1569565931","weight":5},{"id":"1569564233","weight":5},{"id":"1569564989","weight":5},{"id":"1569566095","weight":5},{"id":"1569561085","weight":11},{"id":"1569561795","weight":5},{"id":"1569566437","weight":5},{"id":"1569566939","weight":11},{"id":"1569565151","weight":5},{"id":"1569565055","weight":5},{"id":"1569566233","weight":5},{"id":"1569560997","weight":5},{"id":"1569565463","weight":5},{"id":"1569565415","weight":5},{"id":"1569565549","weight":5},{"id":"1569565575","weight":5},{"id":"1569565919","weight":5},{"id":"1569566267","weight":5},{"id":"1569565421","weight":11},{"id":"1569566237","weight":5},{"id":"1569551905","weight":5},{"id":"1569565457","weight":5},{"id":"1569566911","weight":5},{"id":"1569566933","weight":5},{"id":"1569565389","weight":5},{"id":"1569566057","weight":11},{"id":"1569565889","weight":5},{"id":"1569563725","weight":16},{"id":"1569565165","weight":5},{"id":"1569566555","weight":5},{"id":"1569564141","weight":5},{"id":"1569566973","weight":5},{"id":"1569565031","weight":5},{"id":"1569551541","weight":5},{"id":"1569566067","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T8.4","endtime":"11:10","authors":"Jayakrishnan Unnikrishnan","date":"1341485400000","papertitle":"On Optimal Two Sample Homogeneity Tests for Finite Alphabets","starttime":"10:50","session":"S11.T8: Patterns, Estimation, Hypothesis Testing","room":"Stratton (491)","paperid":"1569564401"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
