{"id":"1569565237","paper":{"title":{"text":"Hierarchies of Local-Optimality Characterizations in Decoding Tanner Codes"},"authors":[{"name":"Nissim Halabi"},{"name":"Guy Even"}],"abstr":{"text":"Abstract\u2014Recent developments in decoding Tanner codes with maximum-likelihood certiﬁcates are based on a sufﬁcient con- dition called local optimality. We deﬁne hierarchies of locally optimal codewords with respect to two parameters. One pa- rameter is related to the minimum distance of the local codes in Tanner codes. The second parameter is related to the ﬁnite number of iterations used in iterative decoding. We show that these hierarchies satisfy inclusion properties as these parameters are increased. In particular, this implies that a codeword that is decoded with a certiﬁcate using an iterative decoder after h iterations is decoded with a certiﬁcate after k · h iterations, for every integer k."},"body":{"text":"Local optimality is often used as a sufﬁcient condition for successful decoding of ﬁnite length codes (see e.g., [1], [2]). In this work we focus on two parameters of the local- optimality characterization for Tanner codes [3]. The ﬁrst parameter is related to the minimum distance of the local codes in (expander) Tanner codes. The second parameter is related to the ﬁnite number of iterations used in iterative decoding, even when number of iterations exceeds the girth. We deﬁne hierarchies of local optimality with respect to these parameters. These hierarchies provide a partial explanation of two questions about successful decoding with ML-certiﬁcates: (1) What is the effect of increasing the minimum distance of the local codes in Tanner codes? (2) What is the effect of increasing the number of iterations beyond the girth in iterative decoding?\nPrevious Work: Density Evolution (DE) is used to study the asymptotic performance of decoding algorithms based on Belief-Propagation (BP) (see e.g., [4], [5]). Convergence of BP-based decoding algorithms was studied in [1], [6]\u2013 [8]. Note that convergence guarantees do not imply suc- cessful decoding after a ﬁnite number of iterations. Korada and Urbanke [9] provide an asymptotic analysis of iterative decoding \u201cbeyond\u201d the girth. Speciﬁcally, they prove that one may exchange the order of the limits in DE-analysis of BP- decoding under certain conditions (i.e., variable node degree at least 5 and bounded LLRs). On the other hand, our work focuses on iterative decoding of ﬁnite length codes using a ﬁnite number of iterations.\nSuboptimal decoding of expander Tanner codes was ana- lyzed in many works (see [10]\u2013[12]). The results in these analyses rely on: (i) the expansion properties of the Tanner graph, and (ii) constant relative minimum distances of the\nlocal codes. The error-correcting guarantees in these analyses improve as the relative minimum distance increases.\nA new local-optimality characterization for a codeword in a Tanner code w.r.t. any MBIOS channel was presented in [3]. A locally-optimal codeword is guaranteed to be both the unique maximum-likelihood (ML) codeword as well as the unique LP- decoding codeword. The characterization of local-optimality for Tanner codes has three parameters: (i) a height h ∈ N, (ii) level weights w ∈ R h + , and (iii) a degree 2 d d ∗ , where d ∗ is the minimum local distance.\nA new message-passing decoding algorithm, called normal- ized weighted min-sum ( NWMS ), was presented for Tanner codes with single parity-check (SPC) local codes [3]. The NWMS decoder is guaranteed to compute the ML-codeword in h iterations provided that a locally-optimal codeword with height h exists. The number of iterations h may exceed the girth of the Tanner graph.\nContribution: We present a variation of local-optimality called strong local-optimality. We prove that if a codeword is strongly locally-optimal, then it is also locally-optimal. Hence, previous results proved for local-optimality [3] hold also for strong local-optimality.\nWe present two hierarchies: (1) A hierarchy of local- optimality based on degrees . The degree hierarchy states that a locally-optimal codeword x with degree parameter d is also locally-optimal with respect to any degree parameter d > d. The degree hierarchy implies that the occurrence of local- optimality does not decrease as the degree parameter increases. (2) A hierarchy of strong local-optimality based on height. The height hierarchy states that if a codeword x is strongly locally- optimal with respect to height parameter h, then it is also strongly locally-optimal with respect to every height parameter that is an integer multiple of h. The height hierarchy proves, for example, that the performance of iterative decoding with an ML-certiﬁcate (e.g., NWMS ) of ﬁnite-length Tanner codes with SPC local codes does not degrade as the number of iterations grows, even beyond the girth of the Tanner graph.\nGraph Terminology: Let G = (V, E) denote an undirected graph. Let N G (v) denote the set of neighbors of node v ∈ V , and let deg G (v) |N G (v)| denote the degree of node v in graph G. A path p = (v, . . . , u) in G is a sequence of vertices such that there exists an edge between every two consecutive nodes in the sequence p. A path p is backtrackless if every two\nconsecutive edges along p do not close a cycle. Let |p| denote the number of edges in p. Let girth(G) denote the length of the shortest cycle in G.\nTanner-codes and ML-decoding: Let G = (V∪J , E) denote an edge-labeled bipartite-graph, where V = {v 1 , . . . , v N } is a set of N vertices called variable nodes, and J = {C 1 , . . . , C J } is a set of J vertices called local-code nodes. We associate with each local-code node C j a linear code C j of length deg G (C j ). Let C J \t C j : 1 j J denote the set of local codes, one for each local code node. We say that v i participates in C j if (v i , C j ) is an edge in E.\nA word x = (x 1 , . . . , x N ) ∈ {0, 1} N is an assignment to variable nodes in V where x i is assigned to v i . The Tanner code C(G, C J ) based on the labeled Tanner graph G is the set of vectors x ∈ {0, 1} N such that the projection of x onto entries associated with N G (C j ) is a codeword in C j for every j ∈ {1, . . . , J}. Let d j denote the minimum distance of the local code C j . The minimum local distance d ∗ of a Tanner code C(G, C J ) is deﬁned by d ∗ min j d j .\nIf the bipartite graph is (d L , d R )-regular, then the graph deﬁnes a (d L , d R )-regular Tanner code. If the Tanner graph is sparse, i.e., |E| = O(N ), then it deﬁnes a low-density Tanner code . Tanner codes with single parity-check (SPC) local codes that are based on sparse Tanner graphs are called low-density parity-check (LDPC) codes .\nLet c i ∈ {0, 1} denote an input to a memoryless channel deﬁned by a conditional probability density function f (y i |c i = a) for a ∈ {0, 1}. For memoryless binary-input output- symmetric (MBIOS) channels, let λ ∈ R N denote the log- likelihood ratio (LLR) deﬁned by λ i (y i ) ln f (y i |c i =0) f (y\nfor every input bit i. For a code C, Maximum-Likelihood (ML) decoding is equivalent to ˆ x ML (y) = arg min x ∈C λ(y), x .\nLocal-Optimality Characterization: A new characterization for local-optimality of Tanner codes was presented in [3] as extension to [2], [13]. Local-optimality is deﬁned in Deﬁni- tion 4.\nDeﬁnition 1 (Path-Preﬁx Tree). Consider a graph G = (V, E) and a node r ∈ V . Let ˆ V denote the set of all backtrackless paths in G with length at most h that start at node r, and let\nˆ E \t (p 1 , p 2 ) ∈ ˆ V × ˆ V p 1 is a preﬁx of p 2 , |p 1 | + 1 = |p 2 | . We identify the empty path in ˆ V with (r). Denote by T h r (G) ( ˆ V , ˆ E) the path-preﬁx tree of G rooted at node r with height h.\nPath-preﬁx trees of G that are rooted in variable nodes are often called computation trees.\nWe use the following notation. Because vertices in T h r (G) are paths in G, we denote vertices in path-preﬁx trees by p and q. Vertices in G are denoted by u, v, r. For a path p ∈ ˆ V , let t(p) denote the last vertex (target) of path p. Denote by Preﬁx + (p) the set of proper preﬁxes of the path p, i.e.,\nLet T h r (G) = ( ˆ V , ˆ E) denote a path-preﬁx tree of a Tanner graph G = (V ∪ J , E). Let ˆ V {p | p ∈ ˆ V , t(p) ∈ V}, and\nˆ J {p | p ∈ ˆ V , t(p) ∈ J }. Paths in ˆ V are called variable paths , and paths in ˆ J are called local-code paths.\nDeﬁnition 2 ( d-tree). Denote by T 2h r (G) = ( ˆ V ∪ ˆ J , ˆ E) the path-preﬁx tree of a Tanner graph G rooted at node r ∈ V. A subtree T ⊆ T 2h r (G) is a d-tree if: (i) T is rooted at (r), (ii) for every local-code path p ∈ T ∩ ˆ J , deg T (p) = d, and (iii) for every variable path p ∈ T ∩ ˆ V, deg T (p) = deg T 2 h\n(p). Let T [r, 2h, d](G) denote the set of all d-trees rooted at r that are subtrees of T 2h r (G).\nDeﬁnition 3 ( w-weighted subtree). Let T = ( ˆ V∪ ˆ J , ˆ E) denote a subtree of T 2h r (G), and let w = (w 1 , . . . , w h ) ∈ R h + \\ {0 h } denote a non-negative weight vector. Let w T : ˆ V → R denote a weight function based on weight vector w for variable paths p ∈ ˆ V deﬁned as follows. If p is an empty variable path, then w T (p) = 0. Otherwise,\nFor any w-weighted subtree w T of T 2h r (G), let π G, T ,w : V → R denote a function whose values correspond to the projection of w T on the Tanner graph G. That is, for every variable node v in G, π G, T ,w (v) \t {p∈T |t(p)=v} w T (p).\nFor a Tanner code C(G), let B (w) d ⊆ [0, 1] N denote the set of all projections of w-weighted d-trees on G. That is,\nVectors in B (w) d are referred to as deviations. For two vectors x ∈ {0, 1} N and f ∈ [0, 1] N , let x ⊕ f ∈ [0, 1] N denote the relative point deﬁned by (x ⊕ f ) i |x i − f i |.\nDeﬁnition 4 (local-optimality, [3]). A codeword x ∈ C(G) is (h, w, d)-locally optimal with respect to λ ∈ R N if for all vectors β ∈ B (w) d ,\nλ, x ⊕ β > λ, x . \t (3) Theorem 5 (local-optimality is sufﬁcient for ML and LP, [3]). Let λ ∈ R N denote the LLR vector received from the channel. If x is an (h, w, d)-locally optimal codeword w.r.t. λ and some 2 d d ∗ , then (1) x is the unique maximum-likelihood codeword w.r.t. λ, and (2) x is the unique optimal solution of the LP-decoder given λ.\nFor two vectors y, z ∈ R N , let \u201c ∗\u201d denote coordinatewise multiplication, i.e., y ∗ z (y 1 · z 1 , . . . , y N · z N ).\nProposition 6 ( [3]). For every λ ∈ R N and every β ∈ [0, 1] N , (−1) x ∗ λ, β = λ, x ⊕ β − λ, x .\nThe following proposition states that the mapping (x, λ) → (0 N , (−1) x ∗ λ) preserves local-optimality.\nProposition 7 (symmetry of local-optimality, [3]). For every x ∈ C, x is (h, w, d)-locally optimal w.r.t. λ if and only if 0 N is (h, w, d)-locally optimal w.r.t. (−1) x ∗ λ.\nLet T q denote the subtree of a path-preﬁx tree T hanging from path q, i.e., the subtree induced by ˆ V q {p ∈ ˆ V ∪ ˆ J | q ∈ Preﬁx + (p) or p = q} (see Figure 1). Let Trim(T , q) denote the trimmed-tree of T induced by q obtained by deleting the subtree T q from T . Formally, Trim(T , q) is the path-preﬁx subtree of T induced by ˆ V ∪ ˆ J \\ ˆ V q . Note that if q is a sibling of q (i.e., q differs from q only in the last edge), then the degree of the parent of q and q decreases as a result of trimming ˆ V q . Hence, w T (q ) < w Trim(T ,q) (q ) for every variable path q ∈ ˆ V q .\nThe proofs of hierarchies presented in the following sections are based on the following lemma.\nLemma 8. Let T denote a subtree of a path-preﬁx tree T 2h r (G). For every path p ∈ T with at least two children in T , there exists at least one child q of p, such that\nLet Λ ⊆ R N denote a set of vectors. Denote by LO C,Λ (h, w, d) the set of pairs (x, λ) ∈ C × Λ such that x is (h, w, d)-locally optimal w.r.t. λ. Formally,\nThe following theorem derives an hierarchy on the \u201cdensity\u201d of deviations in local-optimality characterization.\nTheorem 9 ( d-Hierarchy of local-optimality). Let 2 d < d ∗ . For every Λ ⊆ R N ,\nProof: We prove the contrapositive statement. Assume that x is not (h, w, d + 1)-locally optimal w.r.t. λ. By Propo- sition 7, 0 N is not (h, w, d + 1)-locally optimal w.r.t. λ 0\nsuch that λ 0 , β \t 0. Let T denote the (d + 1)-tree that corresponds to the deviation β.\nConsider the following iterative trimming process. Start with the (d + 1)-tree T and let T ← T ; While there exists a local- code path p ∈ T such that deg T (p) = d + 1 do: T ← Trim(T , q) where q is a child of p such that λ 0 , π G, T ,w\nLemma 8 guarantees that the iterative trimming process halts with a d-tree T whose corresponding deviation β = π G, T ,w satisﬁes λ 0 , β \t λ 0 , β \t 0. We conclude by Proposition 7 that x is not (h, w, d)-locally optimal w.r.t. λ, as required.\nIn this section we introduce a new combinatorial charac- terization named strong local-optimality. We prove that if a codeword is strongly locally-optimal then it is also locally- optimal. The other direction is not true in general.\nDeﬁnition 10 (reduced d-tree). Denote by T 2h r (G) = ( ˆ V ∪ ˆ J , ˆ E) the path-preﬁx tree of a Tanner graph G rooted at node\nis rooted at r, (ii) deg T (r) = deg G (r) − 1, (iii) for every local-code path p ∈ T ∩ ˆ J , deg T (p) = d, and (iv) for every non-empty variable path p ∈ T ∩ ˆ V, deg T (p) = deg T 2 h\nThe only difference between Deﬁnition 2 ( d-tree) to a reduced d-tree is that the degree of the root in a reduced d-tree is smaller by 1 (as if the root itself hangs from an edge).\nLet T red [r, 2h, d](G) denote the set of all reduced d-trees rooted at r that are subtrees of T 2h r (G). For a Tanner code C(G), let B (w) d ⊆ [0, 1] N denote the set of all projections of w-weighted reduced d-trees on G. That is,\nThe following deﬁnition is analogues to Deﬁnition 4 (local- optimality) using reduced deviations instead of deviations.\nDeﬁnition 11 (strong local-optimality). Let C(G) ⊂ {0, 1} N denote a Tanner code with minimum local distance d ∗ . Let w ∈ R h + \\{0 h } denote a non-negative weight vector of length h and let 2 d d ∗ . A codeword x ∈ C(G) is (h, w, d)- strong locally-optimal with respect to λ ∈ R N if for all vectors\nλ, x ⊕ β > λ, x . \t (4) Denote by SLO C,Λ (h, w, d) the set pairs (x, λ) ∈ C ×Λ such\nlocally − optimal w.r.t. λ}. The following lemma states that if a codeword x is strongly\nProof: We prove the contrapositive statement. Assume that x is not (h, w, d)-locally optimal w.r.t. λ. By Proposi- tion 7, 0 N is not (h, w, d)-locally optimal w.r.t. λ 0 (−1) x ∗λ. Hence, there exists a deviation β = π G, T ,w ∈ B (w) d such that\nλ 0 , β 0. Let T denote the d-tree that corresponds to the deviation β.\nDenote by (r) the root of T . By Lemma 8, the root (r) has a child q such that λ 0 , π G, T ,w \t λ 0 , π G, Trim(T ,q),w . Note that Trim(T , q) is a reduced d-tree rooted at r. Moreover, the corresponding reduced deviation β = π G, T ,w satisﬁes\nλ 0 , β \t λ 0 , β 0. We conclude by Proposition 7 that x is not (h, w, d)-strong locally-optimal w.r.t. λ, as required.\nFollowing Lemma 12 and Theorem 5 we have the following corollary.\nCorollary 13 (strong local-optimality is sufﬁcient for both ML and LP). Let C(G) denote a Tanner code with minimum local distance d ∗ . Let h ∈ N + and w ∈ R h + . Let λ ∈ R N denote the LLR vector received from the channel. If x is an (h, w, d)- strong locally-optimal codeword w.r.t. λ and some 2 d d ∗ , then (1) x is the unique maximum-likelihood codeword w.r.t. λ, and (2) x is the unique solution of LP-decoding given λ.\nConsider a weight vector ¯ w ∈ R k ·h , and let ¯ w = ¯ w 1 ◦ ¯ w 2 ◦ . . .◦ ¯ w k denote its decomposition to k weight vectors ¯ w i ∈ R h .\n¯ w ∈ R k ·h is a k-legal extension of w ∈ R h if ∃α ∈ R k such that ¯ w i = α i · w. Note that if ¯ w ∈ R k ·h is geometric, then it is a k-legal extension of ¯ w 1 in its decomposition.\nThe following theorem derives an hierarchy on the height of reduced deviations of strong local-optimality characterization. Theorem 14 ( h-Hierarchy of strong LO). For every Λ ⊆ R N , if ¯ w ∈ R k ·h is a k-legal extension of w ∈ R h , then\nProof: We prove the contrapositive statement. Assume that x is not (k · h, ¯ w, d)-strong locally-optimal w.r.t. λ. Proposition 6 implies that 0 N is not (k·h, ¯ w, d)-strong locally- optimal w.r.t. λ 0 (−1) x ∗ λ. Hence, there exists a reduced deviation β = π G, T , ¯ w ∈ B ( ¯ w ) d such that λ 0 , β \t 0. Let T denote the reduced d-tree that corresponds to the reduced deviation β.\nLet {T i } denote a decomposition of T to reduced d-trees of height 2h as shown in Figure 2, where leaves of a subtree are the roots of other subtrees. Let p i denote the root of a reduced d-tree T i in the decomposition of T . Let order(T i )\np i /h . Namely, the order of T i equals to its level in the decomposition. Note that\nBecause λ 0 , β \t 0, we conclude by averaging that there exists at least one reduced d-tree T ∗ ∈ {T i } of height 2h\nsuch that λ 0 , π G, T ∗ ,w 0. Hence, 0 N is not (h, w, d)-strong locally-optimal w.r.t. λ 0 . We apply Proposition 6 again, and conclude that x is not (h, w, d)-strong locally-optimal w.r.t. λ, as required.\nWe conducted simulations to demonstrate two phenomena. First, we checked the gap between strong local optimality and local optimality. Second, we checked the effect of increasing the number of iterations on successful decoding with ML- certiﬁcates.\nWe chose a (3, 6)-regular LDPC code with blocklength N = 1008 and girth g = 6 [14]. We simulated a set Λ p of 5000 LLR vectors corresponding to the all zeros codeword with respect to a BSC with crossover probability p ∈ {0.04, 0.05, 0.06}. We used unit level weights, i.e., w = 1 h .\nLet SLO 0 N , Λ p (h, w, 2) (resp., LO 0 N , Λ p (h, w, 2) ) denote the set of LLR vectors λ ∈ Λ p such that 0 N is strongly locally- optimal (resp., locally optimal) w.r.t. λ.\nFigure 3 depicts cardinality of SLO 0 N , Λ p (h, w, 2) and LO 0 N , Λ p (h, w, 2) as a function of h, for three values of p. The results suggest that, in this setting, the sets SLO {0 N },Λ p (h, w, 2) and LO {0 N },Λ p (h, w, 2) coincide as h grows. This suggests also that the containment in Lemma 12 is asymptotically tight. That is, for large height h, strong local optimality is very close to local-optimality.\nThe results also suggest that the number of iterations needed to obtain reasonable decoding with ML-certiﬁcates is far greater than the girth. Clearly, the \u201ctree property\u201d that DE analysis relies on does not hold for so many iterations. Indeed, the simulated crossover probabilities are in the \u201cwaterfall\u201d region of the word error rate curve with respect to NWMS . We are not aware of any analytic explanation of this phenomena in ﬁnite length codes.\nAnother result of the simulation is that SLO 0 N , Λ p (h, w, 2) ⊆ SLO 0 N , Λ p (h + 1, w, 2). Namely, once a codeword is strongly locally-optimal for λ with height h, then it is also strongly\nlocally-optimal for any height h > h. This exhibited strength- ening of the height hierarchy result is not true in general. Counterexamples can be obtained for other level weights w and Tanner codes.\nThe degree hierarchy supports the improvement in the lower bounds on the threshold value p of successful LP-decoding over a BSC p as a function of d (see [3, Theorem 27]). These lower bounds are proved by analyzing the probability of a locally-optimal codeword as a function of p and the degree d. For example, consider any (2, 16)-regular Tanner code with minimum local-distance 4 whose Tanner graph has logarithmic girth in the blocklength. The bounds in [3] imply a lower bound on the threshold of p 0 = 0.019 with respect to degree d = 3. On the other hand, the lower bound on the threshold increases to p 0 = 0.044 with respect to degree d = 4.\nThe height hierarchy implies that if a codeword x is (h, w, 2)-strong locally-optimal w.r.t. an LLR vector λ, then it is also strongly locally-optimal with respect to any legal extension of level weights w with larger height h .\nConsider a Tanner code with single parity-check local codes. Assume that x is strongly locally-optimal codeword w.r.t. λ based on a height parameter h. Because strong local- optimality implies local-optimality, following [3, Theorem 16], we conclude that iterative message-passing decoding by NWMS is guaranteed to decode the ML-certiﬁed codeword x after k · h iterations, for every k ∈ N + . This gives the following new insight of convergence. If a codeword x is decoded after h iterations and is certiﬁed to be strongly locally-optimal (and hence ML-optimal), then x is the outcome of NWMS inﬁnitely many times (i.e., whenever the number of iterations is a multiple of h).\nWe present hierarchies of local optimality with respect to two parameters of the local-optimality characterization for Tanner codes [3]. One hierarchy is based on the local code\nnode degrees in the deviations. We prove containment, namely, the set of locally-optimal codewords with respect to degree d + 1 is a superset of the set of locally-optimal codewords with respect to degree d.\nThe second hierarchy is based on the height of the devia- tions. We prove that, for geometric level weights, a strongly locally-optimal codeword is inﬁnitely often strongly locally- optimal. This result implies that a codeword that is decoded with a certiﬁcate using the iterative decoder NWMS after h iterations is decoded with a certiﬁcate after k · h iterations, for every integer k.\nWe deal with terms (a) and (b) separately. Because p / ∈ Preﬁx + (q) for the paths accumulated in term (a), it remains unchanged under trimming children of p from T .\nIt remains to show that p has a child whose trimming does not increase term (b). Let q min denote the child of p, for which the subtree hanging from it has a minimum cost w.r.t. λ. By averaging, trimming the subtree that hangs from q min cannot increase the value of term (b), and the lemma follows."},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565237.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S15.T5.2","endtime":"10:30","authors":"Nissim Halabi, Guy  Even","date":"1341569400000","papertitle":"Hierarchies of Local-Optimality Characterizations in Decoding Tanner Codes","starttime":"10:10","session":"S15.T5: Decoding Algorithms for Codes on Graphs","room":"Kresge Little Theatre (035)","paperid":"1569565237"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
