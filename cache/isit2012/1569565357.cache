{"id":"1569565357","paper":{"title":{"text":"An Outer Bound for the Vector Gaussian CEO Problem"},"authors":[{"name":"Ersen Ekrem"},{"name":"Sennur Ulukus"}],"abstr":{"text":"Abstract\u2014We study the vector Gaussian CEO problem, and provide an outer bound for its rate-distortion region. We obtain our outer bound by evaluating an outer bound for the multi- terminal source coding problem by means of a technique relying on the de Bruijn identity and the properties of the Fisher information. Next, we address the tightness of our outer bound, and show that our outer bound does not provide the rate- distortion region in general. In particular, we provide a speciﬁc example where the rate-distortion region is strictly contained in our outer bound."},"body":{"text":"We study the vector Gaussian CEO problem, where there is a vector Gaussian source which is observed through some lin- ear additive vector Gaussian channels by an arbitrary number of agents. The agents process their observations independently and communicate them to a central unit (the so-called CEO) through orthogonal and rate-limited links (see Figure 1). The goal of the agents is to describe their observations to the CEO in a way that the CEO can reconstruct the source within a given distortion. The trade-off between the rate spent by the agents and the distortion attained by the CEO is characterized by the rate-distortion region, which is unknown in general.\nThe CEO problem is introduced in [1] for a discrete memoryless setting, where the CEO is interested in estimating a discrete source with the minimum Hamming distance. The scalar Gaussian CEO problem is introduced in [2], where there is a scalar Gaussian source which is observed through some linear Gaussian channels by the agents. The CEO wants to estimate the Gaussian source with the minimum mean square error (MMSE). In [2], it is shown that the MMSE decays inversely proportionally with the rate expenditure of the agents, for sufﬁciently large number of agents. The scalar Gaussian CEO problem is further studied in [3], [4], where the entire rate-distortion region is characterized. The achievability of this region follows from the Berger-Tung inner bound [5], and the converse proof relies on the entropy-power inequality. Recently, [6] provided an alternative proof for the sum-rate of the scalar Gaussian CEO problem without invoking the entropy-power inequality.\nAlthough entropy-power inequality is useful to provide converse proofs for scalar Gaussian problems, it might be restrictive for vector Gaussian problems [7], [8]. For the\nvector Gaussian CEO problem, this observation is noticed in [9], where a lower bound for the sum-rate of the vector Gaussian CEO problem is proposed by using the entropy- power inequality. This lower bound is shown to be tight under certain conditions, although it is not tight in general. Recently, [10] provided an outer bound for the rate-distortion region of the vector Gaussian CEO problem when there are only two agents. The outer bound in [10] is obtained by using a generalized version of the extremal inequality in [11].\nSince the outer bound in [10] relies on the extremal in- equality in [11], the generalization of this outer bound [10] to more than two agents requires the generalization of the extremal inequality in [11] as well. Here, we generalize the outer bound in [10] to an arbitrary number of agents without any recourse on the extremal inequality in [11]. To this end, we ﬁrst consider the outer bound provided in [12] for the multi- terminal source coding problem, and evaluate it for the vector Gaussian CEO problem at hand. In this evaluation, we use the de Bruijn identity [13], a connection between the differential entropy and the Fisher information, along with the properties of the MMSE and the Fisher information. This evaluation technique which relies on the de Bruijn identity is useful in the sense that it is able to alleviate some shortcomings of the entropy-power inequality in vector Gaussian problems [8], [14].\nNext, we address the tightness of our outer bound, and show that neither our outer bound nor the outer bound in [10], which is, in fact, a special case of our outer bound, provides the rate-distortion region in general. To show this, we provide a speciﬁc example, where the rate-distortion region is strictly contained in our outer bound. In particular, we consider the parallel Gaussian model, for which we obtain the entire rate- distortion region explicitly by using the outer bound in [12].\nWe then show that the rate-distortion region of the parallel Gaussian model is strictly contained in outer bound, which implies that our outer bound is not tight in general.\nFinally, we note that in our set-up (see Figure 1), the agents observe the vector Gaussian source X through only an additive Gaussian noise, i.e., their observations are Y = X + N . However, our outer bound can be generalized to a broader model [15], [16], where the agents observe some linear com- binations of the vector Gaussian source through an additive Gaussian noise, i.e., their observations are Y = H X + N . These generalizations can be found in [17].\nIn the CEO problem, there are L sensors, each of which getting a noisy observation of a source. The goal of the sensors is to describe their observations to the CEO such that it can reconstruct the source within a given distortion. In the vector Gaussian CEO problem, there is an i.i.d. vector Gaussian source {X i } n i=1 with zero-mean and covariance K X . Each sensor gets a noisy version of this Gaussian source\nwhere {N ,i } n i=1 is an i.i.d. sequence of Gaussian random vec- tors with zero-mean and covariance Σ . Moreover, {N ,i } L are independent ∀i. The distortion of the reconstructed vector is measured by its mean square error matrix\nˆ D n = 1 n\nAn (n, R 1 , . . . , R L ) code for the CEO problem consists of an encoding function at each sensor f n : R M ×n → B n = {1, . . . , 2 nR }, i.e., B n = f n (Y n ) where B n ∈ B n , = 1, . . . , L, and a decoding function at the CEO g n : B n 1 × . . . × B n L → R M ×n , i.e., ˆ X n = g n (B n 1 , . . . , B n L ), where M denotes the size of the vector Gaussian source X.\nSince the MMSE estimator, which is the conditional mean, minimizes the mean square error, the decoding function g n can be chosen as the MMSE estimator. Hence, we have ˆ X i = E X i |{B n } L =1 using which in (2), we get\nˆ D n = 1 n\nHence, a rate tuple (R 1 , . . . , R L ) is said to achieve the distortion D if there exists an (n, R 1 , . . . , R L ) code such that\n1 n\nwhere D is a strictly positive deﬁnite matrix. Throughout the paper, we assume that the distortion matrix D satisﬁes\nwhere the lower bound on the distortion constraint D cor- responds to the MMSE matrix obtained when the CEO has direct access to the observations of the agents {Y } L =1 . In [17, Appendix A.2], we show that imposing the lower bound on D in (5) does not incur any loss of generality, while imposing the upper bound on D in (5) might incur some loss of generality.\nThe rate-distortion region R(D) of the vector Gaussian CEO problem is deﬁned as the closure of all rate tuples (R 1 , . . . , R L ) that can achieve the distortion D.\nThe main result of this paper is the following outer bound on the rate-distortion region R(D):\nTheorem 1 The rate-distortion region of the Gaussian CEO problem R(D) is contained in the region R o (D) which is given by the union of rate tuples (R 1 , . . . , R L ) satisfying\nfor all A ⊆ {1, . . . , L}, where the union is over all positive semi-deﬁnite matrices {D } L =1 ∈ D, and D contains all {D } L =1 matrices satisfying the following constraints\nWe obtain this outer bound by evaluating the outer bound given in [12]. This evaluation relies on the de Bruijn identity along with the properties of the Fisher information and the MMSE. The proof of Theorem 1 can be found in Section III.\nNext, we provide the following inner bound for the rate- distortion region R(D).\nTheorem 2 An inner bound for the rate-distortion region of the vector Gaussian CEO problem is given by the re- gion R i (D) which is described by the union of rate tuples (R 1 , . . . , R L ) satisfying\nfor all A ⊆ {1, . . . , L}, where the union is over all positive semi-deﬁnite matrices {D } L =1 ∈ D.\nThis inner bound is obtained by evaluating the Berger- Tung inner bound [5] by jointly Gaussian auxiliary random variables.\nWe note that for both the outer bound in Theorem 1 and the inner bound in Theorem 2, the feasible sets to which {D } L =1 belong are identical and given by D. On the other hand, rate bounds differ as seen through (6) and (9). Despite this difference, there are cases where the outer and inner bounds match, providing a complete characterization of the rate- distortion region. Here, we note a general sufﬁcient condition under which the outer and inner bounds coincide. If the boundary of the outer bound in Theorem 1 can be attained by {D ∗ } L =1 matrices which achieve the distortion constraint in (7) with equality, then the outer and inner bounds match, giving the rate-distortion region. For example, the outer and inner bounds match for the scalar Gaussian model [17].\nHere, we provide a sketch of the proof of Theorem 1 for L = 2. The proof of Theorem 1 for an arbitrary L can be found in [17]. We ﬁrst state the following outer bound for the rate-distortion of the CEO problem.\nTheorem 3 ( [12, Theorem 1]) The rate-distortion region of the CEO problem R(D) is contained in the union of rate tuples (R 1 , R 2 ) satisfying\nR 1 ≥ I(X; U 1 |U 2 ) + I(Y 1 ; U 1 |X, W ) \t (10) R 2 ≥ I(X; U 2 |U 1 ) + I(Y 2 ; U 2 |X, W ) \t (11)\nwhere the union is over all joint distributions p(x, {y , u } 2 =1 , w) = p(x)p(w) 2 =1 p(y |x)p(u |y , w) satisfying\nWe evaluate this outer bound to obtain the outer bound in Theorem 1 for L = 2. In this evaluation, we use properties of the Fisher information, MMSE and differential entropy along with connections among them. The corresponding background information on these quantities can be found in [17, Section 6.1].\n= 1 2\nUsing [17, Lemma 2] and the fact that jointly Gaussian (X, W, U , Y ) maximizes h(Y |X, W, U ), we have the fol- lowing bounds for the second term in (15)\n≤ 1 2\nSince log |·| is continuous in positive semi-deﬁnite matrices, there exists a matrix D in the following form\nMoreover, using the Cramer-Rao inequality [17, Lemma 1] and the fact that conditioning reduces MMSE, the following bounds on D can be obtained\nI(X; U 1 |U 2 ) = h(X|U 2 ) − h(X|U 1 , U 2 ) \t (22) ≥ h(X|U 2 ) − 1\nwhere (23) comes from the fact that h(X|U 1 , U 2 ) is max- imized by jointly Gaussian (X, U 1 , U 2 ), (24) follows from the monotonicity of log | · | function in positive semi-deﬁnite matrices in conjunction with the distortion constraint in (13), (25) comes from the fact that conditioning cannot increase entropy, and (26) is due to [17, Lemma 2].\nNext, we obtain a lower bound for J −1 (X|U 2 , W ), which, in turn, will yield a lower bound for h(X|U 2 , W ). To obtain a lower bound for h(X|U 2 , W ), we will use an identity between the Fisher information matrix and the MMSE matrix, which holds for additive Gaussian models as we have here. In particular, using [17, Lemma 3], we can get\nMoreover, in view of the monotonicity of log | · | in positive semi-deﬁnite matrices, using (28) in (26), we can get\nwhere the positivity operator comes from the non-negativity of the mutual information. Using (19) and (29) in (10), we can get\n1 2\n1 | (30)\nwhich is the desired bound on R 1 given in Theorem 1. Similarly one can get the desired bound on R 2 as well.\nNext, we consider the sum-rate R 1 + R 2 . To this end, we note that using the maximum entropy theorem and the distortion constraint in (13), one can get\nusing which, and the identities in (19) for the sum-rate bound in (12), we have\nwhich is the desired bound on the sum-rate given in Theo- rem 1.\nFinally, we establish a connection between D and (D 1 , D 2 ), which will complete the proof of Theorem 1. To this end, we note that similar to (28), one can obtain the following lower bound for J −1 (X|U 1 , U 2 , W )\nmmse(X|U 1 , U 2 , W ) \t (34) mmse(X|U 1 , U 2 ) \t (35) D \t (36)\nwhere (34) is due to the Cramer-Rao inequality [17, Lemma 1], (35) comes from the fact that conditioning reduces MMSE, and (36) follows from the distortion constraint in (13). The order in (36) gives us the desired order among D and D; completing the proof.\nHere, ﬁrst, we consider the parallel Gaussian model, and obtain its rate-distortion region. Next, we consider a speciﬁc parallel Gaussian model and show that our outer bound in Theorem 1 is not tight. In other words, we show that, in general, there are rate tuples (R 1 , . . . , R L ) that lie inside our outer bound and are not contained in the rate-distortion region.\nIn the parallel Gaussian model, we have the Gaussian source X i = [X 1,i . . . X M,i ] where {X m,i } M m=1 are zero-mean inde- pendent Gaussian random variables with variances {σ 2 m } M m=1 , respectively. Moreover, the noise at the th sensor N ,i is given by N ,i = [N 1,i . . . N M,i ], where {N m,i } M m=1 are zero- mean independent Gaussian random variables with variances\n{σ 2 m } M m=1 , respectively. In the parallel Gaussian model, there is a separate-distortion constraint on each component of the source as follows\n1 n\nWe note that the constraints on D m in (38) are the scalar versions of the constraints in (5), which were for the vector Gaussian model. For the parallel Gaussian model, we obtain the rate-distortion region R p ({D m } M m=1 ) as follows.\nTheorem 4 The rate-distortion region R p ({D m } M m=1 ) of the parallel Gaussian CEO problem is given by the union of rate tuples (R 1 , . . . , R L ) satisfying\nfor all A ⊆ {1, . . . , L}, where the union is over all {D m } ∀ ,∀m satisfying the following constraints\nSince the distortion constraints in (40) are met with equality, the ﬁrst log(·) in (39) is always positive, and hence, we do not need a positivity operator. We obtain the rate-distortion region of the parallel Gaussian CEO problem in two steps. In the ﬁrst step, we specialize the outer bound in [12] to the parallel model. In the second step, we evaluate the outer bound we obtain in the ﬁrst step, and show that it matches the inner bound given in Theorem 2.\nNext, we consider the case L = M = 2, and provide an example where our outer bound strictly contains the rate- distortion region, i.e., our outer bound includes rate pairs which are outside of the rate-distortion region. In the example we provide, we assume that the following conditions hold:\nσ 2 22 \t (44) 1\nUnder the constraints in (42)-(45) 1 , the rate-distortion region R p (D 1 , D 2 ) can be characterized as follows.\n− µ 2 2\nD 1 \t (48) 0 ≤ D 1 ≤ σ 2 1 , \t = 1, 2 \t (49)\nNext, we ﬁnd an upper bound for our outer bound in Theorem 1 as follows.\nwhere the function f 1 (D 11 , D 21 ) is given by (47) and the set D 1 is given by the union of (D 11 , D 21 ) satisfying (48)-(49).\nNow, we are ready to compare our outer bound with the rate-distortion region for the parallel Gaussian model. Using Corollary 1 and Corollary 2, we have\nwhere (53) comes from the facts that log(·) is strictly concave, and the two terms inside the log(·) functions in (52) are not\nidentical, which is due to the assumption in (44). Equation (54) implies that there are some rate pairs (R 1 , R 2 ) in our outer bound which are outside of the rate-distortion region of the parallel Gaussian model. Hence, our outer bound strictly contains the rate-distortion region of the vector Gaussian CEO problem, i.e., our outer bound is not tight in general.\nHere, we study the vector Gaussian CEO problem and provide an outer bound for its rate-distortion region. To obtain our outer bound, we consider the rather general outer bound for the multi-terminal source coding problem in [12], and evaluate it for the vector Gaussian CEO problem at hand. We accomplish this evaluation by using a technique that relies on the de Bruijn identity along with the properties of the MMSE and Fisher information. Next, we address the tightness of our outer bound and show that our outer bound does not provide the exact rate-distortion region in general. We show this by providing an example where our outer bound strictly includes the rate-distortion region."},"refs":[{"authors":[{"name":"T. Berge"},{"name":"Z. Zhan"},{"name":"H. Viswanathan"}],"title":{"text":"The CEO problem"}},{"authors":[{"name":"H. Viswanatha"},{"name":"T. Berger"}],"title":{"text":"The quadratic Gaussian CEO problem"}},{"authors":[{"name":"Y. Oohama"}],"title":{"text":"Rate-distortion theory for Gaussian multiterminal source coding systems with several side informations at the decoder"}},{"authors":[{"name":"V. Prabhakara"},{"name":"D. Ts"},{"name":"K. Ramchandran"}],"title":{"text":"Rate region of the quadratic Gaussian CEO problem"}},{"authors":[{"name":"S.-Y. Tung"}],"title":{"text":"Multiterminal source coding"}},{"authors":[{"name":"J. Wan"},{"name":"J. Che"},{"name":"X. Wu"}],"title":{"text":"On the sum rate of Gaussian multiterminal source coding: New proofs and results"}},{"authors":[{"name":"H. Weingarte"},{"name":"Y. Steinber"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"The capacity region of the Gaussian multiple-input multiple-output broadcast channel"}},{"authors":[{"name":"E. Ekre"},{"name":"S. Ulukus"}],"title":{"text":"The secrecy capacity region of the Gaus- sian MIMO multi-receiver wiretap channel"}},{"authors":[{"name":"S. Tavilda"},{"name":"P. Viswanath"}],"title":{"text":"On the sum-rate of the vector Gaussian CEO problem"}},{"authors":[{"name":"J. Che"},{"name":"J. Wang"}],"title":{"text":"On the vector Gaussian CEO problem"}},{"authors":[{"name":"T. Li"},{"name":"P. Viswanath"}],"title":{"text":"An extremal inequality motivated by mul- titerminal information theoretic problems"}},{"authors":[{"name":"A. B. Wagne"},{"name":"V. Anantharam"}],"title":{"text":"An improved outer bound for multiterminal source coding"}},{"authors":[{"name":"D. P. Paloma"},{"name":"S. Verdu"}],"title":{"text":"Gradient of mutual information in linear vector Gaussian channels"}},{"authors":[{"name":"E. Ekre"},{"name":"S. Ulukus"}],"title":{"text":"An alternative proof for the capacity region of the degraded Gaussian MIMO broadcast channel"}},{"authors":[{"name":"Y. Oohama"}],"title":{"text":"Distributed source coding of correlated Gassign observa- tions"}},{"authors":[{"name":"Y. Yan"},{"name":"Y. Zhan"},{"name":"Z. Xiong"}],"title":{"text":"The generalized quadratic Gassign CEO problem: New cases with tight rate region and applications"}},{"authors":[{"name":"E. Ekre"},{"name":"S. Ulukus"}],"title":{"text":"An outer bound for the vector Gaussian CEO problem"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565357.pdf"},"links":[{"id":"1569566485","weight":6},{"id":"1569566875","weight":6},{"id":"1569566981","weight":3},{"id":"1569566683","weight":6},{"id":"1569566597","weight":3},{"id":"1569566591","weight":6},{"id":"1569556029","weight":3},{"id":"1569566571","weight":3},{"id":"1569552245","weight":3},{"id":"1569566415","weight":3},{"id":"1569566469","weight":6},{"id":"1569566081","weight":3},{"id":"1569565613","weight":3},{"id":"1569565355","weight":3},{"id":"1569565931","weight":6},{"id":"1569565547","weight":6},{"id":"1569565461","weight":3},{"id":"1569564227","weight":3},{"id":"1569563411","weight":3},{"id":"1569558459","weight":3},{"id":"1569566821","weight":3},{"id":"1569560613","weight":6},{"id":"1569566903","weight":3},{"id":"1569566999","weight":3},{"id":"1569565859","weight":3},{"id":"1569565809","weight":3},{"id":"1569566497","weight":3},{"id":"1569566523","weight":6},{"id":"1569566895","weight":3},{"id":"1569564189","weight":3},{"id":"1569566985","weight":3},{"id":"1569566095","weight":3},{"id":"1569564271","weight":3},{"id":"1569566239","weight":6},{"id":"1569566679","weight":3},{"id":"1569563981","weight":16},{"id":"1569566905","weight":3},{"id":"1569566733","weight":3},{"id":"1569566753","weight":6},{"id":"1569566759","weight":3},{"id":"1569559995","weight":3},{"id":"1569567665","weight":3},{"id":"1569564611","weight":3},{"id":"1569565535","weight":3},{"id":"1569561795","weight":6},{"id":"1569567015","weight":3},{"id":"1569566811","weight":3},{"id":"1569566851","weight":6},{"id":"1569553909","weight":3},{"id":"1569565427","weight":3},{"id":"1569553519","weight":3},{"id":"1569554971","weight":3},{"id":"1569566209","weight":3},{"id":"1569566371","weight":3},{"id":"1569558985","weight":9},{"id":"1569565087","weight":3},{"id":"1569566473","weight":3},{"id":"1569564857","weight":3},{"id":"1569566809","weight":3},{"id":"1569565033","weight":6},{"id":"1569557083","weight":3},{"id":"1569565929","weight":3},{"id":"1569565633","weight":3},{"id":"1569565219","weight":3},{"id":"1569565595","weight":9},{"id":"1569566037","weight":3},{"id":"1569566043","weight":3},{"id":"1569565029","weight":3},{"id":"1569561245","weight":6},{"id":"1569566191","weight":3},{"id":"1569565527","weight":3},{"id":"1569565363","weight":6},{"id":"1569566695","weight":6},{"id":"1569565909","weight":9},{"id":"1569566673","weight":6},{"id":"1569565441","weight":6},{"id":"1569566233","weight":6},{"id":"1569566297","weight":3},{"id":"1569564097","weight":3},{"id":"1569565741","weight":3},{"id":"1569566481","weight":6},{"id":"1569564339","weight":3},{"id":"1569565439","weight":6},{"id":"1569565415","weight":6},{"id":"1569566383","weight":3},{"id":"1569565571","weight":3},{"id":"1569565665","weight":3},{"id":"1569565611","weight":3},{"id":"1569565397","weight":3},{"id":"1569565435","weight":3},{"id":"1569566129","weight":3},{"id":"1569565093","weight":3},{"id":"1569565919","weight":3},{"id":"1569566927","weight":3},{"id":"1569565661","weight":6},{"id":"1569566887","weight":3},{"id":"1569561221","weight":6},{"id":"1569566651","weight":3},{"id":"1569566823","weight":3},{"id":"1569566595","weight":3},{"id":"1569552025","weight":3},{"id":"1569565013","weight":3},{"id":"1569565375","weight":6},{"id":"1569566713","weight":3},{"id":"1569566771","weight":3},{"id":"1569559035","weight":3},{"id":"1569564437","weight":3},{"id":"1569551905","weight":3},{"id":"1569564861","weight":3},{"id":"1569564787","weight":3},{"id":"1569556759","weight":3},{"id":"1569566619","weight":3},{"id":"1569561185","weight":41},{"id":"1569558779","weight":12},{"id":"1569565669","weight":3},{"id":"1569566001","weight":3},{"id":"1569566817","weight":9},{"id":"1569565367","weight":3},{"id":"1569566299","weight":12},{"id":"1569565805","weight":9},{"id":"1569566933","weight":3},{"id":"1569563919","weight":3},{"id":"1569557851","weight":3},{"id":"1569559919","weight":6},{"id":"1569566147","weight":3},{"id":"1569565537","weight":3},{"id":"1569562367","weight":3},{"id":"1569564961","weight":3},{"id":"1569567013","weight":6},{"id":"1569550425","weight":3},{"id":"1569563725","weight":3},{"id":"1569565635","weight":3},{"id":"1569561397","weight":3},{"id":"1569556327","weight":6},{"id":"1569565113","weight":3},{"id":"1569564257","weight":6},{"id":"1569564141","weight":9},{"id":"1569564755","weight":3},{"id":"1569551541","weight":3},{"id":"1569566839","weight":3},{"id":"1569566663","weight":3},{"id":"1569566241","weight":3},{"id":"1569566443","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T1.4","endtime":"18:00","authors":"Ersen Ekrem, Sennur Ulukus","date":"1341250800000","papertitle":"An Outer Bound for the Vector Gaussian CEO Problem","starttime":"17:40","session":"S4.T1: The Slepian-Wolf and CEO Problems","room":"Kresge Rehearsal B (030)","paperid":"1569565357"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
