{"id":"1569566473","paper":{"title":{"text":"Is Gaussian Noise the Worst-Case Additive Noise in Wireless Networks?"},"authors":[{"name":"Ilan Shomorony"},{"name":"A. Salman Avestimehr"}],"abstr":{"text":"Abstract\u2014An important classical result in Information Theory states that the Gaussian noise is the worst-case additive noise in point-to-point channels. In this paper, we signiﬁcantly generalize this result and show that, under very mild assumptions, Gaussian noise is also the worst-case additive noise in general wireless networks with additive noises that are independent from the transmit signals. More speciﬁcally, we prove that, given a coding scheme with ﬁnite reading precision for an AWGN network, one can build a coding scheme that achieves the same rates on an additive noise wireless network with the same topology, where the noise terms may have any distribution with same mean and variance as in the AWGN network."},"body":{"text":"The modeling of background noise in point-to-point wire- less channels as an additive Gaussian noise is well supported from both theoretical and practical viewpoints. In practice, we have witnessed that current wireless systems designed based on the assumption of additive Gaussian noise perform quite well. This is intuitively explained by the fact that, from the Central Limit Theorem, the composite effect of many independent noise sources (e.g., thermal noise, shot noise, etc.) should approach a Gaussian distribution. From a theoretical point of view, Gaussian noise has been proven to be the worst- case noise for additive noise channels. This follows mainly from the fact that the Gaussian distribution maximizes the entropy subject to a variance constraint. More precisely, from the Channel Coding Theorem [1], the capacity of a channel f (y|x) is given by\nIn the case of an additive noise (AN) channel Y = X + Z, where E[Z] = 0 and E Z 2 = σ 2 , the fact that the Gaussian distribution maximizes the entropy implies that h(X|Y ) ≤\nwhere C AWGN is the capacity of the AWGN channel, which is achieved by a Gaussian input distribution. Moreover, a more operational justiﬁcation of the fact that Gaussian is the worst- case noise for additive noise channels was provided in [2], where it was shown that random Gaussian codebooks and nearest-neighbor decoding achieve the capacity of the corre- sponding AWGN channel on a non-Gaussian AN channel.\nOnce we go beyond point-to-point channels, Gaussian noise is only known to be the worst-case additive noise in some special wireless networks, such as the Multiple Access Chan- nel, the Degraded Broadcast Channel and MIMO channels. In all such cases the capacity has been fully characterized and is known to be achievable with Gaussian inputs. Therefore, similar arguments to the one above can be used to show that, in these cases, Gaussian noise is indeed the worst-case additive noise. However, for more general wireless networks where the capacity is unknown, we lack the tools to make such an assertion. The recent constant-gap capacity approximations for the Interference Channel [3] and for single-source single- destination relay networks [4, 5] can only be used to state that Gaussian noise is \u201capproximately\u201d the worst-case additive noise in these cases. Nonetheless, in a leap of faith, most of the research concerning such systems and many other wireless networks views the AWGN channel model as the standard wireless link model. As a result, it remains a fundamental open question whether Gaussian noise is the worst-case additive noise in general wireless networks.\nIn this work, we make signiﬁcant progress towards showing that the Gaussian noise is in fact the worst-case noise for arbitrary wireless networks with additive noises that are inde- pendent of the transmit signals. We consider wireless networks with unrestricted topologies and fairly general trafﬁc demands. We show that any coding scheme with ﬁnite reading precision that achieves a given set of rates on a network with Gaussian additive noises can be used to construct a coding scheme that achieves the same set of rates on a network with same topology and trafﬁc demands, but with non-Gaussian additive noises. A coding scheme is said to have ﬁnite reading precision if all its relaying and decoding functions only depend on the received signals read up to a ﬁnite number of digits after the decimal point. This precision can be chosen arbitrarily large, as long as it is ﬁnite, and is allowed to tend to inﬁnity along the sequence of coding schemes. Hence, this is a very mild restriction, and, in practice, almost all coding schemes satisfy it. Moreover, in [6], we relax this restriction and consider instead a class of coding schemes with inﬁnite reading precision called \u201ctruncatable\u201d coding schemes.\nWe start our coding scheme construction by applying a transformation at the transmit and received signals of all nodes in the network to create an \u201capproximately Gaussian\u201d effective network. The technique resembles OFDM in that it uses the Discrete Fourier Transform to mix together multiple uses of the same channel. This mixing causes the effective\nadditive noise terms to converge in distribution to Gaussian additive noise terms, which are not i.i.d. over time. The dependence between noise realizations is handled through the combination of an interleaving technique and an outer code. The interleaving operation creates multiple blocks of network uses inside which the additive noises are i.i.d. and almost Gaussian. Since our original coding scheme has ﬁnite reading precision, its peformance on the approximately AWGN blocks does not deviate much from its performance on a real AWGN network, and a mutual-information argument can be used to show that our outer codes achieve arbitrarily close to the set of rates of our original coding scheme.\nAn |L|-unicast additive noise wireless network (G, L) con- sists of a directed graph G = (V, E), and a set L ⊂ V × V\nof source-destination pairs. We assume that all sources and destinations are distinct nodes, but more general settings are considered in [6]. All nodes in V which are not a source will function as relays. We associate a real-valued channel gain h u,v with each edge (u, v) ∈ E.\nCommunication in (G, L) is performed over n discrete time steps. At time t = 1, ..., n, each u ∈ V transmits a real-valued signal X u [t], satisfying n t=1 X 2 u [t] ≤ nP , ∀ u ∈ V , for some P ≥ 0. The signal received by node v at time t is\nwhere I(v) = {u ∈ V : (u, v) ∈ E}, and the additive noise N v is assumed to be i.i.d. over time and satisﬁes E[N v ] = 0 and E N 2 v = σ 2 v < ∞. We also assume that the noise terms are independent from all transmit signals and from the noise terms at distinct nodes, and that each N v has an absolutely continuous distribution. If all the additive noises in the network are N (0, σ 2 v ), we say the network is an AWGN network.\nDeﬁnition 1. A coding scheme C with block length n ∈ N and rate tuple R = (R 1 , ..., R |L| ) ∈ R |L| for an |L|-unicast additive noise wireless network consists of:\n1. An encoding function f i : {1, ..., 2 nR i } → R n for each source s i , i = 1, ..., |L|, where each codeword f i (w), w ∈ {1, ..., 2 nR i }, satisﬁes an average power constraint of P .\n2. Relaying functions r (t) v : R t−1 → R, for t = 1, ..., n, for each relay v ∈ V that is not a source, satisfying the average power constraint\n1 n\n3. A decoding function g i : R n → {1, ..., 2 nR i } for each destination d i , i = 1, ..., |L|.\nDeﬁnition 2. A rate tuple R is said to be achievable for an |L|-unicast wireless network (G, L) if there exists a sequence of coding schemes C n = (n, R) for which the probability that at least one decoder makes an error tends to zero, i.e.,\nwhere the message transmitted by source s i , W i , is assumed to be chosen uniformly at random from {1, ..., 2 nR i }, for i = 1, ..., |L|.\nDeﬁnition 3. A coding scheme C = (n, R) has ﬁnite reading precision ρ ∈ N if the relaying/decoding functions of a node v in the network can only depend on\nDeﬁnition 4. Rate tuple R is achievable by coding schemes with ﬁnite reading precision if we have a sequence of coding schemes C n = (n, R), where C n has ﬁnite reading precision ρ n , which achieves rate tuple R.\nThe assumption of ﬁnite reading precision on a coding scheme is certainly mild from a practical point of view. Moreover, in [6], we show that coding schemes with ﬁnite reading precision are in fact optimal for AWGN networks.\nTheorem 1. Suppose a rate tuple R is achievable by coding schemes with ﬁnite reading precision on an AWGN wireless network (G, L). Then it is possible to build a sequence of coding schemes that achieves arbitrarily close to R on the same |L|-unicast additive noise wireless network (G, L) where, for each v, the distribution of N v is replaced with an arbitrary absolutely continuous distribution with E[N v ] = 0 and E N 2 v = σ 2 v .\nTo prove Theorem 1, we start by describing an OFDM-like scheme that is applied to all nodes in the network in section III-A. By applying an Inverse Discrete Fourier Transform (IDFT) to the block of transmit signals of each node, and a Discrete Fourier Transform (DFT) to the block of received signals of each node, we create effective additive noise terms that are weighted averages of the additive noise realizations during that block. This mixture of noises can be shown to converge in distribution to a Gaussian additive noise term. To overcome the fact that the resulting noise terms are not i.i.d. over time, in section III-B, we apply the OFDM-like scheme over multiple blocks, and then we interleave the effective network uses from distinct blocks. This effectively creates several blocks in which the network behaves as an approximately AWGN network (with i.i.d. noises). Then our original code for the AWGN network can be applied to each approximately AWGN block. The fact that this code has ﬁnite reading precision guarantees that, when applied to the approximately AWGN block, its error probability is close to its error probability on a true AWGN network. Finally, we take care of the dependence between the noises of different blocks created in the interleaving operation by using a random outer code for each source-destination pair, and we can show via a mutual-information argument that we can achieve a rate tuple\narbitrarily close to R on the non-Gaussian wireless network. A diagram illustrating the proof steps is shown in Figure 1.\nAssume that a node u ∈ V has b real-valued signals d 0 , d 1 , ..., d b−1 which are the inputs to the effective channels we intend to create. We assume that b is even to simplify the expressions. Node u will \u201cpack\u201d these signals into b complex numbers ˜ d 0 , ..., ˜ d b−1 as follows.\nNext, node u takes the IDFT of ˜ d u = ( ˜ d 0 , ..., ˜ d b−1 ) to obtain X u = IDFT(˜ d u ). We assume that DFT and IDFT refer to the unitary version of the DFT and IDFT. Since ˜ d u is conjugate symmetric, X u is a real vector (in R b ). We will require the original real-valued signals to satisfy\nE d 2 i ≤ P/2, for i = 1, ..., b − 1, \t (2) E d 2 b ≤ P, \t (3)\nTherefore, u may transmit X u over b time-slots, and a receiver node v (i.e., either a destination node or a relay node) will receive, over the same b time-slots,\nBy applying a DFT to its block of b received signals, node v will obtain\nNext, by looking at each component of ˜ Y v , we notice that we have effectively b complex-valued received signals. The additive noise on the th received signal is given by\nBy considering the real and imaginary parts of each component ˜ Y v,i of ˜ Y v , for i = 0, ..., b − 1, separately, we obtain the following 2b − 2 effective received signals:\nh u,v d u,0 + DFT(N v ) 0 , \t (5) ˜ Y v,i =\nfor i = 1, ..., b 2 − 1, \t (6) ˜ Y v,i =\nh u,v d u,b−1 + DFT(N v ) b/2 , \t (8) ˜ Y v,i =\nfor i = b 2 + 1, ..., b − 1, \t (9) ˜ Y v,i = −\nHowever, from the conjugate symmetry of DFT(N v ) (since N v is a real-valued vector), we have that all the received signals from (9) and (10) are repetitions (up to a change of sign) of the received signals in (6) and (7). Therefore, we conclude that we have effectively b distinct real-valued received signals with additive noise. It is important to notice, though, that the additive noise terms are dependent across these b received signals.\nThrough an application of Lindeberg\u2019s Central Limit The- orem [7], the following lemma is proved in [6], showing that the effective additive noises in the received signals (6) and (7) converge in distribution to Gaussian noise terms.\nLemma 1. Let N [0], N [1], N [2], ... be i.i.d. random variables that are zero-mean, have variance σ 2 and have an absolutely continuous distribution, and let\nfor some b ∈ {1, ..., b − 1} \\ {b/2}. Then, Z b converges in distribution to N (0, σ 2 /2) as b → ∞.\nNow consider the additive noise term in (6). It is the real part of (4), which, by Lemma 1, converges in distribution to N (0, σ 2 v /2), as b → ∞. Moreover, Lemma 1 can be restated with sines replacing the cosines, and the same result will hold. Thus, the additive noise in (7) also converges in distribution to N (0, σ 2 v /2). Finally, for the received signals in (5) and (8), it is easy to see that the additive noise in (4) only has a real component, and by the usual Central Limit Theorem, it converges in distribution to N (0, σ 2 v ).\nIn the previous section, we saw that by choosing b suf- ﬁciently large, it is possible to make the effective additive noise at node v arbitrarily close (in the distribution sense) to a zero-mean Gaussian noise with variance σ 2 v /2 or σ 2 v . Notice that, since in (2) we restricted the power used in the network\nuses corresponding to (6) and (7) to P/2, all of our effective channels have the same SNR they would have if the transmit signals had power P and noise variance σ 2 v .\nIn this section, we address the fact that the additive noise at node v in the b effective network uses are dependent of each other. Moreover, we show that our scheme can be implemented without violating the causality of the relays. We consider using the network for a total of bk times, performing the OFDM-like approach from section III-A within each block of b time steps. Then, by interleaving the symbols, it is possible to view the result as b blocks of k network uses. This idea is illustrated in Figure 2. Notice that, within each (interleaved) block of k\nnetwork uses, the additive noises are independent, but they are dependent among distinct blocks.\nSince, from the statement of Theorem 1, the rate tuple R is achievable by coding schemes with ﬁnite reading precision, we may assume that we have a sequence of coding schemes C k = (k, R) with ﬁnite reading precision ρ k , whose error probability when used on the AWGN network is k = P error (C k ) and satisﬁes k → 0 as k → ∞. Now, we consider applying this code over each of the b blocks of length k that we obtained from the interleaving. Notice that, in order to apply code C k on a length-k block other than the ﬁrst or the last one, we will have to divide the output transmit signal of all the nodes by\n2 to satisfy (2), but since the additive noises in these blocks have their variance divided by 2 as well, each node can re-scale its received signal by multiplying it by\n2, and the code performs in the exact same way. It is also important to note that this scheme presents no causality issues. For each i ∈ {1, ..., k}, during times (i − 1)b + 1, ..., ib, a node v transmits the IDFT of the b signals which correspond to the ith signal that node v would transmit in each of the b coding schemes C k that we use. These signals depend on the ﬁrst through (i − 1)th signals from each of the b coding schemes, which were received (after applying the DFT) in the previous i − 1 length-b blocks.\nNow, if b is chosen fairly large, over each (interleaved) block of length k, the noises at all nodes are independent and i.i.d. over time, and are very close to Gaussian in distribution, and, intuitively, the error probability we obtain should be close to k . We let k,b be the largest error probability obtained when we apply C k to one of the b blocks of length k.\nWe let Z b ∈ R k|V | be the random vector associated with the effective additive noises at all nodes in V during this length-k block, assuming that we performed the OFDM-like scheme in blocks of size b. Since each component of Z b is independent and they all converge in distribution to a zero- mean Gaussian random variable, we have that Z b converges in distribution to a Gaussian random vector. We let Z be this limiting distribution, and we know that the component of Z corresponding to node v and time is distributed as N (0, σ 2 v ) (or N (0, σ 2 v /2), depending on the length-k block chosen), for any ∈ {1, ..., k}. Now notice that, if we ﬁx the messages chosen at the sources to be w ∈ |L| i=1 {1, ..., 2 kR i }, then, whether C k makes an error is only a deterministic function of Z b . Therefore, for each w ∈ |L| i=1 {1, ..., 2 kR i }, we can deﬁne an error set A w , corresponding to all realizations of Z b that cause coding scheme C k to make an error. It is important to notice that A w is independent of the actual joint distribution of the noise terms; it only depends on the coding scheme C k . Then we can write\nOur next goal is to show that b,k → k as b → ∞. Recall that a Borel set A ⊆ R m is said to be a µ-continuity set for a probability measure µ on R m , if µ(∂A) = 0, where ∂A is the boundary of A. By the portmanteau Theorem [7], since Z b converges to Z in distribution, we must have lim b→∞ µ b (A) = µ(A) for all µ-continuity sets A, where µ b and µ are the probability measures on R k|V | associated to Z b and Z respectively.\nLemma 2. Suppose we have a coding scheme C = (k, R) with ﬁnite reading precision ρ. Then, for any choice of w ∈\nProof: We ﬁx w and use the fact that C has ﬁnite reading precision ρ to show that the set A w and its complement A c w = R k|V | \\A w can be represented as a countable union of disjoint convex sets, which will then imply the µ-continuity. Recall from Deﬁnition 3 that, in a coding scheme with ﬁnite reading precision ρ, a node v only has access to Y v ρ . Thus, we will call Y v ρ the effective received signal at v. The set\ncan be understood as the set of all possible values of the effective received signals at all nodes in V during a length-k block. It is clear that Y is a countable set for any ﬁnite ρ.\nFor our ﬁxed choice of messages w, the vector y ∈ Y corresponding to the effective received signals at all nodes during the length-k block is a deterministic function of the value of all the noises in the network during the length-k block, z ∈ R k|V | . Therefore, for each y ∈ Y, we deﬁne Q(y) ⊆ R k|V | to be the set of noise realizations z that will result in y being the effective received signals. We claim that Q(y) is a convex set. To see this, consider two noise realizations z, z ∈ Q(y) and ﬁx some α ∈ [0, 1]. We will show that if we replace\none of the components of z with the corresponding component of αz+(1−α)z , the resulting noise realization is still in Q(y). Then, by induction, it will follow that αz+(1−α)z is itself in Q(y). So let us focus on the component corresponding to node v at time . Let y v [ ] ∗ be the noiseless version of the received signal at v at time with its complete binary expansion. Since z and z result in the same y, we have that\nThus, y v [ ] = y v [ ] ∗ + αz v [ ] + (1 − α)z v [ ] ρ , and by re- placing z v [ ] with αz v [ ] + (1 − α)z v [ ], we obtain a noise realization that is still in Q(y), and the claim follows.\nIn [6], it is shown that, for any convex set S, λ(∂S) = 0, where λ is the Lebesgue measure. Moreover, since our mea- sure µ is absolutely continuous, it follows by deﬁnition that λ(S) = 0 ⇒ µ(S) = 0, for any Borel set S. Thus, since λ(∂Q(y)) = 0, we have that µ(∂Q(y)) = 0. This, in turn, clearly implies that\nwhere we use S ◦ to represent the interior of S and S to repre- sent its closure. Next, let Y A w = {y ∈ Y : A w ∩ Q(y) = ∅}. Notice that all noise realizations z ∈ Q(y) will cause all nodes and, in particular, the destination nodes to effectively receive the exact same signals. Therefore, it must be the case that, if A w ∩ Q(y) = ∅, then Q(y) ⊆ A w , which implies that\nMoreover, it is obvious that any noise realization must belong to exactly one set Q(y), and we have\nµ (Q(y) ◦ ) = 1 − µ ∪ y∈Y\\Y Aw Q(y) ◦ ≥ 1 − µ (A c w ) ◦ = µ (A c w ) ◦ c = µ A w ,\nwhere (i) follows from the countability of Y A w and the fact that Q(y 1 ) ∩ Q(y 2 ) = ∅ for y 1 = y 2 , and (ii) follows from (14). We conclude that µ(∂A w ) = µ A w − µ (A ◦ w ) = 0.\nNow it follows from the portmanteau Theorem and Lemma 2 that, for all message choices w, we will have\nwhich implies that b,k → k as b → ∞. Thus, we can apply code C k within each of the b blocks of length k and obtain\na probability of error (within that block) that tends to k as b → ∞. However, since we have a total of b blocks of length k, we make an error if we make an error in any of the b blocks of length k. It turns out that a simple union bound does not work here, since the error probability would be of the form b b,k and we would not be able to guarantee that it tends to 0 as b and k go to inﬁnity. Instead we consider using an outer code for each source-destination pair.\nThe idea is to apply coding scheme C k to each of the b length-k blocks, and then view this as creating a discrete channel for each source-destination pair. More speciﬁcally, for each length-bk block, source s j chooses a symbol (rather than a message) from {1, ..., 2 kR j } b and transmits the b corresponding codewords from C k . Then destination d j will apply the decoder from code C k inside each length-k block and obtain an output symbol also from {1, ..., 2 kR j } b . Notice that, by viewing the input to bk network uses as a single input to this discrete channel, we make sure we have a discrete memoryless channel, and we can use the Channel Coding Theorem. We can view W b j and ˆ W b j as the discrete input and output of the channel between s j and d j . We will then construct a code (whose rate is to be determined) for this discrete memoryless channel between s j and d j by picking each entry uniformly at random from {1, ..., 2 kR j } b . Then, source-destination pair (s j , d j ) can achieve rate\nwhere (i) follows from Fano\u2019s Inequality, since, within each length-k block, we are applying code C k and we have an average error probability of at most b,k .\nThus, by choosing b and k sufﬁciently large, it is possible for each source-destination pair to achieve arbitrarily close to rate R j , and our coding scheme can achieve arbitrarily close to the rate tuple R. This concludes the proof of Theorem 1.\nThe research of A. S. Avestimehr and I. Shomorony was supported in part by NSF Career Award 0953117, NSF Grant CCF-1144000, and AFOSR YIP award FA9550-11-1-0064."},"refs":[{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thomas"}],"title":{"text":"Elements of Information Theory"}},{"authors":[{"name":"A. Lapidoth"}],"title":{"text":"Nearest neighbor decoding for additive non-gaussian noise channels"}},{"authors":[{"name":"R. Etki"},{"name":"D. Ts"},{"name":"H. Wang"}],"title":{"text":"Gaussian interference channel capacity to within one bit"}},{"authors":[{"name":"A. S. Avestimeh"},{"name":"S. Diggav"},{"name":"D. Tse"}],"title":{"text":"Wireless network information ﬂow: A deterministic approach"}},{"authors":[{"name":"S. H. Li"},{"name":"Y. H. Ki"},{"name":"A. El Gama"},{"name":"S. Y. Chung"}],"title":{"text":"Noisy network coding"}},{"authors":[{"name":"I. Shomoron"},{"name":"A. S. Avestimehr"}],"title":{"text":"Worst-case additive noise in wireless networks"}},{"authors":[{"name":"P. Billingsley"}],"title":{"text":"Convergence of Probability Measures"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566473.pdf"},"links":[{"id":"1569566381","weight":5},{"id":"1569566485","weight":5},{"id":"1569564889","weight":5},{"id":"1569565223","weight":5},{"id":"1569566725","weight":5},{"id":"1569565663","weight":11},{"id":"1569565377","weight":5},{"id":"1569566385","weight":11},{"id":"1569565867","weight":5},{"id":"1569565067","weight":11},{"id":"1569559665","weight":5},{"id":"1569566875","weight":5},{"id":"1569559259","weight":5},{"id":"1569565355","weight":11},{"id":"1569566765","weight":11},{"id":"1569565461","weight":5},{"id":"1569564227","weight":5},{"id":"1569566303","weight":5},{"id":"1569564233","weight":5},{"id":"1569566319","weight":5},{"id":"1569566941","weight":11},{"id":"1569558459","weight":5},{"id":"1569565291","weight":5},{"id":"1569564203","weight":11},{"id":"1569565771","weight":5},{"id":"1569560613","weight":5},{"id":"1569566999","weight":5},{"id":"1569566843","weight":5},{"id":"1569558483","weight":5},{"id":"1569556091","weight":5},{"id":"1569565455","weight":16},{"id":"1569566963","weight":5},{"id":"1569566709","weight":16},{"id":"1569564989","weight":5},{"id":"1569565953","weight":5},{"id":"1569564189","weight":5},{"id":"1569564647","weight":5},{"id":"1569563981","weight":5},{"id":"1569566905","weight":5},{"id":"1569566733","weight":5},{"id":"1569555999","weight":5},{"id":"1569559995","weight":5},{"id":"1569565213","weight":5},{"id":"1569566511","weight":16},{"id":"1569565841","weight":5},{"id":"1569566531","weight":11},{"id":"1569561143","weight":5},{"id":"1569565833","weight":11},{"id":"1569564611","weight":11},{"id":"1569566325","weight":11},{"id":"1569566811","weight":11},{"id":"1569553909","weight":5},{"id":"1569566687","weight":5},{"id":"1569566403","weight":22},{"id":"1569566231","weight":5},{"id":"1569566649","weight":5},{"id":"1569558985","weight":5},{"id":"1569566629","weight":5},{"id":"1569565033","weight":5},{"id":"1569563897","weight":16},{"id":"1569565929","weight":5},{"id":"1569566721","weight":5},{"id":"1569555879","weight":16},{"id":"1569565219","weight":5},{"id":"1569558509","weight":5},{"id":"1569556671","weight":5},{"id":"1569566223","weight":5},{"id":"1569566553","weight":11},{"id":"1569564969","weight":5},{"id":"1569565029","weight":5},{"id":"1569565357","weight":5},{"id":"1569562207","weight":5},{"id":"1569566191","weight":11},{"id":"1569565527","weight":5},{"id":"1569567029","weight":5},{"id":"1569566695","weight":5},{"id":"1569566051","weight":5},{"id":"1569565909","weight":5},{"id":"1569555787","weight":5},{"id":"1569565467","weight":5},{"id":"1569566667","weight":5},{"id":"1569566407","weight":11},{"id":"1569560349","weight":5},{"id":"1569565741","weight":5},{"id":"1569566481","weight":22},{"id":"1569566387","weight":5},{"id":"1569560503","weight":5},{"id":"1569566229","weight":5},{"id":"1569566133","weight":5},{"id":"1569563395","weight":5},{"id":"1569551347","weight":5},{"id":"1569566383","weight":11},{"id":"1569565885","weight":5},{"id":"1569565549","weight":5},{"id":"1569565611","weight":16},{"id":"1569566983","weight":5},{"id":"1569566097","weight":5},{"id":"1569566479","weight":5},{"id":"1569565397","weight":5},{"id":"1569565765","weight":5},{"id":"1569565435","weight":5},{"id":"1569566129","weight":16},{"id":"1569564131","weight":5},{"id":"1569566253","weight":5},{"id":"1569566691","weight":11},{"id":"1569566823","weight":5},{"id":"1569566137","weight":11},{"id":"1569565013","weight":5},{"id":"1569566237","weight":5},{"id":"1569565375","weight":5},{"id":"1569566713","weight":11},{"id":"1569566771","weight":5},{"id":"1569566641","weight":11},{"id":"1569564247","weight":11},{"id":"1569551905","weight":5},{"id":"1569556759","weight":16},{"id":"1569561185","weight":11},{"id":"1569566301","weight":5},{"id":"1569565669","weight":16},{"id":"1569563721","weight":5},{"id":"1569560235","weight":11},{"id":"1569566817","weight":5},{"id":"1569564157","weight":5},{"id":"1569566389","weight":5},{"id":"1569566911","weight":11},{"id":"1569564923","weight":5},{"id":"1569566299","weight":11},{"id":"1569564769","weight":5},{"id":"1569566933","weight":5},{"id":"1569563919","weight":5},{"id":"1569557851","weight":5},{"id":"1569565389","weight":5},{"id":"1569565537","weight":5},{"id":"1569567013","weight":22},{"id":"1569560459","weight":5},{"id":"1569565853","weight":5},{"id":"1569564505","weight":5},{"id":"1569565165","weight":5},{"id":"1569564141","weight":5},{"id":"1569566987","weight":11},{"id":"1569566663","weight":11},{"id":"1569564419","weight":16},{"id":"1569566241","weight":11},{"id":"1569566609","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S2.T3.1","endtime":"11:50","authors":"Ilan Shomorony, Salman Avestimehr","date":"1341228600000","papertitle":"Is Gaussian Noise the Worst-Case Additive Noise in Wireless Networks?","starttime":"11:30","session":"S2.T3: Multi-Hop Multi-Flow Wireless Networks","room":"Stratton S. de P. Rico (202)","paperid":"1569566473"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
