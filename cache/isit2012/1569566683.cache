{"id":"1569566683","paper":{"title":{"text":"An Algebraic Framework for Concatenated Linear Block Codes in Side Information Based Problems"},"authors":[{"name":"Felipe Cinelli Barbosa ∗"},{"name":"J¨org Kliewer \u2020"},{"name":"Max H. M. Costa ∗"}],"abstr":{"text":"Abstract\u2014This work provides an algebraic framework for source coding with decoder side information and its dual prob- lem, channel coding with encoder side information, showing that nested concatenated codes can achieve the corresponding rate-distortion and capacity-noise bounds. We show that code concatenation preserves the nested properties of codes and that only one of the concatenated codes needs to be nested, which opens up a wide range of possible new code combinations for these side information based problems. In particular, the practically important binary version of these problems can be addressed by concatenating binary inner and non-binary outer linear codes. By observing that list decoding with folded Reed- Solomon codes is asymptotically optimal for encoding IID q-ary sources and that in concatenation with inner binary codes it can asymptotically achieve the rate-distortion bound for a Bernoulli symmetric source, we illustrate our ﬁndings with a new algebraic construction which comprises concatenated nested cyclic codes and binary linear block codes."},"body":{"text":"Two traditional problems in the ﬁeld of communications are the Wyner-Ziv coding problem [1] and its dual version, the Gel\u2019fand-Pinsker problem [2], [3]. The ﬁrst is an instance of distributed source coding: one source is encoded by taking ad- vantage of the fact that the decoder receives another correlated source as side information. In contrast, the Gel\u2019fand-Pinsker problem is a channel coding problem in which a channel encoder embeds messages by using available channel state information as side information [3]. We will refer to these problems also as source coding with decoder side information (SCSI) for the Wyner-Ziv case, and as channel coding with encoder side information (CCSI) for the Gel\u2019fand-Pinsker problem in the following, respectively.\nThe duality of these problems has been studied in [4] for the Gaussian case, where the authors also analyze how this relationship can be exploited to design dual coset codes. While nested lattice based coset coding schemes for these problems have been proposed for continuous-input (Gaussian) channels [5], in the following we focus on the binary version of these problems, as this is beneﬁcial in many applications which cope with binary data and communication channels, as for example in digital watermarking for the case of CCSI and in distributed video coding for SCSI.\nIn [6] superposition coding was proposed for the binary CCSI case for which random codes and maximum-likelihood (ML) decoding is able to achieve capacity. Differently from superposition coding, nested codes have been used for the bi- nary SCSI case, and a technique based on nested parity check codes has been proposed in [7] which asymptotically achieves the rate-distortion bound for a Bernoulli symmetric source. Recently, in [8] the authors present compound LDPC/LDGM constructions for both problems which asymptotically achieve capacity for the CCSI problem and the rate distortion bound for the SCSI problem with bounded graphical complexity under ML decoding. They show that these compound codes essentially have a joint source-channel coding interpretation. Further, polar codes have been shown to be asymptotically optimal for both problems with bounded decoding complexity [9]. However, their performance for practical block lengths is worse than for other codes of the same length [10]. Finally, other coding schemes for both SCSI and CCSI based on com- mon modulation and coding schemes, as trellis coded quanti- zation/modulation and turbo codes have been presented (see, e.g., [6], [11], [12]).\nThe novel contribution of this paper is an algebraic frame- work which extends the above results for the binary SCSI and CCSI cases to concatenated nested linear block codes. In par- ticular, we show that by concatenating two linear block codes new binary constructions can be obtained which preserve the nested structure either of the outer or of the inner code. This opens up a wide range of possible new code combinations and indicates that code concatenation can alleviate the search for both practical and optimal constructions. We analyze code concatenations for q m -ary outer codes and q-ary inner codes as a binary inner code can be simply obtained by q = 2.\nRecent work by Guruswami and Rudra [13] gives an explicit construction of folded RS (FRS) codes that can achieve list decoding capacity. We show this result implies that if RS codes are used as source codes, the rate-distortion bound is achieved for IID q-ary sources. Together with the fact that concatenated binary codes using outer FRS codes can achieve list decoding capacity for concatenated codes [13], it motivates the use of nested RS codes as outer codes in combination with list decoding for both SCSI and CCSI problems. Finally, based on our ﬁndings we exemplarily present an algebraic concatenated nested coding scheme that asymptotically achieves the rate- distortion and capacity-rate bounds with low encoding and\nThese codes were ﬁrst proposed in [14] under the name of partitioned cyclic codes and can be generally deﬁned as follows.\nDeﬁnition 1 (Nested Linear Block Code). A nested linear block code C is deﬁned such that (i) C ⊂ F N 2 , (ii) C = C 1 + C 2 , (iii) C 1 ∩ C 2 = {0}, where C 1 and C 2 are subcodes.\nIt has been shown in [5], [6], [8] that nested codes are able to achieve the rate-distortion bound for the SCSI problem and symmetric Bernoulli sources and the capacity-noise bound for the CCSI problem and binary symmetric channels (BSCs) as communication channels, respectively. In the following, we revise these results and the use of nested linear block codes in these problems, where we focus on the binary case.\nFor this problem, we consider a BSC with noise vector Z ∼ Bern(p) (BSC(p)) and interference S, representing the channel state, which is uniformly distributed over F N 2 and known a priori at the encoder. The channel output is given by\nwhere E is the transmitted codeword under the input constraint 1\nFor encoding, we assume that without loss of generality subcode C 1 carries the information which is transmitted in K 1 dimensions of the N -dimensional vector space F N 2 . If K = dim( C) according to property (ii) in Deﬁnition 1 we have that K 2 = dim( C 2 ) where K = K 1 + K 2 . Note that a nested parity-check code is simply a dual code of the nested generator code C G (N, K, R) = C 1 (N −K 2 , K 1 , R)+ C 2 (N, K 2 , R+K 1 )\nFor a given information vector encoded in C 1 , there are 2 K 2 possible vectors in C 2 . The encoder now has the task to ﬁnd a vector c 2 in C 2 such that\nS = c 1 + c 2 + E, \t (3) with c 1 ∈ C 1 , such that E satisﬁes the constraint in (2). Otherwise, an encoder error is declared. From (1) we obtain the received vector as\nY = c 1 + c 2 + Z. \t (4) Lemma 1 ([8]). The error probability in recovering c 1 + c 2 from Y approaches zero with increasing N under the con- straint (2) for the transmitted codeword E if the maximal message rate is given as\nK 1 /N = h(W ) − h(p) − ϵ. \t (5) Note that (5) approaches the rates h(W ) − h(p) of\nthe capacity-noise bound R GP (W, p) = u.c.e. {h(W ) − h(p), (0, 0) } where \u201cu.c.e.\u201d denotes the upper convex enve- lope. All other rates on the curve R GP (W, p) can be obtained by time sharing with the point (0, 0).\nThis problem addresses the compression of a symmetric source W ∼ Bern( 1 2 ) by exploiting the knowledge of another correlated source Y as side information at the decoder. The correlation between sources can be represented as W = Y ⊕S where S ∼ Bern(p) is a \u201cseparation\u201d vector corresponding to errors on a virtual BSC(p) modeling the correlation. For the estimate of the source sequence ˆ W we require a constraint on the maximal distortion D, given as\n1 N\nThe encoder receives a sequence of N bits from source W , represented by W. It can be interpreted as\nwhere c ∈ C. We also require 1 N w H (E) ≤ D due to (6), such that the stored version of W is given as ˆ W = c, otherwise an encoder error is declared. We again assume that information is conveyed in K 1 dimensions of the N -dimensional vector space F N 2 , corresponding to code C 1 . Thus, the resulting compression rate is K 1 /N .\nAt the decoder, the encoded information of length K 1 can be recovered as a codeword in C 1 of length N . Because the decoder has access to side information Y it can recover c 2 according to\nThe decoder can then reconstruct ˆ W by considering that c = c 1 + c 2 .\nLemma 2 ([8]). The overall compression rate of the scheme under the distortion constraint in (6) is given as\nfor any ϵ > 0, where p ∗ D = p(1 − D) + D(1 − p) represents binary convolution.\nThe rate K 1 /N in (9) approaches the rate h(p ∗ D) − h(D) of the rate-distortion bound R W Z (D, p) = l.c.e. {h(p ∗ D) − h(D), (p, 0) }, where \u201cl.c.e.\u201d denotes the lower convex enve- lope. All other rates on the curve R W Z (D, p) can be obtained by time sharing with the point (p, 0).\nThe results presented in Section II indicate that nested linear block codes can asymptotically achieve the limits for both SCSI and CCSI problems but does not address how practical capacity-approaching codes for these cases can be obtained. However, the asymptotically capacity-achieving results for compound LDGM/LDPC codes in [8] suggests that code concatenation may result in practical and efﬁcient codes for these applications.\nIn this section we provide an new algebraic framework for nested concatenated codes for which the constructions in [8]\ncan be seen as special cases. In particular, we formally prove that code concatenation preserves the nested code structure, where the inner code serves as translator to a q-ary ﬁeld in such way that the outer code operates in the corresponding q m - ary extension ﬁeld. This especially also covers the practically important binary case for q = 2.\nDeﬁnition 2. Let ϕ : F n Q → F nm q , with Q = q m , be a bijective linear map deﬁned as ϕ(v) = u, where v ∈ F n Q and u ∈ F nm q . This means that a sequence of length n in F Q can be expressed as a q-ary sequence of length nm. If m = 1 we have u = v and there is no mapping.\nDeﬁnition 3. Let u = (u 1 , ..., u l ) , u i ∈ F nm/l q \t , i = 1, ..., l, where 1 ≤ l ≤ n and l is a divisor of mn. Further, let C Ψ (N/l, nm/l, d Ψ ) be a q-ary linear block code. Then, ψ : F nm/l q \t → C Ψ is a bijective linear map such that ψ(u i ) = u i G Ψ , where G Ψ is a generator matrix for C Ψ .\nThis deﬁnition means that the sequence u is partitioned into l groups of nm/l q-ary symbols that are each encoded by C Ψ . Note that this partition corresponds to an (nm/l)-folded code over F nm/l q \t . If l = n, then the groups have length m, and are the q-ary representation of a Q-ary symbol. If l = 1, the entire q-ary sequence u is encoded as a single input message by C Ψ .\nDeﬁnition 4. We deﬁne the extended one-to-one linear map ψ ∗ : F nm q \t → F N q as ψ ∗ (u) \t (ψ(u 1 ), . . . , ψ(u l )) = (u 1 G Ψ , . . . , u l G Ψ ).\nLemma 3. Let v be a codeword of the nested linear block code C(n, k, d) over F Q with Q = q m and C = C 1 + C 2 . The concatenation between C and C Ψ (N/l, nm/l, d Ψ ) yields an equivalent code C eq (N, K, D) over F q according to C eq (N, K, D) = {ψ ∗ (ϕ(v)) } v ∈C where K = km and D ≥ d Ψ d.\nThe proof follows in a straightforward way from sequential concatenation [15]. Note that ϕ(v) is a q-ary codeword of C q (nm, km, d q ), which is the q-ary version of C in the underlying ﬁeld F q .\nThe following proposition represents the main result of this section and states that the nested property as stated in Deﬁni- tion 1 is preserved if the outer code is a nested q-ary linear block code and the inner code is a Q-ary linear block code.\nProposition 1. The concatenation between C and C Ψ produces codewords of an equivalent linear code C eq (N, K) over F q , such that\nProof: 1) According to Lemma 3 we have C eq = {ψ ∗ (ϕ(v)) } v ∈C . Since C = C 1 + C 2 , then C eq = {ψ ∗ (ϕ(v 1 + v 2 )) } v 1 ∈C 1 ;v 2 ∈C 2 . But as ψ ∗ and ϕ are a linear maps and both C 1 and C 2 are also subspaces over the ground ﬁeld F q , as F n Q is equivalent to F nm q , the additivity property of linear mappings yields {ψ ∗ (ϕ(v 1 + v 2 )) } v 1 ∈C 1 ;v 2 ∈C 2 = {ψ ∗ (ϕ(v 1 )) } v 1 ∈C 1\nψ(u) = 0 }, where C q (nm, km) is the equivalent code C in the underlying ﬁeld F q . As ψ ∗ is one-to-one, ker(ψ ∗ ) = {0}, i.e., u = 0. Equivalently, ker(ϕ) = {0}, which means that v = 0. But since C 1 ∩ C 2 = {0}, then v = 0 if and only if v 1 = v 2 = 0. In other words, ψ ∗ (ϕ( C 1 ∩ C 2 = {0})) = {ψ ∗ (ϕ(v 1 )) } v 1 ∈C 1 ∩ {ψ ∗ (ϕ(v 2 )) } v 2 ∈C 2 = {0}.\nBy using a similar approach, it follows from Proposition 1 that its converse also holds, i.e., that the nested property is preserved if the inner code is a nested q-ary linear block code and the outer code is a Q-ary linear block code.\nProposition 2. The concatenation between a linear code C(n, k) over F Q and a linear code C Ψ over F q , C Ψ = C Ψ1 + C Ψ2 and C Ψ1 ∩ C Ψ2 = {0}, produces codewords of an equivalent linear code C eq (N, K) over F q , such that\nwhere ψ ∗ 1 : F nm q → F N q and ψ ∗ 2 : F nm q → F N q are linear maps such that ψ ∗ j (u 1 , . . . , u n ) = (ψ j (u 1 ), . . . , ψ j (u n )), with u i ∈ F nm/l q \t , i = 1, . . . , n, and ψ j : F nm/l q \t → C Ψj , j = 1, 2.\nPropositions 1 and 2 show that properties (i) and (ii) in Deﬁnition 1 still hold after code concatenation, no matter whether the nested code is an inner or outer code. The only requirement is that both subcodes are concatenated with the same outer code, in order to preserve property (ii). We deﬁne the subcodes in the resulting q-ary nested structure of C eq as C eq1 = {ψ ∗ (ϕ(v 1 )) } v 1 ∈C 1 and C eq2 = {ψ ∗ (ϕ(v 2 )) } v 2 ∈C 2 which now instead can be employed in both the SCSI and the CCSI cases.\nWhen employing nested codes to the SCSI problem as in Section II, C eq2 is required to be a good channel code to correct the error formed by the source encoding distortion and the observation error. At the same time, C eq must be a D-distortion source code to output a codeword as close as possible to the information sequence produced by the source with a distortion constraint D. In the case of CCSI, C eq2 takes on the role of a good W -distortion source code whereas C eq is the channel code.\nWhile channel coding can be performed by means of good decoding algorithms, performing source coding with error correcting codes makes it necessary to have complete algorithms that can return the nearest codewords. Motivated by recent results on list decoding of RS codes we will now study the suitability of these codes for source encoding.\nIn [13], Guruswami and Rudra describe an explicit family of codes with a list decoding algorithm that can asymptotically achieve the information-theoretic limit of list decodability, with encoding and decoding performed in polynomial time. The proposed codes are folded RS codes, which can be seen as standard RS codes viewed as codes over a larger alphabet.\nDeﬁnition 5 (ν-Folded Reed-Solomon Code (FRS)). Let α ∈ F q be a primitive element of F q . Let n \u2032 ≤ q − 1 be a multiple\nof ν and 1 ≤ k < n. An FRS code C (ν) (n \u2032 , k) over alphabet F ν q is a folded version of the RS code C(n, k) over F q and is deﬁned as\nwhere n \u2032 = n/ν. In other words, a FRS code is an RS code where ν consecutive symbols each are grouped together.\nThe GR algorithm for FRS codes of rate R allows to list decode in polynomial time up to a fraction of (1 − R − ε) worst-case errors. The folding operation does not change the rate of the RS code (R = k/n = k/n \u2032 ν), thus e \u2032 ν/n = 1 − k/n − ε, so e = e \u2032 ν is the number of correctable errors for the corresponding unfolded RS code [13].\nProposition 3. If the GR list decoding algorithm is used in conjunction with RS codes for source encoding of IID q- ary sources, the probability of encoding errors asymptotically vanishes.\nProof: Starting from an observation that the normalized covering radius t( C)/n of a linear code C(n, k) [16] is t( C)/n ≤ 1 − k/n, which is met with equality by RS codes, we see that e = t( C)−ε, where ε > 0, C(n, k) is the unfolded RS code and t( C) is its covering radius. Because a fraction of errors over F ν q is equivalent to a fraction of errors over F q , the GR algorithm for FRS codes asymptotically corrects a number of errors over F q that is equal to the covering radius of the corresponding unfolded RS code.\nNote that list decoding may not output a single codeword but a list of possible codewords. This does not pose a problem since the source encoder can always pick the one which is closest to the source sequence in Hamming distance.\nUsing concatenated codes for both the CCSI and the SCSI problems, there are two different ways of implementing the source encoding step of ﬁnding a vector c 2 ∈ C eq2 and c ∈ C eq , respectively. The ﬁrst way is to perform separate source encoding for each of the concatenated codes. While RS outer codes in conjunction with the GR algorithm can optimally perform source encoding in F n q , the performance of this strategy also depends on the inner code. Another way is to perform source encoding over the concatenated binary code. In fact, list decoding capacity for binary codes can be asymp- totically achieved if FRS codes are concatenated with random binary linear block codes (BLBC) [13]. This means that every Hamming sphere of radius h −1 (1 − R − ε) has polynomially many codewords. Thus, it is possible to asymptotically achieve the rate-distortion bound for a Bernoulli symmetric source.\nIn the following we provide a general setup which univer- sally addresses the scenario of outer algebraic RS or BCH codes and arbitrary inner BLBCs.\nThe coding scheme for the outer code is based on an algebraic construction of nested cyclic codes. These codes form an ideal in the polynomial ring F q [x]/(x n − 1), where F q [x] is the set of polynomials in x with coefﬁcients from the\nﬁnite ﬁeld F q , where q = 2 m . The polynomial x n − 1 can be factorized as\nwhere r = n − (k 1 + k 2 ). The notation \u201ca(x) ( ·) \u201d is used to indicate the degree of polynomial \u201ca(x)\u201d, and henceforth the argument \u201c(x)\u201d will be omitted in order to simplify notation. Note that for m = 1 we obtain BCH codes, otherwise RS codes are employed.\nThe polynomial g (r) corresponds to a generator polynomial of the code C(n, k 1 + k 2 ), and (gf ) (r+k 1 ) is a generator poly- nomial for C 2 (n, k 2 ). The codewords in C can be expressed as a sum of codewords as follows:\nAt the outer encoder, information i (k 1 −1) 1 \t is encoded using g (r) of code C(n, k 1 + k 2 ), producing v (n −1) 1 \t (zero padded to achieve length n) of the shortened cyclic code C 1 (n − k 2 , k 1 ). In order to allow incorporating binary side information, the sequence v 1 ∈ C 1 is mapped to its binary representation u 1 , which is then partitioned into l groups of nm/l bits that are each encoded by a BLBC code C Ψ (N/l, nm/l). Thus, the resulting codewords c 1 = ψ ∗ (ϕ(v 1 )) have length N and are codewords in C eq1 (N − K 2 , K 1 ).\nEncoding steps: I) (Outer encoding): Encode information i (k 1 −1) 1 \t using the generator g (r) for C(n, k 1 +k 2 ), thus produc- ing a codeword v (n −k 2 −1) 1 \t of C 1 (n − k 2 , k 1 ) padded with k 2 zeros; II) (Code concatenation): Encode l groups of nm/l bits of codeword v 1 (received from the outer encoder) by using the inner code C Ψ , resulting in c 1 ; III) Compute S − c 1 ; IV) Find c 2 ∈ C eq2 according to (3) such that (2) holds; V) Transmit E.\nNote that the encoding complexity is essentially given by step IV, because all other operations are linear. For FRS codes a folding/unfolding step has to be performed before ﬁnding v 2 ∈ C 2 (n, k 2 ) as follows.\nFolding/unfolding step: (i) Code folding σ : F nm 2 → F n/ν 2 mν , σ(u) = u \u2032 , (ii) Code unfolding σ ∗ : F n/ν 2 mν →F n 2 m , σ ∗ (v \u2032 2 ) = v 2 .\nProposition 4. Consider a symmetric Bernoulli source. Source encoding via list decoding of RS/BLBC code C eq2 can asymp- totically achieve a vanishing probability of encoding error. Thus, given a concatenated RS/BLBC channel code C eq which asymptotically achieves capacity on the BSC(p), the resulting joint source-channel coding scheme for the CCSI case achieves the capacity-noise bound R GP (W, p).\nProof: From the rate distortion bound for a symmetric Bernoulli sequence, the rate for the W -distortion source code C eq2 is given as K 2 /N ≥ 1−h(W ). Because list decoding can asymptotically correct an error fraction of h −1 (1 − R 2 − ε), we see that R 2 = 1 − h(W ) − ε asymptotically achieves the rate-distortion bound and therefore results in an encoding error probability which asymptotically tends to zero. Therefore, if the RS/BLBC code C eq achieves capacity on the BSC(p), we\nhave R = 1 − h(p) − ε which results in R 1 = h(W ) − h(p). This is equivalent to the capacity-noise bound R GP (W, p).\nThe channel coding performance of the proposed scheme is essentially the one for the chosen concatenated RS/BLBC code C eq2 . Here we can exploit the fact that some constructions (e.g., RS/LDPC) are capacity approaching, for which effective decoding algorithms exist.\nAfter transmission of E, the decoder receives Y in (1) and the error vector Z is corrected in the same fashion as in any standard concatenated scheme (by using the corresponding decoding algorithms for each code), resulting in an error-free codeword (12). Then, the embedded information i 1 is extracted by a modulo operation and a polynomial division according to\nDecoding steps: I) Receive Y, recover v ∈ C; II) Compute i (k 1 −1) 1 \t as in (13).\nThe encoder receives a sequence of N bits from a Bernoulli symmetric source W , represented by W, which is equivalent to a codeword ˆ W = c in C eq (N, K) plus a \u201cquantization\u201d error E (7). An encoder error is declared if a codeword ˆ W = c cannot be found.\nEncoding steps: I) Receive W, recover v ∈ C; II) Com- pute: i (k 1 −1) 1 \t as in (13).\nAnalogous to the CCSI case, for FRS codes an extra folding/unfolding step must be performed before ﬁnding v ∈ C(n, k), with the difference that now the folded codeword is v \u2032 ∈ C (ν) (n \u2032 , k), so σ ∗ (v \u2032 ) = v. We have the following statement which is analogous to Proposition 4.\nProposition 5. Consider a symmetric Bernoulli source. Source encoding via list decoding of RS/BLBC code C eq can asymp- totically achieve a vanishing probability of encoding error. Thus, given a concatenated RS/BLBC channel code C eq2 which asymptotically achieves capacity on the BSC(p ∗ D), the resulting joint source-channel coding scheme for the SCSI case achieves the rate-distortion bound R GP (W, p).\nThe encoder extracts a polynomial i (k 1 −1) 1 \t of length k 1 (K 1 bits) from v (n −1) (12), so the compression rate is K 1 /N . The encoding steps in this case are essentially the same as the decoding steps of CCSI, but instead of channel decoding we employ source encoding algorithms which dominate the encoding complexity (see encoder step I).\nFor decoding, the steps are analogous to the encoding steps of the CCSI case, with the difference that S becomes Y, and instead of sending the error pattern after ﬁnding c 2 ∈ C eq2 , the information word corresponding to the actual codeword\nˆ W = c 1 + c 2 is stored. Here, channel decoding is employed which dominates the complexity as all other operations are linear.\nWithin the proposed algebraic framework we proved that code concatenation preserves the nested structure of joint\nsource-channel codes. Therefore, the optimal asymptotic per- formance for both binary SCSI and CCSI problems can be universally achieved by concatenation with a linear block code, provided that one of the constituent codes has the necessary nested property. In particular, while in [8] ML decoding is assumed, through a novel RS/BLBC construction with low encoding and decoding complexity we show that list decoding provides the optimal source encoding performance asymp- totically for both problems. At the same time, for channel error correction any capacity-approaching algorithm can be independently used.\nIt is still a challenge to exploit the full potential of con- catenation with practical list decoding algorithms, but separate source and channel encoding is a feasible approach as practical encoding and decoding algorithms exist for each code. Future work will focus on studying other concatenated schemes employing QC-LDPC, polar, and BCH codes as outer codes, which seems to be a promising avenue since these codes have been successfully employed for source coding [9], [15], [17]."},"refs":[{"authors":[{"name":"A. D. Wyner"},{"name":"J. Ziv"}],"title":{"text":"The rate-distortion function for source encoding with side information at the encoder"}},{"authors":[{"name":"S. I. Gel\u2019fand"},{"name":"M. S. Pinsker"}],"title":{"text":"Coding for channel with random parameters"}},{"authors":[{"name":"M. H. M. Costa"}],"title":{"text":"Writing on dirty paper"}},{"authors":[{"name":"S. Pradhan"},{"name":"J. Chou"},{"name":"K. Ramchandran"}],"title":{"text":"Duality between source coding and channel coding and its extension to the side information case"}},{"authors":[{"name":"R. Zamir"},{"name":"S. Shamai"},{"name":"U. Erez"}],"title":{"text":"Nested linear/lattice codes for structured multiterminal binning"}},{"authors":[{"name":"A. Bennatan"},{"name":"D. Burshtein"},{"name":"G. Caire"},{"name":"S. Shamai"}],"title":{"text":"Superposition coding for side-information channels"}},{"authors":[{"name":"S. Shamai"},{"name":"S. Verd´u"},{"name":"R. Zamir"}],"title":{"text":"Systematic lossy source/channel coding"}},{"authors":[{"name":"M. J. Wainwright"},{"name":"E. Martinian"}],"title":{"text":"Low-density codes that are optimal for binning and coding with side information"}},{"authors":[{"name":"S. Korada"},{"name":"R. Urbanke"}],"title":{"text":"Polar codes are optimal for lossy source coding"}},{"authors":[{"name":"N. Hussami"},{"name":"R. Urbanke"},{"name":"S. Korada"}],"title":{"text":"Performance of polar codes for channel and source coding"}},{"authors":[{"name":"A. Liveris"},{"name":"Z. Xiong"},{"name":"C. Georghiades"}],"title":{"text":"Nested convolutional/turbo codes for the binary Wyner-Ziv problem"}},{"authors":[{"name":"Y. Sun"},{"name":"M. Uppal"},{"name":"A. D. Liveris"},{"name":"S. Cheng"},{"name":"V. Stankovic"},{"name":"Z. Xiong"}],"title":{"text":"Nested turbo codes for the Costa problem"}},{"authors":[{"name":"V. Guruswami"},{"name":"A. Rudra"}],"title":{"text":"Explicit codes achieving list decoding capacity: Error-correction with optimal redundancy"}},{"authors":[{"name":"C. Heegard"}],"title":{"text":"Partitioned linear block codes for computer memory with \u201dstuck-at\u201d defects"}},{"authors":[{"name":"R. J. McEliec"}],"title":{"text":"The theory of information and coding"}},{"authors":[{"name":"S. L. G. Cohe"},{"name":"I. Honkal"},{"name":"A. Lobstei"}],"title":{"text":"Covering Codes"}},{"authors":[{"name":"Y. Matsunaga"},{"name":"H. Yamamoto"}],"title":{"text":"A coding theorem for lossy data compression by LDPC codes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566683.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T5.3","endtime":"10:50","authors":"Felipe Cinelli Barbosa, Joerg Kliewer, Max H. M. Costa","date":"1341397800000","papertitle":"An Algebraic Framework for Concatenated Linear Block Codes in Side Information Based Problems","starttime":"10:30","session":"S9.T5: Linear Codes and UEP Codes","room":"Kresge Little Theatre (035)","paperid":"1569566683"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
