{"id":"1569564233","paper":{"title":{"text":"The Porosity of Additive Noise Sequences"},"authors":[{"name":"Vinith Misra"},{"name":"Tsachy Weissman"}],"abstr":{"text":"Abstract\u2014Consider a binary modulo-additive noise channel with noiseless feedback. When the noise is a stationary and ergodic process Z, the capacity is 1 − H(Z) (H(·) denoting the entropy rate). It is shown analogously that when the noise is a deterministic sequence z ∞ , the capacity under ﬁnite-state encoding and decoding is 1 − ρ(z ∞ ), where ρ(·) is Lempel and Ziv\u2019s ﬁnite-state compressibility. This quantity is termed the porosity σ(·) of an individual noise sequence. A sequence of schemes are presented that universally achieve porosity for any noise sequence. These results may be interpreted both as a channel-coding counterpart to Ziv and Lempel\u2019s work in universal source coding, as well as an extension of the work by Lomnitz and Feder and Shayevitz and Feder on communication across modulo-additive channels."},"body":{"text":"Traditionally, universal source coding has been subject to more attention than universal channel coding. The reason for this discrepancy is apparent from Fig. 1. While a source encoder may adjust its encoding to better suit the unknown source, the channel encoder of Fig. 1 has no observation of the unknown channel and is therefore unable to adapt. One may ask for an adaptive decoder, but this leads to a much weaker kind of universality (see, for instance, [1]\u2013[3]).\nHowever, if a noiseless feedback link is added to the system (Fig. 2), both encoder and decoder may adjust to the channel. This permits a level of universality more comparable with that of universal source coding. To draw an even stronger parallel, consider the modulo-additive channel of Fig. 3: one may compare the uncharacterized source of universal compression to the uncharacterized noise of this setting.\nZiv and Lempel\u2019s seminal work [4], [5] can be considered the canonical treatment of universality in source coding. We ﬁnd that an analogous set of questions yields an analogous set of answers for the modulo-additive channel with feedback:\n\u2022 Lempel and Ziv take universality to its logical extreme by allowing the source to be any individual sequence. Here the noise z ∞ is permitted to be any individual sequence.\n\u2022 For a given source sequence x ∞ , Lempel and Ziv show that no ﬁnite-state encoder/decoder can achieve a com- pression rate smaller than the compressibility of x ∞ . Analogously, the converse results here show that no ﬁnite state communication scheme can exceed the noise sequence\u2019s porosity.\n\u2022 Lempel and Ziv construct ﬁnite-state compression schemes that achieve the compressibility for any source sequence. The ﬁnite-state communication schemes {F m } (Sec. IV) serve an identical function here.\nThe notion of competitive universality in channel coding is introduced by Lomnitz and Feder in [6]. The reference class consists of iterated ﬁxed-blocklength (IFB) schemes, which ignore the feedback channel and simply employ block coding across the noisy channel. Rate-adaptive schemes, on the other hand, communicate a ﬁxed number of bits over at most n channel uses. It is proven that IFB schemes can do no better than porosity (rate 1 − ρ(z ∞ )), and a rate-adaptive scheme built upon LZ78 is shown to achieve porosity.\nIn a sense, the results reported here take these statements of competitive optimality a step further: we ask that the achievability schemes not only outperform any elements of the reference class, but that they are elements of the reference class. As IFB codes frequently cannot even achieve porosity for a given noise sequence z ∞ , this requires that the reference class be widened to the class of all ﬁnite-state schemes.\nIn [7], Shayevitz and Feder establish the original results that have sparked much of the subsequent work in this problem. They construct variable-rate, ﬁxed-blocklength schemes, and then demonstrate that these schemes achieve for any noise sequence the empirical capacity, or one minus the ﬁrst- order empirical entropy. We ﬁnd that the universal porosity- achieving schemes {F m } may be constructed from these empirical-capacity-achieving schemes. Note that the achiev- ability structure of [6] is in spirit very similar, but the performance guarantees of Shayevitz and Feder prove more adaptable to the metrics of interest here.\nEswaran et al. [8] extend [7] to demonstrate that even when the feedback is asymptotically zero-rate, the empirical capacity may be still be achieved universally.\nIf the empirical distribution is instead computed in a sliding- window manner, denote\nThe kth order block-by-block empirical entropy is indicated by ˆ H k (x n ) = H ˆ p k (X k ). The sliding-window kth order em- pirical entropy is similarly written as ˆ H k sw (x n ) = H ˆ p k sw (X k ).\nAs shown by Ziv and Lempel [5], the ﬁnite-state compress- ibility of a sequence x ∞ may be written as\nAn analagous quantity may also be introduced: ρ(x ∞ ) = lim inf\nBy Lemma 1 in [9], both expressions may also be written in terms of the block-by-block empirical entropy.\nOperationally, compressibility is the smallest limit supre- mum compression ratio achievable for a sequence (Theorem 3 in [5]). It is not difﬁcult to show, in an analogous manner, that (2) is the smallest possible limit inﬁmum compression ratio. Informed by this, we refer to the original compressibility quan- tity as the worst-case compressibility and the newly introduced limit inﬁmum version as the best-case compressibility.\nThe porosity of a noise sequence z ∞ ∈ Z ∞ is deﬁned in best-case σ(z ∞ ) = log 2 |Z|−ρ(z ∞ ) and worst-case σ(z ∞ ) = log 2 |Z| − ρ(z ∞ ) varieties as well. This name is chosen in analogy with other individual noise sequence properties, such as compressibility and predictability, to reﬂect the passage of information through a noise sequence. In the remainder of this paper, the operational signiﬁcance of porosity is clariﬁed.\nA deterministic additive noise feedback channel, as depicted in Fig. 3, is deﬁned by a noise sequence z ∞ ∈ X ∞ , where X is a ﬁnite alphabet with a modulo addition operator. The channel output at any time i is given by the sum of the noise and the input: y i = x i + z i . Noiseless feedback u i = y i−1\n.. . .. . .. .\ndelays the channel output by one time unit before providing it to the encoder. Without essential loss of generality, we will concern ourselves primarily with the binary-alphabet case, i.e. X = {0, 1}.\nA ﬁnite-state (FS) encoder/decoder scheme for an additive noise channel \u2014 depicted in Fig. 4 \u2014 consists of several components:\n1) An encoder state variable s (e) i and decoder state variable s (d) i , each taking values in a ﬁnite set S.\n3) An iid common randomness source θ i ∼ p θ taking values in a ﬁnite alphabet.\n5) A decoding length function L i = d L (s (d) i , y i , θ i ) that also determines the update of the source pointer: p i+1 = p i + L i .\n7) State-update functions for both the encoder s (e) i+1 \t = f (e) (s (e) i , M p i + p i , y i , θ i ) and decoder\nAt each time step, the encoding function determines the input x i to the channel, the decoding function estimates the ﬁrst L i source symbols that have yet to be estimated (based on the output y i of the channel), and state variables and the source pointer location are updated in anticipation of the next transmission.\nOne may attempt to somewhat generalize class FS by allow- ing for active feedback. The class of ﬁnite-state active feedback (FSAF) schemes accomplishes this: rather than feedback y i−1 , the encoder is provided with the output u i ∈ X of an arbitrary ﬁnite-state feedback channel p(u|y i−1 , s (f) i−1 ). Note that if u i = y i−1 this reduces to class FS.\nIn [9] Lemma 9, it is shown that the class of FSAF schemes is actually equivalent to class FS. The two families may therefore be used interchangeably. For convenience, the converse is proved for class FS, while the achievability scheme given is of class FSAF.\nObserve that FS/FSAF are sufﬁciently general to include the following as special cases:\n1) The class of \u201citerated ﬁxed-length\u201d block schemes, as deﬁned by Lomnitz and Feder [6]. These are simply block codes that ignore the feedback. The common\nrandomness at encoder and decoder allows for randomly generated block codes as well.\n2) Schemes that transmit a variable number of source sym- bols over a ﬁxed number of channel uses, before reseting their state variables and repeating the operation (deﬁned more precisely in Sec. IV as \u201crepetition schemes\u201d).\n3) Schemes that transmit a variable (or ﬁxed) number of source symbols over a variable (but bounded) number of channel uses (including \u201crate-adaptive\u201d schemes [6]).\nSecondly, notice that without certain restrictions in the deﬁnition of class FS, the problem can become trivial:\n1) Suppose that the encoder is permitted to be inﬁnite- state. The system designer may then design the encoder state s (e) i so that the channel input at time i is given by e(i, M p i ) = M p i − z i . The decoder needs merely read the channel output to obtain the message at the maximum possible rate, log |X |.\n2) Suppose that the decoder is permitted to be inﬁnite- state. One may reverse the above construction by having the encoder blindly send the message bits through the channel e(M p i ) = M p i and asking the decoder to cancel out the noise. Once again, the maximum rate log |X | can be achieved for any given noise sequence z ∞ .\n3) Finally, suppose that the ﬁnite-lookahead requirement is nonexistant \u2014 that is, the encoding function can look at the entire untransmitted message stream x i = e(s (e) i , M ∞ p i , y i , θ i ). This is identical to allowing the encoder an inﬁnite number of states: if M ∞ is a Bernoulli(1/2) sequence, then with probability one there exists a one-to-one map between M ∞ i and i.\nChannel coding typically concerns itself with the tradeoff between rate of communication and the frequency of errors. In the individual sequence setting of interest to us, we deﬁne the instantaneous rate and bit-error rate of an FS scheme at time n as\nBest-Case. An FS scheme best-case p-achieves rate/error (R, ) for a noise sequence z ∞ if with at least probability p there exists a sequence of points {n i } ∈ Z + such that lim i→∞ R n i ≥ R and lim i→∞ n i ≤ . In other words, a performance monitor that observes the system at the \u201cright\u201d times will see it achieve (R, ) with probability at least p. If p is 1, we say that the scheme simply best-case achieves (R, ).\nWorst-Case. An FS scheme worst-case p-achieves rate/error (R, ) if with at least probability p both lim inf n→∞ R n ≥ R and lim sup n→∞ n ≤ . In other words, a performance monitor observing the system at any set of sample times will see it achieve (R, ) with probability at least p. If p is one the scheme is said to worst-case achieve (R, ).\nObserve that the randomness in these deﬁnitions has two possible sources: the source sequence M ∞ and the common- information sequence θ ∞ used by the FS scheme. Sometimes the source M ∞ will be a ﬁxed sequence, but this is always made clear from context.\nThe results of this paper are summarized below. Note that while these statements are specialized for the binary-alphabet case, they can easily and intuitively generalize to any ﬁnite alphabet with an addition operation.\n1) A converse that upper-bounds the best-case achievable rate by an FS scheme.\n2) A converse that upper-bounds the worst-case achievable rate by an FS scheme.\n3) A sequence of universal FS schemes {F m } ∞ m=1 that simultaneously achieve the best-case and worst-case converse bounds for any noise sequence z ∞ .\nFormally, each of these three statements corresponds to a theorem:\nTheorem 1: Suppose an FS scheme best-case p-achieves (R, ). If p > 0, then\nTheorem 2: Suppose an FS scheme worst-case p-achieves (R, ). If p > 0, then\nTheorem 3: For an iid Bernoulli(1/2) source M ∞ and noise sequence z ∞ , the scheme F m best-case achieves\nσ(z ∞ ) − δ m (z ∞ ) , m /(σ(z ∞ ) − δ m (z ∞ )), and worst-case achieves\nTheorems 1 and 2 are proven in Section III. In Section IV, we deﬁne the schemes {F m } ∞ m=1 and prove Theorem 3.\nIn order to prove the converse theorems, a series of deﬁni- tions and a useful lemma are ﬁrst required.\nDeﬁnition 1: Let {L i } ∞ i=1 be a bounded sequence of non- negative integers, and let M ∞ , z ∞ , and θ ∞ as usual denote ﬁnite-alphabet sequences. The k-partition of (M ∞ , z ∞ , θ ∞ ) according to {L i } is the sequence of blocks\nDeﬁnition 2: Let x ∞ be a sequence of symbols drawn from a ﬁnite alphabet X . If there exists a series of sample points {n i } ∞ i=1 such that the sequence ˆ p 1 (x)[x n i ] converges to a\ndistribution ˆ p(x), ˆ p(x) is said to be a limiting distribution for x ∞ .\nObserve that for any ﬁnite-alphabet sequence x ∞ at least one limiting distribution exists: Since ˆ p 1 (x)[x n ] is an inﬁnite sequence in a compact set, at least one convergent subsequence must exist.\nDeﬁnition 3: Let z ∞ and θ ∞ be ﬁnite-alphabet sequences. The set M k (z ∞ , θ ∞ ) consists of all binary sequences M ∞ such that there exist partition lengths {L i }, a result- ing k-partition {(M L i , z k , θ k ) i }, and a limiting distribution ˆ p(L, M L , z k ) for the sequence {L i , (M L , z k ) i } such that\nLemma 4: Let z ∞ be a ﬁxed binary sequence, let M ∞ be drawn from an iid Bernoulli(1/2) process, and let θ ∞ be a ﬁnite-alphabet sequence of arbitrary distribution that is independent of M ∞ . Then the probability that M ∞ ∈ M k ((z i , θ i ) ∞ i=1 ) is zero for any k.\nAs the proof is rather involved, we direct the reader to Corollary 7 in [9].\nAlthough the converse results are presented as two distinct theorems, at their heart is the same argument.\nLemma 5: Suppose an s-state -lookahead FS scheme achieves (R, ) on points {n i } for a speciﬁc source sequence M ∞ , a speciﬁc channel noise sequence z ∞ , and a speciﬁc encoder/decoder common information sequence θ ∞ . If for some k ∈ Z + M ∞ is not a member of M k ((z i , θ i ) ∞ i=1 ), and if ˆ H k (z n ) + ˆ H k (θ n ) − ˆ H k ((z i , θ i ) n i=1 ) → n→∞ 0, then\nProof: The general idea in proving this lemma is to turn any given FS scheme into a source encoding/decoding scheme. Consider an FS decoder that achieves (R, ) on some points {n i }, and ignore the minor complication of common randomness θ ∞ . Given only the channel output y ∞ , the decoder produces an estimate of the source sequence M ∞ . Knowing the source sequence and the channel output, the decoder is technically capable of \u201csimulating\u201d the encoder and thereby obtaining both the channel input sequence x ∞ and the noise sequence z ∞ . One may therefore interpret the channel output y ∞ as an encoding of the joint source sequence (M ∞ , z ∞ ). In [9], an argument inspired by this intuition proves the lemma.\nArmed with Lemma 5, it is a relatively straightforward matter to prove Theorems 1 and 2.\nProof of Theorem 1: We ﬁrst note that because θ ∞ is drawn iid and z ∞ is ﬁxed, ˆ H k (z n ) + ˆ H k (θ n ) − ˆ H k ((z i , θ i ) n i=1 ) → 0 with probability one for every k. Furthermore, by Lemma 4, M ∞ / ∈ M k ((z i , θ i ) ∞ i=1 ) with probability one for every k. Therefore, if (R, ) is best-case-achieved with positive proba- bility, it must then be achieved for some speciﬁc (M ∞ , θ ∞ ) such that M ∞ / ∈ M k ((z i , θ i ) ∞ i=1 and ˆ H k (z n ) + ˆ H k (θ n ) −\nˆ H k ((z i , θ i ) n i=1 ) → 0 for every k. Let {n i } be the subsequence on which it is achieved.\n≤ h b ( ) + 1 − lim inf k→∞ 1 k lim inf n→∞ H k (z n ) = h b ( ) + 1 − ρ(z ∞ ).\nProof of Theorem 2: If (R, ) is worst-case-achieved with positive probability p, then it must be worst-case achieved for some (M ∞ , θ ∞ ) such that M ∞ / ∈ M k ((z i , θ i ) ∞ i=1 (be- cause by Lemma 4 this occurs with probability one) and\nˆ H k (z n )+ ˆ H k (θ n )− ˆ H k ((z i , θ i ) n i=1 ) → 0 (because this occurs with probability one when θ ∞ is chosen iid).\nWe may therefore apply Lemma 5 with {n i } = Z + . For any k, we have that\nSince this holds for arbitrary k, we may take the limit inﬁmum of the expression with k → ∞:\nTo construct the universal achievability schemes {F m }, the ﬁnite-extent and ﬁrst-order-empirical-capacity results of Shayevitz and Feder [7] must be extended to inﬁnite-extent and m-th-order empirical capacity. We start by introducing a new subclass of FSAF (repetition schemes), then build an element of this class using the communication scheme of Shayevitz and Feder, and ﬁnally prove that this construction achieves porosity.\nA ﬁnite-extent (FE) scheme F for a channel with alphabet X consists of:\n2) A feedback channel with transition probabilities given by U i ∼ p u i (u i |u i−1 , y i ) and taking values in X , for i ∈ {1, . . . , n}.\n3) A common randomness variable θ drawn from a ﬁnite alphabet, independent of the source, and provided to both encoder and decoder.\n4) Encoding functions x 1 = e 1 (M ∞ , θ), x 2 = e 2 (M ∞ , θ, u 1 ), . . . , x n = e n (M ∞ , θ, u n−1 ).\n5) A decoding length function L = d L (y n , θ, u n−1 ), upper bounded by n log |X |.\nNote that an FE scheme is not a member of classes FS or FSAF \u2014 both of which are inﬁnite-extent.\nA repetition scheme is constructed from an n-extent ﬁnite extent scheme F. Let F(M ∞ , z n ) describe the application of scheme F to source M ∞ and noise block z n . Then the repetition scheme F consists of repeated independent uses of F, i.e. F(M ∞ , z ∞ ) is given by\nIn each block, F is applied to a \u201cvirtual source\u201d consisting of the ﬁrst n bits of the source that have yet to be transmitted and a string of 0s.\nProposition 6: The class of repetition schemes is a subclass of FSAF schemes (and therefore of FS schemes).\nThis follows directly from two properties of repetition schemes:\n\u2022 The block-based structure allows for implementation with ﬁnite-state machines.\n\u2022 A repetition scheme constructed from an n-extent FE scheme has ﬁnite lookahead constant n.\nAt the core of the achievability scheme is a lemma, intro- duced by Shayevitz and Feder [7]:\nLemma 7: (Shayevitz and Feder, 2009) Let X be a ﬁnite alphabet with an addition operation. Then there exists a sequence of n-extent FE schemes F n (X ) with the following worst-case performance guarantees. For any additive noise sequence z ∞ ∈ X ∞ and source sequence M ∞ ∈ {0, 1} ∞ ,\nNote that the only randomness in the above probabilistic statements is due to the randomness in the feedback channel.\nObserve that Lemma 7 concerns itself with only the ﬁrst- order empirical entropy H 1 (z n ). By specializing to binary sequences, this may be replaced by higher-order empirical entropies.\nCorollary 8: For binary additive noise channels with feed- back, there exists a sequence of ﬁnite extent schemes F m with extents N (m) → ∞ and the following performance guarantees:\nProof: For a given m, consider the m-tuple super- symbol channel characterized by inputs X i = x im (i−1)m+1 , noise Z i = z im (i−1)m+1 , and outputs Y i = (x (i−1)m+1 + z (i−1)m+1 , x (i−1)m+2 + z (i−1)m+2 , . . . , x im + z im ). Applying Lemma 7 to channels of this alphabet yields a sequence of schemes F n ({0, 1} m ) with\nn,m → n→∞ 0. \t (8) By (8) we may choose N (m) so that N (m),m → m→∞ 0.\nThe sequence of ﬁnite-extent schemes {F m } ∞ m=1 form the basis of the universal achievability construction.\nDeﬁnition 4: The universal achievability scheme of order m is the repetition scheme F m formed from the N (m)-extent scheme F m .\nCorollary 8 identiﬁes two types of \u201cerror\u201d events in the application of {F m } to a single block: a failure of the rate to exceed the threshold 1 − H 1 (z n ) − (n) and an error in the reconstruction. Let the indicators for these events in the ith block be known as T i and E i respectively. In proving Theorem 3, the probability guarantees for E i (6) and T i (7) must be connected to the relevant performance metrics: best-case rate, worst-case rate, and worst-case error.\nWhile neither sequence E ∞ nor T ∞ is independent (nor identically distributed), both possess certain Markov properties with respect to the source bits M L (i) used in each block. This allows for statements regarding the limiting weighted averages of each of these sequences: e.g. lim sup n→∞ 1 n n i=1 α i E i ≤\nα i with probability one. These limit- ing statements, in turn, can be leveraged to prove the perfor- mance guarantees of Theorem 3. For the full proof, see [9]."},"refs":[{"authors":[{"name":"A. Lapidoth"},{"name":"J. Ziv"}],"title":{"text":"On the universality of the LZ-based decoding algorithm"}},{"authors":[{"name":"A. Lapidoth"},{"name":"P. Narayan"}],"title":{"text":"Reliable communication under channel uncertainty"}},{"authors":[{"name":"S. Shamai"},{"name":"A. Steiner"}],"title":{"text":"A broadcast approach for a single-user slowly fading MIMO channel"}},{"authors":[{"name":"J. Ziv"},{"name":"A. Lempel"}],"title":{"text":"A universal algorithm for sequential data com- pression"}},{"authors":[],"title":{"text":"Compression of individual sequences via variable-rate coding"}},{"authors":[{"name":"Y. Lomnitz"},{"name":"M. Feder"}],"title":{"text":"Universal communication over modulo- additive individual noise sequence channels"}},{"authors":[{"name":"O. Shayevitz"},{"name":"M. Feder"}],"title":{"text":"Achieving the empirical capacity using feedback: Memoryless additive models"}},{"authors":[{"name":"K. Eswaran"},{"name":"A. Sarwate"},{"name":"A. Sahai"},{"name":"M. Gastpar"}],"title":{"text":"Zero-rate feedback can achieve the empirical capacity"}},{"authors":[{"name":"V. Misra"},{"name":"T. Weissman"}],"title":{"text":"The porosity of additive noise sequences"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564233.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S14.T7.4","endtime":"18:00","authors":"Vinith Misra, Tsachy Weissman","date":"1341510000000","papertitle":"The Porosity of Additive Noise Sequences","starttime":"17:40","session":"S14.T7: Topics in Shannon Theory","room":"Stratton (407)","paperid":"1569564233"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
