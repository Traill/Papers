{"id":"1569566081","paper":{"title":{"text":"Reliable versus Unreliable Transmission for Energy Efﬁcient Transmission in Relay Networks"},"authors":[{"name":"Anders Høst-Madsen"},{"name":"Nan Jiang"},{"name":"Yang Yang"},{"name":"Zixiang Xiong"}],"abstr":{"text":"Abstract\u2014A network code is said to be reliable when all trans- missions in the network are (deterministic) functions of the source messages; well-known examples include decode-forward for relay networks. It is said to be unreliable when transmissions depend on the noise realization at nodes; examples include compress- forward and amplify-forward. The deterministic capacity of a network is deﬁned as the supremum of the rates achievable by reliable codes. In this paper we derive the deterministic capacity of some relay networks in the low power regime. The resulting energy per bit is then compared with the one achievable by arbitrary transmission."},"body":{"text":"Coding methods for wireless networks can generally be di- vided into two classes: those where relays process the received signal and forwards it, and those where the relays decode (a function of) the original message, and encodes this into a new signal. In the ﬁrst class (some times denoted estimate- forward) are methods such as amplify-forward and compress- forward [1, Theorem 6] that have wide set of generalizations. The second class (sometime denoted regenerative coding) has as its source the original decode-forward strategy of [1]. What characterizes these methods, as opposed to the ﬁrst class, is that relays decode the original message, or more generally, a function of the original message, reliably, and transmits a message which is a possibly different function of the decoded message (e.g., in [2] the parity information). One way to characterize this class is that the transmission is reliable. Relays decode their messages with a vanishing error probability, and base their transmission on deterministic functions of the messages. We will therefore denote this type of coding as reliable coding. In contrast, amplify-forward type methods introduce further randomness through the noise at the relays. Informally one could say that amplify-forward type methods introduce errors in their transmission streams, while reliable transmission eliminates errors.\nWhy reliable transmission? One way to think of reliable transmission is as follows: nodes decode transmitted pack- ages, with few errors. They then compute functions of these packages, and transmit these new computed packages. In a traditional multi-hop network, the functions are the identity functions. With network coding, the functions operate only\non the contents of the packages. With reliable transmission, the functions operate on both contents of packets and channel coding parts. In all cases, a big advantage is that the layering of the network according to the OSI 7-layer model is main- tained, even if some crosslayer interfacing is needed. On the other hand, general network information theory methods may require completely scrapping the separation between the lower 3-4 layers. It might be worth having that disruption; but to argue that we should be able to quantify the gain achieved by non-reliable transmission. Thus, we need to be able to bound the rate achievable by reliable transmission, even if we want to consider non-reliable transmission.\nThe aim of this paper is to quantify the gain achievable by non-reliable transmission in a few cases. Deterministic capacity was introduced in [3] as the supremum of all rates achievable by reliable transmission. The gain G (energy sav- ings) for non-reliable transmission therefore satisﬁes\nClearly, for (1) we need a bound on reliable codes, which is exactly provided by deterministic capacity.\nWe consider a network with N nodes as in [4, Sec. 15.10]. We denote the transmitted symbol (which might be a vector) at time m at node i by X i [m] and the string of transmitted symbols in the interval 1 . . . k by X i [k] = [X i [1], X i [2], . . . , X i [k]]. Similarly for the received signals Y i [n] and Y i [n] at node i. The output alphabet at node i is X i , the input alphabet Y i , and the nodes are connected through memoryless channels. A (length n) code for the network is deﬁned as in [4, Sec. 15.10]: Node 1, the source has a message W intended for node N that it transmits at rate R; we consider the message a uniform random variable over 1, 2, . . . , 2 nR . The encoder at node 1 is a function X 1 [n] : 1, 2, . . . , 2 nR → X n 1 and the transmission at node 1 is X 1 [n](W ). At node i > 1 the encoder is a function X i [m] : Y m−1 i → X i that depends only on past received\nsymbols, that is the transmission is X i [m](Y i [1] . . . Y i [m−1]), m ∈ {1, . . . , n} . The decoder at node N is a function ˆ\nW : Y n i → 1, 2, . . . , 2 nR , and the performance is measured by the average error probability\nThus far the setup is exactly like in [4, Sec. 15.10]. We now deviate by introducing a corresponding deterministic code:\nDeﬁnition 1. A rate R is said to be achievable by deter- ministic codes if there exists a sequence of (2 nR , n) codes { X i [n], i = 1 . . . N} and a sequence of corresponding deter- ministic codes{ ˆ X i [n], i = 1 . . . N}, so that\nP { ˆ X i [n](W ) = X i [n](Y i [n])} = 0 (5) lim n→∞ P { ˆ W (Y N [n]) = W } = 0. (6)\nThe deterministic capacity is the supremum of all rates R that are achievable by deterministic codes. We say that a sequence of codes X i [n] is reliable if there exist a corresponding sequence of deterministic codes ˆ X i [n] so that (5) is satisﬁed.\nThe deﬁnition of deterministic capacity is abstract and very general. The essence is that it allows arbitrary computations on the data; general capacity allows arbitrary computations on the signals.\nWe will here restrict attention to Gaussian channels as they are representative of wireless networks. The received signal is subject to additive complex Gaussian noise of power BN 0 , where N 0 is the noise power spectral density, and B is the bandwidth. The complex channel gain from node i to node j is c ji . Each node is subject to a power constraint 1 n n k=1 | X i [k]| 2 ≤ P i . In many cases we will consider a total power constraint N i=1 P i ≤ P .\nIn the rest of the paper we will concentrate solely on energy per bit, rather than general capacity. The reason is that for deterministic capacity, see Lemmas 2-3, it is possible in many cases to obtain the limit of capacity for SNR → 0, even if deterministic capacity is not known for general SNR > 0; at the same time, for deterministic capacity, the minimum energy per bit is achieved for SNR → 0 (this may not be true for general capacity). It is therefore possible to evaluate (1) even when general deterministic capacity is unknown.\nIf we denote by C(B) the capacity in bits/s/Hz for a given bandwidth, we can deﬁne the following limit (if it exists)\nwe have multiplied with N 0 / log e to simplify formulas in the following. From this quantity energy per bit can be found by\nIn general, we will denote rates in the low power regime by sans serif.\nLemma 2. Suppose that for each value of B a random (N- vector) variable X (B) that satisﬁes var[X(B)] ≤ P is given. Let Y = c H X (B) + Z, where Z ∼ N (0, N 0 B). Then\nLemma 3. Suppose that for each value of B we are given random variables U(B), V (B),and a vector random variable X (B) that satisfy var[X(B)] ≤ P . Deﬁne\nY 1 = c H 1 X (B) + Z 1 \t (10) Y 2 = c H 2 X (B) + Z 2 \t (11)\nwhere Z 1 and Z 2 are independent, Z 1 , Z 2 ∼ N (0, N 0 B). Suppose that (U(B), V (B)) → X(B) → Y 1 and (U(B), V (B)) → X(B) → Y 2 form Markov chains. Then\nBI(X; Y 2 | U), and BI(U; Y 1 | V ) are increasing functions of B.\n2) We have the following limits. lim B→∞ BI(U(B); Y 1 )\nN 0 ln 2 \t (12) lim\nN 0 ln 2 \t (13) lim\nClaim 1 of Lemma 3 can be used to argue that the (lower bound on) minimum energy per bit is achieved for B → ∞. Then claim 2 can be used to actually compute the limits.\nConsider a network that uses the sequence of reliable codes X i [n], that is, there exists a corresponding sequence of deterministic codes ˆ X i [n] satisfying (5). Deﬁne\nand notice that for deterministic capacity, node i must be able to decode at least W i with asymptotically zero error\nR = lim n→∞ H(W ) n \t (16) R A|B = lim n→∞ H (W i∈A | W j∈B ) n \t (17)\nAssume that the relays are ordered so that | c i1 | ≥ | c j1 | for i ≤ j. For deterministic capacity, each relay must be able to decode a message W i = f i (W ). The channel from the source to the relays forms a degraded broadcast channel with correlated messages. It is clear that if i ≤ j, node i can also decode message W j . We can therefore assume that the message set is also degraded in the sense that there exist functions g i so that W i = g i−1 (W i−1 ). The capacity of the channel from the source to the relays is therefore given by the usual capacity expression for the degraded Gaussian relay channel [6], with R i replaced by R i|i+1,i+2,...N −1 ,\nThe channel from the relays to the destination is a MAC channel with correlated messages. The capacity is not known in general, but in this special case it\u2019s easy to see that we have the outer bound\nThe above outer bound is achievable as follows. Deﬁne independent messages ˜ W 2 , . . . , ˜ W N −1 . The source transmits a superposition of ˜ W 2 , . . . , ˜ W N −1 . Relay i (and therefore also relays 2, . . . , i − 1) decodes message ˜ W i . Relays 2, . . . , i jointly beamform message ˜ W i to the destination. It is im- mediate that this achieves the above outer bound region asymptotically.\nWhat made the problem easy for the diamond relay channel without a direct link is that the source-relay and relay- destination channel are separate. When we add a direct link from the source to the destination, this is no longer true. The channel out of the source is still a type of broadcast channel, but it is no longer degraded. The capacity therefore is not known. The best outer bounds therefore are those of Marton and Körner [7] and Nair and El Gamal [8], which we will modify and apply here. The Marton-Körner type bounds do not have any known general extensions to more than two sources, so we will have to limit results to at most two relays, see Fig. 1. Furthermore, the distributions of auxiliary random variables are in general essentially impossible to ﬁnd. This is where the low power theory, Section III gets into the picture. This allows us to directly ﬁnd bounds on energy, without ﬁnding bounds on capacity.\nWithout loss of generality we can assume that | c 21 | ≥ | c 31 |. First, let us outline an achievable deterministic coding scheme.\nWe consider the following coding scheme, which we will call successive cooperation . We divide the transmission time into three intervals. In the ﬁrst time interval node 1 transmits X 1 1 so that node 2 can decode the message W . In the second time interval node 2 transmits X 2 2 and node 1 transmits X 2 1 = k 1 X 2 2 (where k 1 is a complex constant), so that the signals from nodes 1 and 2 add up coherently at the destination, node 4. Node 3 decodes the message W from Y 1 1 and Y 2 1 , the signals received during the ﬁrst and second time interval.In the third time interval node 3 transmits X 3 and nodes 1 and 2 transmit X 3 1 = k 2 X 3 and X 3 2 = k 3 X 3 , so that all the signals add up coherently at the destination.\nTheorem 4. Consider the diamond relay channel with a total power constraint and assume that | c 21 | ≥ | c 31 | ≥ | c 41 | . Fix the rate R. The total power required to achieve this rate with deterministic codes is then bounded by P ≥ l 3 2 + l 2 2 + | l 11 | 2 , where the right hand side is given by the minimum of the two following solutions\nc 41 | 2 | c 42 | 2 | l 12 | 2 (|c 41 | 2 + |c 43 | 2 ) 2\nc 41 | 2 | l 12 | 2 − | c 42 | 2 | l 22 | 2 | c 41 | 2 + |c 42 | 2 + |c 43 | 2\n(|c 41 | 2 + |c 42 | 2 ) l 2 2 − | c 41 | 2 | l 11 | 2 | c 41 | 2 + |c 42 | 2 + |c 43 | 2\n| 2 (|c 41 | 2 + |c 43 | 2 ) + | c 41 | 2 4|c 42 | 4 + 2|c 42 | 2 | c 43 | 2 + |c 43 | 4 | c\nsuccessive cooperation achieves the deterministic capacity in the low power regime.\nProof: We will provide the principles of the proof. The full details can be found in [9]. We will prove the bounds (compare [8])\nR ≤ I(U; Y 4 ) \t (21) R 3 ≤ I(U 3 ; Y 3 | X 3 ) \t (22)\nR ≤ I(U; Y 4 | U 3 , X 3 ) + I(U 3 ; Y 3 | X 3 ) (23) R ≤ I(X 1 ; Y 2 | X 2 , X 3 ) \t (24)\nfor some joint distribution p(u 3 )p(u|u 3 )p(x 3 | u 3 )p(x 1 , x 2 | u). As an example of the methodology of deterministic capacity we will prove (24); the other bounds follow similar principles.\nThe ﬁrst step is to argue that node 2 can decode the full message W as |c 21 | ≥ | c 31 | ≥ | c 41 |. Node 2 can form\nY 2 [n] + Z 3 [n] = c 31 X 1 [n] + c 31 c\nwhere Z 3 [n] is iid Gaussian noise with power 1 − |c 31 | 2 |c 21 | 2 N 0 B. Since Y 3 [n] has the same distribution as\nY 3 [n], node 2 can decode W 3 with high probability for large n. It then forms\nY 2 [n] + c 42 X 2 [n] + c 43 ˆ X 3 [n] + Z 4 [n] = c 41 X 1 [n] + c 42 X 2 [n] + c 43 ˆ X 3 [n]\nwhere Z 4 [n] is iid Gaussian noise with power 1 − |c 41 | 2 |c 21 | 2 N 0 B and ˆ X 3 [n] is the deterministic code\nˆ Y 4 [n] = c 41 X 1 [n] + c 42 ˆ X 2 [n] + c 43 ˆ X 3 [n] + Z 4 [n](27) ˆ Y 4 [n] = c 41 X 1 [n] + c 42 ˆ X 2 [n] + c 43 ˆ X 3 [n]\nA genie provided with ˆ Y 4 [n] can decode W , as with high probability ˆ Y 4 [n] = Y 4 [n]. Since ˆ Y 4 [n] and ˆ Y 4 [n] have the same distribution 1 , a genie provided ˆ Y 4 [n] can also decode W with high probability. Finally, with high probability Y 4 [n] = ˆ Y 4 [n], and node 2 can therefore decode W with high probability.\nWe will now use this fact to prove (24). For this we need a more careful accounting of errors and we therefore deﬁne E = 1 ˆ X 3 [n] = X 3 [n](Y 3 [n]) or ˆ X 3 [n] = X 3 [n](Y 3 [n]) 0 ˆ X\nNotice that by the deﬁnition of deterministic capacity, Deﬁni- tion 1, P (E = 1) = n . Furthermore, for any random variables A and B\n(where lim n→∞ n = 0 [4]). Denote P 0 = P (E = 0) and P 1 = P (E = 1). With this we get nR\n+I(W ; Y 2 [n]|E = 1)P 1 + H( n ) ≤ n n + I(W ; Y 2 [n]|E = 0)P 0\nIn step (a) we have used (29), in (b) n n has absorbed H(W )P (E = 1) ≤ nR n and H( n ). Step (c) follows from the fact that X 2 [m] is a function of Y 2 [m−1], and, conditioned on E = 0, so is X 3 [m].\nvar[c 42 X 2 (B) + c 43 X 3 (B) +c 41 X 1 (B)|U 3 (B), X 3 (B)]\nHere L = {l ij } is a triangular matrix found through Cholesky factorization. The random variables S are asumptotically un- correlated (but not necessarily independent). We now use that var[S 1 | S 3 ] ≤ var[S 1 ] and similar and var[S 1 | S 3 , U 3 ] = var[S 1 | U 3 ] and similar, as S 3 is a function of U 3 . Since | c 31 | ≥ | c 41 | the bounds are maximized if we set var[S 1 | U 3 ] = 0. Furthermore lim B→∞ var[S i ] = 1, and we put\n+|c 42 l 22 + c 41 l 12 | 2 + |c 41 l 11 | 2 R 3 ≤ | c 31 | 2 | l 11 | 2 + |c 31 | 2 | l 12 | 2 α\nTo get an actual outer bound, these bounds must be max- imized over the parameters l ij and α and R 3 . We ﬁx R and minimize P . The key observation is that it turns out that for any values of the other parameters the sign of ∂P ∂α is independent of α. Any solution must therefore be on the boundaries, that is for α = 0 or α = 1. The solution for α = 0 can be shown to be (19) and for α = 1 to be (18). To prove the ﬁnal claim of the proof the ﬁrst step is to realize that any solution with α = 1 can be achieved with successive cooperation (whereas this is not possible for α = 0). And if the condition (20) is satisﬁed, the power consumption with α = 1 is smaller than the power consumption with α = 0.\nThe approximate capacity of the diamond relay channel without direct link was found in [10]. We compare the energy per bit achieved by deterministic capacity as outlined in Sec- tion IV-B with the outer bound on capacity from [10] as well\nas with amplify-forward [10]. We plot the the performance for the symmetric channel. The result is in Fig. 2.\nFor the 2-relay diamond relay channel with a direct link, we compare the energy corresponding to the outer bound on deterministic capacity given by Theorem 4 with that achievable by amplify-forward for various relay positions, see Fig. 3.\nWe have seen that it is possible to calculate bounds on the energy gain achieved by non-reliable transmission for some examples. This is only possible because we have an abstract deﬁnition of deterministic capacity through Deﬁnition 1; oth- erwise, it would just be a comparison of arbitrary achievable rates. What the results prove is that unreliable transmission is more energy efﬁcient, although perhaps the number of relays have to be large for the gain to be (guaranteed) signiﬁcant."},"refs":[{"authors":[{"name":"T. Cover"},{"name":"A. E. Gamal"}],"title":{"text":"Capacity theorems for the relay channel"}},{"authors":[{"name":"P. Razaghi"},{"name":"W. Yu"}],"title":{"text":"Parity forwarding for multiple-relay networks"}},{"authors":[{"name":"A. Høst-Madsen"}],"title":{"text":"Deterministic capacity of networks"}},{"authors":[{"name":"T. Cove"},{"name":"J. Thoma"}],"title":{"text":"Information Theory, 2nd Edition"}},{"authors":[{"name":"D. Guo"},{"name":"S. Shamai"},{"name":"S. Verdu"}],"title":{"text":"Mutual information and minimum mean-square error in Gaussian channels"}},{"authors":[{"name":"P. Bergmans"}],"title":{"text":"A simple converse for broadcast channels with additive white Gaussian noise"}},{"authors":[{"name":"K. Marton"}],"title":{"text":"A coding theorem for the discrete memoryless broadcast channel"}},{"authors":[{"name":"C. Nair"},{"name":"A. E. Gamal"}],"title":{"text":"An outer bound to the capacity region of the broadcast channel"}},{"authors":[{"name":"A. Høst-Madsen"}],"title":{"text":"Deterministic capacity of networks in the low power regime"}},{"authors":[{"name":"U. Niesen"},{"name":"S. Diggavi"}],"title":{"text":"The approximate capacity of the gaussian n-relay diamond network"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566081.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T3.1","endtime":"10:10","authors":"Anders Høst-Madsen, Nan Jiang, Yang Yang, Zixiang Xiong","date":"1341309000000","papertitle":"Reliable versus Unreliable Transmission for Energy Efficient Transmission in Relay Networks","starttime":"09:50","session":"S5.T3: Energy-Efficient Communication","room":"Stratton S. de P. Rico (202)","paperid":"1569566081"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
