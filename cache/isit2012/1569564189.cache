{"id":"1569564189","paper":{"title":{"text":"The capacity region of the two-receiver vector Gaussian broadcast channel with private and common messages"},"authors":[{"name":"Yanlin Geng"},{"name":"Chandra Nair"}],"abstr":{"text":"Abstract\u2014We develop a new method for showing the optimality of the Gaussian distribution in multiterminal information theory problems. As an application of this method we show that Mar- ton\u2019s inner bound achieves the capacity of the vector Gaussian broadcast channels with common message."},"body":{"text":"Channels with additive Gaussian noise are a commonly used model for wireless communications. Hence computing the capacity regions or bounds on the capacity regions for these classes of channels are of wide interest. Usually these bounds or capacity regions are represented using auxiliary random variables and distributions on these auxiliary random variables. Evaluations of these bounds then become an optimization problem of computing the optimal auxiliary random variables. In several instances involving Gaussian noise channels, it turns out that the optimal auxiliaries and the inputs are Gaussian. However proving the optimality of Gaussian distributions is usually very cumbersome and involves certain non-trivial ap- plications of the entropy-power-inequality(EPI), a non-trivial result in itself.\nFor the two-receiver vector Gaussian broadcast channel with private messages, the capacity region was established[1] by showing that certain inner and outer bounds coincide. This argument was indirect and hence the approach has been hard to generalize to other situations, including the common message case. In the following sections we develop a novel way of proving the optimality of Gaussian input distribution for additive Gaussian noise channels. There are many potential straightforward applications of this new approach which will yield new results as well as recover the earlier results in a simple manner. For the purpose of this article, we will restrict ourselves to two-receiver vector Gaussian channels.\nRemark 1. Due to page limitation, we are unable to do a proper justice to the proofs; the full version is in arXiv [2].\nBroadcast channel[3] refers to a communication scenario where a single sender, usually denoted by X, wishes to communicate independent messages (M 0 , M 1 , M 2 ) to two receivers Y 1 , Y 2 . The goal of the communication scheme is to enable receiver Y 1 to recover messages (M 0 , M 1 ) and receiver\nY 2 to recover messages (M 0 , M 2 ); both events being required to occur with high probability. For previous work one may refer to Chapters 5, 8, and 9 in [4].\nA broadcast channel is characterized by a probability tran- sition matrix q(y 1 , y 2 |x). The following broadcast channel is referred to as the vector additive Gaussian broadcast channel\nIn the above X ∈ R t , G 1 , G 2 are t × t matrices, and Z 1 , Z 2 are Gaussian vectors independent of X.\nA product broadcast channel is a broadcast channel whose transition probability has the form q 1 (y 11 , y 21 |x 1 ) × q 2 (y 12 , y 22 |x 2 ). A vector additve Gaussian product broadcast channel can be represented as\nIn the above Z 11 , Z 12 , Z 21 , Z 22 are independent Gaussian vectors, also independent of X 1 , X 2 .\nRemark 3. In this paper we assume that all our channel gain matrices are invertible. Since the set of all matrices is dense (with respect to say, Frobenius norm) by continuity, our capacity results extend to non-invertible cases.\nWe present some simple claims regarding additive Gaussian channels which will be useful later.\nClaim 1. Consider the following vector additive Gaussian product channel with identical components\nY 1 = GX 1 + Z 1 Y 2 = GX 2 + Z 2\nFurther let Z 1 , Z 2 be independent and distributed as N (0, I). Deﬁne\nProof: The proof is a trivial consequence of the fact that h( ˜ Y, Y ) = h(Y 1 , Y 2 ) and h( ˜ Y, Y | ˜ X, X ) = h( ˜ Z, Z ) = h(Z 1 , Z 2 ) = h(Y 1 , Y 2 |X 1 , X 2 ) where ˜ Z = 1 √ 2 (Z 1 + Z 2 ), Z = 1 √ 2 (Z 1 − Z 2 ).\nRemark 4. An interesting consequence of Gaussian noise is that ˜ Z and Z are again independent and distributed according to N (0, I). Hence ˜ Y, Y can be regarded as the outputs of the Gaussian channel when the inputs are distributed according to ˜ X, X . This observation is peculiar to additive Gaussian channels.\nClaim 2. In vector additive Gaussian product broadcast channels, the random variables Y 11 and Y 22 are independent if and only if X 1 and X 2 are independent.\nWe devise a new technique to show that Gaussian distribu- tion achieves the maximum value of an optimization problem, subject to a covariance constraint. Though some of the results have been known earlier[5], the technique presented here allows us to obtain much broader results.\nThe main idea behind the approach is to show that if a certain X (say zero mean) achieves the maximum value of an optimization problem, then so does 1 √ 2 (X 1 + X 2 ) and\n(X 1 − X 2 ); where X 1 , X 2 are two i.i.d. copies of X. Further we will show that 1 √ 2 (X 1 +X 2 ) and 1 √ 2 (X 1 −X 2 ) have to be independent as well, which forces the initial distribution to be Gaussian, see Theorem 1 in [6] or the full version [2]. To show the ﬁrst step we go to the two-letter version 1 of the channel, use a factorization property of the function involved and then Claim 1 to move from the pair X 1 , X 2 to\nConsider a vector additive Gaussian broadcast channel. For λ > 1 let the following function of p(x) be deﬁned by\ndenote the upper concave envelope 2 of s λ (X). It is a straight- forward exercise to see that\nWe also deﬁne S λ (X|V ) := v p(v)S λ (X|V = v) for ﬁnite V and its natural extension for arbitrary V .\nFor a product broadcast channel q 1 (y 11 , y 21 |x 1 ) × q 2 (y 12 , y 22 |x 2 ) let S λ (X 1 , X 2 ) denote the corresponding upper concave envelope. The following claim is referred to as the \u201cfactorization of S λ (X 1 , X 2 )\u201d.\nClaim 3. The following inequality holds for product broadcast channels\nS λ (X 1 , X 2 ) ≤ S λ (X 1 |Y 22 ) + S λ (X 2 |Y 11 ) ≤ S λ (X 1 ) + S λ (X 2 ).\nFor additive Gaussian noise channels if p(v|x 1 , x 2 ) realizes S λ (X 1 , X 2 ), i.e. S λ (X 1 , X 2 ) = s λ (X 1 , X 2 |V ), and equality is achieved above i.e. S λ (X 1 , X 2 ) = S λ (X 1 ) + S λ (X 2 ), then all of the following must be true\nSince equality holds all inequalities are tight. Hence Y 1 and Y 2 are conditionally independent of V implying that X 1 and X 2 are conditionally independent of V (Claim 1). Hence\n1) Maximizing the concave envelope subject to a covari- ance constraint: Consider an Additive Gaussian Noise broad- cast channel q(y 1 , y 2 |x). For K 0, deﬁne\nClaim 4. There is a pair of random variables (V ∗ , X ∗ ) with |V ∗ | ≤ t(t+1) 2 + 1 such that\nThe goal of this section is to show that a single Gaussian distribution achieves V λ (K), i.e. we can take V to be trivial and X ∼ N (0, K ), K \t K. (This result is known and was ﬁrst shown by Liu and Vishwanath[5] using perturbation based techniques. We use this here as a non-trivial illustration of our technique and then our ﬁnal result in the next section is new.)\nConsider a product channel consisting of two identical components q(y 11 , y 21 |x 1 ) × q(y 12 , y 22 |x 2 ).\nNotation : In the remainder of the section we assume that p ∗ (v, x) achieves V λ (K) , |V | = m ≤ t(t+1) 2 + 1 and X v be a centered random variable (zero-mean) distributed according to p(X|V = v). Further let K v = E(X v X T v ). Then we have\np ∗ (v)K v K and in particular that K v \u2019s are bounded. Claim 5. Let (V 1 , V 2 , X 1 , X 2 ) ∼ p ∗ (v 1 , x 1 )p ∗ (v 2 , x 2 ) be two i.i.d. copies of p ∗ (v, x). We assume that |V | ≤ t(t+1) 2 + 1. Let\nIn the above we take X v 1 and X v 2 to be independent random variables. Then the following hold:\n3) ˜ V , X achieves V λ (K). Proof:\nHere the ﬁrst equality comes because p ∗ (v, x) achieves V λ (K), the second one because (V 1 , X 1 ) and (V 2 , X 2 ) are independent. Equality (a) is a consequence of Claim 1, inequality (c) is a consequence of Claim 3, and the last inequality follows from the following:\nand the deﬁnition of V λ (K). Since the extremes match, all inequalities must be equalities. Hence (b) must be an equality, p(˜ v, ˜ x, x ) achieves S λ ( ˜ X, X ); and since (c) is also equality from Claim 3 we conclude that ˜ X, X are conditionally independent of ˜ V . Furthermore, we also obtain that p(˜ v|˜ x) achieves S λ (˜ x), which from the last inequality matches V λ (K). Similarly for p(˜ v|X ).\nAs a consequence, X v 1 , X v 2 are independent random vari- ables and (X v 1 + X v 2 ) , (X v 1 − X v 2 ) are also independent random variables. Thus (Theorem 1 in [6]) X v 1 , X v 2 are Gaus- sians, say having the same distribution as X v ∼ N (0, K ). Since v 1 , v 2 are arbitrary, all X v i are Gaussians, having the same distribution as X v . Then\nTheorem 1. There exists X ∗ ∼ N (0, K ), K K such that V λ (K) = s λ (X ∗ ).\nRemark : Notice that we never used the precise form of S λ (X) but just used that the implications of Claim 3. In the next section we will deﬁne a new concave envelope that will also satisfy a condition similar to Claim 3, and then establish the optimality of Gaussian.\nCorollary 1. If X ∼ N (0, K) then ∃X ∗ ∼ N (0, K ), K K such that S λ (X) = s λ (X ∗ ) = V λ (K).\nProof: Clearly from Theorem 1 and deﬁnition of V λ (K) we have\nOn the other hand let X ∼ N (0, K − K ) be independent of X ∗ . Note that X ∼ X + X ∗ and\nThe function we considered in the previous section can be used to determine the capacity region of vector Gaussian broadcast channel with only private messages[5]. The function we consider in this section will enable us to determine the capacity region of vector Gaussian broadcast channel with common message as well (see Section III).\nFor λ 0 , λ 1 , λ 2 > 0 and for α ∈ [0, 1] (and ¯ α := 1 − α) consider the following function of p(x) deﬁned by\nFor a product broadcast channel q 1 (y 11 , y 21 |x 1 ) × q 2 (y 12 , y 22 |x 2 ) let T λ (X 1 , X 2 ) denote the corresponding up- per concave envelope. The following claim is referred to as the \u201cfactorization of T λ (X 1 , X 2 )\u201d.\nClaim 6. When λ 0 > λ 1 + λ 2 the following inequality holds for product broadcast channels\nT λ (X 1 , X 2 ) ≤ T λ (X 1 |Y 22 ) + T λ (X 2 |Y 11 ) ≤ T λ (X 1 ) + T λ (X 2 ).\nFor additive Gaussian noise broadcast channels if p(w, x 1 , x 2 ) realizes T λ (X 1 , X 2 ) and equality is achieved above then all of the following must be true\nProof: The proof is similar to proof of Claim 3 and is omitted. Details can be found in [2].\nFor K 0, deﬁne ˆ V\nClaim 7. There exists a pair (W ∗ , X ∗ ) with |W ∗ | ≤ t(t+1) 2 +1 such that ˆ V λ (K) = t λ (X ∗ |W ∗ ).\nNotation : In remainder of the section we assume that p ∗ (w, x) achieves ˆ V λ (K), |W | = m ≤ t(t+1) 2 + 1 and X w be a centered random variable (zero-mean) distributed according to p(X|W = w). Further let K w = E(X w X T w ). Then we have m w=1 p ∗ (w)K w K and in particular that K w \u2019s are bounded.\nClaim 8. Let (W 1 , W 2 , X 1 , X 2 ) ∼ 2 i=1 p ∗ (w i , x i ) be two i.i.d. copies of p ∗ (w, x). We assume that |W | ≤ t(t+1) 2 + 1. Let\nIn the above we take X w 1 and X w 2 to be independent random variables. Then the following hold:\nProof: The proof is similar to proof of Claim 5 and is omitted. Details can be found in [2].\nAs a consequence, X w 1 , X w 2 are independent random vari- ables and (X w 1 + X w 2 ) , (X w 1 − X w 2 ) are also independent random variables. Thus (Theorem 1 in [6]) X w 1 , X w 2 are Gaussians, say X w ∼ N (0, K ). Since w 1 , w 2 are arbitrary, all X w i are Gaussians, having the same distribution as X w . Then\nTheorem 2. There exists X ∗ ∼ N (0, K ), K K such that ˆ V\nCorollary 2. If X ∼ N (0, K) then there exists X 1∗ ∼ N (0, K 1 ) and an independent random variable X 2∗ ∼ N (0, K 2 ), K 1 + K 2 = K K such that T λ (X) = t λ (X 1∗ + X 2∗ ) = ˆ V λ (K) and S λ1+λ2\nProof: Clearly from Theorem 2 and deﬁnition of ˆ V λ (K) we have\nOn the other hand let X ∼ N (0, K − K ) be independent of X ∗ . Note that X ∼ X + X ∗ and\nNow splitting of X ∗ into X 1∗ , X 2∗ is possible by Corollary 1.\nConsider a vector Gaussian broadcast channel with common and private message requirements. Let C be the capacity region. Assume λ 0 > λ 1 + λ 2 . We will seek to maximize the following expression\nRemark 5. The case of maximizing λ 0 R 0 +(λ 1 +λ 2 )R 1 +λ 2 R 2 can be dealt with similarly. On the other hand if λ 0 ≤ (λ 1 + λ 2 ) then it sufﬁces to consider the private messages capacity region. Actually the setting λ 0 ≥ 2λ 1 + λ 2 can be deduced from the degraded message sets capacity region and this is also known; however this will be subsumed in our treatment. Hence the setting we are considering is the only interesting unestablished case.\nIn this section we consider the UVW outer bound[7] and Marton\u2019s inner bound[8] to the capacity region of the broad- cast channel with private and common messages.\nBound 1 (UVW outer bound). The union of rate triples (R 0 , R 1 , R 2 ) satisfying\nover all (U, V, W ) → X → (Y 1 , Y 2 ) forms an outer bound to the broadcast channel.\nDenote the region by Marton\u2019s inner bound as I. (see [2] for the description).\nImpose a covariance constraint K on X and denote I K , C K , O K to be the corresponding inner bound, capacity region, and the outer bound respectively. Trivially we have\nFor any α ∈ [0, 1] observe that (from ﬁrst, third, and fourth constraints of UVW outer bound)\nWe know that the ﬁrst term is maximized when X ∼ N (0, K) and ˆ V λ (K) is achieved by t λ (X 1∗ + X 2∗ ) where\nX 1∗ , X 2∗ are independent and X 1∗ ∼ N (0, K 1 ), X 2∗ ∼ N (0, K 2 ), K 1 + K 2 \t K, and S λ1+λ2\n(X 1∗ ). See Theorem 2 and Corollary 2. Now let W ∗ ∼ N (0, K − (K 1 + K 2 )) be independent of X 1∗ , X 2∗ and let X = W ∗ + X 1∗ + X 2∗ . Observe that this choice attains both maxima simultaneously. For conforming to more standard notation, let us call V ∗ = X 2∗ , thus X = W ∗ + X 1∗ + V ∗ . Thus\nNow using dirty-paper-coding idea (see [2]) choose U ∗ = X 1∗ + AV ∗ with A = K 1 G T (GK 1 G T + I) −1 to have\nProof can be found in full version [2] (and follows from a result in [9]).\nNow using Marton\u2019s inner bound we can always achieve the triples: R 0 = min{I(W ; Y 1 ), I(W ; Y 2 )}, R 1 =\nHence Marton\u2019s inner bound and UVW outer bound match and further the boundary is achieved via Gaussian signalling.\nWe developed a new method to show the optimality of Gaussian distributions. We illustrated this technique for two examples and computed the capacity region of the two-receiver vector Gaussian broadcast channel with common and private messages.\nA lot of this work was motivated by the work on the discrete memoryless broadcast channel; a lot of which was jointly developed with Amin Gohari. The authors are also grateful to Venkat Anantharam, Amin Gohari, Young-Han Kim, and Abbas El Gamal for their comments on early drafts and suggestions on improving the presentation."},"refs":[{"authors":[{"name":"H. Weingarten"},{"name":"Y. Steinberg"},{"name":"S. Shamai"}],"title":{"text":"The capacity region of the gaussian multiple-input multiple-output broadcast channel"}},{"authors":[{"name":"Y. Geng"},{"name":"C. Nair"}],"title":{"text":"The capacity region of the two-receiver vector gaussian broadcast channel with private and common messages"}},{"authors":[{"name":"T. Cover"}],"title":{"text":"Broadcast channels"}},{"authors":[{"name":"A. El Gama"},{"name":"Y.-H. Ki"}],"title":{"text":"Network Information Theory"}},{"authors":[{"name":"T. Liu"},{"name":"P. Viswanath"}],"title":{"text":"An extremal inequality motivated by mul- titerminal information-theoretic problems"}},{"authors":[{"name":"S. G. Ghurye"},{"name":"I. Olkin"}],"title":{"text":"A characterization of the multivariate normal distribution"}},{"authors":[{"name":"C. Nair"}],"title":{"text":"A note on outer bounds for broadcast channel"}},{"authors":[{"name":"K. Marton"}],"title":{"text":"A coding theorem for the discrete memoryless broadcast channel"}},{"authors":[{"name":"Y. Geng"},{"name":"A. Gohari"},{"name":"C. Nair"},{"name":"Y. Yu"}],"title":{"text":"The capacity region of classes of product broadcast channels"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564189.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T2.2","endtime":"17:20","authors":"Yanlin Geng, Chandra Nair","date":"1341248400000","papertitle":"The capacity region of the two-receiver vector Gaussian broadcast channel with private and common messages","starttime":"17:00","session":"S4.T2: Capacity of Broadcast Channels","room":"Kresge Auditorium (109)","paperid":"1569564189"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
