{"id":"1569566501","paper":{"title":{"text":"Combined Decode-Forward and Layered Noisy Network Coding Schemes for Relay Channels"},"authors":[{"name":"Peng Zhong"},{"name":"Mai Vu"}],"abstr":{"text":"Abstract\u2014We propose two coding schemes combining decode- forward (DF) and noisy network coding (NNC) with different ﬂavors. The ﬁrst is a combined DF-NNC scheme for the one- way relay channel which includes both DF and NNC as special cases by performing rate splitting, partial block Markov encoding and NNC. The second combines two different DF strategies and layered NNC for the two-way relay channel. One DF strategy performs coherent block Markov encoding at the source at the cost of power splitting at the relay, the other performs independent source and relay encoding but with full relay power, and layered NNC allows a different compression rate for each destination. Analysis and simulation show that both proposed schemes supersede each individual scheme and take full advantage of both DF and NNC."},"body":{"text":"The relay channel (RC) ﬁrst introduced by van der Meulen consists of a source aiming to communicate with a destination with the help of a relay. In [1], Cover and El Gamal propose the fundamental decode-forward (DF), compress-forward (CF) and combined DF-CF schemes. Lim, Kim, El Gamal and Chung recently put forward a noisy network coding (NNC) scheme [2] for the general multi-source network. NNC is based on CF relaying but involves three new ideas (message repetition, no Wyner-Ziv binning and simultaneous decoding) and outperforms CF for multi-source networks. In [3], Ra- malingam and Wang propose a superposition NNC scheme for restricted relay networks, in which source nodes cannot act as relays, by combining DF and NNC and show some performance improvement over NNC. Their scheme, however, does not include DF relaying rate because of no block Markov encoding.\nThe classical one-way relay channel can be generalized to the two-way relay channel (TWRC), in which two users exchange messages with the help of a relay. In [4], Rankov and Wittneben apply several relay strategies, including decode- forward and compress-forward, to the TRWC. In their pro- posed DF scheme, the two users perform partial block Markov encoding, and the relay sends a superposition of the codewords for the two decoded messages in each block. A different DF strategy is proposed in [5] by Xie, in which the users encode independently with the relay without block Markovity, and the relay sends a codeword for the random binning of the two decoded messages. These two DF schemes do not include each other in general. In [6], Lim, Kim, El Gamal and Chung propose an improved NNC scheme termed \u201dlayered noisy network coding\u201d (LNNC). The relay compresses its\nobservation into two layers: one is used at both destinations, while the other is only used at one destination.\nIn this paper, we ﬁrst propose a combined DF-NNC scheme for the one-way channel. Different from [3], our proposed scheme performs block Markov encoding and hence encom- passes both DF relaying and NNC as special cases. We then propose a combined DF-LNNC scheme for the TWRC. This scheme also includes partial block Markov encoding and, in addition, performs layered NNC. Analysis and numerical results show that this scheme outperforms each individual scheme in [4]\u2013[6] and also the combined scheme in [3].\nThe discrete memoryless two-way relay channel (DM- TWRC) is denoted by (X 1 ×X 2 ×X r , p (y 1 , y 2 , y r |x 1 , x 2 , x r ), Y 1 × Y 2 × Y r ) where (x 1 , y 1 ), (x 2 , y 2 ), (x r , y r ) are input and output signals of user 1, user 2 and the relay, respectively. A (n, 2 nR 1 , 2 nR 2 , P e ) code for a DM-TWRC consists of two message sets M 1 = [1 : 2 nR 1 ] and M 2 = [1 : 2 nR 2 ], three encoding functions f 1,i , f 2,i , f r,i , i = 1, . . . , n and two decoding function g 1 , g 2 deﬁned as follows:\nThe deﬁnitions for error probability, achievable rates and capacity follow standard ones in [7].\nThe one-way relay channel can be seen as a special case of the TWRC by setting M 2 = X 2 = Y 1 = ∅, f 2 = g 1 = null and R 2 = 0.\nwhere Z, Z r ∼ N (0, 1) are independent Gaussian noises, and g, g 1 , g 2 are the corresponding channel gains.\nY r = g r1 X 1 + g r2 X 2 + Z r , \t (2) where Z 1 , Z 2 , Z r ∼ N (0, 1) are independent Gaussian noises and g 12 , g 1r , g 21 , g 2r , g r1 , g r2 are the corresponding channel gains. For both channels, the average input power constraints at each user and the relay are all P .\nIn this section, we propose a coding scheme combing decode-forward [1] and noisy network coding [2] for the one-way relay channel. The source splits its message into two parts, a common and a private message. The common message is different in each block and is decoded at both the relay and destination as in decode-forward, while the private message is the same for all blocks and is decoded only at the destination as in noisy network coding. The source encodes the common message with block Markovity, then superimposes the private message on top. The relay decodes the common message at the end of each block and compresses the rest as in NNC. In the next block, it sends a codeword which encodes both the compression index and the decoded common message of the previous block. The destination decodes each common message by forward sliding-window decoding over two consecutive blocks. Then at the end of all blocks, it decodes the private message by simultaneous decoding over all blocks. Our proposed scheme includes both DF relaying and NNC as special cases and outperforms superposition NNC in [3] in that we use block Markov encoding for the common messages, which provides coherency between source and relay and improves the transmission rate.\nTheorem 1. The rate R = R 10 +R 11 is achievable for the one- way relay channel by combining decode-forward and noisy network coding\nR 10 ≤ min{I(Y r ; U|U r , X r ), I(U; Y |U r , X r ) + I(U r ; Y )} R 11 ≤ min{I(X; Y, ˆ Y r |U, U r , X r ), I(X, X r ; Y |U, U r )\nProof: We use a block coding scheme in which each user sends b − 1 messages over b blocks of n symbols each.\n1) Codebook generation: Fix a joint distribution as in (4). For each block j ∈ [1 : b]:\n\u2022 For each m j−1 , independently generate 2 nR 10 sequences u n j (m j |m j−1 ) ∼ n i=1 p (u i |u r,i ), m j ∈ [1 : 2 nR 10 ].\n\u2022 For each (m j−1 , m j ), independently generate 2 nbR 11 sequences x n j (m|m j , m j−1 ) ∼ n i=1 p (x i | u i , u r,i ), m ∈ [1 : 2 nbR 11 ].\n\u2022 For each (m j−1 , m j , k j−1 ), independently generate 2 n ˆ R sequences ˆy n r,j (k j |k j−1 , m j−1 , m j ) \t ∼\n2) Encoding: In block j, the source sends x n j (m|m j , m j−1 ). Assume that the relay has successfully found compression index k j−1 and decoded message m j−1 of the previous block, it then sends x n r,j (k j−1 |m j−1 ).\n3) Decoding at the relay: At the end of block j, upon receiving y n r,j , the relay ﬁnds a ˆ k j and a unique ˆ m j such that\nwhere T (n) denotes the strong typical set [7]. By the covering lemma and standard analysis, P e → 0 as n → ∞ if\n4) Decoding at the destination: At the end of each block j, the destination ﬁnds the unique ˆ m j−1 such that\nfor all j ∈ [1 : b] and some vector ˆk j ∈ [1 : 2 n ˆ R ] b . As in [2], P e → 0 as n → ∞ if\nR 11 ≤ min{I 1 , I 2 − ˆ R }, where \t (8) I 1 = I(X; Y, ˆ Y r |U, U r , X r )\nBy applying Fourier-Motzkin Elimination to inequalities (6)-(8), the rate in Theorem 1 is achievable.\nRemark 1. In relay decoding (5), we perform joint decoding of both the message and the compression index. If we use sequential decoding to decode the message ﬁrst and then to ﬁnd the compression index, we still get the same rate constraints as in Theorem 1.\nRemark 2. We can check that the rate in (3) is equivalent to the combined DF-CF rate in Theorem 7 [1] for the one- way relay channel. This combined DF-CF scheme is recently shown by Luo et al. [8] to outperform both individual schemes in the Gaussian channel for a small range of SNR. However, combined DF-NNC is expected to outperform combined DF- CF for multi-source networks.\nRemark 3. By setting U r = X r , U = X, ˆ Y r = 0, the rate in Theorem 1 reduces to the decode-forward relaying rate [1] as\nfor some p (x r )p(x|x r )p(y, y r |x, x r ). By setting U = U r = 0, it reduces to the NNC rate [2] as\nRemark 4. The rate constraints in Theorem 1 are similar to those in superposition NNC (Theorem 1 in [3]), but the code distribution (4) is a larger set because of the joint distribution between (x, u, u r ). Hence the achievable rate by the proposed scheme is higher than that in [3]. Speciﬁcally, the scheme in [3] does not include the decode-forward relaying rate in (10).\nWe now evaluate the achievable rate in Theorem 1 for the Gaussian one-way relay channel as in (1).\nCorollary 1. The following rate is achievable for the Gaussian one-way relay channel\nwhere S 1 , S 2 , S 3 , S 4 ∼ N (0, 1) and Z ∼ N (0, Q) are independent, and the power allocations satisfy constraint (12).\nIn this section, we propose a combined scheme based on both decode-forward strategies as in [4] [5] and layered noisy network coding [6] for the two-way relay channel. Each user splits its message into three parts: an independent common, a Markov common and a private message. The independent and Markov common messages are encoded differently at the source and are different for each block, both are decoded at both the relay and destination as in decode-forward. The pri- vate message is the same for all blocks and is decoded only at the destination as in noisy network coding. Each user encodes the Markov common message with block Markov encoding as in [4], then superimposes the independent common message on top of it without Markovity, and at last superimposes the private message on top of both. The relay decodes the two common messages and compresses the rest into two layers: a common and a reﬁnement layer. In the next block, the relay sends a codeword which encodes the two decoded common messages and two layered compression indices. Then at the end of each block, each user decodes two common messages of the other user by sliding-window decoding over two consecutive blocks. At the end of all blocks, one user uses the information of the common layer to simultaneously decode the private message of the other user, while the other\nuser uses the information of both the common and reﬁnement layers to decode the other user\u2019s private message.\nTheorem 2. Let R 1 denote the set of (R 1 , R 2 ) as follows: R 1 ≤ min{I 5 , I 12 } + min{I 5 − I 1 , I 16 }\nR 1 + R 2 ≤ min{min{I 5 , I 12 } + I 15 + I 18 − I 2 + min{I 6 , I 14 }, I 10 + I 15 + I 18 − I 2 ,\nI 10 + min{I 15 − I 1 , I 16 } + min{I 17 − I 2 , I 19 }} 2R 1 + R 2 ≤ min{I 5 , I 12 } + I 15 + I 18 − I 2 + I 10\np (u 2 |w 2 )p(v 2 |w 2 , u 2 )p(x 2 |w 2 , u 2 , v 2 )p(v r |w 1 , w 2 ) p (u r |v r , w 1 , w 2 )p(x r |u r , v r , w 1 , w 2 )\np (ˆy r , ˜y r |y r , x r , u r , v r , w 1 , w 2 , u 1 , v 1 , u 2 , v 2 ), \t (15) where I j are deﬁned in (16)-(21), then R 1 is achievable if user 2 only uses the common layer, while user 1 uses both the common and reﬁnement layers. If the two users exchange decoding layers, they can achieve a corresponding set R 2 . By time sharing, the convex hull of R 1 ∪ R 2 is achievable.\nProof: We use a block coding scheme in which each user sends b − 1 messages over b blocks of n symbols each.\n1) Codebook generation: Fix a joint distribution P ∗ as in (15). Each user l ∈ {1, 2} splits its message into three parts: m l0 , m l1 and m l2 . For each j ∈ [1 : b] and l ∈ {1, 2}\n\u2022 For each m l0,j−1 , m l0,j , m l1,j , independently gener- ate 2 nbR l2 sequences x n l,j (m l2 |m l1,j , m l0,j , m l0,j−1 ) ∼\n\u2022 For each pair (m 10,j−1 , m 20,j−1 ), independently gener- ate 2 n(R 11 +R 21 ) sequences v n r (K|m 10,j−1 , m 20,j−1 ) ∼ n\ni=1 p (v ri |w 1,i , w 2,i ), where K ∈ [1 : 2 n(R 11 +R 21 ) ]. Map each pair (m 11,j−1 , m 21,j−1 ) to one K.\n\u2022 For each vector m j−1 = (m 10,j−1 , m 20,j−1 , m 11,j−1 , m 21,j−1 ), independently generate 2 n ˜ R se- quences u n r,j (t j−1 |m j−1 ) ∼ n i=1 p (u r,i |v r,i , w 1,i , w 2,i ), t j−1 ∈ [1 : 2 n ˜ R ].\n\u2022 For each (t j−1 , m j−1 ), independently generate 2 n ˆ R se- quences x n r,j (l j−1 |t j−1 , m j−1 ) ∼ n i=1 p (x r,i |u r,i , v r,i , w 1,i , w 2,i ), l j−1 ∈ [1 : 2 n ˆ R ].\n\u2022 For each (t j−1 , m j−1 , m j ), independently generate 2 n ˜ R sequences ˜y n r,j (t j |t j−1 , m j−1 , m j ) ∼ n i=1 p (˜y r,i | u r,i , v r,i , w 1,i , w 2,i , u 1,i , u 2,i , v 1,i , v 2,i ), t j ∈ [1 : 2 n ˜ R ].\n\u2022 For each (t j , t j−1 , l j−1 , m j−1 , m j ), independently gen- erate 2 n ˆ R sequences ˆy n r,j (l j |l j−1 , t j , t j−1 , m j−1 , m j ) ∼ n\ni=1 p (ˆy r,i |˜y r,i , x r,i , u r,i , v r,i , w 1,i , w 2,i , u 1,i , u 2,i , v 1,i , v 2,i ), t j ∈ [1 : 2 n ˜ R ].\nm 11,j , m 21,j ). At the end of block j, the relay has decoded m j−1 , m j . Upon receiving y n r,j , it ﬁnds an index pair (ˆt j , ˆ l j ) such that\nAccording to Lemma 1 in [6], the probability that no such (ˆt j , ˆ l j ) exists goes to 0 as n → ∞ if\n˜ R > I ( ˜ Y r ; X r , Y r |U r , V r , U 1 , V 1 , U 2 , V 2 , W 1 , W 2 ) I 1 (16) ˜ R + ˆ R > I ( ˜ Y r ; X r , Y r |U r , V r , U 1 , V 1 , U 2 , V 2 , W 1 , W 2 )+\n3) Relay decoding: At the end of block j, the relay ﬁnds the unique ( ˆ m 10,j , ˆ m 20,j , ˆ m 11,j , ˆ m 21,j ) such that\nR 11 ≤ I(V 1 ; Y r |V r , W 1 , U 1 , W 2 , U 2 , V 2 ) I 3 R 21 ≤ I(V 2 ; Y r |V r , W 2 , U 2 , W 1 , U 1 , V 1 ) I 4\nR 10 + R 11 ≤ I(U 1 , V 1 ; Y r |V r , W 1 , W 2 , U 2 , V 2 ) I 5 R 20 + R 21 ≤ I(U 2 , V 2 ; Y r |V r , W 2 , W 1 , U 1 , V 1 ) I 6 R 11 + R 21 ≤ I(V 1 , V 2 ; Y r |V r , W 1 , U 1 , W 2 , U 2 ) I 7\nR 10 + R 11 + R 21 ≤ I(U 1 , V 1 , V 2 ; Y r |V r , W 1 , W 2 , U 2 ) I 8 R 20 + R 11 + R 21 ≤ I(U 2 , V 1 , V 2 ; Y r |V r , W 1 , W 2 , U 1 ) I 9 R 10 + R 20 + R 11 + R 21 ≤\n4) User decoding: At the end of block j, user 2 ﬁnds the unique ( ˆ m 10,j−1 , ˆ m 11,j−1 ) such that\nR 10 + R 11 ≤ I(U 1 , V 1 ; Y 2 |V r , W 1 , W 2 , U 2 , V 2 , X 2 ) + I(W 1 , V r ; Y 2 |W 2 , U 2 , V 2 , X 2 )\nAt the end of last block b, user 2 uses one compression layer to ﬁnd the unique ˆ m 12 such that\nfor all j ∈ [1 : b] and some vector ˆt j ∈ [1 : 2 n ˜ R ] b . As in [6], P e → 0 as n → ∞ if\nfor all j ∈ [1 : b] and some vectors ˆt j ∈ [1 : 2 n ˜ R ] b ,ˆ l j ∈ [1 : 2 n ˆ R ] b . As in [6], P e → 0 as n → ∞ if\nR 22 + ˜ R + ˆ R ≤ I(X 2 , X r ; Y 1 |X 1 , W 1 , U 1 , V 1 , W 2 , U 2 , V 2 , V r ) + I( ˆ Y r ; X 1 , X 2 , Y 1 | ˜ Y r , X r , U r , W 1 , U 1 , V 1 , W 2 , U 2 , V 2 , V r )\n+ I( ˆ Y r ; X 1 , X 2 , Y 1 | ˜ Y r , X r , U r , W 1 , U 1 , V 1 , W 2 , U 2 , V 2 , V r ) I 18 R 22 ≤ I(X 2 ; ˜ Y r , ˆ Y r , Y 1 |X 1 , U r , X r , W 1 , U 1 , V 1 , W 2 , U 2 , V 2 , V r )\nBy applying Fourier-Motzkin Elimination to inequalities (16)-(21), the rate region in Theorem 2 is achievable.\nRemark 5. By setting V 1 = W 2 = U 2 = V 2 = X 2 = U r = V r = ˜ Y r = 0, we obtain the rate for the one-way channel in Theorem 1 from the region in Theorem 2.\nRemark 6. The proposed combined DF-LNNC scheme in- cludes each schemes in [4]\u2013[6] as a special case. Speciﬁcally, it reduces to the scheme in [4] by setting U 1 = X 1 , U 2 = X 2 , V r = X r , V 1 = V 2 = U r = ˆ Y r = ˜ Y r = 0, to the scheme in [5] by setting V 1 = X 1 , V 2 = X 2 , V r = X r , W 1 = U 1 = W 2 = U 2 = U r = ˆ Y r = ˜ Y r = 0, and to the scheme in [6] by setting W 1 = U 1 = V 1 = W 2 = U 2 = V 2 = V r = 0.\nRemark 7. In our proposed scheme, the Markov common messages bring a coherent gain between the source and the relay, but they require the relay to split its power for each message because of superposition coding. For the independent common messages, the relay can use its whole power to send their bin index, which can then solely represent one message when decoding because of side information on the other message at each destination.\nRemark 8. Rate region for the Gaussian TWRC can be obtained by applying Theorem 2 with the following signaling:\nX 1 = α 1 S 1 + β 1 S 2 + γ 1 S 3 + δ 1 S 4 X 2 = α 2 S 5 + β 2 S 6 + γ 2 S 7 + δ 2 S 8\nα 2 1 + β 2 1 + γ 2 1 + δ 2 1 ≤ P, α 2 2 + β 2 2 + γ 2 2 + δ 2 2 ≤ P, α 2 31 + α 2 32 + β 2 3 + γ 2 3 + δ 2 3 ≤ P, \t (23)\nall S i ∼ N (0, 1) and ˆ Z r ∼ N (0, ˆ Q ), ˜ Z r ∼ N (0, ˜ Q ) are independent. The speciﬁc rate constraints for the Gaussian channel, however, are omitted because of the lack of space.\nWe numerically compare the performance of the proposed combined schemes with the original DF and NNC. The rate regions are obtained by exhaustive simulation, but optimiza- tion can also be used to achieve the maximum rates. Consider the Gaussian channels as in (1) and (2). Assume all the nodes are on a straight line. The relay is at a distance d from the source and distance 1 − d from the destination which makes g 1 = d −γ/2 and g 2 = (1 − d) −γ/2 , where γ is the path loss exponent. Figure 1 shows the achievable rate for the one-way relay channel with P = 10, γ = 3. The combined DF-NNC scheme supersedes both the DF and NNC schemes. It can achieve the capacity of the one-way relay channel when the relay is close to either the source or the destination. Figure 2 shows the sum rate for the two-way relay channel with P = 10, γ = 3. Our proposed scheme achieves larger sum rate than all 3 individual schemes when the relay is close to either user, while reducing to layered NNC when the relay is close to the middle of the two users. Figure 3 shows the achievable rate regions for the Gaussian TWRC using these 4 schemes. The achievable region of our proposed scheme encompasses all 3 individual schemes.\nWe have proposed two combined schemes: DF-NNC for the one-way and DF-LNNC for the two-way relay channels. Both schemes perform message splitting, block Markov encoding, superposition coding and noisy network coding. Each com- bined scheme encompasses all respective individual schemes (DF and NNC or LNNC) and strictly outperforms superpo- sition NNC in [3]. These are initial results for combining decode-forward and noisy network coding for a multi-source network."},"refs":[{"authors":[{"name":"T. Cover"},{"name":"A. El Gamal"}],"title":{"text":"Capacity theorems for the relay channel"}},{"authors":[{"name":"S. H. Lim"},{"name":"Y.-H. Kim"},{"name":"A. El Gamal"},{"name":"S.-Y. Chung"}],"title":{"text":"Noisy network coding"}},{"authors":[{"name":"N. Ramalingam"},{"name":"Z. Wang"}],"title":{"text":"Superposition noisy network coding"}},{"authors":[{"name":"B. Rankov"},{"name":"A. Wittneben"}],"title":{"text":"Achievable rate regions for the two-way relay channel"}},{"authors":[{"name":"L. Xie"}],"title":{"text":"Network coding and random binning for multi-user channels"}},{"authors":[{"name":"S. H. Lim"},{"name":"Y.-H. Kim"},{"name":"A. El Gamal"},{"name":"S.-Y. Chung"}],"title":{"text":"Layered noisy network coding"}},{"authors":[{"name":"A. E. Gama"},{"name":"Y.-H. Ki"}],"title":{"text":"Network information theory"}},{"authors":[{"name":"K. Luo"},{"name":"R. H. Gohary"},{"name":"H. Yanikomeroglu"}],"title":{"text":"On the generalization of decode-and-forward and compress-and-forward for Gaussian relay channels"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566501.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S8.T2.4","endtime":"18:00","authors":"Peng Zhong, Mai Vu","date":"1341337200000","papertitle":"Combined Decode-Forward and Layered Noisy Network Coding Schemes for Relay Channels","starttime":"17:40","session":"S8.T2: Relay Strategies for Network Communications","room":"Kresge Auditorium (109)","paperid":"1569566501"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
