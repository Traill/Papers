{"id":"1569565833","paper":{"title":{"text":"Is Non-Unique Decoding Necessary?"},"authors":[{"name":"Shirin Saeedi Bidokhti"},{"name":"Vinod M. Prabhakaran"},{"name":"Suhas. N. Diggavi"}],"abstr":{"text":"Abstract\u2014In mutiterminal communication systems, signals carrying messages meant for different destinations are often observed together at any given destination receiver. Han and Kobayashi (1981) proposed a receiving strategy which performs a joint unique decoding of messages of interest along with a subset of messages which are not of interest. It is now well-known that this provides an achievable region which is, in general, larger than if the receiver treats all messages not of interest as noise. Nair and El Gamal (2009) and Chong, Motani, Garg, and El Gamal (2008) independently proposed a generalization called indirect or non-unique decoding where the receiver uses the codebook structure of the messages to only uniquely decode its messages of interest. Indirect (non-unique) decoding has since been used in various scenarios. The main result in this paper is to provide an interpretation and a systematic proof technique for why indirect decoding, in all known cases where it has been employed, can be replaced by a particularly designed joint unique decoding strategy, without any penalty from a rate region viewpoint 1 ."},"body":{"text":"Coding schemes for multiterminal systems with many in- formation sources and many destinations try to exploit the broadcast and interference nature of the communication media. A consequence of this is that in many schemes the signals received at a destination carry information, not only about messages that are expected to be decoded at the destination (messages of interest), but also about messages that are not of interest to that destination.\nStandard methods in (random) code design (at the encoder) are rate splitting, superposition coding and Marton\u2019s coding [1], [2]. On the other hand, standard decoding techniques are successive decoding and joint unique decoding schemes [1], [3]. In [3], Han and Kobayashi proposed a receiving strategy which performs a joint unique decoding of messages of interest along with a subset of messages which are not of interest. We refer to a decoder with such a decoding strategy, as a joint unique decoder. It is now well-known that employing such a joint unique decoder in the code design provides an achievable region which is, in general, larger than if the receiver decodes the messages of interest while treating all messages not of interest as noise. Recently, Nair and El Gamal [4] and Chong, Motani, Garg, and El Gamal [5] independently proposed a generalization called indirect or non-unique decoding where the decoder looks for the unique messages of interest while\nusing the codebook structure of all the messages (including the ones not of interest). Such a decoder does not uniquely decode messages not of interest, though it might narrow it to a smaller list. We refer to such a decoder, as an indirect decoder. Coding schemes which employ indirect decoders have since played a role in achievability schemes in different multi- terminal problems such as [6], [7], [8], [9], [10]. It is of interest, therefore, to see if they can achieve higher reliable transmission rates compared to codes that employ joint unique decoders. In this paper, we develop our intuition and ideas within the framework of [4]. While much of the discussion in this paper is conﬁned to this framework, the technique applies more generally to problems studied in [8], [9], [10], as we show in [11].\nIn [4], the idea of indirect decoding is studied in the context of broadcast channels with degraded message sets. Nair and El Gamal consider a 3-receiver general broadcast channel where a source communicates a common message M 0 to three receivers Y 1 , Y 2 , and Y 3 and a private message M 1 only to one of the receivers, Y 1 (Fig 1). They characterize an inner-bound\nDecoder Decoder Decoder\n- - -\n- - - X M\nto the capacity region of this problem using indirect decoding and show its tightness for some special cases. It turns out that the same inner-bound of [4] can be achieved using a joint unique decoding strategy at all receivers. The equivalence of the rate region achievable by indirect decoding and that of joint unique decoding was observed in [4], but it was arrived at by comparing single letter expressions for the two rate regions 2 . This led the authors to express the hope that in general such an equivalence may not exist.\nIn this paper we will provide an interpretation together with a proof technique which, we believe, systematically, shows an equivalence between the rate region achievable through indirect decoders and joint unique decoders in several\nexamples. Our technique is based on designing a special auxiliary joint unique decoder which replaces the indirect decoder and sheds some light on why this equivalence holds. This line of argument is applicable to all known instances where non-unique (indirect) decoding has been employed in the literature [11]. However, we would like to note that analysis using non-unique decoding can often give a more compact representation of the rate-region \u2013 a fact observed in [4], [5] \u2013 which still makes it a valuable tool for analysis.\nWe start this section by brieﬂy reviewing the work of [4] where inner and outer bounds are derived for the capacity region of a 3-receiver broadcast channel with degraded mes- sage sets. In particular, we consider the case where a source communicates a common message (of rate R 0 ) to all receivers, and a private message (of rate R 1 ) only to one of the receivers. A coding scheme is a sequence of ((2 nR 0 , 2 nR 1 ), n) codes consisting of an encoder and a decoder and is said to achieve a rate-tuple (R 0 , R 1 ) if the probability of error at the decoders tend to zero as n → ∞.\nJoint unique decoder vs. Indirect decoder: We consider typical set decoding. A decoder at a certain destination may, in general, examine a subset of messages which includes, but is not necessarily limited to, the messages of interest to that desti- nation. By the term examine, we mean that the decoder will try to make use of the structure (of the codebook) associated with the messages it examines. We say a coding scheme employs a joint unique decoder if every decoder tries to uniquely decode all the messages they consider (and declare an error if there is ambiguity in any of the messages, irrespective of whether such messages are of interest to the destination or not). In contrast, we say that a coding scheme employs an indirect decoder if the decoder tries to decode uniquely only the messages of interest to the destination and tolerates ambiguity in messages which are not of interest. Within this framework, Proposition 5 of [4] establishes an achievable rate region of this problem through a coding scheme that employs an indirect decoder.\nIt turns out that employing a joint unique decoder, one can still achieve the same inner-bound of [4]. In this section, we develop a new proof technique to show this equivalence systematically. The same technique allows us to show the equivalence in all the examples considered in [11].\nA. Indirect decoding in the achievable scheme of Nair and El Gamal\nThe main problem studied in [4] is that of sending two messages over a 3-user discrete memoryless broadcast channel p(y 1 , y 2 , y 3 |x). The source intends to communicate messages M 0 and M 1 to receiver 1 and message M 0 to receivers 2 and 3. Rates of messages M 0 and M 1 are denoted by R 0 and R 1 , respectively. In [4] an inner-bound to the capacity region is proved using a standard encoding scheme based on superposition coding and Marton\u2019s coding, and indirect (or non-unique) decoding. We brieﬂy review this scheme.\n1) Random codebook generation and encoding: To design the codebook, split the private message M 1 into four indepen- dent parts, M 10 , M 11 , M 12 , and M 13 of non-negative rates S 0 , S 1 , S 2 , S 3 , respectively. Let R 1 = S 0 + S 1 + S 2 + S 3 , T 2 ≥ S 2 and T 3 ≥ S 3 . Fix a joint probability distribution p(u, v 2 , v 3 , x).\nRandomly and independently generate 2 n(R 0 +S 0 ) sequences U n (m 0 , s 0 ), m 0 ∈ [1 : 2 nR 0 ] and s 0 ∈ [1 : 2 nS 0 ], each distributed uniformly over the set of typical sequences U n . For each sequence U n (m 0 , s 0 ), generate randomly and con- ditionally independently (i) 2 nT 2 sequences V n 2 (m 0 , s 0 , t 2 ), t 2 ∈ [1 : 2 nT 2 ], each distributed uniformly over the set of conditionally typical sequences V n 2 , and (ii) 2 nT 3 se- quences V n 3 (m 0 , s 0 , t 3 ), t 3 ∈ [1 : 2 nT 3 ], each distributed uniformly over the set of conditionally typical sequences V n 3 . Randomly partition sequences V n 2 (m 0 , s 0 , t 2 ) into 2 nS 2 bins B 2 (m 0 , s 0 , s 2 ) and sequences V n 3 (m 0 , s 0 , t 3 ) into 2 nS 3 bins B 3 (m 0 , s 0 , s 3 ). In each product bin B 2 (m 0 , s 0 , s 2 ) × B 3 (m 0 , s 0 , s 3 ), choose one (random) jointly typical sequence pair (V n 2 (m 0 , s 0 , t 2 ), V n 3 (m 0 , s 0 , t 3 )). If there is no such pair, declare an error whenever the message (m 0 , s 0 , s 2 , s 3 ) is to be transmitted. Finally for each chosen jointly typical pair (V n 2 (m 0 , s 0 , t 2 ), V n 3 (m 0 , s 0 , t 3 )) in each product bin (s 2 , s 3 ), randomly and conditionally independently generate 2 nS 1 se- quences X n (m 0 , s 0 , s 2 , s 3 , s 1 ), s 1 ∈ [1 : 2 nS 1 ], each dis- tributed uniformly over the set of conditionally typical X n sequences. To send the message pair (m 0 , m 1 ), where m 1 is expressed as (s 0 , s 1 , s 2 , s 3 ), the encoder sends the codeword X n (m 0 , s 0 , s 2 , s 3 , s 1 ).\n2) Indirect decoding: Receiver Y 1 jointly uniquely decodes all messages M 0 , M 10 , M 11 , M 12 , and M 13 . Receivers Y 2 and Y 3 , however, decode M 0 indirectly. More precisely,\n\u2022 Receiver Y 1 declares that the message tuple (m 0 , s 0 , s 2 , s 3 , s 1 ) was sent if it is the unique quintuple such that the received signal Y n 1 is jointly typical with \t (U n (m 0 , s 0 ), V n 2 (m 0 , s 0 , t 2 ), V n 3 (m 0 , s 0 , t 3 ), X n (m 0 , s 0 , s 2 , s 3 , s 1 )), where index s 2 is the product bin number of V n 2 (m 0 , s 0 , t 2 ) and index s 3 is the product bin number of V n 3 (m 0 , s 0 , t 3 ).\n\u2022 Receiver Y 2 declares that the message pair (M 0 , M 10 ) = (m 0 , s 0 ) was sent if it ﬁnds a unique pair of indices (m 0 , s 0 ) for which the received signal Y n 2 is jointly typ- ical with (U n (m 0 , s 0 ), V n 2 (m 0 , s 0 , t 2 )) for some index t 2 ∈ [1 : 2 nT 2 ].\n\u2022 Receiver Y 3 is similar to receiver Y 2 with V 3 and t 3 , respectively, instead of V 2 and t 2 .\nThe above encoding/decoding scheme achieves rate pairs (R 0 , R 1 ) for which inequalities (1) to (10) below are satisﬁed for a joint distribution p(u, v 2 , v 3 , x). The reader is referred to [4] for the analysis of the error probabilities.\nR 1 = S 0 + S 1 + S 2 + S 3 \t (1) S 0 , S 1 , S 2 , S 3 ≥ 0, T 2 ≥ S 2 , T 3 ≥ S 3 \t (2) T 2 + T 3 ≥ S 2 + S 3 + I(V 2 ; V 3 |U ) \t (3) S 1 ≤ I(X; Y 1 |U, V 2 , V 3 ) \t (4)\nS 1 + S 2 ≤ I(X; Y 1 |U, V 3 ) \t (5) S 1 + S 3 ≤ I(X; Y 1 |U, V 2 ) \t (6) S 1 + S 2 + S 3 ≤ I(X; Y 1 |U ) \t (7) R 0 + S 0 + S 1 + S 2 + S 3 ≤ I(X; Y 1 ) \t (8) R 0 + S 0 + T 2 ≤ I(U, V 2 ; Y 2 ) \t (9) R 0 + S 0 + T 3 ≤ I(U, V 3 ; Y 3 ). \t (10)\nB. Joint unique decoding sufﬁces in the achievable scheme of Nair and El Gamal\nFix the codebook generation and encoding scheme to be that of Section II-A. We will demonstrate how a joint unique decoding scheme sufﬁces by following these steps:\n(1) We ﬁrst analyze the indirect decoder to characterize regimes where it uniquely decodes all the messages it considers and regimes where it decodes some of the messages non-uniquely.\nLet the rate pair (R 0 , R 1 ) be such that the indirect decoder of receiver Y 2 decodes message M 0 with high probability; i.e., the indirect decoding constraint (9) is satisﬁed. Consider the following two regimes:\n(a) R 0 + S 0 < I(U ; Y 2 ). (b) R 0 + S 0 > I(U ; Y 2 ),\nIn regime (a), it is clear from the deﬁning condition that a joint unique decoder which decodes (M 0 , M 10 ) = (m 0 , s 0 ) by ﬁnding the unique sequence U n (m 0 , s 0 ) such that (U n (m 0 , s 0 ), Y n 2 ) is jointly typical will succeed with high probability. This is the joint unique decoder we may use in place of the indirect decoder for this regime. Notice that in this regime, while the indirect decoder obtains (m 0 , s 0 ) uniquely with high probability, it may not necessarily succeed in uniquely decoding t 2 . Indeed, in this regime insisting on joint unique decoding of U n (m 0 , s 0 ), V n 2 (m 0 , s 0 , t 2 ) could, in some cases, result in a strictly smaller achievable region.\nRegime (b) is the more interesting regime. Here it is clear that simply decoding for (M 0 , M 10 ) and treating all other messages as noise will not work. Indirect decoding must indeed be taking advantage of the codeword V n 2 as well. The indirect decoder looks for a unique pair of mes- sages (m 0 , s 0 ) such that there exists some t 2 for which (U n (m 0 , S 0 ), V n (m 0 , s 0 , t 2 ), Y n 2 ) is jointly typical. One may, in general, expect that there could be several choices of t 2 even in this regime. An important observation is that, in this regime, there is (with high probability) only one choice for t 2 . In other words, in this regime, receiver 2 decodes t 2 uniquely along with m 0 and s 0 . To see this, notice that using inequality (9), we have\nInequalities (9) and (11) together guarantee that a joint unique decoder can decode messages M 0 , M 10 , and M 12 with high probability; In other words, in regime (b) the indirect decoder ends up with a unique decoding of the satellite codeword V n 2 (m 0 , s 0 , t 2 ) with high probability. i.e., we may replace the indirect decoder with a joint unique decoder for messages M 0 , M 10 , M 12 . To summarize loosely, whenever the indirect decoder is forced to derive information from the codeword V n 2 (i.e., when treating V n 2 as noise will not result in correct decoding), the indirect decoder will recover this codeword also uniquely. We make this loose intuition more concrete in Section II-C.\nThe same argument goes through for receiver Y 3 and this shows that insisting on jointly uniquely decoding at all receivers is not restrictive in this problem. Thus, we arrive at the following:\nTheorem 1: For every rate pair (R 0 , R 1 ) satisfying the inner-bound of (1)-(10), there exists a coding scheme employ- ing a joint unique decoder which achieves the same rate pair.\nThe idea behind the proof of Theorem 1 was simple and general. Consider an indirect decoder which is decoding some messages of interest. The message of interest in our problem is M 0 . Along with this message of interest, the decoder might also decode certain other messages, M 10 and M 12 for example. The two main steps of the proof is then as follows.\n(1) Analyze the indirect decoder to determine what messages it decodes uniquely. Depending on the regime of opera- tion, the indirect decoder ends up uniquely decoding a subset of its intended messages, and non-uniquely the rest of its intended messages. For example in regime (a) above, the indirect decoder uniquely decodes only M 0 and M 10 and it might not be able to settle on M 12 . While in regime (b), the indirect decoder ends up decoding all of its three messages M 0 , M 10 , and M 12 uniquely.\n(2) In each regime of operation characterized in step (1), use a joint unique decoder to only decode the messages that the indirect decoder uniquely decodes. In the above proof, this would be a joint unique decoder that decodes M 0 and M 10 in regime (a) and a joint unique decoder that decodes messages M 0 , M 10 , and M 12 in regime (b). Verify that the resulting joint unique decoder does support the corresponding part of the rate region achieved by the indirect decoding scheme.\nThough the idea is generalizable, analyzing the indirect decoder in step (1) is a tedious task. Even for this very speciﬁc problem, it may not be entirely clear how the condition dividing cases (a) and (b) can be derived. Next, we try to resolve this using an approach which generalizes more easily.\nWe take an alternative approach in this section to prove Theorem 1. The proof technique we present here has the same spirit as the proof in Section II-B, but the task of determining which subset of messages should be decoded in what regimes will be implicit rather than explicit as before. To this end, we introduce an auxiliary decoder which serves\nas a tool to help us develop the proof ideas. We do not propose this more complicated auxiliary decoder as a new decoding technique, but only as a proof technique to show sufﬁciency of joint unique decoding in the problem of [4]. We analyze the auxiliary decoder at receiver Y 2 and show that under the random coding experiment, it decodes correctly with high probability if the indirect decoding constraint (9) holds. From this auxiliary decoder and its performance, we will then be able to conclude that there exists a joint unique decoding scheme that succeeds with high probability.\nWe now deﬁne the auxiliary decoder. The auxiliary decoder at receiver Y 2 is a more involved decoder which has access to two component (joint unique) decoders:\n\u2022 Decoder 1 is a joint unique decoder which decodes messages M 0 and M 10 . It ﬁnds M 0 , and M 10 by looking for the unique sequence U n (m 0 , s 0 ) for which the pair (U n (m 0 , s 0 ), Y n 2 ) is jointly typical, and declares an error if there exists no such unique sequence.\n\u2022 Decoder 2 is a joint unique decoder which de- codes messages M 0 , M 10 and M 12 . It ﬁnds M 0 , M 10 , and M 12 by looking for the unique sequences U n (m 0 , s 0 ) and V n 2 (m 0 , s 0 , t 2 ) such that the triple (U n (m 0 , s 0 ), V n 2 (m 0 , s 0 , t 2 ), Y n 2 ) is jointly typical, and declares an error when such sequences do not exist.\nThe auxiliary decoder declares an error if either (a) both component decoders declare errors, or (b) if both of them decode and their decoded (M 0 , M 10 ) messages do not match. In all other cases it declares the (M 0 , M 10 ) output of a component decoder which did not declare an error as the decoded message.\nWe analyze the error probability under the random coding experiment of such an auxiliary decoder at receiver Y 2 and prove that for any > 0, there is a large enough n such that\nInequality (12) shows that for large enough n and under the indirect decoding constraint (9), the auxiliary decoder has an arbitrary small probability of failure.\nWe start by stating the following lemma and the reader is referred to [11] for the proof.\nLemma 1: Fix the probability distribution p U,V,Y (u, v, y) and the typical set T n (U, V, Y ) corresponding to it. Consider a quadruple of sequences (U n , ˜ U n , ˆ V n , Y n ), such that\n\u2022 ˜ U n is independent of (U n , ˆ V n , Y n ) and has the distribu- tion i p U (˜ u i ),\n\u2022 (U n , Y n ) has the joint distribution i p U,Y (u i , y i ), \u2022 (U n , ˆ V n ) has the joint distribution i p U,V (u i , ˆ v i ).\nThen, probability Pr(( ˜ U n , Y n ) ∈ T n (U, Y ), (U n , ˆ V n , Y n ) ∈ T n (U, V, Y )) is upper-bounded by 2 −n(I(U,V ;Y )−6 ) .\nAssume now that (m 0 , s 0 , s 1 , s 2 , s 3 ) = (1, 1, 1, 1, 1) is sent and indices t 1 and t 2 in the encoding procedure are (t 2 , t 3 ) = (1, 1). We analyze in the rest of this section the probability that\nreceiver Y 2 declares M 0 = 1. By the symmetry of the random code construction, the conditional probability of error does not depend on which tuple of indices is sent. Thus, the conditional probability of error is the same as the unconditional probability of error and there is no loss of generality in our assumption.\nConditioned \t on \t (m 0 , s 0 , s 1 , s 2 , s 3 , t 2 , t 3 ) \t = (1, 1, 1, 1, 1, 1, 1), receiver Y 2 makes an error in decoding M 0 only if at least one of the following events occur:\nE 1 : The channel is atypical: the triple (U n (1, 1), V n 2 (1, 1, 1), Y n 2 ) is not jointly typical.\nE 2 : The ﬁrst or the second decoder (uniquely) decodes, but incorrectly: there is a unique pair ( ˜ m 0 , ˜ s 0 ) = (1, 1) such that the triple (U n ( ˜ m 0 , ˜ s 0 ), Y n 2 ) is jointly typical, or there is a unique triple ( ˆ m 0 , ˆ s 0 , ˆ t 2 ) = (1, 1, 1) such that (U n ( ˆ m 0 , ˆ s 0 ), V n 2 ( ˆ m 0 , ˆ s 0 , ˆ t 2 ), Y n 2 ) is jointly typical.\nE 3 : Both decoders fail to decode uniquely and declare errors: there are at least two distinct pairs ( ˜ m 0 , ˜ s 0 ) and ( ˘ m 0 , ˘ s 0 ) such that both pairs (U n ( ˜ m 0 , ˜ s 0 ), Y n 2 ) and (U n ( ˘ m 0 , ˘ s 0 ), Y n 2 ) are jointly typical; and similarly there are at least two distinct triples ( ˆ m 0 , ˆ s 0 , ˆ t 2 ) and ( ˇ m 0 , ˇ s 0 , ˇ t 2 ) such that both triples (U n ( ˆ m 0 , ˆ s 0 ), V n 2 ( ˆ m 0 , ˆ s 0 , ˆ t 2 ), Y n 2 ) and (U n ( ˇ m 0 , ˇ s 0 ), V n 2 ( ˇ m 0 , ˇ s 0 , ˇ t 2 ), Y n 2 ) are jointly typical.\nTherefore, the probability that receiver Y 2 makes an error is upper-bounded in terms of the above events:\nwhere \t (13) \t follows \t because \t Pr(E 1 ) \t = Pr((U n (1, 1), V n 2 (1, 1, 1), Y n 2 ) / ∈ T n ) ≤ \t (ensured by the encoding and the Asymptotic Equipartition Property), and Pr(E 2 |E 1 ) = 0. To upper-bound Pr(E 3 ), we write\n  \n(U n ( ˆ m 0 , ˆ s 0 ), V n 2 ( ˆ m 0 , ˆ s 0 , ˆ t 2 ), Y n 2 ) ∈ A n for some ( ˆ m 0 , ˆ s 0 , ˆ t 2 ) = (1, 1, 1)\n  \n  \n(U n ( ˆ m 0 , ˆ s 0 ), V n 2 ( ˆ m 0 , ˆ s 0 , ˆ t 2 ), Y n 2 ) ∈ A n for some ( ˆ m 0 , ˆ s 0 ) = (1, 1) and ˆ t 2\n  \n  \n(U n ( ˆ m 0 , ˆ s 0 ), V n 2 ( ˆ m 0 , ˆ s 0 , ˆ t 2 ), Y n 2 ) ∈ A n are s.t. ( ˆ m 0 , ˆ s 0 ) = (1, 1) with at least one s.t. ˆ t 2 = 1\n  \nIn the above chain of inequalities, (a) holds because event E 3 is a subset of the event in the right hand side.\nIt is worthwhile to interpret inequality (14). The error event of interest, roughly speaking, is partitioned into the following two events:\n(1) The auxiliary decoder makes an error and the indirect decoder of Section II-A also makes an error.\n(2) The auxiliary decoder makes an error but the indirect decoder of Section II-A decodes correctly. We will show that the probability of this event is small. Note that under this error event, (a) component decoder 1 fails (i.e., it is not possible to decode (M 0 , M 10 ) by treating V n 2 as noise), but still (b) indirect decoder succeeds (i.e., the indirect decoder must be deriving useful information by considering V n 2 ). By showing that this error event has a small probability, we in effect show that whenever (a) and (b) hold, it is possible to jointly uniquely decode the V n 2\ncodeword as well. This makes the rough intuition from Section II-B more concrete.\nTo bound the error probability we bound the two terms of inequality (14) separately. First term of (14) is bounded by the probability of the indirect decoder making an error:\n( ˆ m 0 , ˆ s 0 , ˆ t 2 ), Y n 2 ) ∈ A n for some ( ˆ m 0 , ˆ s 0 ) = (1, 1) and ˆ t 2\n  \n(U n ( ˆ m 0 , ˆ s 0 ), V n 2 ( ˆ m 0 , ˆ s 0 , ˆ t 2 ), Y n 2 ) ∈ A n are s.t. ( ˆ m 0 , ˆ s 0 ) = (1, 1) with at least one s.t. ˆ t 2 = 1\n  \nwhere we have ( ˜ m 0 , ˜ s 0 ) = 1 and ˆ t 2 = 1 in the event in inequality (16) and (a) follows from Lemma 1.\nWe conclude the error probability analysis by putting to- gether inequalities (13), (14), (15), and (17) to obtain that the error probability at the auxiliary decoder is bounded as in inequality (12). So for large enough n, the auxiliary decoder succeeds with high probability if the indirect decoding constraint (9) is satisﬁed; i.e., when the indirect decoder succeeds with high probability.\nOne can now argue that if the auxiliary decoder succeeds with high probability for an operating point, then there also exists a joint unique decoding scheme that succeeds with high probability. The idea is that for all operating points (except in a subset of the rate region of measure zero), each of the two component joint unique decoders 1 and 2 have either a high or a low probability of success. So, if the operating point is such that the auxiliary decoder decodes correctly with high probability, then at least one of the component decoders should also decode correctly with high probability, giving us the joint unique decoding scheme we were looking for. This is summarized in Lemma 2, and the reader is referred to [11] for the proof.\nLemma 2: Given any operating point (except in a subset of the rate region of measure zero), if the auxiliary decoder\nsucceeds with high probability under the random coding experiment, then there exists a joint unique decoding scheme that also succeeds with high probability.\nA similar argument goes through for receiver Y 3 . The random coding argument for the joint unique decoding scheme can now be completed as usual.\nRemark 1: In Sections II-B and II-C, we did not consider cases where R 0 + S 0 = I(U ; Y 2 ) or R 0 + S 0 = I(U ; Y 3 ) (i.e., a subset of measure zero). This is enough since we may get arbitrarily close to such points.\nRemark 2: In Sections II-B and II-C, we ﬁxed the encoding scheme to be that of [4]. The message splitting and the structure of the codebook is therefore a priori assumed to be that of [4], even when R 0 + S 0 < I(U ; Y 2 ) and message M 12 is not jointly decoded at Y 2 . However, in such cases this extra message structure is not required and one can consider message M 12 as a part of message M 11 .\nWe refer the readers to [11] for details on how we may adapt this technique to several instances [8], [9], [10] of non- unique decoding in the literature. We believe this technique may be applicable more generally to show the equivalence of rate regions achievable using random coding employing an indirect decoder and a joint unique decoder."},"refs":[{"authors":[{"name":"A. El Gama"},{"name":"Y. H. Ki"}],"title":{"text":"Network Information Theory"}},{"authors":[{"name":"K. Marton"}],"title":{"text":"A coding theorem for the discrete memoryless broadcast channel"}},{"authors":[{"name":"T. Han"},{"name":"K. Kobayashi"}],"title":{"text":"A new achievable rate region for the in- terference channel"}},{"authors":[{"name":"C. Nair"},{"name":"A. El Gamal"}],"title":{"text":"The capacity region of a class of 3-receiver broadcast channels with degraded message sets"}},{"authors":[{"name":"H. F. Chong"},{"name":"M. Motani"},{"name":"H. K. Garg"},{"name":"H. El Gamal"}],"title":{"text":"On the Han- Kobayashi region for the interference channel"}},{"authors":[{"name":"S. Avestimehr"},{"name":"S. N. Diggavi"},{"name":"D. N. C. Tse"}],"title":{"text":"Wireless network information ﬂow: a deterministic approach"}},{"authors":[{"name":"S. Lim"},{"name":"Y.-H. Kim"},{"name":"A. El-Gamal"}],"title":{"text":"Noisy network coding"}},{"authors":[{"name":"Y. K. Chia"},{"name":"A. El Gamal"}],"title":{"text":"3-receiver broadcast channels with common and conﬁdential messages"}},{"authors":[{"name":"B. Bandemer"},{"name":"A. El Gamal"}],"title":{"text":"Interference decoding for deterministic channels"}},{"authors":[{"name":"C. Nair"},{"name":"A. El Gamal"},{"name":"Y. K. Chia"}],"title":{"text":"An achievability scheme for the compound channel with state noncausally available at the encoder"}},{"authors":[{"name":"S. Saeedi"},{"name":"V. M. Prabhakaran"},{"name":"S. N. Diggavi"}],"title":{"text":"Is non- unique decoding necessary?"}},{"authors":[{"name":"J. Ebrahimi"},{"name":"C. Fragouli"}],"title":{"text":"Combinatorial algorithms for wireless information ﬂow"}},{"authors":[{"name":"M. Goemans"},{"name":"S. Iwata"},{"name":"R. Zenklusen"}],"title":{"text":"An algorithmic framework for wireless information ﬂow"}},{"authors":[{"name":"G. Kramer"},{"name":"J. Hou"}],"title":{"text":"On message lengths for noisy network coding"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565833.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T2.5","endtime":"16:20","authors":"Shirin Saeedi Bidokhti, Vinod M Prabhakaran, Suhas Diggavi","date":"1341244800000","papertitle":"Is Non-Unique Decoding Necessary?","starttime":"16:00","session":"S3.T2: Three-Receiver Broadcast Channels","room":"Kresge Auditorium (109)","paperid":"1569565833"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
