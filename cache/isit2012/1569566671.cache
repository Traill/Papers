{"id":"1569566671","paper":{"title":{"text":"Joint Source-Channel Coding of one Random Variable over the Poisson Channel"},"authors":[{"name":"Albert No"},{"name":"Kartik Venkat"},{"name":"Tsachy Weissman"}],"abstr":{"text":"Abstract\u2014We study the transmission of a single random vari- able across the Poisson channel, which takes a continuous-time waveform {λ t : 0 ≤ λ t ≤ T } as an input, where 0 ≤ λ t ≤ A, for all 0 ≤ t ≤ T . The output of the channel is a non-homogeneous Poisson arrival process with rate λ T . We explore the class of schemes that are optimal in the distortion exponent sense under mean squared loss. We determine a family of optimal encoders for this channel which achieves the minimum mean squared error. In addition, we characterize the distortion exponent for \u2018separation\u2019 based schemes, and also provide an upper bound for the maximal distortion exponent across all joint source channel strategies under this setting."},"body":{"text":"The literature abounds with results pertaining to the Poisson channel, and its numerous applications in ﬁelds ranging from communication theory to neuroscience. Also known as the direct detection optical channel, it is widely studied as the canonical model for optical communication. Several systems based on this channel model have been described in the litera- ture (cf. [1], [2]). The Poisson channel has also been associated with a signiﬁcant body of results in information theory. In [3] and [4], Wyner characterized the capacity and error exponent for the channel, while also providing an explicit construction of a family of exponentially optimal codes for the same. More recently, Atar et. al. in [5], have discovered fundamental links between classical information theoretic quantities and the average estimation loss (under a natural loss function) for this channel.\nThe Poisson channel has a fundamental role to play in \u2018sensory processing\u2019 in neuroscience, where sensory data in the form of neural spike trains are modeled by a Poisson arrival processes (cf. [6]). [7] discusses \u2018efﬁcient recoding\u2019 of information in sensory pathways, which has ramiﬁcations in biological signal processing. Some of the fundamental questions that one wishes to investigate in the biological context are: \u201cto what extent does optimal encoding depend on the statistics of the stimulus?\u201d, and \u201cwhether separation of source and channel is optimal or not?\u201d In this paper, we seek to address some of these questions, from an information theoretic angle.\nWe investigate the problem of transmitting a single ran- dom variable X (representing the environment in the neuro- scientiﬁc context) across the Poisson channel, where our benchmark for performance is the minimum mean squared loss in estimating X based on the channel output process.\nNote that as the time duration of channel use T increases, we expect the loss in estimating X to converge to zero. The \u201cdistortion exponent\u201d of the joint source-channel coding strat- egy captures the rate at which this loss decays exponentially to zero. A larger distortion exponent also indicates a lower system complexity for achieving a target average estimation loss. Thus, a transmission scheme that achieves the maximal distortion exponent is highly desirable.\nThe rest of the paper is organized as follows. In Section II, we provide a detailed description of the problem. Section III deals with the optimal encoding of a random variable for the Poisson channel under a mean squared loss criterion. Our main result shows that a simple family of binary valued encoders is optimal, in a manner that will be made precise. In Section IV, we introduce the notion of separation based schemes for the Poisson channel and explicitly characterize the distortion exponent for such schemes. We also provide an upper bound on the maximal distortion exponent over all joint source channel schemes. We conclude with a summary of our ﬁndings in Section V.\nWe consider the Poisson Channel under a peak power constraint. The channel input is a waveform λ(t), 0 ≤ t ≤ T (which we shall denote throughout, using the shorthand λ T ) which satisﬁes 0 ≤ λ(t) ≤ A, where the parameter A is the peak power. λ(·) deﬁnes the rate function of the output Poisson process, Y T . We consider the problem of transmitting a scalar random variable 1 X, under a minimum mean squared loss criterion. Thus, we are in the setting of a joint source-channel coding problem (see Fig. 1). Our goal is to come up with a coding strategy that achieves the maximal distortion exponent for the Poisson channel.\n1) Distortion Exponent: The distortion exponent α is de- ﬁned by,\nwhere the inﬁmum is over all joint source-channel coding schemes, or more explicitly encoders and decoders, formally deﬁned as follows.\n2) Encoder: Let F [0, T ] denote the set of measurable nonnegative waveforms that reside on the interval [0, T ] and are bounded above by a ﬁxed constant A. The encoder is described by a mapping\n3) Decoder: Let S(T ) denote the space of nondecreas- ing, nonnegative integer-valued step functions on the interval [0, T ]. Thus the output of the channel Y T ∈ S(T ). A decoder is a measurable mapping\nIn the above discussion, we presented the optimal distortion exponent as the target of a good coding strategy. Note that a much stricter criterion for performance would be to achieve a minimum average loss performance, i.e. competing not only in the limiting exponent of the mean squared error for large T , but in the actual expected estimation loss itself, for all values of T .\nThe mean squared error for a given encoder-decoder pair {λ(·), ˆ X(·)} is given by\nIn this scenario, we know that the Bayes estimator minimizes the mean squared loss and is given by the conditional expec- tation of X given the observations Y T . I.e.,\nIn principle, the optimal performance benchmark would be deﬁned as the inﬁmum of the mean squared error, over all possible encoding and decoding functions. Denote by ˜ F [0, T ] the space of right continuous bounded (by constant A) func- tions on the interval [0, T ]. Deﬁne,\nWe now decompose the problem into two parts. We ﬁrst attempt to describe an efﬁcient coding strategy for the Poisson channel, that will be optimal according to the criterion in (6). The second part of the problem is to understand the notion of separation based schemes in this setting, and the corresponding distortion exponent. We discuss these two problems in sections III and IV respectively.\nIn this section, we consider the problem of encoding a random variable to be transmitted across the Poisson channel. Before stating our main theorem, we describe below the family of encoders discussed in [3]. We will use this family of codes and argue that they are essentially optimal in the sense of minimum expected squared loss.\nWyner introduced a family of exponentially optimal codes. We describe the main features of the code below, and refer the reader to [3] for detailed discussions and implications of the same. For ∆ > 0 deﬁne W [0, T ] as follows,\nIn other words, W [0, T ] is the set of encoding wave- forms, denoted by λ T , which are constant on the interval [(i − 1) , i ) for every i ∈ {1, 2, · · · T ∆ } and takes only the values 0 or A. If encoder maps to some λ T ∈ W [0, T ], we say that encoder employs \u201cWyner delta coding strategy\u201d. In order to be explicit, we denote the corresponding output process of the Poisson channel as Y T , whenever the encoder uses the Wyner delta coding strategy. This setting is illustrated in Fig. 2\nHaving set up the notation for the problem, we now describe our main result for this section below:\nThe above result proves that there exists a {0, A} valued equally-spaced binary waveform, that achieves the best per- formance across all encoders in a rich class of encoders for the Poisson Channel. It is striking that this class of schemes is rich enough to guarantee minimum mean squared loss for the continuous time Poisson channel.\nBelow we present two different proofs for Theorem 1, the latter exploiting a recently uncovered information-estimation identity for the Poisson channel.\nFor a given encoding strategy λ T ∈ ˜ F [0, T ], we will construct a Wyner delta approximation λ T ∈ W [0, T ], as follows. For i ≤ t < (i + 1) , deﬁne\nA if (i+1) 0 \t λ s ds > i 0 λ ,s ds + A 0 otherwise\n1) Constructing of an Approximate Estimator: Let N de- note a homogeneous Poisson process with rate 1. Let Y t be a non-homogeneous Poisson process with rate function λ T . It is easy to observe that Y t has a same probability distribution as N t\n. Thus, we can view the Y T process as a function of N and λ T , i.e. Y T = F (N, λ T ). Similarly, we can deﬁne Y T = F (N, λ T ) so that both Y T and Y T are in the same probability space.\nWe wish to argue that the estimator based on the λ ∆ encoding scheme performs as well as the original estimator based on the λ process. Before stating the result, we need to equip the space of output processes S(T ) with a suitable metric, which would then allow us to discuss notions of convergence and continuity on this space. Since Y T is arrival process, it is natural to deﬁne the metric based on arrival times. Deﬁne a(Y T ) to be the number of arrivals of the Y T process. Then, d : S(T ) × S(T ) → [0, ∞] be deﬁned as follows,\n(13) where Y T has arrivals at times {t i } N i=1 and Y T has arrivals at times {t ,i } N i=1 . It is easy to check that d(·, ·) is a metric.\nWe are now ready to state the following Lemma. Lemma 2: lim →0 Y T = Y T almost surely.\n. Now from (10), ∃ ∆ small enough, such that the number of arrivals of the Y T and Y T processes are same, i.e. d(Y T , Y T ) < ∞ almost surely.\nLet N T 0 λ s ds have arrival times τ 1 , τ 2 , · · · , τ n . Deﬁne t i and t ,i such that\nBy applying equations (11) and (12), it is clear that 0 ≤ t i − t ,i < 2 , and therefore, d(Y T , Y T ) < 2 which converges to 0 as → 0.\nArmed with Lemma 2, we now wish to establish the right continuity of the Bayes estimator.\nDeﬁnition 1 (Right continuity of an Estimator): An estimator ˆ X : S(T ) → R is right continuous if\nfor every {Y T n } n≥1 , Y T ∈ S(T ) such that d(Y T n , Y T ) → 0 and the arrival times of Y T and Y T n , which are denoted by {t 1 , · · · t m } and {t n,1 , · · · t n,m } respectively, satisfy t i ≤ t n,i for all n, i.\nLemma 3: For λ T ∈ ˜ F [0, T ], the optimal Bayesian estima- tor ˆ X(y T ) = E[X|Y T = y T ] = f (y T ) is right continuous in the sense of Deﬁnition 1.\nProof: Let N denote the number of arrivals in Y T process and the corresponding arrival times be given by t 1 , · · · , t N . We note that\nwhich can be evaluated explicitly to yield the following expression\nLet Λ(τ n ) = n i=1 λ τ i , by right continuity of λ, Λ(τ n ) is right continuous under the metric d(·, ·). Now, by dominated convergence theorem, we can state that\nTherefore, E[X|Y T = y T ] is right continuous in y T in the sense of Deﬁnition 1.\n2) Wrapping up the proof: It is enough to show that given any λ T ∈ ˜ F [0, T ] and > 0, there exist ∆ > 0 and the corresponding encoder waveform λ T , such that\nwhere E λ and ˆ X λ (Y T ) are expectation and the Bayes optimal estimator respectively, when the encoder employs the coding scheme λ T . More explicitly, ˆ X λ (Y T ) = E λ [X|Y T ]. We deﬁne E λ and ˆ X λ in the same fashion.\nFollowing arguments will ﬁnalize the proof of main theo- rem.\nE λ [(X − ˆ X λ (Y T )) 2 ] ≤E λ [(X − ˆ X λ (Y T ) 2 ] \t (20) <E λ [(X − ˆ X λ (Y T )) 2 ] + . (21)\nIn this section, we present an alternate proof to Theorem 1 which exploits the mismatched estimation and relative entropy equivalence presented in [5].\nIn order to invoke the result in [5, Theorem 4.4], we need to restrict attention to the class of encoder waveforms in ˜ F [0, T ] that are also bounded away from zero. We note that one can approximate any given encoding function by one that is bounded away from zero, while preserving the mean squared error performance of the system.\nThus, without any loss of generality, we consider the modiﬁed class of encoding functions ˜ F δ [0, T ], which is the\ncollection of right continuous waveforms {λ t : 0 < δ < λ t ≤ A, t ∈ [0, T ]}\nLemma 4: Given λ T (·) ∈ ˜ F δ , there exists a sequence of waveforms {˜ λ T n (·)} n≥1 such that ˜ λ T n (x) is continuous on t for all x and Kullback-Leibler divergence between the joint laws 2 converges to zero, in the following manner:\nProof: Let us deﬁne the loss function l : [0, ∞) × [0, ∞) → [0, ∞] given by\nl(x, ˆ x) = x log x ˆ x\nWe now use Theorem 4.4 of [5] to argue that (25) can be equivalently expressed as\nSince λ T (x) is right continuous on t, we can always ﬁnd a sequence of continuous waveforms {˜ λ T n } n≥1 such that\nSince we know that λ t ∈ (δ, A], T 0 l(˜ λ n,t (x), λ t (x))dt is bounded. By dominated convergence theorem,\nUsing the result in Lemma 4, we want to argue that differ- ence of mean squared error between the two encoding schemes converges to zero as well. Pinsker\u2019s inequality [8] provides us with a lower bound for the relative entropy distance in terms of total variance distance between the two laws. The total variation distance between two probability measures P and Q is given by\nwhere G is the sigma algebra on which the probability space is deﬁned. Thus, we have\nUsing Lemma 4 and (31), it is straightforward to argue that lim\nWe now recall an equivalent formulation of the total variation distance in the following Lemma.\nWe can easily generalize the result, such that for every constant c > 0\nLemma 5 tells us that if lim n→0 d T V (P n , Q) = 0, then for any bounded function f , lim n→∞ E P n [f ] − E Q [f ] = 0.\nProof: By an application of (32) and the fact that if X has bounded support, then the conditional variance itself is bounded, we get the required relationship\nClearly, the source might not have bounded support in general, for instance when X is Gaussian. However, it is a straightforward argument to extend the result in Corrolary 6 to the full generality of a random variable with ﬁnite second moment.\nIn this section, we analyze the distortion exponent of the joint source channel schemes for our setting. In addition we explore and explicitly characterize the distortion exponent for the class of \u2018separation\u2019 based schemes, which essentially tell us how much the optimal encoding strategy depends on the statistics of the source.\nDeﬁnition 2 (Separation Based Scheme): A \t separation based scheme is characterized by a source quantizer that performs a quantization of the input random variable X, in a channel agnostic manner. The channel coder then encodes each symbol of the ﬁnite quantizer output, assuming a uniform prior across this symbol alphabet. This scenario is depicted in Fig. 3.\nRecall the deﬁnition of the distortion exponent α in (1). Let us deﬁne α sep to be the distortion exponent, over all separation based strategies, which we have described above. Note trivially, that α ≥ α sep .\nFig. 4: Constructing a channel code using a joint source channel scheme\nIn this section, we present the optimal separation based strategy, which achieves the exponent α sep . We employ the optimal source quantizer Q , followed by the optimal channel coding strategy for the Poisson channel (cf. [3]). Let E S (R) and E W (R) denote the maximal distortion exponent at rate R of the source quantization scheme (i.e., maximal decay rate in n of the mse of a quantizer for X with e nR levels) and the Poisson channel, respectively. Thus, the overall mean squared error can be expressed as\n= 1. Therefore, the corresponding distortion exponent will be given by α sep = min{E W (R), E S (R)}. For example, if the source is a Gaus- sian random variable, and indeed much more generally for continuous random variables under benign regularity condi- tions on the PDFs, it is well known that E S (R) = 2R. Note that we can choose the appropriate R to maximize the distortion exponent and thus, the achievable distortion exponent is max 0≤R≤C W min{E W (R), 2R}, where C W is capacity of the Poisson channel. In [3], Wyner showed that the distortion exponent satisifes, E W (R) = A/4−R if R ≤ R crit where R crit = A 4 log 2.\nFrom now on, we restrict our attention to the cases when E S (R) = 2R. Since the above scheme is clearly the optimum over all separation schemes, we can get an exact formula for\nwhere the maximum is achieved by R ∗ = A 12 . Naturally, this acts as a lower bound for the overall distortion exponent α:\nIn this section we derive an upper bound for the distortion exponent α. The idea is to show that if we have a good joint source channel coding scheme that achieves an distortion exponent ˜ α, one can construct a corresponding channel coding scheme with a ﬁnite message set of size M , that attains the distortion exponent ˜ α. This is illustrated in Fig. 4. Now consider a zero rate communication regime with a ﬁxed\nnumber of messages M , that is independent of T . Let E M denote the maximum distortion exponent in this setting. In [3], it has been shown that\nTherefore, for any joint source channel coding scheme that attains an distortion exponent ˜ α, and for all M ,\n4 \t (41) Thus, we arrive at an upper bound for the maximum distortion exponent α of the Poisson channel, given by\nWe consider the joint source channel coding problem for transmitting a single random variable across the Poisson chan- nel. We establish that given any joint source channel coding scheme λ T (·) from the rich class of functions allowed, there exists a Wyner delta coding scheme λ T ∆ (·) which can perform as well as λ T (·), not only in the distortion exponent sense, but also in expected squared error for a ﬁxed value of T . Further, we explicitly characterize the maximal distortion exponent for the class of separation based schemes for the Poisson channel. We also provide an upper (and lower) bound for the overall distortion exponent in this setting. This is summarized in the following relationship:\nWe are currently working towards closing the gap in our characterization of α. In particular, answering whether the left inequality in (43) is tight, which would imply optimality of separation in a strong distortion-exponent sense. We conjecture that the answer is afﬁrmative."},"refs":[{"authors":[{"name":"I. Bar-David"},{"name":"G. Kaplan"}],"title":{"text":"Information rates of the photon-limited overlapping pulse position modulation channels"}},{"authors":[{"name":"J. E. Mazo"},{"name":"J. Salz"}],"title":{"text":"On optical data communication via direct detection of light pulses"}},{"authors":[{"name":"A. D. Wyner"}],"title":{"text":"Capacity and error exponent for the direct detection photon channel"}},{"authors":[{"name":"A. D. Wyner"}],"title":{"text":"Capacity and error exponent for the direct detection photon channel"}},{"authors":[{"name":"R. Atar"},{"name":"T. Weissman"}],"title":{"text":"Mutual Information, Relative Entropy, and Estimation in the Poisson Channel"}},{"authors":[{"name":"C. Eldar"}],"title":{"text":"A neural network implementing optimal state estimation based on dynamic spike train decoding"}},{"authors":[{"name":"J. Atick"},{"name":"N. Redlich"}],"title":{"text":"Could information theory provide an ecological theory of sensory processing?"}},{"authors":[{"name":"M. Cover"},{"name":"A. Thomas"}],"title":{"text":"Elements of Information Theory"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566671.pdf"},"links":[{"id":"1569566527","weight":6},{"id":"1569565383","weight":26},{"id":"1569566725","weight":6},{"id":"1569566683","weight":6},{"id":"1569566597","weight":6},{"id":"1569566591","weight":6},{"id":"1569552245","weight":6},{"id":"1569565355","weight":13},{"id":"1569565931","weight":6},{"id":"1569551535","weight":13},{"id":"1569565461","weight":6},{"id":"1569564731","weight":6},{"id":"1569564233","weight":13},{"id":"1569565123","weight":6},{"id":"1569565291","weight":6},{"id":"1569565771","weight":13},{"id":"1569566999","weight":13},{"id":"1569566579","weight":6},{"id":"1569566709","weight":6},{"id":"1569564989","weight":6},{"id":"1569566523","weight":6},{"id":"1569565907","weight":6},{"id":"1569566239","weight":6},{"id":"1569558681","weight":20},{"id":"1569565841","weight":6},{"id":"1569561143","weight":6},{"id":"1569566845","weight":6},{"id":"1569566423","weight":6},{"id":"1569567015","weight":6},{"id":"1569566437","weight":6},{"id":"1569553909","weight":6},{"id":"1569562285","weight":6},{"id":"1569553537","weight":6},{"id":"1569565915","weight":6},{"id":"1569553519","weight":6},{"id":"1569554881","weight":6},{"id":"1569565033","weight":6},{"id":"1569565055","weight":6},{"id":"1569565219","weight":13},{"id":"1569566037","weight":6},{"id":"1569566043","weight":6},{"id":"1569565909","weight":6},{"id":"1569565467","weight":6},{"id":"1569566481","weight":6},{"id":"1569565961","weight":6},{"id":"1569560503","weight":6},{"id":"1569565463","weight":6},{"id":"1569565415","weight":6},{"id":"1569557275","weight":6},{"id":"1569565263","weight":6},{"id":"1569565661","weight":6},{"id":"1569565013","weight":6},{"id":"1569565597","weight":6},{"id":"1569566813","weight":6},{"id":"1569566641","weight":13},{"id":"1569551905","weight":6},{"id":"1569565457","weight":6},{"id":"1569558779","weight":13},{"id":"1569564923","weight":6},{"id":"1569566933","weight":6},{"id":"1569563919","weight":6},{"id":"1569559251","weight":6},{"id":"1569550425","weight":6},{"id":"1569564931","weight":13},{"id":"1569564141","weight":6},{"id":"1569565031","weight":6},{"id":"1569564509","weight":6},{"id":"1569551541","weight":6}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T4.3","endtime":"12:30","authors":"Albert No, Kartik Venkat, Tsachy Weissman","date":"1341490200000","papertitle":"Joint Source-Channel Coding of one Random Variable over the Poisson Channel","starttime":"12:10","session":"S12.T4: Classical and Adversarial Joint Source-Channel Coding","room":"Stratton 20 Chimneys (306)","paperid":"1569566671"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
