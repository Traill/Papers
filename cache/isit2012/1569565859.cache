{"id":"1569565859","paper":{"title":{"text":"Transmission of non-linear binary input functions over a CDMA System"},"authors":[{"name":"Elaheh Mohammadi"},{"name":"Amin Gohari"},{"name":"Hassan Aghaeinia"}],"abstr":{"text":"Abstract\u2014We study the problem of transmission of binary input non-linear functions over a network of mobiles based on CDMA. Motivation for this study comes from the application of using cheap measurement devices installed on personal cell- phones to monitor environmental parameters such as air pol- lution, temperature and noise level. Our model resembles the MAC model of Nazer and Gastpar except that the encoders are restricted to be CDMA encoders. Unlike the work of Nazer and Gastpar whose main attention is transmission of linear functions, we deal with non-linear functions with binary inputs. A main contribution of this paper is a lower bound on the computational capacity for this problem. While in the traditional CDMA system the signature matrix of the CDMA system preferably has independent rows, here the signature matrix of the CDMA system is viewed as the parity check matrix of a linear code, reﬂecting our treatment of the interference. We also introduce the problem of Slepian-Wolf compression with the same compression matrix."},"body":{"text":"The problem of decoding functions of sources rather than the sources themselves in a Multiple Access Channel (MAC) has been studied in several works (see for instance [2] and its follow up works, also [3] and [4]). It has been shown that separation is not always optimal in such scenarios, even when the sources are independent [2]. The intuitive reason for this is that the interference caused by other users could be exploited to compute a given function over the air, if the pattern of the interference matches the functions we want to compute.\nAll of the previous models for transmission of functions over MAC (that we know of) do not impose any restrictions on the encoders, except perhaps on the input power. How- ever some promising emerging applications may violate this assumption. We were motivated by one such application to impose a CDMA system as being part of the encoders.\nThe application is monitoring the exposure of humans to environmental parameters such as air pollution, temperature, etc. since the authors are living in one of the world\u2019s most polluted cities. The traditional way of monitoring is to install measurement devices distributed over a given area. Suggestion has been made to install low cost measurement devices on personal cell-phones, e.g. see [1]. Although the focus of this paper is not the application, but we would like to mention a motivation for this application since it may be new (we have not seen it in the literature). Suppose we are interested in the collective exposure of residents to air pollution (not just\npersonal exposures or the general pollution maps). To ﬁnd the answer, it is not sufﬁcient to have a pollution map, but also the population density at the polluted areas at various times in a day. Let us take the average of the measurements by the mobile sensors that are being carried by the residents as they move in the city. There will be just more cell-phones in populated areas and we can simultaneously take into consideration both the pollution and the population density.\nNote that when the mobile system in the application is employing the CDMA system, it is preferable to use the same architecture to transmit functions of measurements by the cell- phones. Thus, we are considering the problem of function transmission over a network of mobiles based on CDMA.\nWe argue that ﬁnding the optimal scheme for transmission of functions can be studied from two different criteria: i.e. maximizing privacy and minimizing transmission rates. Trans- mission of the whole data (rather than a function of it) may not only be bad in terms of transmission rates, but also it may compromise the privacy of cell-phone users. Therefore we can either maximize privacy or minimize transmission rates over all codes that allow reliable function computation. For the Korner-Marton problem [3] the two criteria yield exactly the same answer 1 ; but we believe in general they may be different. Nonetheless in this study we follow the traditional approach\nOur model is shown in Fig. 1 which is similar to the one considered in [2], except for addition of the signature matrices. It is discussed rigorously in Sec. III. But before that in Sec. II, we intuitively discuss our interpretation of the signature matrix of the CDMA system as the parity check matrix of a linear code, demonstrating our treatment of interference. Sec. IV contains our main result, providing a lower bound on the computational capacity for our problem. The bound is expressed in terms of the answer to another problem that we introduce, i.e. the problem of Slepian Wolf with the same compression matrices (discussed in Sec. V). We believe the latter problem can itself be of independent interest. Sec. VI discusses the lower bound of [2] applied to transmission of non-linear functions in our setting. We will not be discussing any upper bounds, but one can derive an upper bound using the ideas in [2] by merging all the transmitters into one node (the same technique used in some versions of the cut-set bound).\nIn this section we discuss our use of the signature matrix as a parity check matrix at a very simplistic level to convey the basic intuitions. Let us assume that we have only three cell-phones. These cell-phones are observing binary random variable O 1 , O 2 and O 3 respectively. The goal of the base station is to recover a boolean function of O 1 , O 2 and O 3 . Let us assume that the cell-phones directly insert their uncoded bits into a CDMA system with a given signature matrix. For instance, if the signature matrix is\nthe signature of the ﬁrst, second and the third cell-phones would be the vectors s 1 = (1, 1) t , s 2 = (1, 0) t and s 3 = (0, 1) t respectively. Assuming tight power control, the receiver gets the vector O 1 s 1 +O 2 s 2 +O 3 s 3 plus some noise. Let us assume that there is no noise for now. In this case, the receiver gets two symbols, the ﬁrst of which is Y 1 = O 1 +O 2 and the second one is Y 2 = O 1 +O 3 . Note that the summation here is real addition in R, and not in the ﬁeld F 2 . Because O i takes values in {0, 1}, Y 1 and Y 2 will be numbers in the set {0, 1, 2}. If Y 1 = 0, we can conclude that O 1 = O 2 = 0. The value of Y 2 would then specify O 3 . Similarly, when (Y 1 , Y 2 ) = (2, 1) we can ﬁgure out O 1 , O 2 and O 3 exactly. However, when (Y 1 , Y 2 ) = (1, 1), there are two possibilities: (O 1 , O 2 , O 3 ) can be (1, 0, 0) or (0, 1, 1). If one were to compute a function f (O 1 , O 2 , O 3 ) at the receiver, the necessary and sufﬁcient condition for doing so would be that f (1, 0, 0) = f (0, 1, 1). Note that this implies that among 2 2 3 plausible boolean functions, half of them are computable with the given signature matrix. Now, observe that if we interpret the signature matrix given in equation (1) as a parity check matrix, the codewords would be the triples (0, 0, 0) and (1, 1, 1). This implies that the triples (1, 0, 0) and (0, 1, 1) form a coset for this codebook, because their mod-2 sum is a codeword. The constraint f (1, 0, 0) = f (0, 1, 1) says that f has to be constant over this coset.\nThe above simple example can be extended to more general setups. It turns out that if we interpret the signature matrix as a parity check matrix, and take a function f that is equal to a constant over any coset of the parity check matrix 2 , we will be able to perfectly recover f when the channel is noiseless. When the channel is noisy, one can overcome noise via pre- coding; this is explained formally in Sec. IV.\nIn this section we deﬁne the communication model for our problem. Some of the notation we encounter as we go along the paper are summarized in Table I. For a r.v. T we use T n as a shorthand for the sequence (T [1], T [2], . . . , T [n]).\nAssume that there are L cell-phones. Let us denote the observation of the i-th cell-phone by r.v. O i taking values in the discrete set O i . R.Vs O 1 , O 2 , . . . , O L are jointly dis- tributed according to a given p O 1 ,O 2 ,...,O L (o 1 , o 2 , . . . o L ). We assume that the L cell-phones are observing i.i.d. repetitions O 1 , O 2 , . . . , O L . The goal of the cell-phones is to enable the base station to recover i.i.d. repetitions of b functions of the observations which we denote by U i (1 ≤ i ≤ b), U i = f i (O 1 , O 2 , . . . , O L ).\n1) An encoder for each cell-phone, mapping O k i (k i.i.d. repetitions of O i ) into a sequence of n bits (denoted by\nThe actual signals transmitted over the air are X nN i for i ∈ [1 : L], that are formed by multiplying each bit of encoder\u2019s output T n i into the signature s i . Note that the length of T n i is n, and the length of s i is N . Since each bit of T n i is multiplied by the whole sequence s i in the CDMA system, the output will be a binary string of length nN , denoted by X nN i .\nusers, we assume that that T j [i] is taking values in {0, 1}, 3 and the variance of Z[i] is σ 2 .\nComputational Capacity: Given a signature length N , A communication rate R N is said to be achievable if there is a se- quence of codes, C n for n ∈ N, all having signatures of length N , such that lim n→∞ P e (C n ) = 0 and lim n→∞ R(C n ) = R N where P e (C n ) and R(C n ) are the probability of error and rate of the code respectively. The computational capacity for a signature length N , C N , is taken to be the supremum of the set of achievable rates for that signature length N .\nIn this section we state our main results. Proof is given in Sec. VIII.\nLet f i (O 1 , . . . , O L ) (1 ≤ i ≤ b) be a set of functions satisfying the property that\nfor any two sequences (o 1 , o 2 , . . . , o L ) and (o 1 , o 2 , . . . , o L ) belonging to the same coset of some parity check matrix H. 4 Without loss of generality we can assume that H has distinct rows h 1 , h 2 , ..., h r . Thus matrix H is of size r × L.\nTheorem 1: For any signature length N > r, the following rate is achievable\nwhere R is the minimum Slepian-Wolf rate with the same compression matrixes (R s.c. SW ), deﬁned in Sec. V, for the choice of V i = j=1:L h i [j]O j (modulo 2). Inequality (a) comes from applying Claim 2 of Sec. V, and is an explicit lower bound expression.\nThe number c is the capacity of a channel with input alphabet W = {0, 1} and output alphabet [− 1 2 , 3 2 ] deﬁned as follows: the output is formed by adding W to a Gaussian noise with variance σ 2 N\n, and then taking it modulo 2, meaning that we add an integer multiple of 2 to it to make it fall into the interval [− 1 2 , 3 2 ). Note that because of the symmetry the capacity occurs at a uniform input distribution.\nTo understand the statement of the main result, we need to introduce the problem of Slepian-Wolf with the same compression matrices. We believe this problem can itself be of independent interest.\nWe ﬁrst begin with the problem in a special case. Suppose we have three correlated binary sources V 1 , V 2 and V 3 jointly\ndistributed according to p(v 1 , v 2 , v 3 ). I.i.d. repetitions of these three sources are observed by three parties, who want to com- municate these i.i.d. repetitions to a fourth party, Alice, using noiseless links of rates R 1 , R 2 , and R 3 . We are interested in the case of R 1 = R 2 = R 3 = R. The minimum possible value of R will be the minimum value of R such that (R, R, R) is in the Slepian-Wolf region. We call this R SW . We know that for any R > R SW we can achieve the rate triple (R, R, R) using linear codes: there are matrices B 1 , B 2 and B 3 (of size nR × n) where the three parties can use and send B 1 V n 1 , B 2 V n 2 and B 3 V n 3 where V n i is a column vector consisting of V i [j] for j ∈ [1 : n]. The multiplication is in the ﬁeld F 2 .\nNow, what if we are interested to ﬁnd a single matrix B, such that having BV n 1 , BV n 2 and BV n 3 we can recover (V n 1 , V n 2 and V n 3 )? The three parties are sending at rates R 1 = R 2 = R 2 = R to Alice using the same compression matrix B. We denote the minimum value of R in this case by R s.c. SW . Clearly R s.c. SW is larger than or equal to R SW (deﬁned in the previous paragraph), because more restrictions are imposed on the deﬁnition of R s.c. SW . But is R s.c. SW always equal to R SW ? We show in Claim 2 that this is not true. The deﬁnition of R s.c. SW can be extended to more than three parties in the natural way.\nUse of the same matrix B to compress correlated data (or Slepian-Wolf with the same compression matrices) arises naturally in our problem. It is also related to the \u201csyndrome technique\u201d whereby a single code based is constructed for distributed compression (see for instance [5][6][7]). And after all, it is interesting to ﬁnd the best compression rate one can achieve if a universal compression code is used by all nodes in a distributed source coding problem.\nWe do not know the exact value of R s.c. SW , but prove a few results about it.\nClaim 1: Let us assume we have only two binary r.v.\u2019s V 1 and V 2 . Let K = V 1 + V 2 (mod 2). Then R s.c. SW for transmission of these two r.v\u2019s is less than or equal to max(H(K), H(V 1 |K)).\nProof: Let R = max(H(K), H(V 1 |K)). Here is the sketch of the proof: let us generate the coordinates of the common compression matrix B (of size nR × n) uniformly and randomly from {0, 1}. Then having BV n 1 and BV n 2 , we can add them modulo two to get B(V n 1 + V n 2 ) = BK n . Since R ≥ H(K), B is a good source code for recovering K n with high probability. Hence we can decode K n ﬁrst. The Slepian-Wolf rate for recovering V n 1 with K n serving as a side information is H(V 1 |K). Since R ≥ H(V 1 |K), B is a good SW code with high probability. Therefore we can ﬁnd V n 1 . Having V n 1 and K n , we can also recover V n 2 .\nClaim 2: There exists V 1 , · · · , V r such that the value of R s.c. SW is strictly larger than R SW . Next, for any p(v 1 , · · · , v r ), R s.c. SW is less than or equal to min(rR SW , max i H(V i )).\nProof: Let V 1 = V 2 = · · · = V r . Then R SW , i.e. the minimum value of R such that (R, R, R) is in the Slepian- Wolf region, is equal to H(V 1 ) r . However, R s.c. SW is equal to H(V 1 ).\nTo show that R s.c. SW ≤ rR SW always holds, we start from an arbitrary code for R SW , and construct another code for\nR s.c. SW . Take an arbitrary code with compression matrices B 1 , B 2 , ..., B r all of size nR × n. Let B to be equal to [B t 1 B t 2 · B t r ] t where t is the transpose operation. One can verify that matrix B is a valid common compression matrix, and is achieving the rate rR for the problem of R s.c. SW . Thus R s.c. SW ≤ rR SW . Note this upper bound on the ratio R s.c. SW R\ncannot be made smaller than r because of the example given at the beginning of this proof.\nTo show the inequality R s.c. SW ≤ max i H(V i ), observe that a random compression matrix of size n[max i (H(V i ) + ] × n allows for recovery of V n i from BV n i (for all i ∈ [1 : r]) with the average probability of error converging to zero. Thus a particular instance should also work.\nIn this section we discuss how our lower bound extends the result of Nazer and Gastpar in [2]. We ﬁnd the set of functions where we can use the result of Nazer and Gastpar, and the lower bound it gives us.\nOur formulation above is similar to the one given by Nazer and Gastpar [2], except that we have a signature matrix here. Nonetheless, if we ﬁx the signature matrices, we can think of a virtual channel between the encoder and decoders that includes the signature matrix. The input to this virtual channel is (T 1 , T 2 , ..., T L ) and the output is Y (1 : N ) = L i=1 T i s i (1 : N )+Z(1 : N ) where the noise vector Z(1 : N ) has covariance matrix σ 2 I. If we use the virtual channel n times, we get an output vector of size nN that we were denoting by Y nN .\nIn this case we can write down the lower bound given in [2] when we have a linear function over a ﬁeld. We are mainly concerned with functions with binary inputs. The only linear function on the ﬁeld F 2 is the XOR function. So this already puts limitations on the lower bounds we can get by [2]. When U i (for 1 ≤ i ≤ b) is the XOR of a subset of the observations O 1 , ..., O L , we get the following lower bound\nwhere the factor N in the denominator comes from our deﬁnition of rate. Because we are free to choose the signatures s 1 , ..., s L we can take maximum of the above expression over all s 1 , ..., s L .\nThe above result works only when the U i s are the XOR func- tions of subsets of O 1 , ..., O L , and it involves a maximization problem that we found hard to do, even when we have linear functions on a ﬁeld.\nTo compute arbitrary nonlinear boolean functions of the observations, Nazer and Gastpar suggest that we increase the ﬁeld size and embed the non-linear function in a linear function deﬁned on a larger space (see Theorem 2 of [2]). Although this would not solve the maximization problem over the signatures s 1 , ..., s L mentioned above, it will result in a lower bound for non-linear functions. In this paper we take an alternative approach of using several linear functions in\nthe same ﬁeld using a particular construction (rather than one single linear function over a larger ﬁeld). In order to transmit several functions over a channel, [2] uses a successive Slepian- Wolf type scheme. Our model allows us to do better than this. Through an appropriate choice of the signature matrix, we can run part of the transmission of the functions in parallel, getting an extra gain compared to the scheme considered by [2].\nThis is not a linear function in the ﬁeld F 2 . Let us assume that N = 2. We can use the signature matrix given in equation 1 since f is constant over all of its cosets. We have V 1 = O 1 + O 2 (mod 2) and V 2 = O 1 + O 3 (mod 2). Therefore\nH(V 1 ) = h(p(O 1 = O 2 )), \t and H(V 2 ) = h(p(O 1 = O 3 )),\nwhere h(·) is the binary entropy function. The main theorem implies the following lower bound.\nThe value of c ≤ 1 depends on σ. For the sake of illustration we assume that σ is such that c = 0.5.\np(O 1 = O 2 ) ∈ {0, 1}, \t and p(O 1 = O 3 ) ∈ {0, 1}.\nThis is expected since in each of the four cases f (O 1 , O 2 , O 3 ) is a constant. It would be interesting to understand the behavior of the lower bound when p(O 1 = O 2 ) and p(O 1 = O 3 ) are not exactly in {0, 1}, but rather in its vicinity. To study this, let us consider the model depicted in Fig. 2 in which O 1 , O 2 and O 3 are assumed to be the result of a random variable B passing through three independent BSC channels, i.e.\nwhere W i \u2019s are binary random variables. p(W i = 1) is the crossover probability of the i th channel. When p(W i = 1) ∈ {0, 1}, the lower bound is ∞. Fig. 3 plots the lower bound R in terms of p(W 2 = 1) and p(W 3 = 1) when p(W 1 = 1) = 0.\nProof of Theorem 1: We create the signature matrix of the CDMA by repeating the matrix H to get a matrix of size N × L. This means that each of the rows h 1 , h 2 , ..., h r would be repeated N r times; extra zeros are padded if N r\nis not an integer. At the receiver, we can look at the received Y \u2019s corresponding to each of the N r repetitions and take their average. This would reduce the variance of noise for that transmission to σ 2 = σ 2 N\n. So, this would be as if the signature matrix is of size r (instead of N ) identical to H, and the noise variance is σ 2 (instead of σ 2 ). We are going to continue assuming that the signature matrix and the parity check matrix are both H.\nAt time i, the cell-phones are sending T 1 [i], T 2 [i], ..., T L [i] respectively. The receiver gets H T 1 [i], T 2 [i], ..., T L [i] t plus noise where the matrix multiplication here is in R. To convert the matrix multiplication from R to that in F 2 , the receiver computes the modulo 2 of each received number (as discussed in the statement of the theorem), mapping it to the interval [− 1 2 , 3 2 ). This would be as if H T 1 [i], T 2 [i], ..., T L [i] t (matrix multiplication in F 2 ) is transmitted but the noise added to this is no longer Gaussian; it is a Gaussian noise mod 2. Number c in the statement of the theorem is the capacity of this channel.\nHaving described the signature matrix, and the decoder\u2019s mod 2 postprocessing of the signal, we now turn our attention to the encoders and the decoder. We can divide the rest of the proof into two parts. The ﬁrst part is a general statement about recovery of the desired functions of the observations from V i \u2019s. This is used in the second part of the proof to design the encoders and the decoder.\n(I) We ﬁrst claim that given any values for (o 1 , o 2 , . . . , o L ), knowing the values of j=1:L h i [j]o j modulo two for i ∈ [1 : r] is sufﬁcient to perfectly recover f i (o 1 , o 2 , . . . , o L ) (1 ≤ i ≤ b). To see this note that having r equations j=1:L h i [j]o j (mod 2) for i ∈ [1 : r] is equivalent to having the product H[o 1 , o 2 , . . . , o L ] t in the matrix form; here the multiplication is in F 2 . Note that the number of equations is r whereas the number of free variables is L, so it ﬁrst seems that the decoder may not be able to ﬁgure out [o 1 , o 2 , . . . , o L ]. The decoder can list the set of all [o 1 , o 2 , . . . , o L ] such that H[o 1 , o 2 , . . . , o L ] t (mod 2) is equal to the received H[o 1 , o 2 , . . . , o L ] t (mod 2). This would be the coset associated to [o 1 , o 2 , . . . , o L ] for the parity check matrix H. Because f i maps all the sequences\nin a coset into the same number, namely f i (o 1 , o 2 , . . . , o L ) are all equal, the receiver will be able to exactly recover f i (o 1 , o 2 , . . . , o L ).\n(II) From the ﬁrst part of the proof we conclude that if we re- liably communicate i.i.d. repetitions of V i = j=1:L h i [j]O j (mod 2) to the receiver, it will be able to reliably recover i.i.d. repetitions of f i (O 1 , O 2 , . . . , O L ) (1 ≤ i ≤ b). Therefore we have translated the original problem into that of communicat- ing linear functions. If we think of the signature matrix as part of a virtual channel between the encoder and decoders, this virtual channel will be a set linear MACs (as deﬁned by [2]) in parallel because of the postprocessing at the receiver. Therefore our setting is not a special case of one considered by Theorem 1 of [2] because the channel is not a single linear MAC. Nonetheless we borrow ideas from [2] to extend the proof of Theorem 1 of [2]; this is not difﬁcult given that the structure of the virtual channel and the linear functions to be computed (i.e. V i s) are prepared to \u201cmatch\u201d.\nIt is possible to ﬁnd a binary matrix B of size (kR + ) × k for the i.i.d. repetitions of (V 1 , V 2 , ..., V r ) such one can recover i.i.d. repetitions of V 1 , V 2 , . . . V L , namely V k 1 , V k 2 , . . . , V k L , from B[V k 1 V k 2 · · · V k L ]. within a probability of error , where by V k 1 we mean a column vector consisting of the k i.i.d. repetitions of V 1 . The multiplication between the column vector V k i and B is done in F 2 . Next we ﬁnd a channel coding matrix G of size kR+ c− × (kR + ) for communicating over a Gaussian channel with variance σ 2 . The i th cell-phone computes GBO k i . It sets this vector of size n = kR+ c− to be T n i . At time j, the random variable T i [j] is multiplied by signature s i . The receiver gets j T i [j]s i plus a noise vector. This is equivalent with getting\nplus noise. Since G is a channel coding matrix, we can recover B[V k 1 V k 2 · · · V k L ] with high probability. From here we can recover V k i because of the property of B mentioned above. Thus, we have a good code. The rate of this code is"},"refs":[{"authors":[{"name":"A. Gelman"}],"title":{"text":"Mobile phones: sensors and sensitivity"}},{"authors":[{"name":"B. Nazer"},{"name":"M. Gastpar"}],"title":{"text":"Computation over Multiple-Access Channels"}},{"authors":[{"name":"J. K¨orner"},{"name":"K. Marton"}],"title":{"text":"How to encode the modulo-two sum of binary sources"}},{"authors":[{"name":"R. Soundararajan"},{"name":"S. Vishwanath"}],"title":{"text":"Communicating Linear Functions of Correlated Gaussian Sources Over a MAC"}},{"authors":[{"name":"S. S. Pradhan"},{"name":"K. Ramchandran"}],"title":{"text":"Distributed source coding using syndromes (DISCUS): design and construction"}},{"authors":[{"name":"V. Toto-Zarasoa"},{"name":"A. Roumy"},{"name":"C. Guillemot"}],"title":{"text":"Rate-adaptive codes for the entire Slepian-Wolf region and arbitrarily correlated sources"}},{"authors":[{"name":"N. Gehrig"},{"name":"P. L. Dragotti"}],"title":{"text":"Symmetric and a-symmetric Slepian-Wolf codes with systematic and non-systematic linear codes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565859.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S8.T6.1","endtime":"17:00","authors":"Elaheh Mohammadi, Amin Aminzadeh Gohari, Hassan Aghaeinia","date":"1341333600000","papertitle":"Transmission of non-linear binary input functions over a CDMA System","starttime":"16:40","session":"S8.T6: New Sequence Constructions","room":"Kresge Rehearsal A (033)","paperid":"1569565859"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
