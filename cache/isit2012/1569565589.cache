{"id":"1569565589","paper":{"title":{"text":"Oblivious Distributed Guessing"},"authors":[{"name":"Serdar Boztas"}],"abstr":{"text":"Abstract\u2014We consider the oblivious distributed guessing of a random variable in three scenarios (single guessor, single constrained guessor, multiple guessors). The optimal guessing schemes in each case are obtained by means of a probability distribution which the guessor(s) use in order to determine their sequence of guesses. Some of the optimal distributions obtained have links to R´enyi\u2019s generalization of Shannon\u2019s entropy,while one of them is related to the r th power means from mathematical analysis."},"body":{"text":"Let X be an unknown discrete random variable X ∈ X with X ﬁnite or countable, and with distribution P. This random variable could, for example, represent an unknown key for a cryptosystem, or an unknown password. In practice, the guessor is not all-powerful and can only ask atomic questions (e.g., query keys/passwords) regarding singletons in X . We assume that a sequence of questions of the form Is X = x? are posed until the ﬁrst YES answer determines the value of the random variable X.\nThe problem of guessing has been investigated in the context of sequential decoding [1] and source-channel coding ([2], [3], [11]) as well as in security applications ([14], [5], [6], [10]). We provide an overview of previous work in the last section. It is also possible to deﬁne guessing in the presence of distortion or source uncertainty ([2], [11], [15]) but we do not pursue this any further here.\nWhile it is attractive to have a number of different guessors working in parallel in trying to obtain the value of the random variable X, there are also some pitfalls in making this approach ﬂexible, in terms of participants entering and leaving the group performing the attack and in terms of partitioning the search space X . This can turn out to be complicated, since it is quite likely that the computational power of each participant (thus the rate at which they can implement the guessing mechanism) can vary a great deal. These factors make the study of oblivious distributed guessing of interest.\nHowever the model we are considering in this paper is also relevant to distributed attacks where a resource being sought\u2013 or a target is being blocked\u2013for example in DDOS (Distributed Denial of Service) attacks. To make this more precise, consider trying to stop the availability of some information which is being made available by an adversary, via various IP addresses on the internet\u2013possibly by using hosting sites, such as Akamai, MegaUpload, etc. In that case, the goal may be to identify the location of as many copies of the resource, which could be a trojan in the adversarial setting as possible and then disable/erase it. Alternatively the goal may be to ﬁnd one copy\nof the resource as quickly as possible. These problems can be framed in terms of the guessing problem we consider, where the location of the resource is identiﬁed with a point in X .\nThe paper is organised as follows. In Section II, we intro- duce the relevant deﬁnitions and notation. In section III, we state the problem we are considering and deﬁne and analyze the guessing algorithm which minimizes the expected number of guesses, in a distributed environment. We also provide a brief discussion of some possible extensions of the result. Section IV concludes the paper, with a discussion of related results in guessing theory and puts the results obtained here in context.\nWe provide some deﬁnitions in this section. A guessing strategy for identifying X is a procedure for generating successive questions of the above type until a YES answer is obtained. Formally, any such procedure can be represented by a function G : X → {1, 2, . . .} where G(k) equals the time index of the question Is X = k?.\nNote that functions G corresponding to valid guessing strategies cannot be totally arbitrary. Clearly, G must be invertible on its range {1, 2, . . .} since only one element may be probed at any given time. Moreover, since we are assuming the answers to the queries Is X = k? are noiseless, it is enough to consider guessing strategies which ask the above question exactly once for each k ≥ 1. This formally corresponds to the mapping G being one-to-one and onto. Any function satisfying these two conditions will be called a guessing function. Every guessing function deﬁnes a valid guessing strategy and conversely.\nAssuming that the guessor knows the probability distribu- tion P (otherwise see [15]), she is interested in minimizing the number of questions required to determine X. This goal can be formalised in a number of ways, such as minimizing a positive moment E[G ρ ] (mostly ρ = 1 is of interest) where\nThe R´enyi entropy of order α of X is a generalization of the Shannon entropy deﬁned by\nand obeys lim α→1 H α (X) = H(X) as well as being strictly decreasing in α unless the Y is uniform on its support.\nBrute force predictability uses the idea of guessing every value of X one by one in order of decreasing probability, when the distribution P(x) is known. We now want to consider the case of an attack that is more distributed, perhaps attacking multiple targets, whose passwords are assumed to come from the same distribution P(x). The question we want to answer is the following:\nGiven P(x), how should the attacker choose a distribution Q(x) in order to optimize some performance criterion, when all the attacker does (resp. attackers do) is to draw random sequential guesses from Q(x)?\nIf there are no constraints on time or memory, the optimal strategy is to minimize the expected number of guesses E[G] and maximize the probability of success at each time index by guessing every point in X in decreasing order of probability.\nLet\u2019s consider a single attacker who is memory constrained and won\u2019t keep track of past guesses, but knows the distribu- tion P which the opponent uses to draw a single value x from X according to P(x).\nThe guessor generates i.i.d. X 1 , X 2 , . . . , from X according to a distribution Q(x) again with the goal of minimizing E[G]. Deﬁne G = min{k : X k = X} as a random variable which denotes the number of guesses before she is successful in exposing X. Note that G = k with probability\nk−1 Q(x). where k ≥ 1, by a success- fail argument. This is because\nNote that now that the guessing scheme is randomized, it is possible to have an unbounded number of guesses, hence\nP(x) Q(x)\nP(x) Q(x)\nquite an interesting result. This means that the distribution Q(x) should be \u201cﬂatter\u201d than P(x). We have thus proved\nProposition 1. The distribution which minimizes the expected number of guesses for a random variable with a nontrivial distribution P, i.e., 0 < P(x) < 1 for all x ∈ X , when a single guessor draws her guesses from the distribution Q is given by\nThis can be generalized to the case of multiple independent attackers against one target, or against multiple targets, and we consider the ﬁrst problem in the rest of this paper.\nNote that the Lagrange multipliers ensure that the solution for Q(x) in Proposition 1 is a minimum, since the expression\nin (Q(x 1 ), . . . , Q(x N )) if we recall that the P(x) are given positive constants. Differentiation of J with respect to Q(x) yields\nwhich leads to the proposition above. Furthermore, note that if we choose Q(x) = P(x) for all x ∈ X which may look like an attractive choice, we obtain E[G] = N which is unexpectedly high. Let us now consider the optimal value of the expectation which the guessor using Proposition 1 achieves. We obtain\nP(x) Q(x)\nwhich provides a new operational deﬁnition of R´enyi entropy of order 1/2 relating it exactly to oblivious guessing. If we compare this to the results in (4) and (5) in section IV, which were derived in [1], [4], we can see that the penalty paid for oblivious memory constrained guessing, as opposed to using the optimal guessing sequence is roughly a factor of 2 in the expectation, and is still much closer to the optimal value than the lower bound given in (5).\nB. Power and Memory Constrained Guessor Minimizing the Probability of Failure\nConsider the case where the guesses are still i.i.d. from Q(x) but the guessor (maybe a sensor network node) decides ahead of time that she will only use L ∈ N guesses. Again we need to ﬁnd the best Q(x), but now the appropriate criterion is minimizing the failure probability in L guesses, namely,\nwhich directly leads to the conditions ∂J\nfor some positive constant µ = λ/L. The second derivative is ∂ 2 J\nand if we once again assume the non-degeneracy condition 0 < Q(x) < 1 for all x ∈ X we conclude that the second derivative is positive indicating that the stationary point determined minimizes the probability P f ail (L). Note that normalization requires that we have x∈X Q(x) = 1 which then yields\nProposition 2. If the attacker is restricted to L guesses, her optimal oblivious strategy is to generate L i.i.d. guesses from the following distribution\nNote that in this case, a power sum related to the probability distribution P(x) is also involved, however this is not a R´enyi entropy since the exponent −1/(L − 1) is negative. Neither is it related to any other kind of generalized entropy in the literature. However, if we rewrite it as below\nwe note that it is related to the weighted power means that are widely used in mathematical analysis [12], namely\nwhich are deﬁned for real quantities r = 0 where a = (a 1 , . . . , a n ) and w = (w 1 , . . . , w n ). Thus we can write the above equation as\nX = {x 1 , . . . , x N }. The power means themselves are also related to the Shannon entropy in the sense that if we let (w 1 , . . . , w n ) and (a 1 , . . . , a n ) be the same ﬁnite probability distribution P then we have\nC. Multiple Memory Constrained Guessors Using Distributed Oblivious Guessing\nWe brieﬂy discuss the case of v ≥ 2 guessors working in parallel, each drawing from the same distribution Q(x), but not coordinating their guesses. First we note that if the guessors use some distribution Q(x) and they collectively work at a rate equal to v times the rate of the single guessor considered in the subsection above, their performance will be within the bounds\nwhere we have used the notation E Q [G v ] for the expected number of guesses when v guessors each use Q(x). We now address the issue of optimizing the distribution Q(x) once v is ﬁxed, instead of using the Q from the subsection above. From the rest of the subsection, we drop the subscript Q from the expectations for simplicity of presentation. Note that we can write\n(1 + k)[(1 − Q(x)) v ] k 1 − (1 − Q(x)) v Q(x)\nif we use the sum of a ﬁnite geometric series. Using once again the generating function identity in (1) yields\nwhich upon differentiation gives ∂J v\nwhich indicates that the optimum distribution Q(x) satisﬁes v(1 − Q(x)) v−1\nLet\u2019s deﬁne R(x) = 1 − Q(x) which takes on values in (0, 1) (note that R is not a probability distribution) which means that we have\nvu v−1 \t (2) deﬁned on (0, 1) we can see that its derivative can be simpli- ﬁed to\nv ) + (v − 1)(1 − u v ) 2 vu v\nwhich is clearly negative for integer v ≥ 2 thus f : (0, 1) → R is a monotone strictly decreasing one-to-one mapping and thus is invertible. We have thus proved\nProposition 3. If v attackers are using distributed guessing their optimal oblivious strategy is to independently generate their guesses according to the distribution\nWe have considered randomized guessing schemes for an unknown random variable under three different regimes and obtained optimal guessing schemes for each case. Here we put our work in context, and discuss related work.\nIt is generally accepted that the Shannon entropy measures the information rate of a source. If we let P(X 1 , . . . , X n ) denote the joint probability distribution of the ﬁrst n symbols output by a discrete source the entropy rate of this source is deﬁned by\n1 n\nwhenever the limit exists. The expectation is taken with respect to the joint distribution P(X 1 = a 1 , . . . , X n = a n ), and historically the term \u2018self-information\u2019 was used for this quantity. We focus on the i.i.d. case this limit is simply\nIn previous work Pliam [14] has argued that Shannon entropy can be arbitrarily far away from the expected number of guesses, and proposed E[G] as a measure of security, calling it \u201cguesswork\u201d. This was also discussed by Malone and Sullivan who pointed out that while the expected number of guesses is closely approximated by the Renyi entropy of order 1/2\u2013 see also the subsection below\u2013it is still possible that a random variable with high expected number of guesses can still be vulnerable to a reasonably high probability of a successful single guess.\nIn this section we ﬁx N = |X | to be a large ﬁnite number and concentrate on the relationship between unpredictability and entropy. Thus X ∈ X can be deﬁned as a scalar random variable whose distribution satisﬁes, without loss of generality,\nWe use the shorthand P(X = k) = p k , for the rest of this paper, where we assume X = {1, 2, . . . , n} and we have identiﬁed the optimal guessing sequence as G(k) = k for k = 1, . . . , N.\nFeder and Merhav [9] investigated the success probability in a single guess and the corresponding limits on Shannon entropy for a given success probability. Following them, we deﬁne the unpredictability as the minimum error probability in guessing X, which we denote by π. Clearly π = 1 − p 1 , and the higher π is, the more unpredictable the random variable X is. Upper and lower bounds on H(Y ) for ﬁxed predictability π, i.e., for distributions obeying (3) and p 1 = 1 − π, can then be obtained. The maximum Shannon entropy H is obtained when\nand this maximum is given by h(π) + π log(N − 1) where h(p) = −p log p − (1 − p) log(1 − p) is the binary entropy function. The minimum H is achieved by more complicated distributions, see Feder and Merhav [9] for a nice ﬁgure and the details.\nIn some contexts, it is more appropriate to allow the opponent an arbitrary number of guesses, e.g., from the design of cryptosystems point of view. There are a number of bounds on E[G] for this scenario. We state two of them, and in the unconditional case where they are simpler, even though corresponding conditional bounds also exist. The expected\nnumber of guesses, for a user with the optimal guessing sequence , E[G] obeys the upper bound [4]\n1 2\n. (5)\nNote that the upper and lower bounds are approximately within a factor ln N/2 of each other. Thus if N = 2 64 the upper and lower bounds are within a factor ln 64/2 ≈ 22.18 which is less than a fraction 2 5−64 = 2 −59 of the size of the search space. In this setting, once again the R´enyi entropy of order 1/2, or H 1/2 (X) plays a role.\nWe also remark that Pliam [14] has also investigated the discrepancy between marginal guesswork, i.e., the number of guesses that the optimal guessor requires to obtain a given success probability, and entropy, and point the reader to [14] for the details.\nThe results we obtained in this paper have provided a complement to the existing results, and have applications in key guessing attacks.\nThe link between guessing and entropy was popularized by Massey [7]. The problem of bounding the expected number of guesses in terms of Renyi entropies was investigated by Arikan in the context of sequential decoding [1]. Pliam independently investigated the relationship between entropy, guesswork and security [14]. Feder and Merhav considered the relationship between predictability and entropy [9]. Pﬁster and Sullivan [13] and Malone and Sullivan [8] also obtained results related to guesswork and entropies.\nIndependent work in this domain has also been brought to the author\u2019s attention. The optimal guessing strategy derived in the case of single guesser with constraint on memory was proposed as an attack strategy in the context of password guessing attacks, which is available online [16].\nThe problem of a cipher with a guessing wiretapper, and the problem of guessing subject to distortion was investigated by Merhav and Arikan [2]. The problem of guessing under source uncertainty was investigated by Sundaresan [15]. Hanawal and Sundaresan have also uniﬁed the work on guessing exponents, using large deviations theory, in [17]. The problem of multi- level guessing was investigated by Merhav, Roth and Arikan [11]. In conclusion, the area of guessing continues to be of interest to a number of applications.\nThis research was supported by the Australian Research Council\u2019s Linkage Projects funding scheme (project number LP110200321). The author would like to thank the referees for useful comments which improved the presentation of the paper."},"refs":[{"authors":[{"name":"E. Arikan; An Inequality on Guessin"}],"title":{"text":"Its Application to Sequential Decoding, IEEE Transactions on Information Theory, 42(1):99-105, 1996"}},{"authors":[{"name":"E. Arika"},{"name":"N. Merhav; Guessing subject to distortio"}],"title":{"text":"IEEE Transac- tions on Information Theory, 44(3):1041-1056, 1998"}},{"authors":[{"name":"E. Arika"},{"name":"N. Merhav; Joint Source-channel Codin"}],"title":{"text":"Guessing with Application to Sequential Decoding, IEEE Transactions on Infor- mation Theory, 44(5):1756-1769, 1998"}},{"authors":[{"name":"S. Boztas¸; Comments on \u2018An Inequality on Guessin"}],"title":{"text":"Its Applica- tion to Sequential Decoding\u2019, IEEE Transactions Information Theory, 43(6):2062-2063, 1997"}},{"authors":[{"name":"S. Dragomi"},{"name":"S. Boztas¸; Some Estimates of the Average Number of Guesses to Determine a Random Variabl"}],"title":{"text":"S"}},{"authors":[{"name":"S. Dragomi"},{"name":"S. Boztas¸; Estimation of Arithmetic Mean"}],"title":{"text":"S"}},{"authors":[{"name":"J. L. Massey; Guessin"}],"title":{"text":"Proc"}},{"authors":[{"name":"D. Malon"},{"name":"G. Sullivan; Guesswor"}],"title":{"text":"W"}},{"authors":[{"name":"M. Fede"},{"name":"N. Merhav; Relations between entrop"},{"name":"N. Merha"}],"title":{"text":"Error Probabil- ity, IEEE Transactions on Information Theory 40(1):259-266, 1994"}},{"authors":[{"name":"N. Merha"},{"name":"M. Rot"},{"name":"E. Arikan; Hierarchical guessing with a ﬁdelity criterio"}],"title":{"text":"R"}},{"authors":[{"name":"S. Mitrinovi´c; Analytic Inequalitie"}],"title":{"text":"D"}},{"authors":[{"name":"C.-E. Pﬁste"},{"name":"G. Sullivan; R´enyi Entrop"}],"title":{"text":"W"}},{"authors":[{"name":"J. O. Pliam; On the incomparability of Entrop"}],"title":{"text":"Marginal Guesswork in Brute-force Attacks, Proc"}},{"authors":[{"name":"R. Sundaresan; Guessing Under Source Uncertaint"}],"title":{"text":"IEEE Transactions on Information Theory 53(1): 269 - 287, 2007"}},{"authors":[{"name":"M. K. Hanawa"},{"name":"R. Sundaresan; Randomised Attacks on Password"}],"title":{"text":"Technical Report TR-PME-2010-11 , Dept"}},{"authors":[{"name":"R. Sundaresan; Guessin"}],"title":{"text":"Compression Subject to Distortion, Tech- nical Report TR-PME-2010-12 , Dept"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565589.pdf"},"links":[{"id":"1569565803","weight":4},{"id":"1569563763","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T6.3","endtime":"12:30","authors":"Serdar Boztas","date":"1341490200000","papertitle":"Oblivious Distributed Guessing","starttime":"12:10","session":"S12.T6: Cryptanalysis and Distributed Guessing","room":"Kresge Rehearsal A (033)","paperid":"1569565589"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
