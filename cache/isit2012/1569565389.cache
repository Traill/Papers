{"id":"1569565389","paper":{"title":{"text":"Shannon Entropy Convergence Results in the Countable Inﬁnite Case"},"authors":[{"name":"Jorge F. Silva"},{"name":"Patricio A. Parada"}],"abstr":{"text":"Abstract\u2014 The convergence of the Shannon entropy in the countable inﬁnity case is revisited and extended in this work. New results are presented that provide necessary and sufﬁcient conditions for the convergence of the entropy in different settings, including scenarios with both ﬁnitely and inﬁnitely supported measures. These results show some connections between the Shannon entropy convergence and the convergence in informa- tion divergence."},"body":{"text":"It is well known that the Shannon entropy in the ﬁnite alphabet case is a continuous functional with respect to the topology of the total variation in the space of probability measures [1], [2]. In fact, this was one of the requirements considered by Shannon in deﬁning this measure [3]. Sur- prisingly, the continuity does not hold if we move from the ﬁnite alphabet to the countable inﬁnite alphabet case [4], [5], [6]. In regard to this, recent results demonstrated the discontinuity of entropy. In particular, Ho et al. [5, Theo- rem 2] showed concrete examples where convergence in χ 2 - divergence and in direct information divergence (I-divergence) [7] (both stronger than total variational convergence [8], [9]) do not imply convergence in the entropy functional. These constructions were extended to other information measures in [5] (mutual information and their conditional versions) proving their discontinuity with respect to the topology of total variation [9]. Interestingly, these illuminating examples consider distributions with ﬁnite support. On the other hand, Harremeus [4] showed the discontinuity of the entropy with respect to the reverse I-divergence [7], and consequently, with respect to the total variational distance 1 .\nThese results raised interest in studying sufﬁcient conditions in a sequence of measures for which entropy convergence is obtained in the countable inﬁnity alphabet case [4], [10], [6]. On this topic we can mention some recent contributions. The ﬁrst is the work of Ho et al. [6], in which the interplay between the entropy and the total variation distance was studied in detail. This work reﬁned the results presented in [5], con- cerning the discontinuity of the entropy, but also found tight bounds to control the entropy difference, |H(P ) − H(Q)|, when the distributions P and Q have a total variation distance\nsmaller or equal to > 0. This result shows that the ﬁnite- support restriction is critical to bound the maximum deviation of the entropy functional, since otherwise the expression is unbounded; furthermore, they proved the continuity of entropy for distributions deﬁned on a ﬁnite and known support [6, The- orem 6]. Complementing this continuity result, Harremoes [4, Theorem 21] obtained convergence of the entropy by imposing a power dominating condition [4, Def. 17] on the limiting probability measure µ, for all the sequences {µ n : n ≥ 0} converging in reverse I-divergence to µ [7].\nAnother view of the problem was provided by Piera et al. [10]. This work focused exclusively on the applications of measure-theoretic convergence results [11], [12] to guarantee the convergence of the entropy, instead of focusing on proving continuity (in total variation or in any strong information topol- ogy [4]) in a subset of the space of probability measures. By applying the dominated convergence theorem [11], [12], [13], they showed sufﬁcient conditions for the entropy convergence requiring that the limiting probability measure µ is neither ﬁnitely supported nor power dominated, and only a ﬁnite entropy for µ was assumed.\nIn this work we follow the approach presented in [10]. We begin by revisiting the ﬁnite support scenario, where we stipu- late necessary and sufﬁcient conditions for the convergence of the entropy. This result is obtained for the case in which the limiting distribution has a ﬁnite but unknown support, which, to the best of our knowledge, has not been addressed before. The results complement the fact that convergence in direct I- divergence is sufﬁcient for the entropy convergence [5], [6], and the fact that controlling the support disagreement between the limiting measure µ and the sequence {µ n : n ≥ 0} is critical to achieve lim n→∞ H(µ n ) = H(µ) [6]. For the more challenging case where the limiting measure µ has an inﬁnite and unknown support, we obtain new results that provide sufﬁcient conditions for the entropy convergence, in the unexplored scenario when µ µ n for all n. In the process, we also obtain convergence of the distributions in direct and reverse I-divergence. These results show that there is a stronger interplay between Shannon entropy and reverse I-divergence than between entropy and direct I-divergence, in terms of the extra conditions needed to guarantee that distribution convergence implies the convergence of the Shannon entropy functional.\nLet X be a countable inﬁnite space, without loss of gen- erality the integers, and let (X, B(X)) denote the measurable space, in which the σ-algebra B(X) is the power set of X. Let P(X) be the collection of probability measures in (X, B(X)). Let µ and v be in P(X), if µ is absolutely continuous with respect to v 2 , denoted by µ \t v, then dµ dv (x) denotes the Radon-Nikodym (RN) derivative of µ with respect to v 3 [13], [14]. In this context AC(X|v) denote the collection of µ ∈ P(X) where µ v. Every µ ∈ P(X) is absolutely continuous with respect to the counting measure λ in X (or the Lebesgue measure ) 4 , where its RN derivate f µ (x) ≡ dµ dλ (x) = µ ({x}) is the probability mass function (pmf). For any µ ∈ P(X) we denote its support by A µ ≡ supp(µ) ≡ {x ∈ X : f µ (x) > 0}, and we denote by\nthe collection of probability measures with ﬁnite support. Then, it is simple to verify the following:\nf µ (x) > 0. \t (2) Let M(X) denote the collection of functions from X to R.\nthe space of µ integrable functions, where the L 1 -norm of g ∈ l 1 (µ) is given by:\nLet µ and v in P(X), the total variation distance of µ and v is given by [9]\nFrom Sheffe\u2018s identity [15], this metric can be expressed in terms of the L 1 norm of the involved pmf\u2018s, i.e.,\nA stronger notion of similarity between distributions was proposed by Kullback and Leibler [16] (see also [17], [18], [14], [8]) as an indicator of discrimination information. The Kullback-Leibler divergence or I-divergence of µ with respect to v is given by:\nlog dµ dv\nf µ (x) log f µ (x) f v (x)\nFor D(µ||v) to be well-deﬁned, it is necessary that µ v (or equivalently that A µ ⊂ A v ). It is well known that D(µ||v) ≥ 0, and that D(µ||v) = 0 if, and only if, µ = v. Pinsker\u2018s inequality offers a relationship between the I-divergence and the total variational distance [19], [18], [8]. More precisely, ∀µ, v ∈ P(X),\n2 ln 2 · V (µ, v) 2 ≤ D(µ, v). \t (8) Finally for any given v ∈ P(X), let H(X|v) denote the\ncollection of probability measures where the I-divergence with respect to v is well deﬁned, i.e.,\nlog dµ dλ\n(11) to be the Shannon entropy of µ [1], [14], [20]. Hence H(X) is the space of ﬁnite entropy distributions, where it is simple to verify that: F (X) ⊂ H(X); P(X) = H(X); and that there are inﬁnite-supported distributions with ﬁnite entropy, i.e., F (X) c ∩ H(X) = ∅ [1], [14].\nIn the following, we denote the space of bounded real- valued functions form X to R by M b (X).\nDeﬁnition 1: A sequence {µ n : n ∈ N} ⊂ P(X) is said to converge weakly to µ ∈ P(X), if ∀f ∈ M b (X),\nThe standard notation for this convergence is µ n ⇒ µ 5 . It is simple to verify that µ n ⇒ µ is equivalent to the point-wise convergence of the measures (or the weak topology in P(X)) given by: lim n→∞ µ n ({x}) = µ({x}) for all x ∈ X [10].\nDeﬁnition 2: A sequence {µ n : n ∈ N} ⊂ P(X) is said to converge in total variation (or variation) to µ ∈ P(X), if\nV (µ n , µ) = 0. \t (13) By (5) the convergence in total variation implies the uniform convergence of the measures, i..e,\nIt is well-known that the convergence in total variations is stronger than the weak convergence, however when X is countable these two notions are equivalent [10, Lemma 3].\nFinally, the last notion to mention is the convergence in I-divergence introduces by Barron et al. [7].\nDeﬁnition 3: Let {µ n : n ∈ N} be a sequence in H(X|µ) with µ ∈ P(X). We say that {µ n : n ∈ N} converges to µ in reverse I-divergence if,\nAlternatively, let {µ n : n ∈ N} be in P(X) and µ ∈ P(X) such that µ \t µ n , for all n. Then {µ n : n ∈ N} is said to converge to µ in direct I-divergence if,\nD(µ||µ n ) = 0. \t (16) From Pinsker\u2018s inequality in (8), the convergence in direct and reverse I-divergence is stronger than total variational convergence. The reciprocal result is not true [4], [8].\nThe following two sections will be devoted to ﬁnding con- ditions in a sequence {µ n : n ∈ N} and its limiting measure µ (in total variation) that guarantee the convergence of the Shannon entropy functional, i.e., lim n→∞ H(µ n ) = H(µ).\nWe start with the scenario where µ ∈ F (X), i.e., the support of the limiting measure is ﬁnite but unknown. In this context if µ n µ, then µ n ⇒ µ sufﬁces to obtain the convergence of the entropy and convergence of the distribution in the reverse I-divergence sense, as stated in [10, Sec. 5].\nThe next result does not impose either that µ µ n for all n, or that µ n \t µ for all n. Furthermore, we obtain necessary and sufﬁcient conditions for the convergence of the entropy reﬁning the previously mentioned result.\ni) There exists N > 0 such that µ \t µ n (A µ ⊂ A µ n ) ∀n ≥ N , and furthermore\nµ(·|B) denote the conditional probability with respect to the event B ⊂ X.\nRemark 1: 1) First note that in this ﬁnite-supported case, µ is eventually absolutely continuous with respect to µ n , and then, total variational convergence is equivalent to the convergence in direct I-divergence (from i)). 2) The sufﬁcient and necessary condition needed in ii) has to do with ensuring a vanishing expression for the tail component of the entropy of µ n , restricted to the support disagrement A µ n \\ A µ . 3) The convergence in direct I-divergence is not sufﬁcient to guarantee the convergence of the entropy. Concrete examples demonstrating this point have been presented in [5] for the ﬁnitely supported scenario. 4) From the examples in [5], controlling the support discrepancy A µ n \\ A µ seems to be a critical ingredient for achieving the convergence of the entropy. This fact is formalized in Theorem 1 point ii).\nNote that H (µ n (·|A µ n \\ A µ )) ≤ log |A µ n \\ A µ | [1], then we can state the following corollary.\nRemark 2: From µ n \t ⇒ \t µ, we have that lim n→∞ µ n (A µ n \\ A µ ) = 0 (see proof of Theorem 1), then if A µ n \\ A µ is uniformly in n bounded by a ﬁnite set, i.e., ∃M > 0, ∃B ⊂ X a ﬁnite set such that A µ n \\ A µ ⊂ B for all n > M , then we have the entropy convergence from Corollary 1.\nTheorem 1 also implies the entropy convergence result stated at the beginning of this section.\nCorollary 2: Under the assumptions of Theorem 1 if we add the condition that µ n µ, then\nand the following three convergence criteria are equiv- alent: lim n→∞ V (µ n , µ) = 0, lim n→∞ D(µ n ||µ) = 0 and lim n→∞ D(µ||µ n ) = 0.\nIf µ n \t µ ∈ F (X) then D(µ n ||µ) < ∞ and we can use the following identity:\n(19) However, considering that µ n is not absolutely continuous with respect to µ and vice versa, we have that:\nThe ﬁrst term of the RHS of (20) is upper bounded by M µ · V (µ n , µ), where\nwhich vanishes as n goes to inﬁnity. The second term tends to zero as by the uniform convergence of µ n to µ, we have that\n− 1 = 0. For the third term, note that by the point-wise convergence and the fact that |A µ | is ﬁnite, it is simple to verify that there exists N < ∞ such that ∀n ≥ N , A µ ⊂ A µ n (i.e., µ \t µ n eventually). Hence A µ \\ A µ n is the empty set eventually, which implies that the\nthird term is zero eventually and, consequently, zero in the asymptotic regime. Consequently we have that,\nTo conclude, the expression in the RHS of (22) is equivalent to (18) from deﬁnition of conditional probability and the fact that lim n→∞ µ n (A µ n \\ A µ ) · log (1/µ n (A µ n \\ A µ )) = 0. Finally (17) is taken directly from the uniform convergence of the sequence to µ and the fact µ µ n eventually in n.\nB. Revisiting Examples of Entropy Discontinuity and Conver- gence\nThe proof of the discontinuity of the entropy in the count- able inﬁnity case presented in [5, Theorem 2], was based on the analysis of the following construction:\nwith α > 0 and n ≥ 1. In fact, Ho et al.[5, Sec. III] showed that lim n→∞ D(µ o ||µ n ) = 0 for µ o ≡ {1, 0, . . .} ∈ F (X), where lim n→∞ H(µ n ) = α > H(µ o ) = 0. From this exam- ple, the proof of the discontinuity of the entropy functional at all probability measures was derived [5, Appendix A, pp. 5372]. Then it is of interest to revisit this key example in light of the presented ﬁnite-supported results.\nIt is a simple exercise to verify that this discontinu- ity derives directly from Theorem 1, as we are in the ﬁ- nite support scenario where µ n ⇒ µ o = {1, 0, . . .} and lim n→∞ µ n (A µ n \\ A µ o ) · H (µ n (·|A µ n \\ A µ o )) = α > 0. Furthermore from Theorem 1, we can stipulate necessary and sufﬁcient conditions for the entropy convergence (or non- convergence) for a more general construction that includes (23) as a particular case. In particular, let us consider the sequence of measures\nfunction of (a n ) n≥1 , a sequence in (0, 1) being o(1), and (k n ) n≥1 , a sequence in N \\ {0} with k n → ∞ (or (1/k n ) being o(1)). Note that lim n→∞ µ n (a n , k n ) = µ o , H(µ n |A µ n \\ A µ o ) = log k n , and µ n (A µ n \\ A µ o ) = a n for all n ≥ 1. Hence from Theorem 1, for the convergence of the entropy it is necessary and sufﬁcient that lim n→∞ a n · log k n = 0 (equivalent to say that log(k n ) is o(1/a n )). In particular, for a n = K n τ and k n = n θ with K > 0, τ > 0 and θ > 0 the convergence is obtained, while for a n = α log n and k n = n θ\nwith α > 0 and θ ∈ N the convergence is not obtained, which is the case in (23). Finally for a n = 1 n β and k n = γ n for β > 0 and γ > 0, the convergence is obtained if and only if β > 1, this scenario was constructed and analyzed in [5, Sec. VII] using the marginal distributions of a Markov chain, obtaining a congruent conclusion.\nHere we consider the scenario where the support of µ could be inﬁnite and unknown, i.e., |A µ | = ∞. In this context Piera et al. [10] introduced the following result.\nTHEOREM 2: [10, Theorem 4] Let µ ∈ H(X) and let us consider that {µ n : n ≥ 0} ⊂ AC(X|µ). If µ n ⇒ µ and\nInterpreting the result, to obtain the convergence of the entropy without imposing a ﬁnite support assumption on the limiting measure µ, a uniform bounding condition (UBC), µ- almost surely, was added in (25). This UBC allows the use of the dominated convergence theorem, and it is strictly needed in that sense [12], [13], [11] 6 . As a consequence of adding the UBC, the convergence on reverse I-divergence is also obtained, reafﬁrming the relationship reported in Corollary 2 between entropy convergence and convergence in reverse I-divergence in the regime when µ n µ for all n.\nConcerning the reciprocal case when µ µ n for all n, we state the following result.\nTHEOREM 3: Let µ ∈ H(X) and a sequence of measures {µ n : n ≥ 1} ⊂ H(X), such that µ µ n for all n ≥ 1. If\n< ∞ iii) lim n→∞ x∈A\n= 0, then, µ ∈ H(X|µ n ) for all n ≥ 1, and\nNote that D(µ||µ n ) < ∞ from the UBC in ii). Let us write D(µ||µ n ) by\n. Note that φ n (x) is bounded µ-almost everywhere from ii), and φ n (x) → 0 point-wise from i). Then by the bounded convergence theorem [12, Th. 1.4], lim n→∞ D(µ||µ n ) = 0. Concerning the second term in (26),\n(x)dµ(x) where ˜ f 1\n· (f µ n (x)/f µ n (x) − 1) ∈ l 1 (µ). From ii) both terms log f µ (x) f\nand (f µ n (x)/f µ n (x) − 1) are upper bounded µ-almost everywhere. Then as ˜ f 1 n (x) → 0 point-wise from i), this integral tends to zero by the bounded convergence theorem.\nOn the other hand, (30) can be written as X ˜ f 2 n (x)dµ(x) where ˜ f 2 n (x) ≡ 1 A µ (x) · log 1 f\n· (f µ n (x)/f µ n (x) − 1) ∈ l 1 (µ). Hence from ii) there exists ˜ M < ∞ such that\nGiven that µ ∈ H(X) and the fact that ˜ f 2 n (x) → 0 point-wise from i), the integral in (30) tends to zero by the dominated convergence theorem [12, Th. 1.8]. Then (27) tends to zero and integrating the results,\nRemark 3: It is important to mention that from i) and ii), we have that µ ∈ H(X|µ n ) ∀n, and that lim n→∞ D(µ||µ n ) = 0. Hence, the tail vanishing entropy conditions in iii) is needed to achieve the entropy convergence emphasizing, as in the ﬁnite-supported result in Theorem 1, the non-sufﬁciency of the convergence in direct I-divergence for the entropy convergence in the regime when µ µ n for all n.\nProposition 2: Given the weak convergence µ n ⇒ µ, a sufﬁcient conditions to obtain iii) in Theorem 3 is that there exists a ﬁnite set B and N > 0 such that A µ n \\ A µ ⊂ B for all n ≥ N . (The proof is omitted for the space constraints.)\nThen in the inﬁnite-supported scenario for µ, being able to control the support disagreement is critical to obtain the convergence of the entropy. A direct corollary of this result is the following:\nCorollary 3: Under the general assumption of Theorem 3, if A µ = X (consequently A µ n = X for all n), then under the conditions i) and ii) of Theorem 3, lim n→∞ D(µ n ||µ) = 0, lim n→∞ D(µ||µ n ) = 0 and lim n→∞ H(µ n ) = H(µ).\nProof: The important thing to notice is that the UBC of Theorem 3 (condition iii)) is stronger than the UBC of Theorem 2 in (25), so then the convergence in reverse I- divergence is obtained from Theorem 2.\nFinally note that in this last scenario, there is no support dis- agreement between µ and µ n , for any ﬁxed n, which implies a strong relationship between convergence in I-divergence and entropy convergence.\nWe are currently working on the application of these ideas and results into the problems of entropy estimation and esti- mation of distributions, consistently in information divergence.\nThis material is based upon work supported by awards of CONICYT-Chile, Fondecyt 1110145. We thanks the anony- mous reviewer for valuable comments and suggestions.\nTHEOREM 4: [12, Th. 1.4, pp. 10] Let {f n } be a se- quence of measurable functions uniformly bounded with re- spect to a probability measure µ, if f n converges to f in measure 7 , then lim n→∞ f n dµ = f µ."},"refs":[{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, Wiley Interscience, New York, 1991"}},{"authors":[{"name":"B. As"}],"title":{"text":"Robert  Information Theory, Dover Publications, 1965"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[{"name":"P. Herremoe"}],"title":{"text":"Entropy"}},{"authors":[{"name":"W. Yeung"}],"title":{"text":"On the discontinuity of the Shannon information measures"}},{"authors":[{"name":"W. Yeung"}],"title":{"text":"The interplay between entropy and variational distance"}},{"authors":[{"name":"L. Gy¨orﬁ"},{"name":"C. van der Meulen"}],"title":{"text":"Distribution estimation consistent in total variation and in two types of information divergence"}},{"authors":[{"name":"I. Csisz´a"},{"name":"C. Shield"}],"title":{"text":"Paul  Information theory and Statistics: A tutorial , Now Inc"}},{"authors":[{"name":"L. Devroy"},{"name":"G. Lugos"}],"title":{"text":"Combinatorial Methods in Density Estima- tion , Springer - Verlag, New York, 2001"}},{"authors":[],"title":{"text":"On convergence properties of Shannon entropy"}},{"authors":[{"name":"P. R. Halmo"}],"title":{"text":"Measure Theory, Van Nostrand, New York, 1950"}},{"authors":[{"name":"S. Varadha"}],"title":{"text":"S"}},{"authors":[],"title":{"text":"Leo Breiman, Probability, Addison-Wesley, 1968"}},{"authors":[{"name":"R. M. Gra"}],"title":{"text":"Entropy and Information Theory, Springer - Verlag, New York, 1990"}},{"authors":[{"name":"H. Scheff´e"}],"title":{"text":"A useful convergence theorem for probability distribution"}},{"authors":[{"name":"S. Kullback"},{"name":"R. Leibler"}],"title":{"text":"On information and sufﬁciency"}},{"authors":[{"name":"S. Kullbac"}],"title":{"text":"Information theory and Statistics, New York: Wiley, 1958"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"Information-type measures of difference of probability distributions and indirect observations"}},{"authors":[{"name":"S. Kullback"}],"title":{"text":"A lower bound for discrimiantion in terms of variation"}},{"authors":[{"name":"J. Beirlant"},{"name":"E. Dudewics"},{"name":"L. Gy¨orﬁ"},{"name":"C. van der Meulen"}],"title":{"text":"Nonparametric entropy estimation: An overview"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565389.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S1.T8.2","endtime":"11:10","authors":"Jorge Silva, Patricio Parada","date":"1341226200000","papertitle":"Shannon Entropy Convergence Results in the Countable Infinite Case","starttime":"10:50","session":"S1.T8: Information Theoretic Tools and Properties","room":"Stratton (491)","paperid":"1569565389"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
