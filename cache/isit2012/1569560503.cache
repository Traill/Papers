{"id":"1569560503","paper":{"title":{"text":"On Decoding Error Exponent of Gaussian Channel with Noisy Feedback: Nonexponential Number of Messages"},"authors":[{"name":"Marat V. Burnashev"},{"name":"Hirosuke Yamamoto"}],"abstr":{"text":"Abstract\u2014For information transmission a discrete time chan- nel with independent additive Gaussian noise is used. There is also feedback channel with independent additive Gaussian noise, and the transmitter observes without delay all outputs of the forward channel via that feedback channel. Transmission of nonexponential number of messages is considered and the achievable decoding error exponent for such a combination of channels is investigated. It is shown that for any ﬁnite noise in the feedback channel the achievable error exponent is better than similar error exponent of the no-feedback channel. Method of transmission/decoding used in the paper strengthens the earlier method used by authors for BSC. In particular, for small feedback noise, it allows to get the gain up to 23.6% (instead of 14.3% earlier for BSC)."},"body":{"text":"We consider the discrete time channel with independent additive Gaussian noise, i.e. if x = (x 1 , . . . , x n ) is the input codeword then the received block y = (y 1 , . . . , y n ) is\nwhere ξ = (ξ 1 , . . . , ξ n ) are independent N (0, 1)\u2013Gaussian random variables, i.e. Eξ i = 0, Eξ 2 i = 1. It is also assumed that there is a noisy feedback channel, and the transmitter observes (without delay) all outputs of the forward channel via this noisy feedback channel\nwhere η = (η 1 , . . . , η n ) are independent (and independent of ξ) N (0, 1)\u2013Gaussian random variables, i.e. Eη i = 0, Eη 2 i = 1, and the value σ = 0 is given. No coding is used in the feedback channel (i.e. the receiver simply re-transmits all received outputs to the transmitter). In other words, the feedback channel is \u201cpassive\u201d (see Fig. 1).\n 6\n- x \t y\nwhere A is a given constant. For short, we denote by AWGN(A) the channel (1) with constraint (3) without feed- back, and by AWGN(A, σ) that channel with noisy feedback (2).\nSince Shannon\u2019s paper [1] it has been known that even noiseless feedback does not increase the capacity of the Gaussian (or any other memoryless) channel. However, feed- back can improve the decoding error probability (or simplify effective transmission methods). In the case of noiseless feed- back possibility of such improvement of the decoding error probability with respect to no-feedback channel was shown for a number of channels in [3]\u2013[10].\nIn case of noisy feedback possibility of such improvement with respect to no-feedback channel was ﬁrst shown in [11], [12] (for binary symmetric channel). The purpose of this paper is to get similar (in fact, much stronger) results for Gaussian channel.\nWe consider the case when the overall transmission time n and M = e o(n) equiprobable messages {θ 1 , . . . , θ M } are given. After the moment n, the receiver makes a decision ˆ θ on the message transmitted. We are interested in the best possible decoding error exponent (and whether it can exceed similar exponent of the channel without feedback).\nSuch case of nonexponential (on n) number M (i.e. R = 0) was investigated in [11], [12] (for BSC channel). Later in [13], [14] those results were generalized to positive rates R.\nSome related results or topics for channels with noiseless feedback can be found in [2\u201312], and for the case of noisy feedback \u2013 in [15], [16] (see also discussion in [12]).\nFor comparison with the paper results we remind results ob- tained earlier in [11]\u2013[14]. There the direct and feedback chan- nels were BSC(p) and BSC(p 1 ), respectively. It was shown in [11]\u2013[14] that there exists a critical value p crit (p, R) > 0 such that if p 1 < p crit (p, R) then it is possible to improve decoding error exponent of the no-feedback channel. If, in particular, both R and p 1 are small then gain is 14.3%. In order to get such improvement the transmission/decoding method with one \u201cswitching\u201d moment was developed and investigated.\nThe method of [11], [12] was applied to Gaussian channel AWGN(A, σ) in [18] with similar to [11], [12] results (in particular, with the same asymptotic gain of 14.3%).\nRemark 1: The transmission method used in [11]\u2013[14] re- duces the problem to testing of two most probable (at some ﬁxed moment) messages. It was mentioned in [12, Remark 1] and [14, Remark 3] that such method is not optimal even for one switching moment.\nIn the paper, still using one \u201cswitching\u201d moment, we essentially improve the transmission/decoding method from [11]\u2013[14]. We show that for any feedback noise intensity σ 2 < ∞ it is possible to improve the best error exponent of the AWGN(A) channel without feedback. The transmis- sion/decoding method with one \u201cswitching\u201d moment, giving such improvement, is described in Section II. It strengthens the method introduced by authors earlier in [11]\u2013[14]. Of course, if σ is not small then the improvement is small (but strictly positive).\nRemark 2: We consider the case when feedback noise in- tensity σ > 0 is ﬁxed and does not depend on the number of messages M . The case when σ is such that σ 2 M is small represents, in a sense, the noiseless feedback case (cf. [18]).\nx i y i , x 2 = (x, x), d (x, y) = x − y 2 . A subset C = {x 1 , x 2 . . . , x M } with\nx i 2 = An, i = 1, 2, . . . , M is called a (M, A, n)\u2013code of length n.\nFor a code C = {x i } denote by P e (C) the minimal possible decoding error probability\nwhere P (e|x i ) \u2013 conditional decoding error probability pro- vided x i was transmitted, and minimum is taken over all decoding methods.\nIf M = M n is the number of messages, then for AWGN(A) channel denote by P e (M, A, n) the minimal possible decoding error probability for the best (M, A, n)\u2013code. Introduce the exponent (in n) of that function\n1 n\nSimilarly, for AWGN(A, σ) channel let P e (M, A, σ, n) be the minimal possible decoding error probability for the best transmission method. Introduce the function\nIn the paper we consider the case when (ln M n )/n → 0 as n → ∞ (it corresponds to zero-rate of transmission).\nIt is also known that if σ = 0 (i.e. noiseless feedback) then [7] for all ﬁxed M\nIn other words, for large (but nonexponential) M noiseless feedback gives 100% of error exponent improvement over similar error exponent of the no-feedback Gaussian channel.\nFor AWGN(A, σ) channel denote by F 1 (M, A, σ) the error exponent for the transmission method with one switching moment, described in Section II. Clearly, F 1 (M, A, σ) ≤ F (M, A, σ) for all M, A, σ.\nIn order to avoid bulky general formulations we present results only in two extreme cases: small σ and large σ. Our main results are as follows.\nTheorem . Let ln M = o(n), n → ∞. Then: a) If σ → 0 then the formula holds\n5) ≈ 0.236, then for large M formula (6) gives 23.6% of improvement with respect to the no-feedback channel .\nIn other words, for any non-exponential M n and any σ < ∞ we get an improvement of the no-feedback error exponent.\nRemark 3: Using the transmission strategy with one ﬁxed switching moment, it is possible to get better gain than in (6) (elaborating more intensively dependence among random variables involved). But computations become too much bulky. Much more interesting would be to investigate transmission strategies with several (or increasing) number of switching moments.\nWe use the transmission strategy with one ﬁxed switching moment at which the coding function will be changed. The transmission method used earlier in [11]\u2013[14], (and in [18]) reduces the problem to testing of two most probable (at some ﬁxed moment) messages. We improve that strategy in both transmission and decoding stages.\nFor simplicity, we consider the case M ≤ (n + 2)/2. We partition the total transmission time [1, n] on two phases: [1, M − 1] (phase I) and [M, 2M − 2] (phase II). Thus the total length of the code used is 2M − 2. The remaining time [2M − 1, n] is not used. After moment 2M − 2 the receiver makes a decision in favor of the most probable message θ i (based on all received on [1, 2M − 2] signals).\nEach of M codewords {x i } of length 2M − 2 have the form x i = (x i , x i ), where x i has length M − 1 (to be used on phase I) and x i also has length M − 1 (to be used on phase II). Similarly, the received block y can be represented as y = (y , y ), where y is the block received on phase I and y is the block received on phase II. Denote by z the received (by the transmitter) block on phase I. The codewords ﬁrst parts {x i } are ﬁxed, while the second parts {x i } will depend on the block z received by the transmitter on phase I.\nArrange the distances {d(x i , y ) = y − x i 2 , i = 1, . . . , M } for the receiver after phase I in the increasing order, denoting\n(case of tie has zero probability). Let also x (1) , x (2) , . . ., x (M ) be the corresponding ranking of codewords {x } after phase I for the receiver, i.e x (1) is the closest to y codeword, etc.\nLet also x (1)t , x (2)t , . . . , x (M )t be the corresponding rank- ing of codewords {x } after phase I for the transmitter, i.e x (1)t is the closest to z codeword, etc.\nTransmission. On phase I the transmitter uses a simplex code of M codewords {x i } of length M −1 such that x i 2 = A 1 .\nFor phase II we set a number τ 0 > 0. Based on the received block z the transmitter selects three most probable codewords x (1)t , x (2)t , x (3)t and calculates for them the value d (3)t − d (2)t = τ A 3 ≥ 0. Depending on the value τ the transmitter uses the following code {x k } with x k 2 = A 2 , k = 1, . . . , M :\nthen on phase II the transmitter uses the same simplex code of M codewords {x i } of length M − 1. Case 2 : If after phase I\nthen on phase II the transmitter uses another code {x k }: a) two most probable messages θ i , θ j have opposite codewords x i = −x j of length M − 1 which have non- zero coordinates only at moment M − 2;\nb) remaining M − 2 messages {θ k } use a simplex code of M −2 codewords {x k } of length M −3 trailed by two 0\u2019s at moments M −2, M −1. All those M −2 codewords {x k } are orthogonal to the ﬁrst two codewords (x i , x j ).\nThis transmission method generalizes the method used in [14]. The code used in case I resolves the case when after phase I three most probable codewords x (1)t , x (2)t , x (3)t are approximately equiprobable.\nRemark 4: The assumption M ≤ (n + 2)/2 was used only to simplify description of the codes (simplex) used on phases I and II. In fact, all results remain valid if for a code C on phase I we have min i=j x i − x j 2 ≥ 2A 1 (1 + o(1)) as n → ∞ (and similarly for phase II). That condition can be fulﬁlled if M = e o(n) .\nDecoding. Due to noise in the feedback channel the re- ceiver does not know exactly codewords x (1)t , x (2)t , x (3)t and the value τ , and therefore it does not know the code used on phase II. But based on the received block y , the receiver may evaluate the conditional probabilities of code- words x (1)t , x (2)t , x (3)t and of the value τ , and so get the probabilities with which each code was used.\nAs a result, based on whole received block y, the receiver uses maximum likelihood decoding, evaluating posterior prob- abilities {p(y|x i )} and making decision in favor of most probable message θ i . Such full decoding is described below.\nIf x true is the true codeword then y = x true + ξ and ξ = (ξ , ξ ) = (ξ 1 , . . . , ξ n ), where all {ξ i } are independent N (0, 1)\u2013Gaussian random variables. If x true = x 1 , then\nThe receiver makes decision after moment n using whole received block y. If after phase I the difference d (3) − d (2) is rather close to τ 0 A 3 (see (10) and (11)) then due to noise in the feedback link the receiver cannot be sure which code was used by the transmitter on phase II (since lists {x (1) , x (2) } and {x (1)t , x (2)t } may turn out to be different). But based on\ny the receiver knows the probability distribution of the code used by the transmitter on phase II. Then in the decoding it should take into account that distribution.\nIf θ true = θ 1 , then for decoding error probability P e we have\np y θ i p y θ 1\n≤ M P ln p y θ 2 p y θ 1\np y θ 2 p y θ 1\np y y , θ 2 p y y , θ 1\nMain difﬁculty represents investigation of random variable Y . For that purpose denote\nwhere | · | represents the cardinality of a set. Sets Z 2 , Z 3 , Z 4 describe all possible relations between pairs {x 1 , x 2 } and {x (1)t , x (2)t } of most probable messages for the receiver and the transmitter, respectively. Note that only the cardinality of the intersection {x 1 , x 2 } {x (1)t , x (2)t } matters in the evaluation of Y .\np y y , θ 2 p y y , θ 1\nwhere x 1 , x 2 depend on k (via Z k ). If θ true = θ 1 , then y = x 1 + ξ , and\np y y , θ 2 p y y , θ 1\nFurther proof of Theorem consists of evaluating proba- bilities {p k } and values {B k }. It is rather computationally involved and general formulas are quite bulky. For that reason we present resulting formulas for {B k } only when σ → 0 and M → ∞. In that case we set τ 0 → 0 and get (as n → ∞)\nWe set β such that both terms under minimization become equal, i.e. set\nA developed version of this paper with detailed proofs will appear as [19].\nThe authors wish to thank the University of Tokyo for supporting this joint research."},"refs":[{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"The zero error capacity of a noisy channel"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"Probability of Error for Optimal Codes in Gaussian Channel"}},{"authors":[{"name":"R. L. Dobrushin"}],"title":{"text":"Asymptotic bounds on error probability for message transmission in a memoryless channel with feedback"}},{"authors":[{"name":"M. Horstein"}],"title":{"text":"Sequential decoding using noiseless feedback"}},{"authors":[{"name":"E. R. Berlekam"},{"name":"D. Thesi"}],"title":{"text":"Block coding with noiseless feedback, Ph"}},{"authors":[{"name":"J. P. M. Schalkwijk"}],"title":{"text":"A coding scheme for additive noise channels with feedback"}},{"authors":[{"name":"M. S. Pinsker"}],"title":{"text":"The probability of error in block transmission in a memoryless Gaussian channel with feedback"}},{"authors":[{"name":"M. V. Burnashev"}],"title":{"text":"Data transmission over a discrete channel with feedback: Random transmission time"}},{"authors":[{"name":"M. V. Burnashev"}],"title":{"text":"On a Reliability Function of Binary Symmetric Channel with Feedback"}},{"authors":[{"name":"H. Yamamoto"},{"name":"K. Itoh"}],"title":{"text":"Asymptotic performance of a modiﬁed Schalkwijk\u2013Barron scheme for channels with noiseless feedback"}},{"authors":[{"name":"M. V. Burnashev"},{"name":"H. Yamamoto"}],"title":{"text":"On BSC, noisy feedback and three messages"}},{"authors":[{"name":"M. V. Burnashev"},{"name":"H. Yamamoto"}],"title":{"text":"On zero-rate error exponent for BSC with noisy feedback"}},{"authors":[{"name":"M. V. Burnashev"},{"name":"H. Yamamoto"}],"title":{"text":"Noisy Feedback Improves the BSC Reliability Function"}},{"authors":[{"name":"M. V. Burnashev"},{"name":"H. Yamamoto"}],"title":{"text":"On reliability function of BSC with noisy feedback"}},{"authors":[{"name":"S. C. Draper"},{"name":"A. Sahai"}],"title":{"text":"Noisy feedback improves communication reliability"}},{"authors":[{"name":"Y.-H. Kim"},{"name":"A. Lapidoth"},{"name":"T. Weissman"}],"title":{"text":" The Gaussian channel with noisy feedback"}},{"authors":[{"name":"A. Tchamkerten"},{"name":"E. Telatar"}],"title":{"text":"Variable length coding over an unknown channel"}},{"authors":[],"title":{"text":"On the AWGN channel with noisy feedback and peak energy constraint"}},{"authors":[{"name":"M. V. Burnashev"},{"name":"H. Yamamoto"}],"title":{"text":"On reliability function of Gaussian channel with noisy feedback: zero rate"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569560503.pdf"},"links":[{"id":"1569566381","weight":5},{"id":"1569566527","weight":5},{"id":"1569565383","weight":10},{"id":"1569565223","weight":10},{"id":"1569566725","weight":5},{"id":"1569565663","weight":10},{"id":"1569566385","weight":10},{"id":"1569565867","weight":5},{"id":"1569564605","weight":5},{"id":"1569559617","weight":15},{"id":"1569566981","weight":5},{"id":"1569566683","weight":10},{"id":"1569565551","weight":5},{"id":"1569566943","weight":5},{"id":"1569556029","weight":5},{"id":"1569566571","weight":5},{"id":"1569552245","weight":15},{"id":"1569566415","weight":10},{"id":"1569566469","weight":5},{"id":"1569565355","weight":10},{"id":"1569564469","weight":5},{"id":"1569566765","weight":5},{"id":"1569565461","weight":5},{"id":"1569564731","weight":5},{"id":"1569566671","weight":5},{"id":"1569564233","weight":15},{"id":"1569563411","weight":15},{"id":"1569566941","weight":15},{"id":"1569565291","weight":10},{"id":"1569564203","weight":5},{"id":"1569556713","weight":5},{"id":"1569566467","weight":5},{"id":"1569565771","weight":5},{"id":"1569566999","weight":5},{"id":"1569566843","weight":5},{"id":"1569566579","weight":5},{"id":"1569566089","weight":5},{"id":"1569565455","weight":21},{"id":"1569566709","weight":15},{"id":"1569564989","weight":15},{"id":"1569551763","weight":5},{"id":"1569565953","weight":5},{"id":"1569565709","weight":5},{"id":"1569566269","weight":5},{"id":"1569566985","weight":5},{"id":"1569566095","weight":5},{"id":"1569565907","weight":5},{"id":"1569559565","weight":10},{"id":"1569563307","weight":5},{"id":"1569558681","weight":10},{"id":"1569565213","weight":10},{"id":"1569567665","weight":10},{"id":"1569566581","weight":5},{"id":"1569565833","weight":5},{"id":"1569565667","weight":5},{"id":"1569566423","weight":5},{"id":"1569567015","weight":5},{"id":"1569566437","weight":5},{"id":"1569553909","weight":15},{"id":"1569559111","weight":5},{"id":"1569553537","weight":5},{"id":"1569565915","weight":5},{"id":"1569553519","weight":10},{"id":"1569566425","weight":5},{"id":"1569554881","weight":5},{"id":"1569566209","weight":10},{"id":"1569565655","weight":5},{"id":"1569566909","weight":5},{"id":"1569566473","weight":5},{"id":"1569565033","weight":15},{"id":"1569565055","weight":5},{"id":"1569555879","weight":5},{"id":"1569565219","weight":15},{"id":"1569554759","weight":5},{"id":"1569566553","weight":15},{"id":"1569565029","weight":5},{"id":"1569562207","weight":5},{"id":"1569565527","weight":5},{"id":"1569566603","weight":10},{"id":"1569565467","weight":10},{"id":"1569566655","weight":5},{"id":"1569566673","weight":5},{"id":"1569565441","weight":5},{"id":"1569566317","weight":5},{"id":"1569566501","weight":5},{"id":"1569565439","weight":5},{"id":"1569566133","weight":5},{"id":"1569562551","weight":5},{"id":"1569563395","weight":5},{"id":"1569566901","weight":5},{"id":"1569565415","weight":10},{"id":"1569566383","weight":5},{"id":"1569565571","weight":5},{"id":"1569565885","weight":5},{"id":"1569564411","weight":5},{"id":"1569566293","weight":5},{"id":"1569565665","weight":5},{"id":"1569566873","weight":5},{"id":"1569565765","weight":5},{"id":"1569565435","weight":5},{"id":"1569566261","weight":5},{"id":"1569565215","weight":5},{"id":"1569565181","weight":5},{"id":"1569565865","weight":10},{"id":"1569566737","weight":5},{"id":"1569561221","weight":5},{"id":"1569566253","weight":5},{"id":"1569564305","weight":10},{"id":"1569566823","weight":10},{"id":"1569565013","weight":5},{"id":"1569565375","weight":10},{"id":"1569566713","weight":5},{"id":"1569566813","weight":10},{"id":"1569566641","weight":15},{"id":"1569563975","weight":5},{"id":"1569551905","weight":10},{"id":"1569556759","weight":10},{"id":"1569565271","weight":5},{"id":"1569566397","weight":5},{"id":"1569558779","weight":5},{"id":"1569565233","weight":5},{"id":"1569565729","weight":5},{"id":"1569564923","weight":5},{"id":"1569566299","weight":10},{"id":"1569563919","weight":10},{"id":"1569566577","weight":10},{"id":"1569557851","weight":5},{"id":"1569565389","weight":5},{"id":"1569562367","weight":5},{"id":"1569566847","weight":5},{"id":"1569560459","weight":5},{"id":"1569565853","weight":5},{"id":"1569550425","weight":5},{"id":"1569565165","weight":5},{"id":"1569566413","weight":5},{"id":"1569565113","weight":5},{"id":"1569566375","weight":5},{"id":"1569565143","weight":5},{"id":"1569564257","weight":5},{"id":"1569564931","weight":5},{"id":"1569564141","weight":5},{"id":"1569564509","weight":15},{"id":"1569551751","weight":5},{"id":"1569566067","weight":10},{"id":"1569566825","weight":5},{"id":"1569566443","weight":5},{"id":"1569566727","weight":5},{"id":"1569565515","weight":5},{"id":"1569560581","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T8.5","endtime":"13:10","authors":"Marat V Burnashev, Hirosuke Yamamoto","date":"1341579000000","papertitle":"On Decoding Error Exponent of Gaussian Channel with Noisy Feedback: Nonexponential Number of Messages","starttime":"12:50","session":"S16.T8: Error Exponents","room":"Stratton (491)","paperid":"1569560503"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
