{"id":"1569566433","paper":{"title":{"text":"Application of Information-Type Divergences to  Constructing Multiple-priors and Variational  Preferences "},"authors":[{"name":"A. Ahmadi-Javid "}],"abstr":{"text":"Abstract\u2014   This  paper  studies  broad  classes  of  variational  and  multiple-priors  preferences  incorporating  divergence  measures  that  are  generalizations  of  the  relative  entropy.  It  is  shown  that  the  proposed  information-type  preferences  can  be  calculated  through  convex  optimization  problems  with  a  small  number  of  real variables. \nKeywords-   Information-type  divergnecs,  Generalized  relative  entropy,  Kullback-Leibler  divergence,  Variational  preferences,  Multiple-priors preferences, Decision thoery under ambiguty  "},"body":{"text":"The  notion  of  ambiguity,  which  was  first  raised  in  the  decision-theory  literature  by  Elsberg  in  1961  [6],  is  the  main  motivation  for  going  beyond  the  traditional  expected  utility  preferences  that  were  axiomatized  by  von  Neumann  and  Morgenstern  [13].  Hence,  in  last  two  decades,  various  preferences that try to model ambiguity have been introduced  and  axiomatized.  Gilboa  and  Schmeidler  [7]  proposed  multiple-priors  (or  maxmin  expected  utility)  preferences.  Hansen  and  Sargent  [8]  studied  multiplier  preferences  that  have  been  recently  axiomatized  by  Strzalecki  [15].  Maccheroni et al. [11] proposed and axiomatized a wider class  of preferences, called variational preferences, that includes as  special  cases  both  multiplier  and  mean-variance  preferences  introduced  by  Markowitz  [12],  on  their  domains  of  monotonicity.  Chateauneuf  and  Faro  [5]  introduced  homothetic  preferences.  Cerreia-Vioglio  et  al.  [4]  studied  the  class  of  uncertainty-averse  preferences  that  includes  all  the  above-listed preferences as special cases. \ntype  divergences  to  construct  a  coherent  risk  measure,  which  is  an  important  notion  in  the  mathematical-finance  literature.  In  this paper, we follow  on this idea to establish information-\ntype preferences under ambiguity, which are highly important  in  the  economic  literature.  Here,  we  explore  three  broad  classes  of  multiple-priors  and  variational  preferences  that  are  built by exploiting generalizations of the relative entropy. Our  analysis  shows  that  all  preferences  in  these  three  classes  can  be  computed  by  solving  convex  optimization  problems  with  only one or two real variables. \nThe  paper  is  organized  as  follows.  Notation  and  mathematical  formulation  are  in  Section  2.  Sections  3  and  4 \nrespectively consider classes of  variational  and multiple-priors  preferences  incorporating  weighted  Csiszar\u2019s  divergences.  Section  5  studies  a  wide  class  that  includes  all  of  the  preferences presented in  the two  preceding sections.  Section  6  concludes the paper. \nLet  ( \t ) P , , F Ω \t  be  a  probability  space  where  Ω  is  a  set  of  all  simple events,  F  is a  σ -algebra of subsets of  Ω , and  P  is a  reference probability measure on  . F  Suppose that  ∆  is the set  of  all  finitely  additive  probability  measures  on  ( ) . ,F Ω   Moreover,  suppose  that  L  is  the  set  of  all  Borel  measurable  functions  (random  variables)  \t ℜ → Ω : X \t ,  \t L Χ ⊆  is  a  subspace  including  all  real  numbers,  and  ∞ L  is  the  set  of  all  bounded Borel measurable functions. \nWe  define the utility function  \t ℜ → Χ : u \t  as a  measurable  function,  which  is  usually  assumed  to  be  non-decreasing  and  concave.  The  expected  utility  preference,  multiple-priors  (or  maxmin  expected  utility),  multiplier  and  variational  preferences are respectively defined as follows:  ( ) \t ( ) ( \t )\n  where  θ  is  a  positive  constant,  ℑ  is  a  convex  subset  of  ∆ ,  [ )\n∞ → ∆ , 0 : c \t  is  a  convex  function,  and  ( ) \t dP dP dQ dP dQ P Q D KL \t       = ∫ ln : \t  is  the  relative  entropy  of  Q  \nwith  respect  to  P ,  or  the  Kullback\u2013Leibler  divergence  from  Q  to  P . \nThe  generalized  relative  entropy  of  Q  with  respect  to  , P   denoted  by  ( ) Q P H g , ,  is  an  information-type  divergence  measure, called (Csiszar\u2019s) g-divergence, from  Q  to  : P  \nwhere  g  is a convex function with  ( ) 0 1 = g \t . This quantity is  an important non-symmetric (or directed) divergence measure  (see  [10]  for  more  details).  Note  that  ( ) 0 , ≥ Q P H g \t ,  and \nFollowing  [11],  the  weighted  g-divergence  \t ( ) Q P H w g , , \t ,  which  is  a  generalization  of  the  g-divergence  ( ) Q P H g , ,  may  be defined as follows: \nwhere  w  is  a  density  function  over  Ω  with  respect  to  the  reference probability measure  P . See [11] for more comments  on this type of divergence.  \nwhere  ( ) \t ( ) Q P H Q c \t w g , , θ = \t  with  0 ≥ θ \t . This class is a subclass  of variational preferences that was recently considered in [11];  however, no result was given on how such preferences can be  computed  practically.  In  the  following,  we  provide  a  useful  representation  for  this  class,  which  can  be  used  to  compute  such divergence preferences practically. \nwhere  * g  is the  conjugate (the  Legendre\u2013Fenchel transform)  of  g . \nProof.   This  result  can  be  proven  by  implementing  a  strong  Lagrangian  duality  theorem  for  the  following  optimization  problem,  implied  by  holding  a  generalized  Slater\u2019s  constraint  qualification [9] (a similar discussion is presented in the proof  of Lemma 1.3 of [3]), \nand  applying  the  result  [15,  Theorem  14.60]  that  allows  to  interchange integration and minimization.                               □ \nX w g u θ , , , DP \t  be  computed  by  solving  the  following  one- dimensional optimization problem: \nwhich  can  be  straightforwardly  reformulated  to  a  convex  optimization problem. \nIn  the  next  corollary,  we  investigate  the  divergence  preference  associated  with  the  weighted  relative  entropy.  For  this  case,  the  above  optimization  problem  can  be  solved  explicitly. \nintroduce  the  following  preference,  called  constrained  divergence preference:  ( ) \t ( )\nWe  are  again  able  to  represent  this  class  via  an  optimization  problem  with  a  small  number  of  real  decision  variables. \nTheorem 3.  Let  \t ] \t ] ∞ ∞ − → ℜ \t , : g \t  be a closed convex function  with  ( ) 0 1 = g \t ,  u  be  a  bounded  measurable  function,  w  be  a  density  function  over  Ω   with  respect  to  the  reference  probability measure  P , and let  \t 0 ≥ β \t . Then, for  \t ∞ ∈L X \t , \n( ) \t ( ) \t         \t    \n   \n  \n \t    \n  \t + − − − =\nβ \t t X u g w t X\nProof.  The  proof  follows  from  a  strong  Lagrangian-duality  theorem for the following optimization problem: \nBy Theorem 3, one can see that the constrained divergence  preference  \t ( ) X w g u β , , , DAP \t  is the optimal value of the following  convex optimization problem: \n   \n   \n  \n \t    \n  \t + + − −\nη \t t X u g w t\nThe  constrained  divergence  associated  with  the  weighted  relative entropy can simplified to the formula in the following  corollary. \n  \n= > =\n. 0 0\n0 ln\nx x x x x g\nIn  this  section,  we  consider  the  following  broad  class  of  preferences, called mixed divergence preferences:  ( )\nwhere  \t 0 ≥ θ \t  and  \t 0 ≥ β \t .  One  can  see  that  this  class  is  a  subclass of variational preferences; indeed,  ( ) \t ( )\nwhich  shows  that  the  class  of  mixed  divergence  preferences  includes  both  of  the  classes  studied  in  Sections  3  and  4.  Fortunately,  a  useful  presentation  for  this  class  can  again  be  provided  as  follows.  Using  this  theorem,  we  can  compute  a  mixed divergence preference by solving a convex optimization  problem with two real variables. \nTheorem  5.   Let  \t ] \t ] ∞ ∞ − → ℜ \t , : , h g \t  be  closed  convex  functions  with  ( ) ( ) 0 1 1 \t = = h g \t ,  u  be  a  bounded  measurable  function,  w  be a density function over  Ω   with respect to the  reference  probability  measure  P ,  and  let  0 ≥ θ \t  and  \t 0 ≥ β \t .  Then, for  \t ∞ ∈L X \t , \n  \n  \n  \n \t    \nβ θ \t t X u k w t X\nOur  results  show  how  information-type  divergences  can  be  used to construct a vast class of interesting preferences that are  efficiently  computable.  Incorporating  other  types  of  divergences,  e.g.  Bregman  divergences,  and  developing \nrepresentations  along  the  same  lines  of  this  paper  may  be  a  very important direction for future research. Also, studying the  following  class  of  homothetic  preferences,  called  divergence  homothetic preferences:   ( )\nwhere  \t [ ) ∞ → , 0 : ∆ d \t , is another avenue of future research.   "},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566433.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T8.5","endtime":"16:20","authors":"Amir Ahmadi-Javid","date":"1341244800000","papertitle":"Application of Information-Type Divergences to Constructing Multiple-priors and Variational Preferences","starttime":"16:00","session":"S3.T8: Directed Information, Common Information, and Divergence","room":"Stratton (491)","paperid":"1569566433"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
