{"id":"1569566147","paper":{"title":{"text":"Capacity Achieving Linear Codes with Random Binary Sparse Generating Matrices over the Binary Symmetric Channel"},"authors":[{"name":"A. Makhdoumi Kakhaki,3"},{"name":"H. Karkeh Abadi,4"},{"name":"P. Pad,5"},{"name":"H. Saeedi"},{"name":"F. Marvasti"},{"name":"K. Alishahi"}],"abstr":{"text":"Abstract\u2014In this paper, we prove the existence of capacity achieving linear codes with random binary sparse generating matrices over the Binary Symmetric Channel (BSC). The results on the existence of capacity achieving linear codes in the literature are limited to the random binary codes with equal probability generating matrix elements and sparse parity-check matrices. Moreover, the codes with sparse generating matrices reported in the literature are not proved to be capacity achieving for channels other than Binary Erasure Channel. As opposed to the existing results in the literature, which are based on optimal maximum a posteriori decoders, the proposed approach is based on a different decoder and consequently is suboptimal. We also demonstrate an interesting trade-off between the sparsity of the generating matrix and the error exponent (a constant which determines how exponentially fast the probability of error decays as block length tends to inﬁnity). Based on our results, we also propose a channel coding rate achievable by linear codes at a given block length and error probability. Moreover, we prove the existence of capacity achieving linear codes with a given (arbitrarily low) density of ones on rows of the generating matrix. In addition to proving the existence of capacity achieving sparse codes, an important conclusion of our paper is to prove that any arbitrarily selected sequence of sparse generating matrices is capacity achieving with high probability."},"body":{"text":"The Shannon coding theorem [1] states that for a variety of channels with a given capacity C, if the information transmis- sion rate R over the channel is below C, there exists a coding scheme for which the information can be transmitted with an arbitrarily low probability of error. For Discrete Memoryless Channels (DMC), it has been shown [2] that the probability of error can be bounded between two exponentially decaying functions of the codeword blocklength, n. In this theorem, there is no constraint on the codes in terms of linearity. In [3], a simpler proof of the Shannon theorem has been provided. The existence of capacity achieving linear codes over the Binary Symmetric Channel (BSC) was shown by Elias [4] where it was also proved that random linear codes have the same error exponent as random codes. A similar result has been obtained in [5]. It was recently shown in [6] that the\nerror exponent of a typical random linear code can, in fact, be larger than a typical random code, implying a faster decaying of error as n increases. Some bounds on the decoding error probability of linear codes have been derived in [7]. The result reported in [4]-[7] are all based on the fact that the elements of generating matrices of the capacity achieving linear codes should be one or zero with equal probability; therefore the generating matrix of such approaches are not sparse. 1 More- over, most papers on capacity achieving sparse linear codes are concentrated on codes with sparse parity-check matrices. In particular, an important class of codes called Low-Density Parity-Check (LDPC) codes [8], [9] have been of major interest in the past decade. While these codes have sparse parity-check matrices, they do not necessarily exhibit sparse generating matrices which are the focus of this paper. In [10]- [11], some Low-Density Generating-Matrix (LDGM) schemes have been proposed which have performance approaching the capacity. 2 Some other related literature on the codes with sparse generating matrices having performance close to ca- pacity includes [12]-[14]; in [12], a capacity-achieving scheme has been proposed based on serially concatenated codes with an outer LDPC code and an inner LDGM code. However, the generating matrix corresponding to the concatenation is not necessarily sparse. On the other hand, rateless codes have been proposed in [13] and [14] which have sparse generating matrices but are only proved to be capacity achieving over the Binary Erasure Channel (BEC).\nIn this paper, using a novel approach, we prove the existence of capacity achieving linear codes with sparse generating ma- trices that can provide reliable communications over the BSC\nat rates below the channel capacity. The proof is accomplished by ﬁrst deriving a lower bound on the probability of correct detection for a given generating matrix and then by taking the expectation of that lower bound over all possible generating matrices with elements 1 and 0 with probability ρ and 1 − ρ, respectively. By showing that this expectation goes to one as n approaches inﬁnity, we prove the existence of linear capacity achieving codes. To show the sparsity, we extend this result by taking the expectation over a subset of matrices for which the density of ones could be made arbitrarily close to any target ρ. We then prove a stronger result that indicates the existence of capacity achieving linear codes with the same low density of ones in each row of the generating matrix. In addition to proving the existence of capacity achieving sparse codes, we also show that for a sufﬁciently large code length, no search is necessary in practice to ﬁnd the desired deterministic matrix. This means that a randomly chosen code can have the desired error correcting property with high probability. This is done by proving that the error probability of a sequence of codes, corresponding to a randomly selected sequence of sparse generating matrices tends to zero as n approaches inﬁnity, in probability. This important result is then extended to generating matrices with low density rows. As opposed to the existing results in the literature, which are based on Maximum A Posteriori (MAP) decoders, the proposed proofs are based on a suboptimal decoder, 3 which makes our approach also novel from decoder point of view.\nAlthough in reality the block length of codes is ﬁnite, in order to prove that a class of codes is capacity achieving, we assume that the block length goes to inﬁnity. An interesting question is that for a given error probability and block length, how close to capacity the rate of the code can be. An upper bound for the coding rate achievable at a given block length and error probability is the sphere packing bound (see Equation (5.8.19) in [4]). In [15], for a given block length and error probability performance, the authors have obtained a lower bound on the achievable rate called Random Coding Union (RCU) bound. In this paper, we compare the rate of our sparse linear codes with these bounds. We also demonstrate an interesting trade-off between the sparsity of the generating matrix and the error exponent such that the sparser the matrix, the smaller the error exponent becomes.\nIt is important to note that we rigorously prove the existence of capacity achieving linear codes for a constant ρ resulting in a non-vanishing density of ones on the generating matrix as n tends to inﬁnity. However, we have made a conjecture that if we choose ρ(n) of O( log n √ n ), the resulting codes can still be capacity achieving, which implies a vanishing density of ones. It is worth mentioning that in the full version of this paper [16], we have proved similar results on the existence of capacity achieving linear sparse codes over the BEC. In particular, we have been able to prove that to have capacity achieving generating matrices, ρ(n) can be of O( log n n ). This implies that the number of ones in the generating matrix is\nabout n log n which is asymptotically less than n 3/2 log n, the number of ones in the case of BSC.\nThe organization of the paper is as follows: In the next sec- tion, some preliminary deﬁnitions and notations are presented. In Sections III we present our theorems for BSC and Section IV concludes the paper. Due to lack of space, we have omitted the proof of theorems and propositions. They can be found in the full version of the paper [16].\nConsider a Discrete Memoryless channel (DMC) which is characterized by X and Y as its input and output alphabet sets, respectively, and the transition probability function P(y|x), where x ∈ X is the input, and y ∈ Y is the output of the channel. In this paper, we consider the binary case where X = {0, 1}. A binary code C(n, k) of rate R is a mapping from the set of 2 k k-tuples X i to n-tuples Z i , 0 ≤ i ≤ 2 k − 1, where X i ∈ {0, 1} k , Z i ∈ {0, 1} n , and the code rate R is deﬁned as the ratio of k by n. Since we are only interested in Linear Codes, the mapping is fully speciﬁed by an n ×k binary matrix A = {A ij } (the generating matrix), and encoding is accomplished by a left multiplication by A:\nwhere the calculations are in GF (2). The vector Z i is then transmitted through the DMC. Decoding is deﬁned as recover- ing the vector X i from the possibly corrupted received version of Z i .\nIn this paper the employed decoding scheme relies on the a posteriori probability distribution. Let A be the generating matrix. For a received vector Y = y, the decoder allocates a random vector such as X = x as the original transmitted message with the conditional probability P(X = x|Y = y). Clearly, the probability of correct detection using A as the generating matrix is\nP(x i ) P(y j |x i ) P(x i |y j ) = ∑\nNote that the optimal decoder is a MAP decoder which allocates argmax x P(X = x|Y = y) and that the probability of correct detection using MAP is more than or equal to the probability of correct detection in (1). Throughout the paper, the index i in X i and Z i may be dropped for more clarity. For the sake of convenience, the following notations are used for the remainder of the paper.\nDeﬁnition 1: Let A n ×k be the set of all binary n × k matrices. The density of an A ∈ A n ×k is deﬁned as the total number of ones within the matrix divided by the number of its elements (nk). A matrix with a density less than 0.5 is called sparse; the smaller the density, the sparser the matrix becomes.\nDeﬁnition 2: Let each entry of each element of A n ×k has a Bernoulli(ρ) distribution, 0 < ρ < 1. 4 This scheme induces a probability distribution on the set A n ×k , denoted by Bernoulli(n, k, ρ). For the rest of paper, we consider this distribution on the set A n ×k .\nNote that as n approaches inﬁnity, the typical matrices of A n ×k have a density close to ρ.\nConsider a BSC with cross-over probability ϵ. The ca- pacity of this channel is given by C = 1 − h(ϵ), where h (ϵ) = −ϵ log ϵ − (1 − ϵ) log (1 − ϵ). We suppose that R, the rate of the code, is less than C. In this section, we prove the existence of capacity achieving linear codes with arbitrarily sparse generating matrices over the BSC. We prove the existence by showing that the average error probability over such generating matrices tends to zero as n approaches inﬁnity.\nAssume that we encode a message vector X to generate the codeword AX. Note that X is chosen uniformly from the set {0, 1} k . Due to the effect of error in the BSC, each entry of the transmitted codeword AX can be changed from 0 to 1 and vice versa. These changes can be modeled by adding 1 to erroneous entries of AX (in GF(2)). Therefore, the error of a BSC with cross-over probability ϵ can be modeled by a binary n-dimensional error vector N with i.i.d. entries with Bernoulli(ϵ) distribution. Thus, if the output of the channel is shown by Y , the following equation models the channel:\nIn the following theorem, a lower bound for the average probability of correct detection over the set A n ×k , is obtained.\nTheorem 1: Consider a BSC with cross-over probability ϵ. A lower bound for the average probability of correct detection over all n × k generating matrices with Bernoulli(n, k, ρ) distribution is given by\n(3) An important result of this theorem is that if we ﬁx the\naverage error probability, we can ﬁnd the maximal achievable rate for a given block length over a given channel. Fig. 1 is a plot of the coding rate versus n for different values of\nρ for a BSC with ϵ = .11. Note that for this value of ϵ, the capacity of BSC is equal to 0.5. This plot is numerically evaluated from Theorem 1 where the average probability of error is set to 10 −3 . As can be seen, at block length of 1000, we can achieve the rate of 0.4 which is about 80% of the capacity. Another interesting observation of this ﬁgure is that when the block length n increases, the achievable coding rate becomes independent of the density ρ. The signiﬁcance of this observation is that sparse generating matrices can replace non-sparse ones for large block sizes which implies a simpler encoder structure.\nFig. 2 shows the comparison of our result to the sphere packing bound and the RCU bound of [15] for ϵ = .11 and average probability error of 10 −3 . As can be seen, the rate of our codes follows both bounds pretty closely. It is important to note that the RCU bound guarantees a lower bound on the achievable rate for codes which are not necessarily linear. Consequently, it is not surprising that the rate of our linear codes have not achieved the RCU bound.\nNow using the results of Theorem 1, we want to prove the existence of capacity achieving linear codes. In the following theorem, we will show that the expected value of the correct detection probability over all generating matrices from A n ×k approaches 1 indicting the existence of at least one linear capacity achieving code.\nThe performance of linear codes is determined by the error exponent which is deﬁned as follows:\nDeﬁnition 3: The error exponent of a family of codes C of rate R is deﬁned as\nIf the limit is greater than zero, the average error probability of the proposed codes decreases exponentially to zero as n increases. The error exponent is an index such that the larger\nthe error exponent, the faster the probability of error decays as n increases. Based on our observation, there is an interesting relation between the error exponent of the codes constructed by generating matrices with Bernoulli(n, k, ρ) distribution and the values of ρ. In Fig. 3, we have plotted the average probability of error versus n for various values of ρ and a ﬁxed code rate. As can be seen, the error exponent which is equal to the slope of the curves, increases as ρ increases (the generating matrix become less sparse). In other words, although the probability of error for sparse codes goes to to zero exponentially as n increases; this decrease is not as fast as high density codes.\nDeﬁnition 4: Let W (A) be the number of ones in a given binary matrix A and η be an arbitrary positive constant. T η n ×k is deﬁned as a subset of A n ×k for which | W (A) nk − ρ| < η, η > 0. By choosing a sufﬁciently small η, the set T η n ×k is in fact a subset of A n ×k which contains matrices having density of ones arbitrarily close to any given ρ. Note that the probability distribution on T η n ×k is induced from the probability distribution on A n ×k .\nIn Theorems 1 and 2, we proved the existence of capacity achieving codes for any value of ρ. We did not explicitly prove the existence of sparse capacity achieving codes. However, us-\ning concentration theory [17], we can see that for a sufﬁciently large n, a randomly chosen matrix from A n ×k is in the subset T η n ×k with high probability. In other words, we can state the following proposition which implies the existence of capacity achieving codes which are sparse.\nProposition 1: Let T η n ×k be the set of typical matrices deﬁned in Deﬁnition (4). We then have\nDeﬁnition 5: We deﬁne R n ×k as the set of all binary n ×k matrices with rows that have kρ ones. We also consider a uniform distribution on the set R n ×k for the rest of the paper. In the next theorem, we will prove a stronger result on capacity achieving sparse codes. We show the existence of capacity achieving matrices with rows containing exactly kρ ones. In other words, the density of ones in each row is exactly equal to ρ. This also implies that the generating matrix has a density of ones exactly equal to ρ. In Theorem 3, we shall derive a lower bound on the average probability of correct detection and in Theorem 4 we will prove that this lower bound tends to one. This shows that the average probability of error over the set R n ×k approaches zero, implying the existence of capacity achieving codes with generating matrices taken from R n ×k .\nTheorem 3: For a binary symmetric channel with cross- over probability ϵ, a lower bound for the expected value of the probability of correct detection over all generating matrices in R n ×k is given by\nIn Theorems 1 and 2, we proved the existence of capac- ity achieving linear codes with generating matrices having Bernoulli(n, k, ρ) distribution by showing that the average probability of error over all generating matrices tends to zero as n approaches inﬁnity. This implies that we may have to perform a search over A n ×k to ﬁnd such a matrix. Assume that we simply pick matrices randomly for each n from the set A n ×k . This constitutes a sequence of n × nR matrices. Now consider the resulting sequence of error probabilities corresponding to the sequence of generating matrices. In the following proposition, we shall prove that the limit of this sequence is zero in probability, i.e., a sequence of randomly chosen matrices is capacity achieving with high probability.\nThis suggests that for sufﬁciently large n, no search is neces- sary to ﬁnd a desired deterministic generating matrix.\nProposition 2: Let {A n ×nR } ∞ n=0 be the sequence of matri- ces, where A n ×nR is selected randomly from A n ×nR . If we denote the error probability of the generating matrix A n ×nR over BSC by p e (A n ), then p e (A n ) converges in probability to zero as n tends to inﬁnity.\nNote 1: If we use the result of Theorem 4, we can extend Proposition 2 to the case where we construct the matrix sequence by choosing the matrices from the set R n ×k . In other words, in order to have capacity achieving sequences of generating matrices for BSC with arbitrarily low density rows, we can simply pick generating matrices randomly from R n ×k .\nAt this stage, we have been able to rigorously prove the existence of capacity achieving sparse linear codes over the BSC. However for a given ρ, although the density of ones can be made arbitrarily small, it does not go to zero even when n approaches inﬁnity. Let us assume the case where ρ is a decreasing function of n such that lim n →∞ ρ(n) = 0, resulting in zero density of ones as n goes to inﬁnity. In the following conjecture, we will propose a result indicating that this assumption can in fact be true. Although, we have not been able to rigorously prove the conjecture, a sketch of the proof has been presented in [16].\nConjecture 1: For any ρ(n) of O( log n √ n ), by assuming the Bernoulli(n, k, ρ(n)) distribution on the set A n ×k , we have\nIn this paper, a novel approach to prove the existence of capacity achieving sparse linear codes over the BSC was proposed. In Theorem 1, we derived a lower bound on the average probability of correct detection over the set A n ×k . In Theorem 2, we proved that the average probability of error over A n ×k tends to zero. Then we proved the existence of sparse capacity achieving codes in Proposition 2. In Theorem 3, we derived a lower bound on the average probability of correct detection over the set R n ×k . Using this lower bound in Theorem 4, we proved the existence of capacity achieving codes with generating matrices with the same density in each row. In Proposition 2 and its preceding note, we showed that the error probability of codes corresponding to any randomly chosen sequence of generating matrices tends to zero in probability. This implies that for a sufﬁciently large n, a randomly chosen matrix from A n ×k and R n ×k will have the average error correcting capability. In addition, we conjectured that Theorem 2 can hold for the case where ρ is of O( log n √ n ). This implies that for a capacity achieving code over a BSC, the density of the generating matrix can approach zero. We also demonstrated an interesting trade-off between the sparsity of the generating matrix and the error exponent indicating that a sparser generating matrix results in a smaller error exponent. We also observed that ﬁxing the average bit error rate and ϵ, the rates for the codes with generating matrices of higher\ndensities are closer to capacity for small block sizes. For larger block sizes, however, the rate becomes independent of the generating matrix density. In our proofs, we have used a suboptimal decoder while previous works in the literature were based on a MAP decoder. This implies that we can get stronger results if we use the optimal MAP decoder.\nFor future work, one can try to rigorously prove Conjecture 1 and possibly extend it to the case of matrices in the set R n ×k . The improvement in the bounds using a MAP decoder can be an interesting topic to investigate. The extension of the results to other memoryless channels is another challenging topic to be explored. A very interesting work is to analytically derive the error exponent to prove the trade-off between error exponent and sparsity of the generating matrix.\nThe authors would like to thank Professor G. D. Forney for his valuable comments and suggestions and Mr. R. Farhoudi for his comments about proof of theorems."},"refs":[{"authors":[{"name":"C. E. Shanno"}],"title":{"text":"A mathmatical theory of communications, Bell Systems Technical Journal, vol"}},{"authors":[{"name":"R. M. Fan"},{"name":"T. Pres"}],"title":{"text":"Transmisson of Information, The M"}},{"authors":[{"name":"R. Gallager"}],"title":{"text":"A simple derivation of the coding theorem and some applications"}},{"authors":[{"name":"R. G. Gallage"}],"title":{"text":"Information Theory and Reliable Communication, John Wiley and Sons Inc"}},{"authors":[{"name":"M. Mezar"},{"name":"A. Montanar"}],"title":{"text":"Information, physics, and computation, Oxford University Press, USA, 2009, pp"}},{"authors":[{"name":"A. Barg"},{"name":"G. D. Forney"}],"title":{"text":"Random codes: Minimum distances and error exponents"}},{"authors":[{"name":"G. Poltyrev"}],"title":{"text":"Bounds on the decoding error probability of linear binary codes via their spectra"}},{"authors":[{"name":"R. G. Gallager"}],"title":{"text":"Low density parity check codes"}},{"authors":[{"name":"D. J. C. MacKay"},{"name":"R. M. Neal"}],"title":{"text":"Near Shannon limit performance of low density parity check codes"}},{"authors":[{"name":"F. J. Vazquez-Araujo"},{"name":"M. Gonzalez-Lopez"},{"name":"L. Castedo"},{"name":"J. Garcia- Frias"}],"title":{"text":"Capacity approaching low-rate LDGM codes"}},{"authors":[{"name":"J. Garcia-Frias"},{"name":"Z. Wei"}],"title":{"text":"Approaching Shannon performance by iterative decoding of linear codes with low-density generator matrix"}},{"authors":[{"name":"H. Chun-Hao"},{"name":"A. Anastasopoulos"}],"title":{"text":"Capacity-achieving codes with bounded graphical complexity and maximum likelihood decoding"}},{"authors":[{"name":"M. Luby"}],"title":{"text":"LT codes"}},{"authors":[{"name":"A. Shokorollahi"}],"title":{"text":"Raptor codes"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"H. Vincent Poor"},{"name":"S. Verdu"}],"title":{"text":"Channel Coding Rate in the Finite Blocklength Regime"}},{"authors":[{"name":"A. Makhdoumi Kakhaki"},{"name":"H. Karkeh Abadi"},{"name":"P. Pad"},{"name":"H. Saeedi"},{"name":"F. Marvasti"},{"name":"K. AlishahiA"}],"title":{"text":"Capacity Achieving Linear Codes with Random Binary Sparse Generating Matrices"}},{"authors":[{"name":"A. Demb"},{"name":"O. Zeitoun"}],"title":{"text":"Large Deviation Techniques and Application, Springer, 2009"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566147.pdf"},"links":[{"id":"1569566567","weight":2},{"id":"1569565383","weight":2},{"id":"1569564635","weight":2},{"id":"1569566875","weight":2},{"id":"1569559617","weight":2},{"id":"1569566855","weight":2},{"id":"1569566697","weight":2},{"id":"1569565091","weight":2},{"id":"1569552245","weight":10},{"id":"1569564481","weight":2},{"id":"1569564805","weight":13},{"id":"1569566469","weight":2},{"id":"1569566081","weight":2},{"id":"1569565613","weight":2},{"id":"1569565355","weight":7},{"id":"1569565461","weight":7},{"id":"1569566739","weight":2},{"id":"1569558459","weight":2},{"id":"1569566497","weight":2},{"id":"1569566787","weight":2},{"id":"1569566523","weight":2},{"id":"1569566749","weight":2},{"id":"1569564613","weight":2},{"id":"1569567009","weight":2},{"id":"1569566095","weight":5},{"id":"1569566239","weight":2},{"id":"1569566679","weight":2},{"id":"1569566617","weight":2},{"id":"1569559565","weight":2},{"id":"1569566905","weight":5},{"id":"1569566733","weight":2},{"id":"1569566753","weight":2},{"id":"1569563307","weight":2},{"id":"1569555999","weight":5},{"id":"1569567665","weight":5},{"id":"1569565257","weight":2},{"id":"1569566437","weight":2},{"id":"1569558901","weight":2},{"id":"1569565735","weight":2},{"id":"1569553909","weight":2},{"id":"1569565915","weight":5},{"id":"1569552251","weight":2},{"id":"1569566425","weight":5},{"id":"1569566909","weight":10},{"id":"1569566809","weight":2},{"id":"1569566629","weight":2},{"id":"1569566447","weight":2},{"id":"1569563897","weight":2},{"id":"1569565279","weight":2},{"id":"1569566003","weight":2},{"id":"1569556671","weight":5},{"id":"1569566037","weight":2},{"id":"1569564973","weight":2},{"id":"1569565469","weight":13},{"id":"1569565357","weight":2},{"id":"1569565933","weight":2},{"id":"1569562207","weight":2},{"id":"1569567033","weight":5},{"id":"1569566233","weight":2},{"id":"1569566667","weight":2},{"id":"1569565463","weight":2},{"id":"1569562551","weight":2},{"id":"1569566901","weight":5},{"id":"1569551347","weight":7},{"id":"1569565415","weight":2},{"id":"1569555367","weight":2},{"id":"1569566805","weight":2},{"id":"1569559199","weight":5},{"id":"1569565665","weight":2},{"id":"1569566779","weight":2},{"id":"1569565093","weight":7},{"id":"1569565385","weight":2},{"id":"1569565241","weight":2},{"id":"1569566927","weight":2},{"id":"1569565661","weight":2},{"id":"1569565865","weight":2},{"id":"1569566267","weight":2},{"id":"1569566737","weight":2},{"id":"1569566253","weight":2},{"id":"1569565353","weight":2},{"id":"1569564305","weight":2},{"id":"1569564291","weight":2},{"id":"1569565177","weight":2},{"id":"1569565597","weight":2},{"id":"1569565293","weight":2},{"id":"1569559035","weight":2},{"id":"1569564247","weight":2},{"id":"1569565457","weight":2},{"id":"1569556759","weight":2},{"id":"1569566619","weight":2},{"id":"1569565271","weight":2},{"id":"1569561185","weight":2},{"id":"1569565367","weight":2},{"id":"1569564281","weight":5},{"id":"1569561713","weight":2},{"id":"1569557851","weight":2},{"id":"1569559919","weight":2},{"id":"1569565035","weight":5},{"id":"1569559597","weight":2},{"id":"1569566273","weight":2},{"id":"1569565889","weight":5},{"id":"1569551539","weight":2},{"id":"1569561397","weight":7},{"id":"1569566413","weight":2},{"id":"1569565707","weight":7},{"id":"1569566375","weight":15},{"id":"1569564141","weight":2},{"id":"1569566973","weight":2},{"id":"1569561579","weight":5},{"id":"1569565031","weight":2},{"id":"1569564755","weight":2},{"id":"1569551541","weight":2},{"id":"1569565895","weight":2},{"id":"1569566067","weight":5},{"id":"1569566609","weight":2},{"id":"1569566113","weight":5},{"id":"1569566727","weight":2},{"id":"1569565315","weight":2}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T4.1","endtime":"17:00","authors":"Abbasali Makhdoumi Kakhaki, Hossein Karkeh Abadi, Pedram Pad, Hamid Saeedi, Farokh Marvasti, Kasra Alishahi","date":"1341247200000","papertitle":"Capacity Achieving Linear Codes with Random Binary Sparse Generating Matrices over the Binary Symmetric Channel","starttime":"16:40","session":"S4.T4: Structured Codes","room":"Stratton 20 Chimneys (306)","paperid":"1569566147"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
