{"id":"1569566913","paper":{"title":{"text":"Principal Component Pursuit with Reduced Linear Measurements"},"authors":[{"name":"Arvind Ganesh ∗"},{"name":"Kerui Min ∗"},{"name":"John Wright \u2020"},{"name":"and"},{"name":"Yi Ma ∗\u2021"}],"abstr":{"text":"Abstract\u2014In this paper, we study the problem of decomposing a superposition of a low-rank matrix and a sparse matrix when a relatively few linear measurements are available. This problem arises in many data processing tasks such as aligning multiple images or rectifying regular texture, where the goal is to recover a low-rank matrix with a large fraction of corrupted entries in the presence of nonlinear domain transformation. We consider a natural convex heuristic to this problem which is a variant to the recently proposed Principal Component Pursuit. We prove that under suitable conditions, this convex program guarantees to recover the correct low-rank and sparse components despite reduced measurements. Our analysis covers both random and deterministic measurement models. 1"},"body":{"text":"Recently, low-rank matrix recovery and approximation has become an increasingly popular area of research, as low-rank models have been applied to many real problems in which high-dimensional data samples are assumed to lie on a lower- dimensional subspace. For instance, they have been success- fully employed in various problems such as face recognition [1], system identiﬁcation [2], and information retrieval [3].\nThis popularity has been further boosted by the recent discovery that convex optimization techniques can efﬁciently and correctly recover a low-rank matrix L 0 ∈ R m×n from corrupted observations D . = L 0 + S 0 , where S 0 ∈ R m×n is a sparse matrix whose entries can have arbitrary magnitude [4], [5], [6]. More speciﬁcally, it has been shown that under rather broad conditions, the following convex program succeeds in recovering L 0 and S 0 from D:\n3 , and λ > 0 is a weighting factor. This program has been dubbed Principal Component Pursuit (PCP) in [4]. Furthermore, follow-up works have shown that PCP is stable in the presence of additive Gaussian noise [7], and can recover L 0 even when the corruption matrix S 0 is not sparse [8].\nIn practice, however, the PCP framework is limited by the measurement model assumed. For instance, in image data, we often have to deal with nonlinear transformations as well. In other words, we could have D ◦ τ = L + S for some\nunknown transformation τ (in some parametric group). This model arises in computer vision applications such as batch image alignment [9] and texture rectiﬁcation [10]. To deal with the nonlinearity introduced by the unknown transform, [10], [9] proposed to linearize the constraint D ◦ τ = L + S, and solve for the transformation τ iteratively via the following convex program:\nwhere the J i \u2019s denote the image Jacobians with respect to the transformation parameters. 4 Although this convex program was proposed in the same spirit as PCP, the constraint is now different, and hence, the theoretical guarantees for PCP given in [4], [5], [6] do not directly apply to this case.\nWe can also interpret (2) in the following manner. Let Q denote the orthogonal complement to the linear subspace spanned by the Jacobian span(J 1 , . . . , J p ). Clearly, we can rewrite (2) as follows:\nwhere P Q is the orthogonal projection operator onto the subspace Q. This is a variation to (1) in which instead of observing the entire corrupted matrix D, we only observe q = mn − p \u201creduced linear measurements\u201d of D. If Q is a linear subspace of special matrices with support in Ω ⊂ [m] × [n], then we have the special case of recovering L 0 from D, when only a subset of entries in D are available. This case is akin to the low-rank matrix completion problem [11], [12], [13], and has been analyzed in [4], [14]. A more general linear measurement model was recently considered in [15], where a greedy algorithm was proposed to recover L 0 and S 0 . However, to the best of our knowledge, the convex program in (3) with a general subspace Q is still an open problem.\nThe convex program (3) poses a very fundamental theoreti- cal problem: how many general linear measurements of D are required to reliably recover the underlying low-rank matrix, even when the number of corruptions is quite large? We note that in the context of the applications in [10], [9], the number of reduced measurements is directly related to the dimension\nof the transformation group. Hence, from that viewpoint, we would like to know how large the deformation group can be for successful recovery. In addition, we believe the results will be useful for many other problems in communications and signal processing when only such linear measurements are available.\nIn this work, we show that the convex program (3) does indeed succeed in recovering the low-rank and sparse compo- nents of D, under fairly broad conditions similar to those in [4]. In addition, we give results for random and deterministic measurement models, respectively. Although we follow a sim- ilar proof strategy as [4], the new constraint poses signiﬁcant difﬁculties and requires some non-trivial modiﬁcations to the techniques used in [4]. Due to lack of space, we will only provide a sketch of the proof for the main results here. We refer the interested reader to [16] for the detailed proof.\nWe begin by outlining our assumptions on L 0 , S 0 and the subspace Q. The ﬁrst two assumptions are designed to avoid ambiguous situations, e.g., when the target matrices L 0 and S 0 are simultaneously low-rank and sparse: 5\n\u2022 incoherence model for L 0 . Let L 0 = U ΣV ∗ be the reduced singular value decomposition of L 0 . We assume\n \nmax i∈[m] U ∗ ¯ e i 2 2 ≤ µr/m, max j∈[n] V ∗ e j 2 2 ≤ µr/n,\n∞ -norm 6 , and ¯ e i and e j denote standard basis vectors in R m and R n , respectively.\n\u2022 random support model for S 0 . Each entry (i, j) is included in Ω = supp(S 0 ) independently with probability ρ > 0. We write Ω ∼ Ber(ρ).\nWe say that a subspace S ⊆ R m×n is ν-coherent if there exists an orthonormal basis {G i } for S satisfying max i G i 2 ≤ ν/ min{m, n}.\nIn this paper, we consider two different assumptions on the p-dimensional subspace Q ⊥ :\n\u2022 random subspace model for Q ⊥ . We assume that Q ⊥ has an orthonormal basis G 1 , G 2 , . . . , G p ∈ R m×n that is chosen uniformly at random from all possible orthobasis sets of size p in R m×n . It can be shown that each of the G i \u2019s is identical in distribution to H/ H F , where the entries of H ∈ R m×n are i.i.d. according to a Gaussian distribution with mean 0 and variance 1/mn.\n\u2022 deterministic subspace model for Q ⊥ . Under this mod- el, Q ⊥ is a ﬁxed ν-coherent subspace, for some ν ≥ 1.\nThe main results of this work show that under each of these assumptions, (3) correctly recovers L 0 , S 0 :\nTheorem 1 (Random Reduction). Fix any C p > 0, and let Q ⊥ be a p-dimensional random subspace of R m×n (n ≤ m),\nL 0 a rank-r, µ-incoherent matrix, and supp(S 0 ) ∼ Ber(ρ). Then, provided that\nwith high probability (L 0 , S 0 ) is the unique optimal solution to (3) with λ = m −1/2 . Here, C r > 0 and ρ 0 ∈ (0, 1) are numerical constants.\nThus, the convex program (3) can still recover the low-rank matrix L 0 under essentially the same conditions as PCP [4], while tolerating up to a constant fraction of errors. The scaling in this result covers several applications of interest: in [10], p is constant, while in [9], p = O(n).\nRemark 1. In Theorem 1, \u201cwith high probability\u201d means with probability at least 1 − β(C p )m −c , with c > 0 numerical.\nIn a related work [17], we show that the convex program (3) also works with highly compressive measurements: dim(Q) only needs to be on the order of (mr + k) log 2 m, within a polylogarithmic factor of the intrinsic degrees of freedom of the unknowns (L 0 , S 0 ). On the other hand, that result does not guarantee correct recovery when a constant fraction of the entries are corrupted. Theorem 1 shows that this is possible when Q is random and p linear in n. In fact, when p = O(1), this is possible even with no random assumption on Q:\nTheorem 2 (Deterministic Reduction). If Q ⊥ is a ν-coherent p-dimensional subspace of R m×n (n ≤ m ≤ αn), L 0 is a rank-r, µ-incoherent matrix, and supp(S 0 ) ∼ Ber(ρ), with high probability (L 0 , S 0 ) is the unique optimal solution to (3) with λ = m −1/2 , provided that\nand ρ < ρ 0 , where ρ 0 and C r are positive numerical constants. Remark 2. Here, \u201cwith high probability\u201d means with proba- bility at least 1 − β(p, α, ν)m −c , with c > 0 numerical.\nThe deterministic subspace model is a signiﬁcantly weaker assumption than the random subspace model. Consequently, recovery is guaranteed for matrices of much lower rank under the former model. Nevertheless, Theorem 2 could be more useful from a practical standpoint since in real applications such as [10], [9], random subspaces are seldom encountered, and the rank of the matrix to be recovered is often small.\nIn this section, we sketch the main ideas of the proof and refer the readers to [16] for more details. We ﬁrst ﬁx some notation. Given the reduced SVD of L 0 = U ΣV ∗ , let T ⊂ R n×n denote the linear subspace {U X ∗ + Y V ∗ | X ∈ R n×r , Y ∈ R m×r }. By a slight abuse of notation, we also denote by Ω the linear subspace of matrices with support in Ω. We let P T and P Ω denote the projection operators onto T and Ω, respectively.\nThe ﬁrst step in the proof is to establish a sufﬁcient condition for (L 0 , S 0 ) to be the optimal solution to (3).\nLemma 1 (Dual Certiﬁcate). Suppose that dim(Q ⊥ ⊕ T ⊕ Ω) = dim(Q ⊥ ) + dim(T ) + dim(Ω). Let Γ = Q ∩ T ⊥ so that Γ ⊥ = Q ⊥ ⊕ T . Assume that P Ω P Γ ⊥ < 1/2 and λ < 1. Then, (L 0 , S 0 ) is the unique optimal solution to (3) if there exists a pair (W, F ) ∈ R m×n × R m×n satisfying\nwith P T W = 0, W < 1/2, P Ω F = 0, F ∞ < 1/2, and P Ω M F ≤ 1/4.\nThus, in order to prove Theorems 1 and 2, it is sufﬁcient to produce a dual certiﬁcate W ∈ R m×n satisfying\n     \n    \nP Q ⊥ W = −P Q ⊥ (U V ∗ ), W < 1/2,\nP Ω (U V ∗ − λsgn(S 0 ) + W ) F ≤ λ/4, P Ω ⊥ (U V ∗ + W ) ∞ < λ/2.\nWe construct the dual certiﬁcate W in a similar fashion as in [4]. However, the modiﬁed constraint in (3) adds signiﬁcant difﬁculty to various technical parts of the proof.\n1) Construction of W L using the golﬁng scheme. We note that Ω c ∼ Ber(1 − ρ) by assumption. Suppose that Ω 1 , Ω 2 , . . . , Ω j 0 are independent support sets such that Ω j ∼ Ber(q) for all j. Then, Ω c and j 0 j=1 Ω j have the same probability distribution if ρ = (1 − q) j 0 . Starting with Y 0 = 0, we iteratively deﬁne\nand set W L = P Γ Y j 0 , where j 0 = 2 log m . 2) Construction of W S by least squares. We set\nW S = arg min X X F s.t. P Ω X = λsgn(S 0 ), P Γ ⊥ X = 0.\n(10) 3) Construction of W Q by least squares. We set\nW Q = arg min X X F s.t. P Q ⊥ (U V ∗ + X) = 0, P Π X = 0,\n(11) where Π = Ω ⊕ T .\nIt is not difﬁcult to see that W = W L + W S + W Q satisﬁes the ﬁrst two conditions in (8). We now present three lemmas that together establish that W also satisﬁes the remaining inequality conditions in (8).\nLemma 2. Suppose that the assumptions of either Theorem 1 or Theorem 2 hold true. Then, the matrix W L obeys, with high probability,\nLemma 3. In addition to the assumptions of Lemma 2, assume that the signs of the non-zero entries of S 0 are\ni.i.d. symmetrically distributed in the set {+1, −1}. Then, the matrix W S obeys, with high probability,\nLemma 4. Suppose that the assumptions of either Theorem 1 or Theorem 2 hold true. Then, the matrix W Q obeys, with high probability,\nThe key component of our proof is to show that under the given assumptions, the subspaces Q ⊥ , T and Ω are pairwise incoherent with each other i.e., to bound the operators norms of P Q ⊥ P T , P T P Ω and P Q ⊥ P Ω . Once these pairwise subspace incoherences are established, we will also have to bound the operator norms of P Ω P Γ ⊥ and P Q ⊥ P Π , where Γ ⊥ = Q ⊥ ⊕ T and Π = Ω ⊕ T . To obtain the latter bounds, we will use the following simple linear algebraic result.\nLemma 5. Let S 1 , S 2 and S 3 be any three linear subspaces in R m×n satisfying dim(S 1 ⊕ S 2 ⊕ S 3 ) = dim(S 1 ) + dim(S 2 ) + dim(S 3 ), and P S 1 P S 2 ≤ α 1,2 < 1, P S 2 P S 3 ≤ α 2,3 < 1\nand P S 3 P S 1 ≤ α 3,1 < 1. We deﬁne S = S 1 ⊕ S 2 . Then, we have\nOnce we have derived all the aforementioned operator norm bounds, the proof methodology is similar to that adopted in [4]. We highlight the general strategy below:\n1) Proof of Lemma 2. The construction of W L is almost identical to the one used in Section 3.2 in [4], except for the fact that in place of the subspace T in [4], we now have Γ ⊥ . While the proofs for the second and third parts of the Lemma 2 are almost identical to those in [4], the proof to bound W L differs signiﬁcantly. This is because the subspace T ⊥ satisﬁes P T ⊥ X ≤ X\nfor any X ∈ R m×n . Since the subspace Γ does not enjoy this property, we employ a novel method to bound\n2) Proof of Lemma 3. This proof follows the same outline as the one in Section 3.3 in [4]. Once again, the main difference is that the T in [4] has been replaced with Γ ⊥ in our work. Employing the bound that we derived on P Ω P Γ ⊥ , we use the convergent Neumann series to express W S in analytical form and analyze it in the same way as was done in [4].\nTheorems 1 and 2, we restate a lemma here from [4] that proves that the subspaces T and Ω are somewhat incoherent.\nLemma 6 (Corollary 2.7 in [4]). Let Ω ∼ Ber(ρ), and L 0 a rank-r, µ-incoherent matrix. Then, with high probability, we have P Ω P T 2 ≤ ρ + , provided that 1 − ρ ≥ C 0 −2 µr log m n\nIn this section, we sketch the proofs of Lemmas 2 and 4 under the assumptions of Theorem 1. First, we present two bounds that can be derived under the random subspace model. Lemma 7. Assume the conditions of Theorem 1, and that p < mn/4. Then, with high probability,\nThe proof is quite lengthy. We brieﬂy outline the key difference from [4], which arises in bounding W L . The golﬁng iteration aims to control the residual\nfor all j, whp. The certiﬁcate W L depends on the errors (Z j ) through the expression\nRecall Γ = T + Q ⊥ . Let R = P T ⊥ Q ⊥ . Then T ⊥ R and Γ = T + R. So P Γ = P T + P R . Using that for any X,\nFor the second term, we use the matrix Bernstein inequality [18] to show that with high probability\nwith C 1 , C 2 numerical. For small C r , W L ≤ 1/4. The other bounds in Lemma 2 are established as in [4].\nUsing Lemmas 5-7, we have \t k≥0 (P Q ⊥ P Π P Q ⊥ ) k ≤ 4/3, with high probability.\nWe bound P Q ⊥ (U V ∗ ) F as follows. If we vector- ize all matrices, then P Q ⊥ has the same distribution as H(H ∗ H) −1 H ∗ , where H ∈ R mn×p is an i.i.d. N (0, 1/mn) matrix. Therefore,\nFrom Lipschitz concentration ([19] Prop. 2.18) and bounds on expected singular values Gaussian matrices [20], we have\nr, H ∗ vec(U V ∗ ) is a p-dimensional i.i.d. N (0, r/mn) vec- tor. Using Jensen\u2019s inequality and [19] Prop. 2.18,\nUnder the stated assumptions, this is ≤ 1/8 for large m. For any (i, j), | W Q , ¯ e i e ∗ j | is bounded by\nUsing the same procedure that we employed above to bound P Q ⊥ (U V ∗ ) F and applying a union bound over (i, j), we\nIn this section, we present the bounds corresponding to the ones in Section IV. These bounds are crucial to prove Lemmas 2, 3 and 4 under the assumptions of Theorem 2. Throughout this section, we will assume that Q ⊥ is ﬁxed subspace that is ν-coherent for some ν > 0.\nwhere (21) holds provided that ρ < ρ 0 and ν 2 p 3 log m/n ≤ C, for some numerical C > 0 and ρ 0 ∈ (0, 1).\nProof: The proof of (20) follows from rank(P T X) ≤ 2r ∀X, and H¨older\u2019s inequality. The proof of (21) is more involved. We present the basic ideas here.\nConsider the linear operator A . = P Q ⊥ P Ω P Q ⊥ − ρP Q ⊥ . Clearly, E [A] = 0 since Ω ∼ Ber(ρ). Let δ ij be a sequence of independent Bernoulli random variables such that\nand ⊗ denotes the outer or tensor product between matrices. Using the fact A ⊗ B ≤ A F B F , we can show that\nWe then invoke the matrix Bernstein inequality [18] to show that A ≤ ρ with high probability. The desired result follows by noting that P Q ⊥ P Ω P Q ⊥ = A + ρP Q ⊥ .\na) Proof Sketch: Lemma 2: We follow the same proof technique that we used in Section IV-A. But the proof here is simpler. In fact, when p = O(1), we do not need the subspace R (deﬁned in Section IV-A) to be ν-coherent.\nb) Proof Sketch: Lemma 4: The basic outline of the proof is the same as that described in Section IV-B. We present only the relevant bounds here. Using the ν-coherence of Q ⊥ , it is easy to show that P Q ⊥ (U V ∗ ) F ≤ 2νpr 2 /n. Using Lemmas 7 and 5, we can show that\nwith high probability, for ρ sufﬁciently small. Once again, using the Neumann series for W Q along with Lemmas 7 and 5, it is not difﬁcult to show that\nEmploying the above three bounds in the same manner as done in Section IV-B, we obtain W Q ≤ 8νpr 2 /n, and\nP Ω c W Q ∞ ≤ 2 2ν 2 p 2 r 2 /n 2 + 4 2µνr 3 /n 2 . Since λ = 1/\nm, and m ≤ αn, to conclude P Ω c W Q ∞ ≤ λ/8, we require C \t αν 2 p 2 r 2 /n + αµνpr 3 /n ≤ 1, giving the scaling r = O(n 1/3 ) in the theorem.\nIn this paper, we have given a ﬁrst theoretical corroboration for the success of the convex programming heuristic in two cases of practical interest. However, there are still plenty of open problems. For the deterministic case, our bounds (and rates) can likely be improved using more reﬁned arguments. Perhaps more importantly, it is important to investigate the\nextent to which Q ⊥ arising in real applications are operator- incoherent. It may be possible to tailor the assumptions to better ﬁt real imaging applications. For the random subspace case, our results are close to the best possible, in terms of the rank of the unknown matrix and the number of corruptions that can be tolerated. However, it would be interesting to know if more reﬁned arguments can show recovery from constant fractions of error, even with constant fractions of measurements missing (p = O(mn)), as might occur in compressed sensing. Finally, while we have focused here on noise-free analysis of exact recovery, extensions to the noisy and compressible cases are obviously of signiﬁcant practical interest."},"refs":[{"authors":[{"name":"J. Wright"},{"name":"A. Yang"},{"name":"A. Ganesh"},{"name":"Y. Ma"},{"name":"S. Sastry"}],"title":{"text":"Robust face recognition via sparse representation"}},{"authors":[{"name":"M. Fazel"},{"name":"H. Hindi"},{"name":"S. Boyd"}],"title":{"text":"Rank minimization and applications in system theory"}},{"authors":[{"name":"C. Papadimitriou"},{"name":"P. Raghavan"},{"name":"H. Tamaki"},{"name":"S. Vempala"}],"title":{"text":"Latent semantic indexing: A probabilistic analysis"}},{"authors":[{"name":"E. Cand`es"},{"name":"X. Li"},{"name":"Y. Ma"},{"name":"J. Wright"}],"title":{"text":"Robust principal component analysis?"}},{"authors":[{"name":"V. Chandrasekaran"},{"name":"S. Sanghavi"},{"name":"P. Parrilo"},{"name":"A. Willsky"}],"title":{"text":"Rank- sparsity incoherence for matrix decomposition"}},{"authors":[{"name":"D. Hsu"},{"name":"S. M. Kakade"},{"name":"T. Zhang"}],"title":{"text":"Robust matrix decomposition with sparse corruptions"}},{"authors":[{"name":"Z. Zhou"},{"name":"X. Li"},{"name":"J. Wright"},{"name":"E. Cand`es"},{"name":"Y. Ma"}],"title":{"text":"Dense error correction for low-rank matrices via principal component pursuit"}},{"authors":[{"name":"A. Ganesh"},{"name":"J. Wright"},{"name":"X. Li"},{"name":"E. Cand`es"},{"name":"Y. Ma"}],"title":{"text":"Dense error correction for low-rank matrices via principal component pursuit"}},{"authors":[{"name":"Y. Peng"},{"name":"A. Ganesh"},{"name":"J. Wright"},{"name":"W. Xu"},{"name":"Y. Ma"}],"title":{"text":"RASL: Robust Alignment by Sparse and Low-rank decomposition"}},{"authors":[{"name":"Z. Zhang"},{"name":"A. Ganesh"},{"name":"X. Liang"},{"name":"Y. Ma"}],"title":{"text":"TILT: Transform Invariant Low-rank Textures"}},{"authors":[{"name":"E. Cand`es"},{"name":"B. Recht"}],"title":{"text":"Exact matrix completion via convex optimza- tion"}},{"authors":[{"name":"E. Cand`es"},{"name":"T. Tao"}],"title":{"text":"The power of convex relaxation: Near-optimal matrix completion"}},{"authors":[{"name":"D. Gross"}],"title":{"text":"Recovering low-rank matrices from few coefﬁcients in any basis"}},{"authors":[{"name":"X. Li"}],"title":{"text":"Compressed sensing and matrix completion with constant proportion of corruptions"}},{"authors":[{"name":"A. Waters"},{"name":"A. Sankaranarayanan"},{"name":"R. Baraniuk"}],"title":{"text":"SpaRCS: Recover- ing low-rank and sparse matrices from compressive measurements"}},{"authors":[{"name":"A. Ganesh"},{"name":"K. Min"},{"name":"J. Wright"},{"name":"Y. Ma"}],"title":{"text":"Principal component pursuit with reduced linear measurements"}},{"authors":[{"name":"J. Wright"},{"name":"A. Ganesh"},{"name":"K. Min"},{"name":"Y. Ma"}],"title":{"text":"Compressive principal component pursuit"}},{"authors":[{"name":"J. Tropp"}],"title":{"text":"User-friendly tail bounds for sums of random matrices"}},{"authors":[{"name":"M. Ledou"}],"title":{"text":"The Concentration of Measure Phenomenon"}},{"authors":[{"name":"M. Rudelson"},{"name":"R. Vershynin"}],"title":{"text":"Non-asymptotic theory of random matrices: extreme singular values"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566913.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T9.2","endtime":"15:20","authors":"Arvind Ganesh, Kerui Min, John Wright, Yi Ma","date":"1341327600000","papertitle":"Principal Component Pursuit with Reduced Linear Measurements","starttime":"15:00","session":"S7.T9: Compressive Sensing","room":"Stratton West Lounge (201)","paperid":"1569566913"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
