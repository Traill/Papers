{"id":"1569566831","paper":{"title":{"text":"Alternating Markov Chains for Distribution Estimation in the Presence of Errors"},"authors":[{"name":"Farzad Farnoud (Hassanzadeh)"},{"name":"Narayana P. Santhanam"},{"name":"Olgica Milenkovic"}],"abstr":{"text":"Abstract\u2014We consider a class of small-sample distribution estimators over noisy channels. Our estimators are designed for repetition channels, and rely on properties of the runs of the observed sequences. These runs are modeled via special types of Markov chains, termed \u201calternating Markov chains\u201d. We show that alternating chains have redundancy that scales sub-linearly with the lengths of the sequences, and describe how to use a distribution estimator for alternating chains for the purpose of distribution estimation over repetition channels."},"body":{"text":"Estimating the distribution of a source with a large alphabet based on a small number of observations is a problem of sig- niﬁcant interest in molecular biology, neuroscience, physics, statistics, and learning theory. In order to address this problem, throughout the years a number of sophisticated estimators were developed, including those by Good and Turing [1], [2] and Orlitsky et al. [3], [4], to cite a few. The idea behind these estimators is to use frequencies of symbol frequencies, rather than simple frequency counts standardly used for Maximum Likelihood (ML) estimation.\nAn additional problem in this estimation setting arises when some of the observations are inaccurate. Since most known distribution estimators are based on frequency counts, errors that change these counts may have a signiﬁcant bearing on the accuracy of the method. A particularly interesting case is when the counts are changed by consecutive repetitions of some symbols. In [5], we described a collection of distri- bution estimators, based on Expectation Maximization (EM), both for channels with known and channels with unknown repetition parameters. The focal point of the study was a class of sequences, termed alternating sequences, generated by a Markov chain of special topology. The goal of this work is twofold. The ﬁrst goal is to establish a rigorous analyt- ical framework for evaluating the redundancy of alternating Markov chains. The second goal is to describe how to use alternating sequence distribution estimators for distribution estimation over repetition channels. In particular, we exhibit block and sequential estimators for alternating sequences that have vanishing redundancy and provide good quality estimates in the presence of repetition errors.\nIt is important to observe that although alternating se- quences are generated by a Markov chain, their redundancy cannot be accurately estimated using the methods developed in [6]. This is due to the fact that the bound of [6] are too general to give useful redundancy characterizations for special classes of Markov chains. Surprisingly, the special class of alternating Markov sequences has properties that may be analyzed using tools developed for i.i.d. sequences, with some appropriate modiﬁcations. Our analysis reveals that alternating Markov chains have sub-linear pattern redundancy in the sequence length n, scaling as c √ n + log(n), for some constant c. This is a counterpart to the examples described in [6], where Markov chains of redundancy of order n log n were constructed using permutation patterns.\nThe paper is structured as follows. In Section II, we introduce the problem of distribution estimation over noisy channels and the notion of alternating sequences. In Section III, we review the basic ideas and terminology behind the proposed estimation method, including the notions of patterns and proﬁles. Sections IV-A and IV-B are devoted to deriving upper and lower bounds on the redundancy of block estimators for alternating sequences, respectively. A sequential estimator for alternating sequences is presented in Section V. Section VI describes how to determine the source distribution, observed through a noisy channel, based on the estimated probabilities of alternating sequences.\nDue to space limitations, for a number of results the proofs are either omitted or outlined only. The complete proofs may be found in the archive version of the manuscript.\nConsider a sample sequence x generated by an i.i.d. source S deﬁned over a large-cardinality alphabet A. Suppose that the source S has distribution p S . The estimator observes an erroneous version of x, denoted by y. The errors are modeled as arising from a channel C with input x and output y. What are the ultimate performance limits for estimating the distribution of the source, given that the length of x is small compared to |A| or comparable to |A|?\nEstimating the distribution of a source based on a noise-free short sequence x is a challenging task. ML estimators perform\npoorly in this setting as they typically overestimate probabili- ties of seen symbols, while they underestimate probabilities of unseen symbols. More appropriate solutions for this scenario are due to Good and Turing [1], [2] and Orlitsky et al. [3], [4].\nThere are two approaches one can follow to address an even more difﬁcult family of problems, namely that of small-sample distribution estimation in the presence of errors.\nIn one scenario, one may try to ﬁrst denoise the output sequence y, so as to obtain a reduced-noise estimate ˆ x of x, and then apply a small-sample distribution estimator to ˆ x. Note that ˆ x depends on both y and p S , and thus one needs to estimate p S in order to estimate x and vice versa. This \u201cestimation loop\u201d may be resolved via the use of iterative methods that alternate in improving estimates for x and p S (see our companion paper [5]). In another scenario, one may try to ﬁrst estimate the distribution of y and then reconstruct the distribution of x by \u201cinversion\u201d of the noisy channel. We henceforth pursue the second line of reasoning.\nThe focal point of our inversion study is a special class of Markov sequences that arise in the study of distribution estimation over repetition channels. A repetition channel is a channel which outputs several copies of each input symbol. The number of copies is a random variable with a prede- termined distribution of known or unknown parameters. One important property of repetition channels is that they maintain the identity and order of symbols in the sequence, and only alter the symbols\u2019 runlengths. As an example, the sequence x =\u2018committee\u2019 passed through a repetition channel may be observed as y =\u2018ccommmiitttee\u2019. The alternating sequence of a sequence x, V (x), is a sequence obtained from x by replacing each run of x by one single symbol. We refer to V (x) = V (y) as the alternating sequence of x and denote it by v. Note that v is a Markov sequence and its corresponding Markov chain is referred to as an alternating Markov chain.\nThroughout the rest of the paper, we reserve the symbol N to denote the length of the source output x. We also use m to denote the cardinality of the alphabet A, which may be inﬁnite, and n to denote the length of the alternating sequence v.\nThe pattern ψ = Ψ (x) of a sequence x is obtained by replacing each symbol by its order of appearance in x. The proﬁle of ψ of a pattern is a vector Φ(ψ) = (ϕ 1 , · · · , ϕ n ), where ϕ i is the number of symbols that appear i times in ψ. We use the shorthand notation Φ(x) to denote Φ(Ψ(x)). Notational confusion can be avoided by noting whether the argument of Φ(·) is a sequence or a pattern.\nFor example, the proﬁle of the pattern ψ = 1232421 is ϕ = (2, 1, 1, 0, 0, 0, 0), since 3 and 4 appear once, 1 appears twice, and 2 appears three times. Note that many patterns may have the same proﬁle.\nRemark: Observe that every proﬁle of patterns of length n corresponds to a partition of the integer n. The number of\nparts of size i is ϕ i and n i iϕ i = n. In the example above, ϕ = (2, 1, 1, 0, 0, 0, 0) corresponds to an (unordered) partition of 7 into two parts of size 1, one part of size 2, and one part of size 3, i.e., 7=1+1+2+3. In the correspondence between partitions and proﬁles, the number of parts of size µ equals the number of symbols that appear µ times. The number of partitions of n is denoted by p (n).\nIt is clear that ψ is the pattern of an alternating sequence v of length n if and only if ψ i = ψ i+1 , for 1 ≤ i ≤ n − 1.\nLet I n be the collection of i.i.d. distributions over length n sequences. Consider a probability distribution p over A that assigns probability 0 < p s ≤ 1 to s ∈ A. Then, the distribution induced by p over A n is denoted by p n and for all x ∈ A n , it equals\nEvery distribution p over A also induces a distribution p I n Ψ , p I n Ψ ψ := p n x : Ψ (x) = ψ ,\nover patterns of length n. The set of all such induced distri- butions is denoted by I n Ψ . We simply write p (x) and p ψ if dropping the subscripts and superscripts causes no confusion.\nFurthermore, p induces a probability distribution over alter- nating sequences of length n, which for v = v 1 · · · v n takes the form\n1 − p v j−1 . \t (1) The set of all such induced distributions is denoted by V n . The induced distribution p V n Ψ over alternating patterns of length n and the set V n Ψ are deﬁned similarly to their unconstrained sequence counterparts. Note that the family p V n (v) is Marko- vian.\nThe ﬁrst issue we address is the relationship between the length of the source sequence and the length of the corre- sponding alternating sequence. The following lemma will be useful four our subsequent derivations.\nLemma 1. Assuming all symbol probabilities are smaller than 1 / 2 , we have\nSketch of proof: By deﬁning the exposure martingale of the number of runs of x adapted to the ﬁltration obtained by revealing elements of x one by one, and using Azuma\u2019s inequality, one can show that\nP [n ≤ E[n] − 2λ √ N ] < e −λ 2 /2 . \t (3) Furthermore, assuming all symbol probabilities are smaller than 1/2, it can be shown that E[n] ≥ N/2. Then, the claimed result follows from (3), with λ =\nThe second property, stated in the following lemma, con- cerns patterns with same probability. The lemma that follows provides the means for analyzing alternating sequences using the techniques developed for i.i.d sequences, which allows for signiﬁcant simpliﬁcations when compared to the general Markov process analysis.\nLemma 2. Let ψ = ψ 1 ψ 2 · · · ψ n and ψ = ψ 1 ψ 2 · · · ψ n be two alternating patterns with proﬁle ϕ such that the multiplicity of ψ n is equal to the multiplicity of ψ n . For any i.i.d. distribution p, we have p(ψ) = p(ψ ).\nProof: Let the alphabet be A = {a 1 , a 2 , · · · } and let the probability of a i be p i . Assume ψ n = a and ψ n = b, with a, b ∈ [k], where k is the number of elements appearing in ψ. Let the multiplicity of i ∈ [k] in ψ be denoted by µ i and the multiplicity of i in ψ be denoted by µ i . Note that, by assumption, µ a = µ b .\nFurthermore, let f be a bijection between the set {1, 2, · · · , k} and a subset A ⊆ A of size k. This bijection determines which symbol of the alphabet goes to which symbol in the pattern. Then the probability assigned to pattern ψ is\nwhere the summation is over all bijections between the set {1, 2, · · · , k} and a subset A ⊆ A of size k. There exists a permutation g over [k] with g(b) = a and, g(j) = i for all j ∈ [k]\\{b}, such that µ i = µ j . Then, by letting f (·) = f(g(·)), we have\nBy summing both sides of the equality over all bijections between the set {1, 2, · · · , k} and a subset A ⊆ A of size k, it follows that p ψ = p ψ .\nWe start by deriving an upper bound on the worst case redundancy of alternating sequences, deﬁned, with respect to a collection of distributions P, as\nwhere U(p) is the support set of the distribution p, q(u) is the probability assigned to u by the estimator q, and p(u) is the probability of u with respect to the distribution p. As already pointed out, alternating sequences are ﬁrst-order\nMarkov processes. Prior results by Dhulipala and Orlitsky [6] showed that in general, the per-symbol pattern redundancy of a ﬁrst-order Markov process may be unbounded. For the particular case of alternating sequences, however, we show that the per-symbol pattern redundancy tends to zero.\nSuppose that P is a collection of distributions over patterns of length n and let Ψ(P) denote the set of patterns with positive probability with respect to some distribution in P. Denote the set of proﬁles of patterns in Ψ(P) by Φ (P). Also, for a pattern ψ n of length n, let the set of all alternating patterns with the same proﬁle as the proﬁle of ψ n be denoted by ˜ Ψ −1 (Φ (ψ n )).\nIf Ψ(P) is partitioned into M classes such that any distri- bution in P assigns the same probability to all patterns in the same class, then the worst case redundancy is bounded by [7]\nLet ˜ Ψ n := Ψ(V n Ψ ) and ˜ Φ n := Φ(V n Ψ ). It is easy to see that every proﬁle of an alternating sequence corresponds to a unique partition and hence ˜ Φ n ≤ p (n).\nTheorem 3. The worst case redundancy of V n Ψ grows at most linearly with √ n. More precisely,\nProof: From Lemma 2, all patterns with the same proﬁle and the same multiplicity of the last element, have the same probability. Hence, for any distribution p and any pattern ψ ∈ ˜ Ψ n ,\nwhere L(ψ) denote the number of patterns with the same proﬁle and the same multiplicity of the last element as ψ.\nIn the triple summation above, the index m corresponds to the multiplicity of the last element ψ n of ψ . By deﬁnition of L(ψ ), we have\nThe theorem then follows from p (n) ≤ e π ( 2 3 ) 1 2 n 1 2 [8, pp. 8\u2013102].\nIn subsection IV-A, we saw that the redundancy of patterns of alternating sequences is O ( √ n). Here, we show that it is bounded from below by a constant multiple of n 1/3 .\nLemma 4. Let ψ = 1ψ 1 1ψ 2 · · · 1ψ n/2 , for even n, and ψ = 1ψ 1 1ψ 2 · · · 1ψ n/2 1, for odd n, be an alternating pattern. For a function r n ≥ 1 of n, we have\n(12) where ϕ is the proﬁle of the pattern ψ 1 ψ 2 · · · ψ n/2 .\nProof: Note that since ψ is an alternating pattern, we have ψ j = 1 for 1 ≤ j ≤ n/2 . Consider the alphabet A = {a, s 1 , · · · , s m } , where m is the largest number appearing in ψ minus one. Let v be a sequence with pattern ψ starting with symbol a. Let p be a distribution deﬁned as\n, \t i = a, 1 − 1 2r n , \t i = a,\nwhere µ (i, v) is the number of occurrences of i in v. First, suppose that n is even. We then have\nThe position of a is ﬁxed in v, but the symbols {s 1 , · · · , s m } that appear the same number of times can be swapped with- out changing the pattern Ψ (v) of v. This can be done in\nϕ µ ! ways. Hence, there are n/2−1 µ=1 ϕ µ ! sequences with pattern ψ and probability p (v). Since ϕ n/2 ≤ 2, we\nwhere the inequality follows since p a ≥ 1/2. The number of sequences with pattern ψ and starting with a is n/2 µ=1 ϕ µ !. Theorem 5. For the collection of distributions V n Ψ ,\n \n \nSuppose ﬁrst that n is even. Let Φ n k be the set of proﬁles whose largest parts are of size k. Since Φ n n/2 ⊆ Φ (V n Ψ ),\n \n \nwhere \t Ψ A ψ \t is \t the \t alternating \t pattern 1, ψ 1 + 1, 1, ψ 2 + 1, · · · , 1, ψ n/2 + 1 induced by ψ.\nIn the previous section we studied the problem of assigning probabilities to patterns of a certain length without prior infor- mation. In this section, we address a more practical problem. Namely, given a pattern ψ n−1 of length n−1, what is our best estimate q ψ n |ψ n−1 of the probability of ψ n being the next observed symbol, for ψ n ∈ {1, 2, · · · , 1 + max i≤n−1 ψ i }. This sequential estimator assigns probabilities to patterns of length n in a natural way, q (ψ n ) = n i=1 q ψ i |ψ i−1 , where ψ 0 is the empty string.\nWe present a sequential estimator q 1/2 for patterns of alternating sequences which is based on a sequential estimator for patterns of i.i.d. sequences presented in [7] by Orlitsky et al. Let\nbe the largest probability assigned to ψ n by any distribution in V n Ψ and let q be as deﬁned in (7), i.e.,\n2 3\nFor an alternating pattern ψ i , let ˜ Ψ n ψ i \t := z ∈ ˜ Ψ n : z 1 z 2 · · · z i = ψ i be the set of alternating\npatterns of length n ≥ i whose ﬁrst i elements are the same as ψ i . Accordingly, from q (z) , z ∈ ˜ Ψ n , we deﬁne the distribution\nTheorem 6. The worst case redundancy of the sequential estimator q 1/2 is bounded by\nFrom Lemmas 7 and 8, the proofs of which are omitted, we obtain\n2 3\nThe theorem follows after some simple algebra and by noting that h n < 2n.\nLemma 7. For an alternating pattern ψ n 1 , we have ˆ p ψ n 1 (ψ n 1 )\n2 3\nIn this section, we explain how to reconstruct the noiseless source probabilities from estimates of probabilities provided by alternating sequences. First, recall from Lemma 1, that with high probability, n is of the same order as N and thus, with high probability, the length n of the alternating sequence is large if the length of the source sequence is large.\nAssume that the source has alphabet A = {a 1 , a 2 , · · · } with probability p a j for element a j . Suppose p a i a j is the probability of observing a j after a i in the alternating sequence and assume that the correct values of p a 1 a 2 and p a 2 a 1 are given. We have\nGiven p a 1 a j for j ≥ 2, the remaining probabilities may be obtained by noting that p a1aj p a1a2 = p aj p a2 and thus\nAlthough the presented estimators for alternating sequences do not ﬁnd probabilities with zero error, we are justiﬁed in assuming that the estimates obtained from these estimators are \u201cclose\u201d to the correct values since their redundancy is vanishing. Hence, the estimates of the probabilities p a i a j of the alternating sequence may be used to obtain estimates for probabilities p i of the source sequence as explained in the previous sections.\nAcknowledgment: The work was supported by the NSF STC-CSoI 2011 and NSF CCF 0809895, and AFRLDL-EBS"},"refs":[{"authors":[{"name":"I. J. Good"}],"title":{"text":"The population frequencies of species and the estimation of population parameters"}},{"authors":[{"name":"W. A. Gale"},{"name":"G. Sampson"}],"title":{"text":"Good-Turing smoothing without tears"}},{"authors":[{"name":"A. Orlitsky"},{"name":"N. Santhanam"},{"name":"J. Zhang"}],"title":{"text":"Always good turing: asymp- totically optimal probability estimation"}},{"authors":[{"name":"A. Orlitsky"},{"name":"N. Santhanam"},{"name":"K. Viswanathan"},{"name":"J. Zhang"}],"title":{"text":"Convergence of proﬁle based estimators"}},{"authors":[{"name":"F. Farnoud"},{"name":"O. Milenkovic"},{"name":"N. Santhanam"}],"title":{"text":"Small-sample distribution estimation over sticky channels"}},{"authors":[{"name":"A. Dhulipala"},{"name":"A. Orlitsky"}],"title":{"text":"Universal compression of markov and related sources over arbitrary alphabets"}},{"authors":[{"name":"A. Orlitsky"},{"name":"N. Santhanam"},{"name":"J. Zhang"}],"title":{"text":"Universal compression of memoryless sources over unknown alphabets"}},{"authors":[{"name":"G. E. Andrew"}],"title":{"text":"The theory of partitions"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566831.pdf"},"links":[{"id":"1569566875","weight":14},{"id":"1569564605","weight":7},{"id":"1569565227","weight":7},{"id":"1569564481","weight":7},{"id":"1569566415","weight":7},{"id":"1569565547","weight":14},{"id":"1569564903","weight":7},{"id":"1569566709","weight":7},{"id":"1569566787","weight":28},{"id":"1569564311","weight":7},{"id":"1569565199","weight":7},{"id":"1569566643","weight":7},{"id":"1569566719","weight":7},{"id":"1569566513","weight":7},{"id":"1569565521","weight":7},{"id":"1569566711","weight":7},{"id":"1569565319","weight":14},{"id":"1569564919","weight":7},{"id":"1569565177","weight":7},{"id":"1569566595","weight":7},{"id":"1569565375","weight":7},{"id":"1569565541","weight":7},{"id":"1569564787","weight":21},{"id":"1569561713","weight":14},{"id":"1569562367","weight":7},{"id":"1569563725","weight":14},{"id":"1569565113","weight":7}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T8.2","endtime":"10:30","authors":"Farzad Farnoud, Narayana Prasad Santhanam, Olgica Milenkovic","date":"1341483000000","papertitle":"Alternating Markov Chains for Distribution Estimation in the Presence of Errors","starttime":"10:10","session":"S11.T8: Patterns, Estimation, Hypothesis Testing","room":"Stratton (491)","paperid":"1569566831"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
