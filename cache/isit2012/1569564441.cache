{"id":"1569564441","paper":{"title":{"text":"Tractable Bayesian Social Learning on Trees"},"authors":[{"name":"Yashodhan Kanoria"},{"name":"Omer Tamuz"}],"abstr":{"text":"Abstract\u2014We study a model of Bayesian agents in social networks who learn from the actions of their neighbors. Agents attempt to iteratively estimate an unknown \u2018state of the world\u2019 s from initial private signals, and the past actions of their neighbors in the network. We investigate the computational problem the agents face in implementing the (myopic) Bayesian decision rule. When private signals are independent conditioned on s, and when the social network graph is a tree, we provide a new \u2018dynamic cavity algorithm\u2019 for the agents\u2019 calculations, with computational effort that is exponentially lower than a naive dynamic program.\nWe use this algorithm to perform the ﬁrst numerical simulations of Bayesian agents on networks with hundreds of nodes, and observe rapid learning of s in some settings."},"body":{"text":"The importance of social learning in networks has been demonstrated in a wide variety of settings, such as the adoption of agricultural technology in Ghana [1], and choice of contraceptives by European women [2].\nAccordingly, developing and understanding models of social learning has been a goal of theoretical economics for the past few decades (cf., [3], [4] and references therein). Typical models in this context assume a pure information externality ; agent payoffs depend only on the action they choose and an underlying \u2018state of the world\u2019, and not on the actions of others. Agents observe the actions of their \u2018neighbors\u2019, but payoffs are not observed (or observed with noise) ex interim. The premise in such models is that \u201cactions speak louder than words\u201d \u2013 agents learn by observing each others\u2019 actions, and not by communicating directly.\nWe consider a model that features repeated bidirec- tional interaction between fully Bayesian agents con- nected by a social network. Our model is a specialization of the model of Gale and Kariv [5]. We consider a group of Bayesian agents, each with a private signal that carries information on an unknown state of the world s. The individuals form a social network, so that each observes the actions of some subset of others, whom we call her neighbors. The agents must repeatedly choose between a set of possible actions, the relative merit of which depends on the state of the world s. The agents iteratively learn by observing their neighbors\u2019 actions,\nand picking an action that is myopically optimal, given their information. Gale and Kariv [5] showed that, in this model, agents converge to the same action under some conditions. Related work [6] sheds more light on the phenomenon of agreement on actions and the conditions in which it arises.\nWe are interested in the following questions in the context of this model, which have not been previously addressed:\n(I) What action do the agents converge to, e.g., what is the distribution of this consensus action?\n(II) What are the dynamics of such interactions, e.g., what is the rate of agreement/convergence?\n(III) Are the computations required of Bayesian agents feasible?\nEven in the simple case of two states of the world, binary private signals and two possible actions, the required calculations appear to be very complicated. A na¨ıve dynamic programming algorithm (cf. Proposi- tion III.1) is exponential in the number of individuals. Since at iteration t one may consider only agents at distance t, then in graphs of maximum degree d (on which we focus) the number of individuals to consider is O(min(n, d t )), and the computational effort required of each individual to compute their action at time t is t2 O(min(n,d t )) . This grows very rapidly, restricting previous simulations to networks of three nodes [5]\nWe describe a novel algorithm for the agents\u2019 calcu- lation in our model, when the social network graph is a tree . This algorithm has running time that is exponen- tially smaller than the na¨ıve dynamic program, reducing the computational effort to 2 O(min(n,td)) .\nUsing our algorithm we are able to run numerical simulations of the social learning process. This extends the work of Gale and Kariv [5], who simulated the process for three agents, to much larger networks 1 . We use our algorithm to investigate questions (I) and (II): We numerically evaluate the probability that the agents learn the optimal action, and its progress with time. We observe rapid learning of the optimal action in certain\nWe conjecture that on regular trees, the error proba- bility under Bayesian updates is no larger than the error probability under a different \u2018majority\u2019 update rule, in which agents adopt the opinion of the majority of their neighbors in the previous round. Our numerical results support this conjecture. We prove that for the majority update rule, the number of iterations needed to estimate s correctly with probability 1 − is O(log log(1/ )), for regular trees of degree at least ﬁve. Assuming the conjecture, the computational effort required of Bayesian agents drops from quasi-polynomial in 1/ (using the na¨ıve dynamic program) to polynomial in log(1/ ) (i.e., polylogarithmic in 1/ ), making Bayesian learning com- putationally tractable. Thus, our results shed new light on question (III), suggesting a positive answer in the case of trees.\nThe restriction of the discussion to tree or tree-like social networks certainly excludes many natural set- tings that tend to exhibit highly clustered social graphs. However, in some cases artiﬁcially constructed networks have no or few loops by design; these include highly hierarchical organizations, as well as some physical communication networks. Furthermore, the fact that this non-trivial class of networks does not present a major computational hurdle for fully Bayesian calculations is in itself somewhat surprising.\nSee the full version of the paper [7] for a more detailed discussion, literature survey and proof details.\nTechnical contributions. A key technique used in this paper is the dynamic cavity method, introduced by Kanoria and Montanari [8] in their study of \u2018majority updates\u2019 on trees. This technique is a dynamical version of the cavity method of statistical physics. Our algo- rithmic and analytical approach leveraging the dynamic cavity method should be applicable to a range of models involving iterative updates on trees.\nOur second main technical contribution is our proof, using a dynamic cavity type approach, of doubly expo- nentially fast convergence of majority dynamics on reg- ular trees. This result should be of independent interest.\nThe model we consider is a simpliﬁed version of the model of social learning introduced by Gale and Kariv [5].\nConsider a directed graph G = (V, E), representing a network of n = |V | agents, with V being the set of agents and E being the social ties between them. A directed edge (i, j) indicates that agent i observes agent j. In most of this paper, we study the special case of undirected graphs, where relationships between agents are bidirectional.\nAgents attempt to learn the true state of the world s ∈ S, where S is ﬁnite. The information available\nto them are their private signals x i , where x i ∈ X and X is ﬁnite. We assume a general distribution of (s, x 1 , . . . , x n ), under the condition that private signals are independent conditioned on s, i.e. P [s, x 1 , . . . , x n ] = P [s] i∈V P [x i |s] .\nIn each discrete time period (or round) t = 0, 1, . . . , agent i chooses action σ i (t) ∈ S, which we call a \u2018vote\u2019. Agents observe the votes cast by their neighbors in G. Thus, at the time of voting in round t ≥ 1, the information available to an agent consists of the private signal she received initially, along with the votes cast by her neighbors in rounds up to t − 1. In each round, each agent votes for the state of the world that she currently believes is most likely, given the Bayesian posterior distribution she computes.\nFormally, let σ t i = (σ i (0), σ i (1), . . . , σ i (t)) denote all of agent i\u2019s votes, up to and including time t. Let ∂i denote neighbors of agent i, not including i, i.e., ∂i = {j : (i, j) ∈ E}. We write σ t ∂i = {σ t j } j∈∂i , i.e., σ t ∂i are the votes of i\u2019s neighbors up to and including time t. Then the agents\u2019 votes σ i (t) are given by\nwhere, if the maximum is attained by more than one value, some deterministic tie breaking rule is used. We denote σ i = (σ i (0), σ i (1), . . .).\nNote that σ i (t) is a deterministic function of x i and σ t−1 ∂i . We denote this function g i (t) : X × |S| t|∂i| → S:\nFor convenience, we also deﬁne the vector function g t i that returns the entire history of i\u2019s votes up to time t, g t i = (g i,0 , g i,1 , . . . , g i,t ), so that\nThe full version [7] motivates our model in the context of rational agents, and also presents a detailed compari- son with the model of Gale and Kariv [5].\nA fairly straightforward dynamic programming algo- rithm can be used to compute the actions chosen by agents in our model. The proposition below states the computational complexity of this algorithm.\nProposition III.1. On any graph G, there is a dynamic programming (DP) based algorithm that allows agents to compute their actions up to time t with computational effort t2 O(min(n,(d−1) t )) , where d is the maximum degree of the graph.\nThe algorithm and proof is included in the full version [7] of this paper. This proposition provides the bench- mark that we compare our other algorithmic results to.\nIn particular, we do not consider this algorithm a major contribution of this work.\nA key advantage of the DP algorithm is that it works for any graph G. The disadvantage is that the computa- tional effort required grows doubly exponentially in the number of iterations t.\nOur main result concerns the computational effort needed when the graph G is a tree (i.e., a graph with no loops). We show that computational effort exponentially lower than that of the naive DP sufﬁces in this case.\nTheorem III.2. In a tree graph G with maximum degree d, each agent can calculate her actions up to time t with computational effort t2 O(min(n,td)) .\nThe algorithm we use employs a technique called the dynamic cavity method [8], previously used only in analytical contexts. Section IV contains a description of the algorithm and analysis leading to Theorem III.2.\nWe would like to thank our anonymous referee for pointing out that it may also be possible to prove Theorem III.2 using Bayesian Networks (BN). The proof would involve constructing the appropriate BN and showing that its tree-width is min(n, td).\nAn apparent issue is that the computational effort required is exponential in t; typically, exponentially growing effort is considered as large. However, in this case, we expect the number of iterations t to be typically quite small, for two reasons: (1) In many settings, agents appear to converge to the \u2018right\u2019 answer in a very small number of iterations [5]. If is the desired probability of error, then assuming a reasonable conjecture (Con- jecture III.4), we show that computational effort only polylog(1/ ) is required on trees. Having obtained an approximately correct estimate, the agents would have little incentive to continue updating their beliefs. (2) In many situations we would like to model, we might expect only a small number (e.g., single digit) number of iterative updates to occur, irrespective of network size etc. For instance, voters may discuss an upcoming election with each other over a short period of time, ending on the election day when ballots are cast.\nSince an agent gains information at each round, and since she is Bayesian, then the probability that she votes correctly is non-decreasing in t, the number of rounds. We say that the agent converges if this probability converges to one, or equivalently if the probability that the agent votes incorrectly converges to zero.\nWe say that there is doubly exponential convergence to the state of the world s if the maximum single node error probability max i∈V P [σ i (t) = s] decays with round number t as\nThe following is an immediate corollary of Theorem III.2.\nCorollary III.3. Consider iterative Bayesian learning on a tree of with maximum degree d. If we have doubly exponential convergence to s, then computational effort that is polynomial in log(1/ ) (i.e., polylogarithmic in 1/ ) sufﬁces to achieve error probability P [σ i (t) = s] ≤\nNote that if weaken our assumption to doubly ex- ponential convergence in only a subset V c ⊆ V of nodes, i.e., max i∈V c P [σ i (t) = s] = exp − Ω(b t ) , we still obtain a similar result with nodes in V c efﬁciently learning s.\nWe state below, and provide numerical evidence for, a conjecture that implies doubly exponential convergence of iterative Bayesian learning.\n1) Bayesian vs. \u2018majority\u2019 updates: We conjecture that on regular trees, iterative Bayesian learning leads to lower error probabilities (in the weak sense) than a very simple alternative update rule we call \u2018majority dynamics\u2019[8]. Under this rule, the agents adopt the action taken by the majority of their neighbors in the previous iteration. Our conjecture seems natural since the iterative Bayesian update rule chooses the vote in each round that (myopically) minimizes the error probability. We use σ i (t) to denote votes under the majority dynamics.\nConjecture III.4. Consider binary s ∼ Bernoulli(1/2), and binary private signals that are independent identi- cally distributed given s, with P [x i = s] = 1 −δ for some δ ∈ (0, 1/2). Let the majority dynamics be initialized with the private signals, i.e., σ i (0) = x i for all i ∈ V . Then on any inﬁnite regular tree, for all t ≥ 0, we have\nUsing a dynamic cavity approach, we show doubly ex- ponential convergence for majority dynamics on regular trees (the full version [7] contains a proof):\nTheorem III.5. Consider binary s ∼ Bernoulli(1/2), and binary initial votes σ i (0) that are independent iden- tically distributed given s, with P [σ i (0) = s] = 1 −δ for some δ ∈ (0, 1/2). Let i be any node in an (undirected) d regular tree for d ≥ 5. Then, under the majority dynamics,\n\u2022 We have doubly exponential convergence for itera- tive Bayesian learning on regular trees with d ≥ 5, implying that for any > 0, an error probability\ncan be achieved in O(log log(1/ )) iterations with iterative Bayesian learning.\n\u2022 Combining with Corollary III.3), we see that the computational effort that is polylogarithmic in (1/ ) sufﬁces to achieve error probability 1/ .\nThis compares favorably with the quasi-poly(1/ ) (i.e., exp polylog(1/ ) ) upper bound on computa- tional effort that we can derive by combining Conjecture III.4 and the na¨ıve dynamic program described. Indeed, based on recent results on subexponential decay of error probability with the number of private signals being aggregated [9], it would be natural to conjecture that the number of iterations T needed to obtain an error probability of obeys (d − 1) T ≥ C log(1/ ) for any C < ∞, for small enough. This would then imply that the required computational effort using the na¨ıve DP on a regular tree of degree d grows faster than any polynomial in 1/ .\nSince we are unable to prove our conjecture, we instead provide numerical evidence for it (see the full version of the paper), which is consistent with our conjecture over different values of d and P [x i = s]. The full version also discusses difﬁculties in proving the conjecture.\nWe would like to emphasize that several of the error probability values could be feasibly computed only be- cause of our new efﬁcient approach to computing the decision functions employed by the nodes. Our numeri- cal results indicate very rapid decay of error probability on regular trees (cf. questions (I) and (II) in Section I).\nFigure 1 plots decay of error probabilities in regular trees for iterative Bayesian learning with P [x i = s] = 0.3 Each of the curves (for different values of d) in the plot of log( − log P [σ i (t) = s]) vs. t appear to be bounded below by straight lines with positive slope, suggesting doubly exponential decay of error proba- bilities with t. The empirical rapidity of convergence, particularly for d = 5, 7, is noteworthy. See the full version [7] for more numerical results.\nIn this section we develop the dynamic cavity algo- rithm leading to Theorem III.2. We present the core construction and key technical lemmas in Section IV-A. In Section IV-B, we show how this leads to an efﬁcient algorithm for the Bayesian computations on tree graphs, and prove Theorem III.2.\nAssume in this section that the graph G is a tree with ﬁnite degree nodes. For j ∈ ∂i, let G j→i = (V j→i , E j→i ) denote the connected component contain- ing node j in the graph G with the edge (i, j) removed. That is, G j→i is j\u2019s subtree when G is rooted at i.\nWe consider a modiﬁed process where agent i is replaced by a zombie agent who takes a ﬁxed sequence of actions τ i = (τ i (0), τ i (1), . . .), and the true state of the world is assumed to be some ﬁxed s. Furthermore, this \u2018ﬁxing\u2019 goes unnoticed by the agents (except i, who is a zombie anyway) who perform their calculations assuming that i is her regular Bayesian self. Formally:\nτ i (t) \t for j = i , g j,t (x j , σ t−1 ∂j ) for j = i .\nWe denote by Q [A||τ i , s] the probability of event A in this modiﬁed process.\nThis modiﬁed process is easier to analyze, as the processes on each of the subtrees V j→i for j ∈ ∂i are independent: Recall that private signals are independent conditioned on s, and the zombie agent ensures that the subtrees stay independent of each other. This is formalized in the following claim, which is immediate to see:\nClaim IV.1. For any i ∈ V , s ∈ S and any trajectory τ i , we have\n(Since σ t j is unaffected by τ i (t ) for all t > t, we only need to specify τ t i , and not the entire τ i .)\nNow, it might so happen that for some number of steps the \u2018zombie\u2019 agent behaves exactly as may be expected of a rational player. More precisely, given σ t−1 ∂i , it may be the case that τ t i = g t i x i , σ t−1 ∂i for some x i . This event provides the connection between the modiﬁed process and the original process, and is the inspiration for the following theorem.\nTheorem IV.2. Consider any i ∈ V , s ∈ S, t ∈ N, trajectory τ i and σ t−1 ∂i . For any x i such that P [x i |s] > 0, we have\nUsing Eqs. (3) and (4), we can write the posterior on s computed by node i at time t, in terms of the probabilities Q [·||·]:\n= P [s] P [x i |s] P σ t−1 ∂i |s, x i = P [s] P [x i |s]\nWe have therefore reduced the problem of calculating σ i (t) to calculating Q [·||·]. The following theorem is the heart of the dynamic cavity method and allows us to perform this calculation:\nTheorem IV.3. For any i ∈ V , j ∈ ∂i, s ∈ S, t ∈ N, τ t i and σ t j , we have Q σ t j τ t i , s =\nThe proof of this theorem is similar to the proof of Lemma 2.1 in [8], where the dynamic cavity method is introduced and applied to a different process.\nWe now describe how to perform the agents\u2019 cal- culations. At time t = 0 these calculations are trivial. Assume then that up to time t each agent has calculated the following quantities:\n1) Q σ t−1 j \t τ t−1 i , s , for all s ∈ S, for all i, j ∈ V such that j ∈ ∂i, and for all τ t−1 i and σ t−1 j .\nNote that these can be calculated without making any observations \u2013 only knowledge of the graph G, P [s] and P [x|s] is needed.\nAt time t + 1 each agent makes the following calcu- lations:\n1) Q σ t j τ t i , s for all s, i, j, σ t j , τ t i . These can be calculated using Eq. (7), given the quantities from the previous iteration.\n2) g t+1 i (x i , σ t ∂i ) for all i, x i and σ t ∂i . These can be calculated using Eqs. (5) and (6) and the the newly calculated Q σ t j τ t i , s .\nSince agent j calculates g t+1 i for all i, then she, in particular, calculates g t+1 j . This allows her to choose\nthe (myopic) Bayes optimal action in rounds up to t + 1, based on her neighbors\u2019 past actions. A simple calculation yields the following lemma.\nLemma IV.4. In a tree graph G with maximum degree d, the agents can calculate their actions up to time t with computational effort n2 O(td) .\nIn fact, each agent does not need to perform calcu- lations for the entire graph. It sufﬁces for node i to calculate quantities up to time t for nodes at distance t − t from node i (there are at most (d − 1) t−t such nodes). A short calculation yields an improved bound on computational effort, stated in Theorem III.2. The proof of Theorem III.2 is relatively straightforward and is provided in the full version [7] of this paper.\nAcknowledgments. We would like to thank A. Monta- nari, E. Mossel and A. Sly for valuable discussions, and the anonymous referees for their valuable comments.\nY. Kanoria was supported by a 3Com Corporation Stanford Graduate Fellowship. Omer Tamuz is supported by a Google Europe Fellowship in Social Computing."},"refs":[{"authors":[{"name":"T. Conley"},{"name":"C. Udry"}],"title":{"text":"Social learning through networks: The adoption of new agricultural technologies in ghana"}},{"authors":[{"name":"H. Kohler"}],"title":{"text":"Learning in social networks and contraceptive choice"}},{"authors":[{"name":"S. Goya"}],"title":{"text":"Connections: An Introduction to the Economics of Networks "}},{"authors":[{"name":"D. Acemoglu"},{"name":"M. A. Dahleh"},{"name":"I. Lobel"},{"name":"A. Ozdaglar"}],"title":{"text":"Bayesian learning in social networks"}},{"authors":[{"name":"D. Gale"},{"name":"S. Kariv"}],"title":{"text":"Bayesian learning in social networks"}},{"authors":[{"name":"D. Rosenberg"},{"name":"E. Solan"},{"name":"N. Vieille"}],"title":{"text":"Informational externali- ties and emergence of consensus"}},{"authors":[{"name":"Y. Kanoria"},{"name":"O. Tamuz"}],"title":{"text":"Tractable bayesian social learning"}},{"authors":[{"name":"Y. Kanoria"},{"name":"A. Montanari"}],"title":{"text":"Majority dynamics on trees and the dynamic cavity method"}},{"authors":[],"title":{"text":"Subexponential convergence for information aggregation on regular trees"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564441.pdf"},"links":[{"id":"1569565883","weight":5},{"id":"1569565867","weight":5},{"id":"1569565551","weight":5},{"id":"1569555999","weight":5},{"id":"1569566657","weight":5},{"id":"1569566489","weight":5},{"id":"1569566425","weight":11},{"id":"1569554971","weight":5},{"id":"1569566257","weight":5},{"id":"1569566223","weight":5},{"id":"1569566245","weight":5},{"id":"1569566983","weight":5},{"id":"1569565093","weight":5},{"id":"1569566737","weight":5},{"id":"1569565353","weight":5},{"id":"1569566595","weight":5},{"id":"1569566755","weight":5},{"id":"1569564437","weight":11},{"id":"1569565529","weight":5},{"id":"1569566397","weight":5},{"id":"1569567691","weight":5},{"id":"1569565861","weight":5},{"id":"1569562367","weight":5},{"id":"1569565997","weight":5},{"id":"1569566825","weight":5},{"id":"1569566443","weight":5},{"id":"1569566727","weight":5},{"id":"1569565315","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S15.T8.1","endtime":"10:10","authors":"Yashodhan Kanoria, Omer Tamuz","date":"1341568200000","papertitle":"Tractable Bayesian Social Learning on Trees","starttime":"09:50","session":"S15.T8: Tree Learning","room":"Stratton (491)","paperid":"1569564441"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
