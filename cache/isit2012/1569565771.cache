{"id":"1569565771","paper":{"title":{"text":"Mutual Information and Relative Entropy over the Binomial and Negative Binomial Channels"},"authors":[{"name":"Camilo G. Taborda"},{"name":"Fernando P´erez-Cruz"}],"abstr":{"text":"Abstract\u2014We study the relation of the mutual information and relative entropy over the Binomial and Negative Binomial channels with estimation theoretical quantities, in which we extend already known results for Gaussian and Poisson channels. We establish general expressions for these information theory concepts with a direct connection with estimation theory through the conditional mean estimation and a particular loss function."},"body":{"text":"closely related with the conditional mean estimation through the mean square error (mse) [1]. The derivative of the mutual information, as a function of the signal to noise ratio (snr), is proportional to the minimum mse (mmse) regardless the input distribution, i.e.,\n= 1 2\nmmse(snr), (1)\nsnrx, 1) and P X is any distribution such that E P X [X 2 ] < ∞.\nThis result can be extended to the relative entropy between two distributions P Y and Q Y , that are, respectively, the transformation of two generic input distributions P X and Q X through a Gaussian channel [2]. The derivative of the relative entropy between the marginals P Y and Q Y with respect to the snr is\n2 (mse Q (snr) − mmse(snr)) = 1\nwhere mse Q (snr) represents the mismatch mean square error incurred when we estimate X from Y assuming a prior distribution Q X different to the actual P X , i.e.,\nGuo et al. [3] have extended these results for the Poisson channel. In this analogy, conditioned on X = x, Y ∼ P(γx), where X is a positive random variable and γ represents an input scaling factor. In this case the derivative of the input- output mutual information with respect to γ is given by\nThis result was extended to the relative entropy between two marginals P Y and Q Y in [4], in which the authors showed that it can be rewritten following the Gaussian channel results by exchanging the square-loss function by (a, ˆ a) = a log(a/ˆ a)−(a−ˆ a). The derivative of relative entropy between the marginals P Y and Q Y , which are respectively, random transformations of P X and Q X through a Poisson channel can be expressed by\nIn this paper, we show that these results can be extended to the Binomial and Negative Binomial channels. Although these distributions are similar the results are dissimilar enough to be presented separately. The derivative is taken with respect to a different parameter and the loss functions are also different.\nFor the Binomial channel, in which the outcome Y is governed by the sum of n independent Bernoulli trials and a probability of success p = αx/n where X is a positive bounded random variable, we ﬁnd expressions for the deriva- tive of mutual information and relative entropy with respect to α in terms of the expectation of the following loss function:\na(n − ˆ a) ˆ a(n − a)\nBesides the actual proofs, the only remarkable difference between the results in this paper with respect to the Gaussian and Poisson distributions is that the expectation of the loss function is taken with respect to a Binomial distribution with\nIn the second part of the paper, we show that these rela- tionships can be extended to a Negative Binomial channel. In this case, conditioned on X = x, Y ∼ N B(r, p(αx)), where p(αx) stands for the probability of success of each Bernoulli trial for a ﬁxed number of failures r. On the one hand, setting p(αx) = αx, as in the Binomial channel yields a closed-form expression without a connection with a well-behaved loss- function. On the other hand, setting p(αx) = αx/(αx + r), such that the mean of the distribution is αx (as in previous results), we can derive similar results to those for the Gaussian, Poisson and Binomial channels, using the loss function:\na(r + ˆ a) ˆ a(r + a)\nThe rest of the paper is divided as follows. We, respectively, present the results for the Binomial and Negative Binomial Channels in Sections II and III. Both sections are divided in three subsections. In Subsections II-A and III-A, we show that the proposed loss functions have similar properties to the square loss and the loss used for the Poisson channel relations. The results obtained for the mutual information are shown in Subsections II-B and III-B, and the extensions for the relative entropy are presented in Subsections II-C and III-C. We conclude the paper in Section IV with a discussion.\nIn this section we propose a natural loss function that is closely related with the functional that describes the relative entropy and mutual information over the Binomial channel. In addition, we formulate different properties fulﬁlled by this loss function, showing by this way similarities between the behavior stated for the Gaussian and Poisson channel and the scheme proposed for the Binomial channel.\nLemma 1. The function n (a, ˆ a) proposed in (7) has the following properties:\n1) Non-negativity: n (a, ˆ a) ≥ 0 with equality if and only if a = ˆ a.\n3) Unboundedness of loss for underestimation: For any a ∈ (0, n), lim ˆ a→0 + n (a, ˆ a) = ∞.\n4) Unboundedness of loss for overestimation: For any a ∈ (0, n), lim ˆ a→n − n (a, ˆ a) = ∞.\n| a=ˆ a (a − ˆ a), where φ n (a) = a log a n−a . This loss is the difference between the points φ n (a) and its ﬁrst order Taylor approximation at φ n (ˆ a). Due to the strictly convexity of the function φ n (a) we can state the non-negativity of the function n (a, ˆ a), see Section 3 in [5]. Properties 2, 3 and 4 are straightforward.\nThe unboundedness behavior of the loss function when ˆ a → 0 + and ˆ a → n − for some a ∈ (0, n) is a desired property when measuring goodness of reconstruction of a bounded quantity, not possessed by the more common loss functions.\nLemma 2. Consider a positive random variable A over the interval (0, n) such that E A log A n−A < ∞. Then, for any ˆ a ∈ (0, n)\n= E A log A(n − E[A]) (n − A)E[A]\n+ n(ˆ a − a) n − ˆ a\nThe main consequence of this lemma is that the mean minimizes the expected loss, i.e.,\nwhich is a property possessed by all the Bregman divergences [6]. Notice in this case that\nThe Binomial distribution is a discrete probability distribu- tion that measures the number of successes in n independent trials of a Bernoulli random variable with a probability of success p. The framework considered in this paper assumes that p = αx/n where x is a realization of a positive bounded random variable X, distributed over the set X = [a, b] where a, b ∈ R + , and n is a positive integer value. Accordingly the conditional probability mass of the channel is given by\nNotice that the constraint over the parameter p makes that 0 < αx < n for all values of x ∈ X . Therefore, the set of feasible values of α is given by the set A = {α ∈ R + |0 < αb < n}.\nTheorem 1. Let X be a random variable distributed over the set X = [a, b] where a, b ∈ R + with E P X X log X 1−X/b < ∞. Consider a Binomial channel with parameters (n, αx/n). Then, for a ﬁxed n ∈ N and for all α ∈ A, the derivative of the input-output mutual information with respect to α is given by\n \n \n= 1 α\n[·] represents the expectation with respect to the joint distribution P XY when P Y |X is the sum of n−1 Bernoulli trials with probability of success p = αx/n, i.e.\nNotice that the −1 only affects the number of trials of the underlying Bernoulli random variable, but the probability of success still depends on n and not n − 1.\nThe theorem is proven in [7]. The deﬁnition in (15) implies that the expectations carried out at the right hand side of (14) consider that P Y |X is a Binomial distribution with n − 1 trials, while on the left hand side of (14), the mutual information employs the conditional distribution P Y |X with n independent Bernoulli trials.\nThe non-negativity property of the loss function n (a, ˆ a) let us state that the mutual information increases as long as the scaling factor increases. Theorem 1 is the Binomial counterpart of (1) and (4) to the Gaussian and Poisson channels respectively.\nSince 0 < αx/n < 1/2 for all x ∈ X the result follows by noting that (I) and (II) in (17) are strictly positive terms.\nNow, consider two probability distributions, P X and Q X deﬁned over a positive and bounded set X . Furthermore, consider a random transformation that takes an outcome of X and produces an output Y according to a Binomial distribution of n independent Bernoulli trials and probability of success αx/n, i.e. conditioned on X = x, Y ∼ B(n, αx/n). The output Y can follow two different distributions P n Y and Q n Y , which have been respectively generated from P X and Q X . For the sake of clarity, superscript n on the notation of the marginals P n Y and Q n Y , makes reference to the number of independent Bernoulli trials used to construct the conditional P Y |X that generates each output distribution.\nTheorem 2. Let X be a random variable over the set X = [a, b] where a, b ∈ R + distributed according to P X or Q X with E P X X log X 1−X/b < ∞ and E Q X X log X 1−X/b < ∞,\nand consider a Binomial channel with parameters (n, αx/n). Then, for a ﬁxed n ∈ N and for all α ∈ A, the derivative of the relative entropy between the marginals P n Y and Q n Y with respect to α is given by\n(18) where\nThe theorem is proven in [7]. One immediate implication of this theorem is that, due to the non-negativity property of the loss function, the relative entropy between the marginals increases as a function of the scaling factor α.\nIn this section we propose a natural loss function that is closely related with the functional that describes the relative entropy and mutual information over the Negative Binomial channel. In addition we formulate different properties fulﬁlled by the loss function proposed, showing by this way similarities between the behavior stated for the Gaussian, Poisson and Binomial channels and the scheme proposed for the Negative Binomial channel.\nLemma 3. The function r (a, ˆ a) in (8) has the following properties:\n1) Non-negativity: r (a, ˆ a) ≥ 0 with equality if and only if a = ˆ a.\n3) Unboundedness of loss for underestimation: for any a > 0, lim ˆ a→0 + r (a, ˆ a) = ∞.\n| a=ˆ a (a − ˆ a), where φ r (a) = a log a r+a . Due to the strictly convexity of the function φ r (a) we can state the non-negativity of the function r (a, ˆ a), see Section 3 in [5]. Properties 2 and 3 are straightforward.\nThe unboundedness behavior of the loss function when ˆ a → 0 + for some a > 0 is a desired property when measuring goodness of reconstruction of non-negative quantities, not possessed by the more common loss functions.\nLemma 4. For any non-negative random variable A such that E A log A r+A < ∞ and any ˆ a ∈ (0, ∞),\nE[ r (A, ˆ a)] = E A log A(r + ˆ a) (r + A)ˆ a\n+ r(ˆ a − a) r + ˆ a\nEquation (22) indicates that the functional E[ r (A, ˆ a)] is quite similar to its counterparts developed for the Gaussian, Poisson and Binomial channels, where, in the former we use the square loss function and in the second we use a loss function based on the convex conjugate of the log-moment generating function of the Poisson distribution. One consequence of the previous analysis is the fact that the conditional expectation E[A|A ] is the unique minimizer of the loss function r (A, ˆ a) when we construct an estimator of A based on the observation A . This property, as has been shown in [6] is common to all the Bregman loss functions.\nThe Negative Binomial distribution is a discrete probability distribution of the number of successes in a sequence of Bernoulli trials before a number of failures r occur. In this framework we set the parameter p of each Bernoulli trial so that the mean of the conditional distribution is equal to αx, as has been common in the previous results obtained for the Poisson and Binomial channels. Accordingly the conditional probability mass of the channel is given by\n(23) where r ∈ N is a ﬁxed integer value, α > 0 is an input scaling parameter and x is a realization of a positive random variable X. Notice that in this case the set of feasible values of α is the set A = R + .\nTheorem 3. Let X be a positive random variable with E P X X log X r+X < ∞ and consider a Negative Binomial channel with parameters (r, αx/(αx + r)). Then, for a ﬁxed value r ∈ N and for all α ∈ A, the derivative of the input-\n \n \n[·] represents the expectation with respect to the joint distribution P XY where P Y |X is a Negative Binomial distribution with parameters (r + 1, αx/(αx + r)), i.e.,\ng(X, Y ) r + y y\n(25) Notice that the +1 only affects the number of failures of\nthe underlying Bernoulli random variable, but the probability of success still depends on r and not r + 1.\nThe theorem is proven in [7]. For the sake of clarity, notice that the conditional distribution P Y |X used at the left hand side of (24) uses as parameters (r, αx/(αx + r)) while at the right hand side the conditional distribution used correspond to a Negative Binomial with parameters (r + 1, αx/(αx + r)). An immediate consequence of the result obtained in (24) is that the mutual information increases as long as the the input scaling factor increases.\n(26) Proof: Due to the previous theorem the derivative of the\nClearly, since the random variable X is positive, both terms at the right hand side of (27) (I and II) are positive, therefore we can state the conclusion given by the corollary.\nA similar reasoning in the Gaussian channel shows that the derivative of the input-output mutual information with respect to an input scaling factor α is upper bounded by a half the second moment of the input X. For more details, see [8].\nLet P X and Q X be two distributions over a positive bounded random variable X. Consider a random trans- formation of the Negative Binomial type with parameters\n(r, αx/(αx+r)). We denote as P r Y and Q r Y each marginal dis- tribution followed by the output of the random transformation under the distributions P X and Q X , respectively. Superscript r makes reference to the number of failures in the conditional distribution of the channel P Y |X .\nTheorem 4. Let X be a positive random variable, which can be distributed according to P X or Q X with E P X X log X r+X < ∞ and E Q X X log X r+X < ∞.\nConsider a Negative Binomial channel with parameters (r, αx/(αx + r)). Then, for a ﬁxed value r ∈ N and for all α ∈ A, the derivative of the relative entropy between the marginals P r Y and Q r Y with respect to α is given by\n= 1 α\nmle Q (α) = 1 α\nOne immediate implication of this theorem is that, due to the non-negativity property of the loss function, the relative entropy between the marginals increases as long as the scaling input increases.\nIn this paper, we have proven the relation between the relative entropy and estimation theoretic quantities holds for the Binomial and Negative Binomial Channels. For the Gaus- sian distribution the connection is through the well-known square loss, while the Poisson and Negative Binomial two loss functions for nonnegative reals are needed and a bounded nonnegative loss function is needed for the Binomial channel. All these relationships present some common properties. First, the derivative is taken with respect to the mean parameter of the conditional distribution between the input and output. This result becomes obvious when we deal with the Negative Binomial channel. Second, the conditional mean minimizes the proposed loss functions, which is a unique property of the Bregman Divergences [6]. Third, the derivative is proportional to the expectation of the loss function, between the input and its conditional mean estimate. Two obvious questions arise: Can we extend these results to other distributions and loss-functions? Is there a unique representation that links the conditional distribution between X and Y and the loss function? We believe that the second question can be answered using the relation between the exponential families and the\nBregman divergences [9] and, if so, it might also provide an answer to the ﬁrst question, as well. Although the loss-function used for the Binomial and Negative Binomial channels is different from the ones proven in [9] and it would lead to a different connection between exponential families and the Bregman divergences."},"refs":[{"authors":[{"name":"D. Guo"},{"name":"S. Shamai"},{"name":"S. Verd´u"}],"title":{"text":"Mutual Information and Minimum Mean-Square Error in Gaussian Channels"}},{"authors":[{"name":"S. Verd´u"}],"title":{"text":"Mismatched Estimation and Relative Entropy"}},{"authors":[{"name":"D. Guo"},{"name":"S. Shamai"},{"name":"S. Verd´u"}],"title":{"text":"Mutual Information and Conditional Mean Estimation in Poisson Channels"}},{"authors":[{"name":"R. Atar"},{"name":"T. Weissman"}],"title":{"text":"Mutual Information, Relative Entropy, and Estimation in the Poisson Channel"}},{"authors":[{"name":"S. Boy"},{"name":"L. Vandenbergh"}],"title":{"text":"Convex Optimization"}},{"authors":[{"name":"A. Banerjee"},{"name":"X. Guo"},{"name":"H. Wang"}],"title":{"text":"On the Optimality of Conditional Expectation as a Bregman Predictor"}},{"authors":[{"name":"C. G. Taborda"},{"name":"F. P´erez-Cruz"}],"title":{"text":"Mutual Information and Relative Entropy over the Binomial, Negative Binomial and Poisson Channels"}},{"authors":[{"name":"D. Guo"},{"name":"Y. Wu"},{"name":"S. Shamai"},{"name":"S. Verd´u"}],"title":{"text":"Estimation in Gaussian Noise: Properties of the Minimum Mean-Square Error"}},{"authors":[{"name":"A. Banerjee"},{"name":"S. Merugu"},{"name":"I. S. Dhillon"},{"name":"J. Ghosh"}],"title":{"text":"Clustering with Bregman Divergences"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565771.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T7.4","endtime":"18:00","authors":"Camilo G. Taborda, Fernando Pérez-Cruz","date":"1341250800000","papertitle":"Mutual Information and Relative Entropy over the Binomial and Negative Binomial Channels","starttime":"17:40","session":"S4.T7: Capacity of Finite-Alphabet Channels","room":"Stratton (407)","paperid":"1569565771"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
