{"id":"1569566661","paper":{"title":{"text":"Degree-guided Map-Reduce Task Assignment with Data Locality Constraint"},"authors":[{"name":"Qiaomin Xie"},{"name":"Yi Lu"}],"abstr":{"text":"Abstract\u2014The map-reduce framework is used in many data- intensive parallel processing systems. Data locality is an impor- tant problem with map-reduce as tasks with local data complete faster than those with remote data. We propose a degree- guided task assignment algorithm, which uses very little extra information than the currently implemented Random Server algorithm. We analyze a simple version of the degree-guided algorithm, called Peeling algorithm, and the Random Server algorithm in a discrete-time model using evolution of random graphs. We characterize the thresholds below which no queueing takes place and compute the effective service rates for both algorithms. The degree-guided algorithm achieves the optimal performance in the region of practical interest and signiﬁcantly outperforms the Random Server algorithm. The performance characteristics derived from discrete time model are conﬁrmed with simulation in continuous time."},"body":{"text":"Cluster computing systems, such as MapReduce [1] and Hadoop [2], have become a popular framework for data- intensive applications. Large clusters consisting of tens of thousands of machines [3] have been built for web indexing and searching; small and mid-size clusters have also been built for business analytics and corporate data warehousing [4]. In clusters of all sizes, throughput and job completion time are important metrics for computation efﬁciency.\nTo facilitate parallel computation, a ﬁle is divided into many small chunks. Each chunk is replicated three times by default and placed on different servers. The default number is determined based on availability concerns [5]: The ﬁrst chunk is placed on a randomly chosen server; the second chunk is placed on a random server in the same rack to protect against server failures, and the third chunk is placed on a random server outside the rack to protect against rack failures. When a task is placed on a server where data are not locally available, it will need to retrieve data from one of the remote servers hosting the replicas. This increases the completion time of the task. As a result, placing tasks as close as possible to data is a common practice of data-intensive systems, referred to as the data locality problem [1].\nData locality is an important problem as it signiﬁcantly affects system throughput and job completion times. The current scheduling algorithms in Hadoop are based on the Random Server algorithm [6], which depends on a large number of outstanding tasks to achieve high data locality. As a result, with light to medium load, which is the region online\nsystems operate in today, the Random Server algorithm results in unnecessary delay of tasks.\nIn this paper, we propose a degree-guided task assign- ment algorithm that signiﬁcantly outperforms the Random Server algorithm at light to medium load. We analyze the degree-guided algorithm and the Random Server algorithm in a discrete-time model using evolution of random graphs. We show that the degree-guided algorithm has the same performance as the optimal maximum matching algorithm below a threshold, and converges to that of the Random Server algorithm at high load. We characterize the thresholds below which no queueing takes place for both algorithms. The degree-guided algorithm experiences queueing at a higher load than the Random Server algorithm and has a signiﬁcantly higher effective service rate in the region of light to medium load. The performance characteristics are further veriﬁed with simulation in continuous time.\nThis paper is organized as follows. In Section II, we describe the algorithms and the model used for analysis. Section III derives the mean-ﬁeld model of the assignment procedure for a single time slot. We show the performance of the Random Server algorithm and the Peeling algorithm over one time slot in Section IV. Section V characterizes the ﬁxed points and the thresholds below which no queueing takes place for both algorithms. We evaluate these two algorithms via simulation in continuous time in Section VI.\nConsider a discrete-time model for a system with m parallel servers. At the beginning of each time slot, a constant number of tasks arrive at the system. Each task processes one ﬁle chunk, which is replicated d times and placed on d randomly selected servers.\nThe placement of data can be modelled by bipartite graphs G with n task nodes and m server nodes, as illustrated in Fig. 1. An edge between task node i and server node j indicates the presence of data for task i on server j. And we deﬁne the degree of a server node as the number of unassigned tasks that have data on it. In particular, we consider the default data replication scheme of 3 replicas for each data chunk, i.e, d = 3. The scheme can be easily extended to a variable number of replicas for different data chunks, as proposed in [3], [7].\nAssume that each server can process only one task at any time. Task assignment is performed at the beginning of a slot. All tasks are assigned unless there are no more idle servers, and the remaining tasks are kept in a queue. For a task assigned to a server with its data, the service time is assumed to follow geometric distribution with parameter p, and the parameter is q for tasks processed at servers without its data, where q < p. We refer to servers serving tasks with local data as p-server and servers serving tasks without local data as q-server.\n1. Random Server algorithm: Whenever there are outstand- ing tasks in the system, an idle server is chosen randomly. If there exist multiple tasks whose data is replicated on this server, one task is uniformly selected from the set. Otherwise, this server is assigned for a randomly selected task. This models the FIFO scheduler currently used in Hadoop clusters [2], [7], which assigns tasks following exactly the same rule.\n2. Degree-guided algorithm: An idle server with the least non-zero degree is sampled if outstanding tasks are present. And this server is assigned for a task that\u2019s randomly selected from all of its connected tasks. When all idle servers are of degree 0, a server is selected randomly for a random task. The least non-zero degree of the selected server ensures that the task is assigned to a p-server. In addition, it maintains the connections of unassigned tasks to the remaining idle servers to the utmost extend, which increases the probability of assigning these tasks to p-servers.\nIn this paper, we consider a simple version of the degree- guided algorithm, called Peeling algorithm. With the Peeling algorithm, an idle server that has local data for only one out- standing task is assigned to this task. The procedure continues until no server of degree 1 exists, which is referred to as the peeling stage . Then the assignment procedure continues with the Random Server algorithm, which is called the random stage . Note that the Peeling algorithm is equivalent to the degree-guided algorithm until the peeling stage stops.\nWe need the following deﬁnitions to derive the mean-ﬁeld models for the Random Server algorithm and the Peeling algorithm. In this section, we focus on the analysis of these two algorithms over one time slot.\nDeﬁnition 1: Degree Distributions from A Node Per- spective. Given a graph G with n task nodes and m server nodes, let L i denote the number of task nodes of degree i, i = 0, 1, · · · , l max , and R j the number of server nodes of degree j, j = 0, 1, · · · , r max , where l max and r max are the largest degree of task nodes and server nodes respectively. So i L i = n and i R i = m. The degree distributions from a node perspective are deﬁned by the pair (L, R), where L = {L 0 , L 1 , · · · , L l max } and R = {R 0 , R 1 , · · · , R r max }.\nDeﬁnition 2: The Standard Ensemble G (L, R). Given the degree distributions (L, R), we deﬁne an ensemble of bipartite graphs G (L, R) in the following way. Each graph in G (L, R) has the degree distributions (L, R). As a node of degree i has i sockets from which the i edges emanate, there are a total of i iL i = j jR j sockets on each side. Label the sockets on each side with numbers {1, · · · , i iL i }. Let σ be a permutation on {1, · · · , i iL i }, and associate σ to a bipartite graph by connecting the j-th socket on the task nodes to the σ (j)-th socket on the server nodes. We deﬁne a probability distribution over the set of graphs by placing the uniform probability distribution on σ. The deﬁnition is the same as the standard ensemble for LDPC codes [8], pg 78.\nWe model the assignment procedure as an evolution of the random graph ensemble. Let s ∈ N denote the assignment step, which starts at zero and increases by one for every task assigned. At each step, all edges connected to the selected server and those connected to the assigned task are removed from the graph. This procedure results in a sequence of resid- ual graphs, denoted by G (L(s), R(s)), where (L(s), R(s)) are the degree distributions of unsigned tasks and remaining idle servers at step s. No edge is left at the end of the assignment as either all tasks are assigned or no idle server remains. Consider a system with n unassigned tasks and k idle servers before the assignment (k ≤ m). Then L i (0) n\nfollows a binomial distribution B (d, k m ) and R i (0) k a binomial distribution B (dn, 1 m ) truncated at r max . Hence l max = d and we can set r max a ﬁxed constant as there is a limited amount of storage space on each server. Let M p (s) and M q (s) denote the increased numbers of p-servers and q-servers at the end of step s respectively. Deﬁne the scaled time τ = s w , where w = nd denotes the total number of edges in the initial graph. Let γ i (τ ) = R i (wτ ) w , l i (τ ) = L i (wτ ) w , m p (τ ) = M p (wτ ) w , m q (τ ) = M q (wτ ) w , which together determine the assignment path for a single slot. We obtain the following theorem. Full proof can be found in [9].\nTheorem 1: Evolution of Residual Graph for the As- signment Algorithms. The expected assignment paths of the two algorithms are described by the two sets of differential equations respectively: Random Server algorithm:\ne (τ ) \t , for 0 ≤ i ≤ l max − 1 \t (1)\nc (τ ) \t (3) where v (τ ) = j l j (τ ), c(τ ) = j γ j (τ ), and e(τ ) =\njl j (τ ) = j jγ j (τ ). Peeling algorithm:\njl j (τ ) , for 2 ≤ i ≤ l max − 1, (4) dγ 1 (τ )\nfor 2 ≤ i ≤ r max − 1, \t (6) m p (τ ) = τ and m q (τ ) = 0 \t (7)\nWith probability at least 1 − O(n 1 6 e − √ dn c 3 ), the assignment path of a speciﬁc instance has maximum L 1 -distance from the expected assignment path at most O (n − 1 6 ) from the start of the process until either the total number of nodes in the residual graph has reached size ηn, where η is an arbitrary strictly positive constant, or the algorithm gets stuck.\nBy solving these differential equations, we can obtain the expected increased fractions of p-servers and q-servers after the assignment, which provide an efﬁcient way to evaluate the performance of these two algorithms.\nLet (f i , f p , f q ) denote the fraction of idle servers, p- servers and q-servers at the beginning of a time slot, hence f i , f p , f q ∈ [0, 1] and f i + f p + f q = 1. We ﬁrst show the property of function σ p (r, f i ) = M p (n) m , which denotes the increased fraction of p-servers at the end of assignment process for a system with n m = r and f i percent of idle servers before the assignment. We then evaluate the performance of the two algorithms against the maximum matching, which is optimal.\nFirst, we show that for the Random Server algorithm, the function σ p (r, f i ) does not depend on the fraction of idle servers available at the beginning of the slot, so long as all tasks are assigned eventually.\nTheorem 2: Independence of f i . Consider the Random Server algorithm for a system with m servers and n = mr unassigned tasks. Let the fraction of servers be (f i , f p , f q ) before the assignment. If r < f i , the increased fraction of p-servers, σ p (r, f i ), is independent of f i .\nWe sketch the proof of Theorem 2 here and full proof can be found in [9]. Note that the evolution of the assignment procedure only depends on the degree distribution of tasks nodes and servers nodes, and the sequence of idle servers sampled. The random selection of idle servers, it makes no difference to determine the sequence of idle servers sampled I n before the assignment. Then restrict the original graph to the tasks nodes and I n , denoted by G . With I n ﬁxed, σ p is determined by G , which has the same degree distributions for different f i . Hence the resulting σ p is the same.\nFigure 2 shows the plots of σ p (r) for the Random Server algorithm and the plots of σ p (r, f i ) for the Peeling algorithm with different values of f i . Note that σ p (r, f i ) = M p (n) m can be obtained by solving the differential equations in Theorem 1 numerically. At the same load, the increased fraction of p- servers by the Peeling algorithm is larger than the Random Server algorithm.\nNote that for Peeling algorithm with f i = 1, there exists an obvious point for r, above which σ p (r, f i ) decreases before increases again. Below the threshold, γ 1 (τ ) > 0 for τ ∈ [0, min{ 1 d , f i dr }]. That is, servers of degree-1 are available throughout the assignment procedure so the peeling stage doesn\u2019t stop. Hence σ p (r, f i ) = min{r, f i }, which equals r if r < f i . With f i < 1, however, some tasks are only connected to occupied servers, which will result in the emergence of random stage and hence decrease σ p .\nWe also observe that σ p (r) increases monotonically for the Random Server algorithm. We have the theorem below.\nTheorem 3: Monotonicity. Consider the Random Server algorithm for a system with m servers and n = mr tasks arriving. Let f i = 1 and r < f i . Then the increased fraction of p-servers, σ p (r), strictly increases with r.\nThe idea of proof for Theorem 3 is induction. We show that σ p ( n m ) < σ p ( n +1 m ) for n, m ∈ N using coupling. For detailed proof, please refer to [9].\nThe objective of task assignment is to assign as many tasks as possible to a server with local data, which is a matching problem. The following theorem shows the performance of the maximum matching algorithm, which is optimal.\nTheorem 4: [10] Consider a system where n, m → ∞ with r = n m ﬁxed and each task has its data on d randomly selected servers. Let y be the unique solution to the equation:\n1 − e − y − ye − y \t (8) and r ∗ = \t y d (1−e −y ) d−1 . If r ≤ r ∗ , all tasks are assigned to p-servers; if r ≥ r ∗ , proportion of p-servers assigned is:\nx = (1 − e − drx ) d− 1 \t (10) For the case of d = 3, we obtain r ∗ = 0.918. The\nperformance of maximum matching is showed in Fig. 2. We can see that with all servers idle initially, the Peeling algorithm achieves the optimal performance as maximum matching, if r is below the threshold for the emergence of the random stage. And the improvement of the Peeling algorithm over the Random Server algorithm is signiﬁcant. Above the threshold, the Peeling algorithm deteriorates from the optimal matching, but still performs better than the Random Server algorithm. We have the following lemma.\nLemma 1: The Peeling algorithm achieves the performance of optimal matching when the load for the system is below the threshold for the emergence of the random stage.\nIn this section, we characterize the ﬁxed points of the system evolution when the load is below the threshold that no task remains in the queue after the assignment.\nConsider a system with r = n m ﬁxed and n, m → ∞ . Let (π i , π p , π q ) denote the equilibrium values of (f i , f p , f q ) before the assignment without tasks queueing, and (ˆ π i , ˆ π p , ˆ π q ) the equilibrium values after the assignment.\nTheorem 5: Fixed point characterization. For both the Random Server algorithm and the Peeling algorithm, deﬁne\nwhere g (π i ) is different for these two algorithms. If no queueing takes place, π i satisﬁes\nπ i = g(π i ) + r and g (π i ) > 0. \t (11) And the ﬁxed point is\nProof. With n = mr and no tasks in the queue, at the ﬁxed point, all n tasks are assigned, which yields the following:\nˆ π q = π q + r − σ p (r, π i ), ˆ π i = π i − r.\nWith the geometric distribution for the service time, a task leaves a p-server with probability p and leaves a q-server with probability q, we have\nπ i = ˆ π i + ˆ π p p + ˆ π q q, π p = ˆ π p (1 − p), π q = ˆ π q (1 − q).\nTo ensure all tasks assigned, we need π i > r, which yields Eq. (11). Substituting π i in the above equations yields the ﬁxed point.\nRemark. For the Random Server algorithm with π i > r, Theorem 2 indicates that σ p (r, π i ) only depends on r. So the computation of its ﬁxed point can be simpliﬁed.\nFrom Theorem 5, we can deﬁne the threshold for queueing. Deﬁnition 3: Threshold for Queueing . We deﬁne the\nBy solving the mean ﬁelds equations in Theorem 1, we obtain a table of σ p (r, f i ). With the values of σ p (r, f i ), we can obtain the queueing threshold ρ ∗ . For instance, with p = 0.8 and q = 0.4, ρ ∗ = 0.695 for the Random Server algorithm and ρ ∗ = 0.765 for the Peeling algorithm. Fig. 3 plots the thresholds for these two algorithms against q with p = 0.8. It shows that with the same p and q, the threshold under the Random Server algorithm is smaller than that under the Peeling algorithm. In addition, the thresholds for both algorithms increase to 1 as q increases towards p, since the system behaves as a homogeneous system when q almost equals p.\nwhich measures the efﬁciency of the servers. When the load is below the queueing threshold, µ e can be obtained from Theorem 5. We do not have explicit characterization of the ﬁxed points at high load. Instead, we iterate the mean-ﬁeld equations over multiple time slots to obtain the ﬁxed point.\nFig. 4 shows the effective mean service rate at the ﬁxed points for both algorithms with p = 0.6 and q = 0.2. As soon as queueing takes place, the mean service rate for the Peeling algorithm decreases drastically and converges to that of the Random Server algorithm. This is due to the lack of degree- one server nodes and the peeling stage stops when there are still a large number of unassigned tasks.\nWe evaluate the Peeling algorithm against the Random Server algorithm via simulation in continuous time. Consider a system of m parallel servers. Tasks arrive at the system as a Poisson process with rate-mλ. The service time of a task assigned to a server with(without) local data is assumed to i.i.d. with distribution B p (·) (B q (·)) with mean 1 p ( 1 q ). Tasks are assigned in the following ways:\nRandom Server algorithm: When an arriving task sees some idle servers, an idle server is randomly selected for this task; otherwise this task joins the queue. When a server becomes idle, if its degree is zero, a task is sampled from the unassigned pool uniformly randomly; otherwise a task is selected randomly from the tasks that have data replicated on this server. If no unassigned task exists in the system, the server just stays idle.\nPeeling algorithm: When a task arrives, if no idle servers are available, it joins the queue; otherwise a server with the least non-zero degree is selected to process this task. If all idle servers are of degree 0, this tasks is assigned to a randomly selected idle server. When a server becomes idle, it follows the same rule as the Random Server algorithm.\nConsider exponential service time distribution. Fig. 5 shows the effective mean service rate. Similar to the results in Fig. 4, which is obtained from the computation using mean-ﬁeld equations in the discrete-time model, the Peeling algorithm outperforms the Random Server algorithm at low to moderate\nload, while converges to the Random Server algorithm with load increasing. Note that these two algorithms show similar performance trend in different models. Hence performance analysis under the discrete-time model provides insight for the performance of these algorithms in real scenario, and also offers guidelines for the design of efﬁcient algorithms.\nWe proposed a degree-guided task assignment algorithm that is shown to signiﬁcantly outperform the Random Server algorithm over the region of light to medium load. The future work is two-fold: (1) We plan to solve the ﬁxed point problem at high load. (2) We are interested in designing an algorithm that outperforms the Random Server algorithm at high load."},"refs":[{"authors":[{"name":"J. Dean"},{"name":"S. Ghemawat"}],"title":{"text":"MapReduce: Simpliﬁed data processing on large clusters"}},{"authors":[],"title":{"text":"Apache \t Hadoop"}},{"authors":[{"name":"G. Ananthanarayanan"},{"name":"S. Agarwal"},{"name":"S. Kandula"},{"name":"A. Greenberg"},{"name":"I. Stoica"},{"name":"D. Harlan"},{"name":"E. Harris"}],"title":{"text":"Scarlett: Coping with skewed popularity content in MapReduce clusters"}},{"authors":[{"name":"J. Dixon"}],"title":{"text":"Pentaho, Hadoop, and data lakes"}},{"authors":[{"name":"M. Satyanarayanan"}],"title":{"text":"A survey of distributed ﬁle systems"}},{"authors":[{"name":"M. Zaharia"},{"name":"D. Borthakur"},{"name":"J. Sen Sarma"},{"name":"K. Elmeleegy"},{"name":"S. Shenker"},{"name":"I. Stoica"}],"title":{"text":"Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling"}},{"authors":[{"name":"C. Abad"},{"name":"Y. Lu"},{"name":"R. Campbell"}],"title":{"text":"Dare: Adaptive data replication for efﬁcient cluster scheduling"}},{"authors":[{"name":"T. Richardso"},{"name":"R. Urbank"}],"title":{"text":"Modern Coding Theory"}},{"authors":[{"name":"Q. Xie"},{"name":"Y. Lu"}],"title":{"text":"Analysis of map-reduce task assignment with data locality constraint"}},{"authors":[{"name":"M. L. C. Bordenave"},{"name":"J. Salez"}],"title":{"text":"Matchings on innite graphs."}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566661.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S6.T4.4","endtime":"12:50","authors":"Qiaomin Xie, Yi Lu","date":"1341318600000","papertitle":"Degree-guided Map-Reduce Task Assignment with Data Locality Constraint","starttime":"12:30","session":"S6.T4: Distributed Applications","room":"Stratton 20 Chimneys (306)","paperid":"1569566661"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
