{"id":"1569561623","paper":{"title":{"text":"An Innovative Approach for Analysing Rank Deﬁcient Covariance Matrices"},"authors":[{"name":"Gabriel H. Tucci"},{"name":"Ke Wang"}],"abstr":{"text":"Abstract\u2014The estimation of a covariance matrix from an insufﬁ- cient amount of data is one of the most common problems in ﬁelds as diverse as multivariate statistics, wireless communications, signal processing, biology, learning theory and ﬁnance. In [13], a new approach to handle rank deﬁcient covariance matrices was suggested. The main idea was to use dimensionality reduction in conjunction with an average over the Stiefel manifold. In this paper we further continue in this direction and consider a few innovative methods that show considerable improvements with respect to more traditional ones such as diagonal loading. One of the methods is called the Ewens estimator and uses a randomization of the sample covariance matrix over all the permutation matrices with respect to the Ewens measure. The techniques used to attack this problem are broad and run from random matrix theory to combinatorics."},"body":{"text":"The estimation of a covariance matrix from an insufﬁcient amount of data is one of the most common problems in ﬁelds as diverse as multivariate statistics, wireless communications, signal processing, biology, learning theory and ﬁnance. For instance, the covariation between asset returns plays a cru- cial role in modern ﬁnance. The covariance matrix and its inverse are the key statistics in portfolio optimization and risk management. Many recent ﬁnancial innovations involve complex derivatives, like exotic options written on the mini- mum, maximum or difference of two assets, or some structured ﬁnancial products, such as CDOs. All of these innovations are built upon, or in order to exploit, the correlation structure of two or more assets. In the ﬁeld of wireless communications, covariance estimates allow us to compute the direction of arrival (DOA), which is a critical task in smart antenna systems since it enables accurate mobile location. Another application is in the ﬁeld of biology and involves the interactions between proteins or genes in an organism and the joint time evolution of their interactions.\nTypically the covariance matrix of a multivariate random variable is not known but has to be estimated from the data. Estimation of covariance matrices then deals with the question of how to approximate the actual covariance matrix on the basis of samples from the multivariate distribution. Simple cases, where the number of observations is much greater than the number of variables, can be dealt by using the sample covariance matrix. In this case, the sample covariance matrix\nis an unbiased and efﬁcient estimator of the true covariance matrix. However, in many practical situations we would like to estimate the covariance matrix of a set of variables from an insufﬁcient amount of data. In this case the sample covariance matrix is singular (non-invertible) and therefore a fundamen- tally bad estimate. More speciﬁcally, let X be a random vector X = (X 1 , . . . , X m ) T ∈ C m×1 and assume for simplicity that X is centered. Then the true covariance matrix is given by\nConsider n independent samples or realizations x 1 , . . . , x n ∈ C m×1 and form the m × n data matrix M = (x 1 , . . . , x n ). Then the sample covariance matrix is an m × m non-negative deﬁnite matrix deﬁned as\nIf n → +∞ and m is ﬁxed, then the sample covariance matrix K converges (entry\u2013wise) to Σ almost surely. Whereas, as we mentioned before, in many empirical problems, the number of measurements is less than the dimension (n < m), and thus the sample covariance matrix is almost surely rank deﬁcient (singular). Our target in this paper is to recover the true covariance matrix Σ from K under the condition n < m. The conventional treatment of covariance singularity artiﬁcially converts the rank deﬁcient sample covariance matrix into an invertible (positive deﬁnite) covariance by the simple expedi- ent of adding a positive diagonal matrix, or more generally, by taking a linear combination of the sample covariance and the identity matrix. This procedure is variously called \u201cdiagonal loading\u201d or \u201cridge regression\u201d [5], [17]. Consider αK + βI m as an estimate of Σ, where α, β are called loading parameters. The resulting matrix is positive deﬁnite (invertible) and preserves the eigenvectors of the sample covariance. The eigenvalues of αK + βI m are uniformly rescaled and shifted versions of the eigenvalues of K. There are many methods of choosing the optimum loading parameters, see [11], [14] and [15].\nIn Marzetta, Tucci and Simon\u2019s paper [13], a new approach to handle singular covariance matrices was suggested. Let p ≤ n be a parameter, to be estimated later, and consider the set of all p × m one\u2013sided unitary matrices\nThe topology on Ω p,m is the subspace topology inherited from C p×m . With this topology Ω p,m is a compact manifold, called the Stiefel manifold, whose dimension is dim(Ω p,m ) = 2mp− p 2 . Endow Ω p,m with the Haar measure, that is, the uniform distribution on the set Ω p,m . Then deﬁne the operators\nwhere the expectation is taken with respect to the Haar measure. Surprisingly, it was found that\nwhich is the same as diagonal loading. Moreover, the prop- erties of invcov p (K) were investigated. In particular, it was shown that if K is decomposed as K = U DU ∗ , with D = diag(d 1 , . . . , d n , 0, . . . , 0), then\nIn other words, invcov p (K) preserves the eigenvectors of K, and transforms all the zero eigenvalues to a non\u2013zero constant. Formulas to compute the values of λ i and µ were provided and the asymptotic behavior of invcov p (D) was studied using techniques from free probability.\nIn this paper, we investigate new methods to estimate rank deﬁcient covariance matrices. In Section II, we consider a new approach, called the Ewens estimator, to estimate Σ. In this one, the average is taken over the set of all m × m permutation matrices with respect to the Ewens measure. The explicit formula for the Ewens estimator is computed by a combinatorial argument. In Section III, we combine the ideas of the ﬁrst two methods, extend the deﬁnition of permutation matrices to get p × m unitary matrices, and deﬁne two new operators K θ,m,p and ˜ K θ,m,p to estimate Σ and Σ −1 respectively. We provide an explicit formula for K θ,m,p and an inductive formula to compute ˜ K θ,m,p . In Section IV, it is assumed that Σ has some special form, i.e. tridiagonal Toeplitz matrix or power Toeplitz matrix and we study its asymptotic behavior under the Ewens estimator. In this Section we also present some simulations under the different methods to test the effect of the parameters.\nLet S m be the set of permutations of the set [m] := {1, . . . , m}. The Ewens measure is a probability measure on the set of permutations that depends on a parameter θ > 0. In this measure, each permutation has a weight proportional to its total number of cycles. More speciﬁcally, for each permutation σ in S m its probability is equal to\nwhere θ > 0 and K(σ) is the number of cycles in σ. The case θ = 1 corresponds to the uniform measure. This measure has recently appeared in mathematical physics models (see e.g. [2] and [6]) and has recently started to gain insight into the cycle structures of random permutations.\nLet σ be a permutation in S m , the corresponding permutation matrix M σ is an m×m matrix deﬁned as M σ (i, j) = 1 σ(i) (j). If we denote e i to be the 1 × m vector with the i\u2013th entry equal to 1 and all others entries equal to 0, then\n \n \nwhich is, of course, a unitary matrix. Given the sample covariance matrix K we deﬁne the new estimator for Σ as\nwhere the expectation is taken with respect to the Ewens measure of parameter θ.\nTheorem 1. Let K = (a ij ) be an m × m complex matrix. Then K θ = E(M σ KM ∗ σ ) is an m × m matrix such that the diagonal terms satisfy\nRemark 1. If θ = 1 then K 1 = α ee T m + β(I m − ee T m ) where α = eKe T m = i,j a ij m , β = Tr(K)−α m−1 and e = (1, 1, . . . , 1). This has already been computed in [20], Prop. 2.2. If K = D = diag(d 1 , . . . , d m ), then\nIn this Section we combine the ideas of the ﬁrst two methods to create a third hybrid method. First, we extend the deﬁnition of a permutation. For an integer p ≤ m, let\nThe size of the set S p,m is m! (m−p)! and in the case p = m, S m,m is the set of all permutations on [m]. For σ ∈ S p,m , the associated p × m matrix takes the form\n   \n   \nwhere e σ(i) = (e 1 σ(i) , e 2 σ(i) , . . . , e m σ(i) ) is a 1 × m row vector with the σ(i)\u2013th entry 1 and all others equal to 0. Notice that V σ V T σ = I p and\nNext, we use the Ewens measure on S m to deﬁne a probability on the set S p,m . For each σ ∈ S p,m , consider the set\nIn other words, Ω σ is the set of all permutations in S m whose restriction to the set {1, 2, . . . , p} is equal to σ. Recall that p θ,m is the Ewens measure on S m with parameter θ. Now we deﬁne the probability of σ ∈ S p,m as\nwhere (V σ KV T σ ) + is the Moore−Penrose pseudoinverse of the matrix V σ KV T σ . We use K θ,m,p as an estimate for Σ and\n˜ K θ,m,p for Σ −1 . Now we show a few results on these new estimators.\nTheorem 2. Let A = (a ij ) be an m × m complex matrix. Then K θ,m,p is an m × m matrix with diagonal entries equal to\n \nand non\u2013diagonal entries, assuming i < j (if j < i exchange i and j in the following expression), equal to\n      \n     \nRemark 2. If A is a diagonal matrix A = diag(d 1 , . . . , d m ), then\n(θ + 1)a 11 \t θa 12 \t a 13 θa 21 \t (θ + 1)a 22 a 23\nNow we consider the estimate ˜ K θ,m,p as in Equation (11). First we analyze the case when K is diagonal.\nObtaining a close form expression for Equation (11) in the general case seems to be much more challenging. However, we are able to give an inductive formula with the help of a result of Kurmayya and Sivakumar\u2019s [10]. Unfortunately, we omit these results for space limitations.\nIn this Section we study the performance of our estimators and compare them with traditional methods. We focus on the case where the true covariance matrix has a Toeplitz structure. More speciﬁcally, we focus on the following two types of Toeplitz matrices.\nConsider an m × m symmetric tridiagonal Toeplitz matrix of the form\n     \nb 1 b b 1\n     \n     \n1 \t α \t α . . . α m−1 α \t 1 \t α · · · α m−2 .. . \t . . . . . . . . . .. .\nα m−2 α m−3 · · · 1 \t α α m−1 α m−2 · · · α \t 1\n     \nC. Preliminaries on the asymptotic behavior of large Toeplitz matrices\nWe ﬁrst collect some basic deﬁnitions and theorems regarding large Toeplitz matrices from Albrecht B¨ottcher and Bernd Silbermann\u2019s book [4]. For an inﬁnite Toeplitz matrix of the form\n   \na 0 a −1 a −2 . . . a 1 a 0 a −1 . . . a 2 a 1 a 0 . . .\n   \nfor 0 ∈ [0, 2π]. Let A m be the m × m principal minor of the matrix A. Given a Borel subset E ⊂ C we deﬁne the measures\nwhere χ E is the characteristic function of the set E and {λ (m) j } m j=1 are the eigenvalues of A m . The following classical result holds.\nTheorem 4 (Corollary 5.12 in [4]). If a ∈ L ∞ is real-valued, then the measures µ m given by (12) converge weakly to the measure µ deﬁned by (13).\nD. Asymptotic Behavior of Toeplitz Matrices under Ewens Estimator\nwhere ϕ ∈ [0, 2π]. By Theorem 1.2 in [4], the spectrum of B as m tends to inﬁnity is supported on the interval [1 − 2b, 1 + 2b]. On the other hand, by Theorem 1, we have that\n         \n0 1 3 3 · · · 3 2 1 0 2 4 · · · 4 3 3 2 0 2 · · · 4 3 .. . \t . . . . . . . . . \t .. .\n3 4 4 · · · 0 2 3 3 4 4 · · · 2 0 1 2 3 3 · · · 3 1 0\n         \n     \nb 0 b b 0\n     \nIf θ is a ﬁxed constant greater than 1 then as m → ∞, b(θ − 1)\nTherefore, B θ and (1 − 2 m )I m + 2 m ee T are asymptotically equivalent matrices (see Chapter 2, [9]) and by Theorem 2.6 in [9],\nwhere δ t is the Dirac measure at the point t. A more interesting situation happens when θ = βm for a ﬁxed constant β. In this case,\nTherefore, the limit eigenvalue distribution is supported on the interval 1−2b β 2 (β+1) 2 , 1+2b β 2 (β+1) 2 . The Figure below shows the estimated density function for the spectrum as θ changes.\nSimilar results where obtained for the power Toeplitz matrix A α .\nIn this Section we present some simulations to test the perfor- mance of our estimators. Let A α be an m × m Toeplitz matrix with entries a i,j = α |i−j| . Given n observations we want to recover the matrix A α . We construct the sample covariance matrix K and proceed to recover A α in terms of the operators\ninvcov p (K) and E(M σ KM ∗ σ ). Let A α , m, p, n, K and θ as before and deﬁne the following performance functions\nf (m, n, α, p) = A α − (p/m)invcov p (K) −1 2 , g(m, n, α, p) = A −1 α − (m/p)invcov p (K) 2 , F (m, n, α, θ) = A α − E(M σ KM ∗ σ ) 2 .\nIn Figures 2 and 3, we show the performance of the estimators for different values of p and θ. It was observed in [13] that the estimator invcov p outperforms the more standard and classical estimator of diagonal loading for optimal loading parameters as in Ledoit and Wolf [11] by computing f (m, n, α, p), for the different values of p, and comparing them with ||A α −K LW || 2 . The same type of experiments were performed on a variety of different scenarios as well. We can observe how the Ewens estimator outperforms the invcov p estimator for the optimum values of p and θ. The next Figures show the behavior of the previous functions for the different parameter values."},"refs":[{"authors":[{"name":"Z. D. Bai"}],"title":{"text":"Methodologies in spectral analysis of large-dimensional random matrices, a review"}},{"authors":[{"name":"V. Bet"},{"name":"Y. Velenik"}],"title":{"text":"D, Ueltschi and  Random permutations with cycle weights Ann"}},{"authors":[{"name":"A. B¨ottche"},{"name":"S. M. Grudsky"}],"title":{"text":"Spectral properties of banded Toeplitz matrices"}},{"authors":[{"name":"A. B¨ottche"},{"name":"B. Silbermann"}],"title":{"text":"Introduction to large truncated Toeplitz matrices"}},{"authors":[{"name":"N. R. Drape"},{"name":"H. Smith"}],"title":{"text":"Applied Regression Analysis (Wiley Series in Probability and Statistics"}},{"authors":[{"name":"N. Ercolan"},{"name":"D. Ueltschi"}],"title":{"text":"Cycle structure of random permutations with cycle weights, 2011"}},{"authors":[{"name":"H. Fulton"}],"title":{"text":"Representation Theory, Springer, 1991"}},{"authors":[{"name":"J. Galambos"}],"title":{"text":"Advanced Probability Theory"}},{"authors":[{"name":"R. M. Gray"}],"title":{"text":"Toeplitz and circulant matrices: A review"}},{"authors":[{"name":"T. Kurmayy"},{"name":"C. Sivakumar"}],"title":{"text":"K"}},{"authors":[{"name":"O. Ledoi"},{"name":"M. Wolf"}],"title":{"text":"Some hypothesis tests for the covariance matrix when the dimension is large compared to the sample size"}},{"authors":[{"name":"I. Macdonald"}],"title":{"text":"Symmetric functions and Hall Polynomials"}},{"authors":[{"name":"T. Marzett"},{"name":"G. Tucc"},{"name":"S. Simon"}],"title":{"text":"A random matrix\u2013theoretic approach to handling singular covariance estimates"}},{"authors":[{"name":"X. Mestre"}],"title":{"text":"Improved estimation of eigenvalues and eigenvectors of covariance matrices using their sample estimates"}},{"authors":[{"name":"X. Mestr"},{"name":"M. A. Lagunas"}],"title":{"text":"Diagonal loading for ﬁnite sample size beamforming: an asymptotic approach"}},{"authors":[{"name":"R. Muirhead"}],"title":{"text":"Aspects of Multivariate Statistical Theory"}},{"authors":[{"name":"C. D. Richmon"},{"name":"R. Rao Nadakudit"},{"name":"A. Edelman"}],"title":{"text":"Asymptotic mean squared error performance of diagonally loaded capon-mvdr processor"}},{"authors":[{"name":"B. Sagan"}],"title":{"text":"The Symmetric Group: Representations"}},{"authors":[{"name":"R. P. Stanley et al"}],"title":{"text":"Enumerative Combinatorics: Volume 2"}},{"authors":[{"name":"M. A. G. Viana"}],"title":{"text":"The covariance structure of random permutation matrices"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569561623.pdf"},"links":[{"id":"1569566091","weight":3},{"id":"1569551535","weight":3},{"id":"1569554971","weight":3},{"id":"1569566237","weight":3},{"id":"1569559597","weight":3},{"id":"1569565337","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S14.T8.3","endtime":"17:40","authors":"Gabriel H. Tucci, Ke Wang","date":"1341508800000","papertitle":"An Innovative Approach for Analysing Rank Deficient Covariance Matrices","starttime":"17:20","session":"S14.T8: High-Dimensional Inference","room":"Stratton (491)","paperid":"1569561623"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
