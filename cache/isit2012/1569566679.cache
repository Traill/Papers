{"id":"1569566679","paper":{"title":{"text":"Repairable Fountain Codes"},"authors":[{"name":"Megasthenis Asteris"},{"name":"Alexandros G. Dimakis"}],"abstr":{"text":"Abstract\u2014We introduce a new family of Fountain codes that are systematic and also have sparse parities. Although this is impossible if we require the code to be MDS, we show it can be achieved if we relax our requirement into a near-MDS property. More concretely, for any we construct codes that guarantee that a random subset of (1 + )k symbols sufﬁces to recover the original symbols with high probability. Our codes produce an unbounded number of output symbols, creating each parity independently by linearly combining a logarithmic number of input symbols.\nThis structure has the additional beneﬁt of logarithmic locality: a single symbol loss can be repaired by accessing only O(log k) other coded symbols. This is a desired property for distributed storage systems where symbols are spread over a network of storage nodes. Our mathematical contribution involves analyzing the rank of sparse random matrices over ﬁnite ﬁelds. We rely on establishing that a new family of sparse random bipartite graphs have large matchings with high probability."},"body":{"text":"Fountain codes [1], [2], [3] form a new family of linear erasure codes with several attractive properties. For a given set of k input symbols, a Fountain code produces a potentially limitless stream of output symbols, each created independently of others as a random combination of input symbols according to a given distribution. Ideally, given a randomly selected subset of (1 + )k encoded symbols, a decoder should be able to efﬁciently recover the original k input symbols with high probability (w.h.p.) for some small overhead . Independent and random construction of encoded symbols allows decentral- ized encoding and dynamic adjusting in the number of coded symbols. Further, Fountain codes can be used with efﬁcient encoding and decoding algorithms.\nCurrent cloud storage systems are starting to use erasure coding techniques, typically Reed-Solomon codes [4]. In this paper we investigate and design Fountain codes for such distributed storage applications.\nOne important property of distributed storage codes is efﬁcient repair [5]: when a single encoded symbol is lost it should be possible to reconstruct it without communicating too much information from other coded symbols. A related property is that of locality of each symbol: the number of other code symbols that need to be accessed to reconstruct that symbol [4], [6], [7], [8].\nIn addition, it is highly desired that distributed storage codes are systematic, i.e., the original information symbols appear in\nthe encoded sequence. This enables the reading of symbols without decoding and is a practical requirement for most storage applications. Raptor codes, a class of Fountain codes, can be transformed into a systematic form [3] by introducing a change of variables. Unfortunately, due to this two layer encoding, the parity symbols are no longer sparse in the input symbols.\nOur Contribution: We introduce a new family of Fountain codes that are systematic and also have parity symbols with logarithmic sparsity. We show that this is impossible if we require the code to be MDS, but is possible if we allow a near- MDS property similar to the probabilistic guarantees provided by LT and Raptor codes.\nMore concretely, for any > 0 we construct codes that guarantee that a random subset of (1 + )k symbols sufﬁces to recover the original symbols w.h.p. Our codes produce an unbounded number of output symbols, creating each parity independently by linearly combining a logarithmic number of input symbols.\nThis structure provides also logarithmic locality: each sym- bol in our codes is repairable by accessing only O(log k) other coded symbols. The disadvantage of our codes is less efﬁcient decoding complexity, since the peeling decoder cannot be used for our construction.\nTechnically, we rely on a novel random matrix result: we show that systematic matrices with independent parity columns and logarithmic density have full rank submatrices of near optimal size. Our analysis builds on the connections of matrix determinants to ﬂows on random bipartite graphs, using techniques from [9], [10]. Our key technical result is showing that a new family of sparse random graphs have matchings w.h.p.\nGiven k input symbols, elements of a ﬁnite ﬁeld F q , we want to encode them into n symbols using a linear code. Linear codes are described by a k × n generator matrix G over F q , which when multiplied by an input vector u ∈ F 1× k q\nproduces a codeword v · = uG ∈ F 1× n q . We want G to have the following properties:\n\u2022 Systematic form , i.e., a subset of the columns of G forms the identity matrix, I, which implies that the input symbols are reproduced in the encoded sequence.\n\u2022 Rateless property , i.e., each column is created indepen- dently. The number n of columns does not have to be\n\u2022 MDS property , i.e., any k columns of G have rank k, implying that any subset of k encoded symbols sufﬁces to retrieve the input.\n\u2022 Good locality . G has locality l if each column can be written as a linear combination of at most l other columns. If the code is systematic, then sparse parities sufﬁce to obtain good locality [8].\nAny sufﬁciently large subset of encoded symbols should allow recovery of the original data. In the case of MDS codes, an information theoretically minimum subset of k encoded symbols sufﬁces to decode. It can be easily shown, however, that the generator matrix of a systematic MDS code affords no zero coefﬁcient in the parity columns. Consider a parity column with a zero coefﬁcient in the i-th position: this parity column along with any k −1 systematic columns excluding the one corresponding to the i-th input symbol, form a singular matrix. Therefore, if parities are deliberately sparse in the input symbols, seeking to improve the code\u2019s locality, the \u201cany k\u201d property has to be relaxed.\nIn LT codes, the ﬁrst practical realizations of Fountain codes invented by Luby [2], the average degree of the output symbols, i.e., the number of input symbols combined into an output symbol, is O (log k). Note, however, that sparsity in this case does not imply good locality, since LT codes lack systematic form.\nShokrollahi in [3] introduced Raptor codes, a different class of Fountain codes. Building on LT, Raptor codes decreased the per symbol encoding and decoding cost (which corresponds to average degree of encoded symbols) to a constant. A systematic ﬂavour of Raptor codes is provided in [3], but the parity symbols are no longer sparse in the input symbols.\nGummadi in his thesis [11] also considers the use of Fountain codes for storage applications and suggests sparse systematic variants of LT and Raptor codes. His constructions have the disadvantage of requiring an overhead that cannot be made arbitrarily small but rather is bounded by 1 + δ, (δ > 0) and 0.25 respectively. On the other hand, the advantage of this construction is that efﬁcient decoding is still possible.\nWe introduce a new family of Fountain codes that are systematic and also have sparse parities. Each parity symbol is a random linear combination of up to d(k) randomly chosen input symbols. Due to their randomized nature, our codes provide a probabilistic guarantee on successful decoding of k = (1 + )k randomly selected encoded symbols, for arbitrarily small > 0 (near-MDS). Requiring decoding of a random set of k encoded symbols to be successful w.h.p. (that is with vanishingly small probability of failure as k grows), we show that a sparsity level of O (log k) is achievable. Our main result, which is asymptotic in k, is established in Theorem 1, at the end of this section.\nIt is useful to describe our randomized construction through a correspondence to a bipartite graph G(U, V, E), depicted in Fig. 1. The set of nodes U on the left side corresponds to the input symbols and the set V on the right corresponds to the encoded symbols. An edge (i, j) ∈ E if the input symbol i ∈ U is one of the symbols combined into the encoded symbol j ∈ V . Evidently, each of the k ﬁrst encoded symbols is connected to a single, distinct input symbol (systematic part). The remaining nodes in V correspond to parity symbols. Each parity node j ∈ V randomly and independently selects (with repetition) a subset N (j) of the input symbols as follows: an input symbol is selected uniformly and independently from U and added in N (j) and this procedure is repeated d(k) times. Therefore, |N(j)| will be smaller than d(k) if the same data node is selected twice. In fact, the size of the set N (j) is exactly the number of coupons a coupon collector would have after purchasing d(k) coupons from a set of k coupons. It is not hard to see that when d(k) k, |N(j)| will be approximately equal to d(k) w.h.p. Each parity symbol is a random linear combination of the input symbols it is connected to:\nwhere the coefﬁcients f ij are selected uniformly and indepen- dently over F q .\nwhere v is a 1 × n vector of encoded symbols, u is a 1 × k input symbol vector and G is a k × n matrix of the form G = [ I | P ], i.e., the k ﬁrst columns correspond to the identity matrix (systematic part). For j ∈ {k + 1, . . . , n}, the j-th column of G (i.e., each column of the parity matrix, P) has |N(j)| ≤ d(k) nonzero entries. To retrieve the k input symbols based on k encoded symbols, we need k out of the latter to be linearly independent combinations of the former. Therefore, the key property required for successful decoding is that a randomly selected matrix G S , consisting of k columns of G (including any combination of systematic and parity parts) has full rank w.h.p.\nSo far, we have seen that our construction is systematic and has the rateless property: parity symbols can be created ran- domly and independently and hence any number of symbols can be produced dynamically. Its locality is directly related to d(k), which is clearly a measure of the sparsity of G: the repair of any erased parity symbol involves as few as d(k) systematic symbols, while similarly, the repair of a systematic symbol involves a parity symbol covering the erased symbol and up to d(k) − 1 other systematic symbols contributing to the same parity.\nOur main contribution is identifying how small d(k) can be to ensure that a randomly selected submatrix G S is full rank w.h.p.\nTheorem 1. Let G = [ I | P ] be a random matrix with independent columns constructed as described. Then, d(k) = c log k is sufﬁcient for a random k ×k submatrix G S of G to be full rank, i.e. contain an invertible k × k submatrix G S k , with probability 1 − k q − o(1), for some positive constant c.\nTheorem 2. (Converse) If each parity column of G is gener- ated independently, then d(k) = Ω(log k) is necessary for a random k × k submatrix G S of G to be full rank w.h.p.\nFrom the two theorems, it follows that our codes achieve optimal locality with a logarithmic degree for every parity symbol. Original data is reconstructed in O(k 3 ) using Maxi- mum Likelihood (ML) decoding, which corresponds to solving a linear system of k equations in GF(q). Note, however, that the Wiedemann algorithm [12] can reduce complexity to O k 2 log k on average, exploiting the sparsity of the linear equations, with negligible extra memory requirement. Finally, note that in order to achieve vanishingly small probability of failure as k grows, the size of the ﬁeld must grow accordingly.\nProof of Theorem 1. The main body of this section is dedicated to the proof of Theorem 1, stating that when G is constructed as described in section IV, a randomly selected k × k submatrix G S is full rank w.h.p. More formally,\nIn the following, we exploit a connection between deter- minants and perfect matchings (P.M.\u2019s) in bipartite graphs. In section IV, we showed the correspondence of the ran- domly constructed matrix G to an unbalanced bipartite graph G = (U, V, E). The submatrix G S corresponds to a subgraph G S = (U S = U, V S , E S ), depicted in Fig. 2, containing all k nodes of U on the left side, k out of the n nodes of V\non the right side, and the subset E S of the edges incident only to nodes in these sets. Similarly, any k × k submatrix G S k of G S corresponds to a smaller, balanced bipartite graph, G S k = (U, V S k , E S k ), with k nodes on each side. G S k can be regarded as the Edmond\u2019s matrix of the corresponding bipartite\nLemma 1. The determinant of G S k is nonzero if and only if there exists a perfect matching in G S k , i.e.\ndet (G S k ) = 0 ⇔ ∃ perfect matching M in G S k . (5) Proof: We use the following expression for the determi-\nwhere S n is the set of all permutations on {1, . . . , n} and sgn (π) is the sign of permutation π. There is a one to one correspondence between a permutation π ∈ S n and a candidate P.M. (u 1 , v π(1) ), . . . , (u n , v π(n) ) in G S k . Note that if the candidate P.M. does not exist in G S k , i.e. some edge (u i , v π(i) ) / ∈ E S k then the term corresponding to π in the summation is 0. Therefore, we have:\nwhere P is the set of perfect matchings in G S k . This is clearly zero if P = ∅, i.e., if G S k has no P.M. If G S k has a P.M., there exists a π ∈ P and the term corresponding to π is\ni=1 a i,π(i) = 0. Additionally, there is no other term in the summation containing the exact same set of variables and this term cannot be cancelled out. In this case det (G S k ) = 0, which concludes the proof of the lemma.\nHowever, G S k is not an actual Edmond\u2019s matrix; its entries are (randomly selected) elements of a ﬁnite ﬁeld F q , not indeterminates. There are two substantially different cases in which det (G S k ) = 0:\n\u2022 The determinant polynomial is identically zero which occurs if and only if G S has no P.M., or\n\u2022 it is not identically zero (i.e., G S k has a P.M.), but the selected coefﬁcients correspond to a root of the polynomial.\nIn other words, in contrast to the use of indeterminates, an unfortunate selection of the random coefﬁcients of G S k can lead to zero determinant even when G S k has a P.M.\nTaking into account that a P.M. in some subgraph G S k of G S is a P.M. M in G S and vice versa, the probability we are interested in can be written as\n \nNonexistence of a P.M. in G S , implies that we cannot ﬁnd k nodes in V S that can be perfectly matched with the k nodes of U . In that case, no submatrix G S k can have nonzero determinant, i.e. β = 1. On the other hand, existence of a P.M. M in G S , implies the existence of a submatrix G S k that will most probably have a nonzero determinant, depending on the randomly selected coefﬁcients of G S k . The determinant of G S k corresponding to M , is a polynomial of degree exactly k and the probability that it equals zero can be bounded using the Schwartz-Zippel Theorem [13], by k q , where q is the number of elements in the ﬁnite ﬁeld from which the coefﬁcients of G S k are drawn. A step further, the probability Pr ( G S k : det(G S k ) = 0 | ∃M), i.e., that no invertible matrix G S k exists despite the existence of a P.M., can be upper bounded by the same quantity. Hence α ≤ k q . Continuing from (8), we have:\nwhere the fact that Pr ( M ) = o(1) is based on Lemma 2, provided in section V-A. This completes the proof of theorem 1.\nProof of Theorem 2 An input symbol is covered by an encoded symbol, if it participates with a nonzero coefﬁcient in the formation of the latter. In order to be able to retrieve the original k symbols from a subset of k encoded symbols, it is imperative that the subset covers all input symbols. It is a standard result in balls and bins analysis that covering k bins w.h.p. requires throwing Ω (k log k) balls. Here, each of the k encoded symbol \u201cthrows d(k) balls\u201d. Therefore, we need k · d(k) = (1 + )k · d(k) = Ω (k log k) from which the theorem becomes obvious.\nLemma 2. The bipartite graph G S = (U, V S , E S ) correspond- ing to the submatrix G S of G has a perfect matching with probability 1 − o(1) as k → ∞.\nProof: We establish an upper bound on the probability that a perfect matching (P.M.) between U and V S does not\nexist. (Recall that, since |U| < |V S |, a P.M. is a matching that saturates all nodes in U ). In particular, we require the probability to vanish asymptotically with a rate 1/poly(k) and show that the value d(k) = O(log k) used in constructing the bipartite graph sufﬁces to achieve that.\nLet V s denote the subset of V S corresponding to systematic encoded symbols ( 0 ≤ |V s | ≤ k). We can assume that all nodes in V s are used in the P.M.: each saturates a distinct node in U , leaving us with more options for saturating the remaining nodes in U . Since k = (1 + )k > k, V S will deﬁnitely have a nonempty subset corresponding to parity (nonsystematic) symbols, denoted by V ns = V S \\V s . Let U s denote the subset of U that is saturated by V s and U ns = U \\U s . Since U s and V s are matched, a P.M. between U and V S exists if and only if a P.M. exists between U ns and V ns .\nThe probability that a P.M. does not exist equals the probability that there exists a contracting set of nodes in U ns . For simplicity, let s = |V s | and consequently, |V ns | = k − s. Let E i be the event that there exists a set of i nodes in U ns that contracts, i.e., has at most i −1 neighbours in V ns . This is equivalent to at least k − s − (i − 1) nodes in V ns being only adjacent to nodes in U ns other than the i nodes of interest. Then,\nUsing the fact that n k ≤ 2[ nH ( k n ) +log 2 ( n) ], where H(·) is the binary entropy function, we have: k−s k−s−i ≤ 2 B (1) k,s ( i) and\nThe entire sum can be bounded by k − s times the largest among these bounds. In other words,\nwhere B (3) k,s (i) = d(k) k − s − i + 1 log 2 k−i k . We re- quire Pr ( P.M. in G S ) to vanish asymptotically faster than\nLet N and D denote the numerator and denumerator of the right hand side of inequality (13). In the following, we show that N D = O (ln(k)) and hence choosing d(k) = c ln(k) for some constant c sufﬁces to satisfy (13) and achieve the vanishing probability.\n\u2022 For the numerator, N , we have the following upper bound:\nwhere the inequality is due to the monotonicity of the logarithm and the fact that f (x) = xH x−y x is increas- ing with respect to x when 0 ≤ y ≤ x.\nwhere (γ) is due to ln(1 + x) > x x+1 for x > −1, x = 0 (here, x = i k−i > 0).\nWe examine the ratio N D in parts: (i) For the ﬁrst part:\n(iii) For the third and ﬁnal part, since for i = 1 the entropy in this term is zero, we need only consider i ≥ 2:\nwhere the last inequality holds when log 2 (k) ≥ 1 + ln(2) log 2 (1 + ).\nCombining (15), (16) and (17), it is evident that using d(k) = c log(k), where c ∝ 1/ , sufﬁces to make Pr( P.M. in G S ) = o(1) which completes the proof."},"refs":[{"authors":[{"name":"J. W. Byers"},{"name":"M. Luby"},{"name":"M. Mitzenmacher"},{"name":"A. Rege"}],"title":{"text":"A digital fountain approach to reliable distribution of bulk data"}},{"authors":[{"name":"M. Luby"}],"title":{"text":"Lt codes"}},{"authors":[{"name":"A. Shokrollahi"}],"title":{"text":"Raptor codes"}},{"authors":[{"name":"O. Khan"},{"name":"R. Burns"},{"name":"J. Plank"},{"name":"W. Pierce"},{"name":"C. Huang"}],"title":{"text":"Rethinking erasure codes for cloud ﬁle systems: Minimizing i/o for recovery and degraded reads"}},{"authors":[{"name":"A. Dimakis"},{"name":"P. Godfrey"},{"name":"Y. Wu"},{"name":"M. Wainwright"},{"name":"K. Ramchandran"}],"title":{"text":"Network coding for distributed storage systems"}},{"authors":[{"name":"F. Oggier"},{"name":"A. Datta"}],"title":{"text":"Self-repairing homomorphic codes for dis- tributed storage systems"}},{"authors":[{"name":"D. Papailiopoulos"},{"name":"J. Luo"},{"name":"A. Dimakis"},{"name":"C. Huang"},{"name":"J. Li"}],"title":{"text":"Simple regenerating codes: Network coding for cloud storage"}},{"authors":[{"name":"P. Gopalan"},{"name":"C. Huang"},{"name":"H. Simitci"},{"name":"S. Yekhanin"}],"title":{"text":"On the locality of codeword symbols"}},{"authors":[{"name":"P. Erd˝os"},{"name":"A. R´enyi"}],"title":{"text":"On random matrices"}},{"authors":[{"name":"A. G. Dimakis"},{"name":"V. Prabhakaran"},{"name":"K. Ramchandran"}],"title":{"text":"Decentralized erasure codes for distributed networked storage"}},{"authors":[{"name":"R. Gummadi"}],"title":{"text":"Coding and scheduling in networks for erasures and broadcast"}},{"authors":[{"name":"D. Wiedemann"}],"title":{"text":"Solving sparse linear equations over ﬁnite ﬁelds"}},{"authors":[{"name":"R. Motwani"},{"name":"P. Raghavan"}],"title":{"text":"Algorithms and theory of computation handbook"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566679.pdf"},"links":[{"id":"1569566567","weight":6},{"id":"1569566485","weight":3},{"id":"1569566725","weight":3},{"id":"1569566385","weight":3},{"id":"1569566875","weight":6},{"id":"1569566683","weight":3},{"id":"1569565091","weight":15},{"id":"1569566591","weight":3},{"id":"1569566571","weight":6},{"id":"1569559967","weight":3},{"id":"1569565613","weight":3},{"id":"1569565355","weight":3},{"id":"1569565837","weight":3},{"id":"1569559541","weight":3},{"id":"1569565317","weight":3},{"id":"1569566319","weight":3},{"id":"1569566941","weight":3},{"id":"1569565771","weight":3},{"id":"1569566157","weight":3},{"id":"1569566903","weight":3},{"id":"1569566999","weight":3},{"id":"1569565859","weight":3},{"id":"1569564249","weight":3},{"id":"1569565809","weight":15},{"id":"1569566843","weight":6},{"id":"1569566579","weight":3},{"id":"1569566795","weight":3},{"id":"1569566895","weight":18},{"id":"1569566617","weight":6},{"id":"1569566753","weight":6},{"id":"1569555999","weight":6},{"id":"1569565841","weight":3},{"id":"1569566423","weight":3},{"id":"1569566811","weight":3},{"id":"1569558901","weight":3},{"id":"1569565735","weight":3},{"id":"1569562285","weight":3},{"id":"1569564209","weight":3},{"id":"1569566425","weight":6},{"id":"1569566649","weight":6},{"id":"1569565559","weight":3},{"id":"1569566127","weight":3},{"id":"1569565087","weight":9},{"id":"1569564857","weight":21},{"id":"1569566913","weight":3},{"id":"1569566809","weight":3},{"id":"1569565847","weight":15},{"id":"1569565929","weight":3},{"id":"1569565055","weight":6},{"id":"1569565633","weight":6},{"id":"1569565279","weight":3},{"id":"1569565219","weight":6},{"id":"1569566003","weight":12},{"id":"1569565185","weight":6},{"id":"1569565469","weight":6},{"id":"1569565357","weight":3},{"id":"1569555787","weight":3},{"id":"1569565441","weight":3},{"id":"1569565311","weight":3},{"id":"1569566275","weight":3},{"id":"1569566857","weight":6},{"id":"1569566177","weight":3},{"id":"1569565523","weight":3},{"id":"1569566983","weight":3},{"id":"1569566779","weight":9},{"id":"1569566261","weight":3},{"id":"1569565093","weight":9},{"id":"1569566927","weight":9},{"id":"1569565661","weight":3},{"id":"1569566887","weight":42},{"id":"1569564919","weight":3},{"id":"1569566917","weight":3},{"id":"1569565353","weight":6},{"id":"1569564291","weight":3},{"id":"1569566547","weight":3},{"id":"1569566595","weight":3},{"id":"1569566639","weight":3},{"id":"1569566771","weight":3},{"id":"1569566641","weight":3},{"id":"1569564437","weight":6},{"id":"1569564861","weight":21},{"id":"1569565457","weight":3},{"id":"1569566487","weight":3},{"id":"1569565529","weight":3},{"id":"1569561185","weight":6},{"id":"1569566397","weight":3},{"id":"1569565669","weight":3},{"id":"1569560235","weight":3},{"id":"1569566817","weight":3},{"id":"1569566601","weight":6},{"id":"1569566147","weight":3},{"id":"1569565561","weight":3},{"id":"1569566847","weight":3},{"id":"1569565113","weight":3},{"id":"1569564257","weight":6},{"id":"1569566973","weight":3},{"id":"1569566987","weight":3},{"id":"1569565031","weight":3},{"id":"1569558697","weight":6},{"id":"1569566825","weight":3},{"id":"1569564807","weight":3},{"id":"1569566443","weight":6},{"id":"1569560581","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T5.1","endtime":"11:50","authors":"Megasthenis Asteris, Alex Dimakis","date":"1341401400000","papertitle":"Repairable Fountain Codes","starttime":"11:30","session":"S10.T5: Rateless Codes","room":"Kresge Little Theatre (035)","paperid":"1569566679"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
