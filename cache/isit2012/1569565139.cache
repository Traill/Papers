{"id":"1569565139","paper":{"title":{"text":"Network Coded Gossip with Correlated Data"},"authors":[{"name":"Bernhard Haeupler ∗"},{"name":"Asaf Cohen ∗"},{"name":"Chen Avin"},{"name":"Muriel M´edard"}],"abstr":{"text":"Abstract\u2014We design and analyze gossip algorithms for net- works with correlated data. In these networks, either the data to be distributed, the data already available at the nodes, or both, are correlated. Although coding schemes for correlated data have been studied extensively, the focus has been on characterizing the rate region in static memory-free networks. In a gossip- based scheme, however, nodes communicate among each other by continuously exchanging packets according to some underlying communication model. The main ﬁgure of merit in this setting is the stopping time \u2013 the time required until nodes can successfully decode. While Gossip schemes are practical, distributed and scalable, they have only been studied for uncorrelated data.\nWe wish to close this gap by providing techniques to analyze network coded gossip in (dynamic) networks with correlated data. We give a clean framework for oblivious network models that applies to a multitude of network and communication scenarios, specify a general setting for distributed correlated data, and give tight bounds on the stopping times of network coded protocols in this wide range of scenarios."},"body":{"text":"In this paper, we design and analyze information dissemi- nation algorithms in communication networks with correlated data. In these networks, either the data to be distributed, the data already available at the nodes, or both, are correlated. This problem arises in a many networking applications, such as sensor, peer-to-peer or content distribution networks. One such example is a large set of distributed temperature sensors with a clock at the receiver. Both the temperatures at different sensor locations and the time at which a measurement is taken have high correlations among each other.\nWhile the current information theory literature includes several coding schemes for correlated data, the focus in these works is mainly on characterizing the rate region - the set of achievable rates. On the other hand, recent work in the net- working literature offers a multitude of efﬁcient, decentralized and address-oblivious schemes for information dissemination (e.g., randomized gossip). Unfortunately these schemes treat the data as uncorrelated and neglect any available information at the receivers. The focus of this paper is to close this gap and give tools for analyzing gossip-based algorithms in networks with correlated data.\n1) Related Work: Distributed compression has been studied in information theory mainly through small, canonical prob- lems. In [1], Slepian and Wolf considered the problem of separately encoding two correlated sources and joint decoding.\nIn [2] and [3] the problem of compression with a rate-limited helper is considered. In [4], Ho et al. considered the multicast problem with correlated sources which can be viewed as extending the Slepian-Wolf problem to arbitrary networks through network coding. Further extensions appeared in [5] and [6]. In all these studies the goal is to characterize the rate region for (ﬁxed) static and memory-free networks, that is, the set of required capacities needed for a multicast.\nGossip schemes were ﬁrst introduced in [7] as a simple and decentralized way to disseminate a piece of information in a network. A detailed analysis of a class of these algorithms is given in [8]. In these schemes nodes communicate by continuously picking communication partners in a randomized fashion and then forwarding the information. The main ﬁgure of merit is the stopping time \u2013 the time needed for all nodes to be informed. Such randomized gossip-based protocols are attractive due to their locality, simplicity, and structure-free nature, and have been offered in the literature for various tasks. For the task of distributing multiple messages [9] introduced algebraic gossip , a network coding-based gossip protocol in which nodes exchange linear combinations of their available messages. This idea was extended to arbitrary networks in [10] and [11]. Haeupler [12] proved tight bounds for the stopping time of algebraic gossip for various models, including (adversarial) dynamically changing networks [13] and nodes with limited memory [14]. Improved bounds for non-uniform gossip were given in [15]. The projection analysis developed in [12] will play a key role in this paper.\nTo our knowledge this paper is the ﬁrst to combine these two strains of research and analyze gossip based protocols in networks with correlated data. Our contributions are manifold: First, we give a clean and general framework for oblivious net- work models in Section III and deﬁne a setting for a correlated environment in Section II. In this general setting we extend the projection analysis of [12] by making a connection between the coefﬁcient vectors a node knows and the amount and type of information it has learned. This results in simple, direct and self-contained proofs of tight bounds on the stopping time in the canonical models of one source and side information at the receivers, as well as two correlated sources. In Section V we then give results for the general scenario of multiple sources and side information. We do this by providing tight bounds on the time required for any set of (fractional) capacities to be induced by the (random) packet exchanges generated\nin an oblivious network model. This allows to transform results on the rate region of static memory-free networks (e.g., [4], [6]) into bounds on the stopping times of gossip-based algorithms. These capacity bounds are interesting in their own right and have the potential to be useful in other information dissemination problems. Due to space limitation, the complete proofs are given in [16].\nIn this section we state the broadcast problem, deﬁne the network and communication model and specify the nature of the source and side information.\n1) Network and Communication Model: For simplicity, we will assume that the network consists of a ﬁxed set V of n nodes. Communication takes place in synchronous rounds. In each round, each node v decides on a packet p v of s bits to be sent out (possibly using randomness). Given the current state of the network, the network model then speciﬁes (possibly using randomness) which packet will be received by which node in the current round. This corresponds to a probability distribution over directed edge sets where a directed edge (v, u) means that the packet p v is successfully received by node u. Nodes are assumed to have unlimited memory (for schemes with limited buffers see [14]). We denote the active edge set of directed edges chosen for round t with E t .\n2) Source and Side Information: We assume random vari- ables {X i } k i=1 ∪ {Y v } v∈V that are arbitrarily correlated ac- cording to some known memoryless joint distribution. To generate the messages and side information we take l i.i.d. samples from these variables and denote the resulting message vectors with x 1 , . . . , x k and the side information vectors with y v . We assume that the k messages are initially distributed to nodes (i.e., sources) such that each vector is given to at least one node. We also assume that each node v ∈ V (or terminal) is given the vector y v as side information. Given this initial information distribution, we are interested in the time when nodes are able to decode x 1 , . . . , x k based on their side information and the packets exchanged with other nodes.\n3) The Encoding and Decoding Schemes: For a given ﬁeld size q and slack δ > 0 we assume throughout that nodes employ the following coding scheme: Prior to communication, source nodes perform random binning, that is, for every 1 ≤ i ≤ k each node receiving the message vector x i applies the same random mapping into 2 l(H(X i )+δ) bins. The resulting bin indices (the same for every node initialized with x i ) are interpreted as vectors of length h = l log q (H(X i ) + δ) over the ﬁnite ﬁeld F q . These vectors are split into h log q s blocks of s log q symbols in F q each, for a total of s bits per block.\nDuring the communication phase nodes sends out random linear combinations (over F q ) of these blocks as packets. To keep track of the linear combination contained in a packet one coefﬁcient for each block of each message is introduced and sent in the header of each packet. As in all prior works on distributed network coding (e.g., [9], [10], [11], [12], [14], [15]), we assume that s log q is sufﬁciently large compared to\nthe number of coefﬁcients. This renders the overhead of the header negligible leaving a packet size of s bits as desired.\nAt each node, independent linear equations on the blocks are collected for decoding. We denote with S v the subspace spanned by all coefﬁcient vectors received at node v. We also use the following notion of knowledge from [12]:\nDeﬁnition 1. Node v knows a coefﬁcient vector µ iff S v is not orthogonal to µ, i.e, there exists c ∈ S v such that c, µ = 0.\nLemma 1 ([6], [3]). Let X ∈ X and Y ∈ Y be two arbitrarily correlated random variables and let x, y be two vectors that are created by taking l i.i.d. samples from their joint distribu- tion. Suppose, for some > 0 and for some constant δ > 0, all possible sequences in X l are randomly and uniformly divided to at least 2 l(H(X|Y )+δ) bins. Then joint typicality decoding correctly decodes x with probability at least 1 −\nfor l = Ω(log −1 ) using y and any (H(X|Y ) + δ)l bits of information on the bin index of the bin in which the true x resides.\nIn particular, Lemma 1 asserts that having access to the side information y, the message vector x can be decoded with high probability using any l s (H(X|Y ) + δ) linearly independent equations on the blocks describing the bin index of x.\nIn this section we introduce the deﬁnition of an oblivious network model. This gives a clean and very general framework capturing a wide variety of communication and (dynamic) network settings. While this was already somewhat implicit in [12] we restrict ourselves to networks without adaptive adversarial behavior. This greatly facilitates the much cleaner framework presented in this section.\nDeﬁnition 2. A network model is oblivious if the active edge set E t of time t only depends on t, E t for any t < t and some randomness. An oblivious network model is i.i.d. if the active edge set E t is sampled independently for every t from the same distribution.\nThe following are examples of oblivious (and i.i.d.) network models: In the (Uniform) Gossip Model [10], [11], [12] one has an underlying (directed) graph G and in each round each node picks a random neighbor as a communication partner. A node then sends (PUSH) or requests (PULL) a packet from its partner or both (EXCHANGE). The Rumor Mongering [9] or Random Phone Calls Model [8] is a well-studied special case of this in which G is complete, that is, nodes pick a random node as a communication partner. It is easy to include more sophisticated features in an oblivious network model. Random packet losses in wired networks, or characteristics of radio networks like half-duplex transmission, collisions, packet loss rates depending on SNR and more can be easily modeled by removing edges according to (randomized) rules. An interesting example of non-i.i.d. oblivious network models are (edge-)markovian evolving graphs [17].\nFor any oblivious network model M we can deﬁne a random ﬂooding process F (M, p, S). Informally, this process describes which nodes are informed over time if initially only nodes in S are informed and from there on every informed node informs all its communication partners (as speciﬁed by M ). The only modiﬁcation to this standard infectious process is the parameter p which adds an independent probability of p for each transmission to be overheard.\nDeﬁnition 3. Let M be an oblivious network model, p be a probability of fault and S ⊆ V be a starting set of nodes. We deﬁne the ﬂooding process F (M, p, S) to be the random process S 1 ⊆ S 2 ⊆ . . . that is characterized by S 1 = S and for every time t we deﬁne S t+1 by taking each of the (directed) edges E t speciﬁed by M for time t independently with probability 1 − p to obtain E t and then we set S t+1 = {v ∈ V | ∃u ∈ S t : (u, v) ∈ E t ∨ v = u}.\nNote, that Deﬁnition 3 is only well deﬁned if M is oblivious. Furthermore, F becomes a time-homogeneous Markov chain if M is also i.i.d. Also, as long as for every time t the union over the edges in M from t to inﬁnity is almost surely connected then F is absorbing with the unique absorbing state V . We say the ﬂooding process F stops if it reaches this absorbing state and we denote the time this happens with the random variable S F . The next deﬁnition pairs this ﬂooding time with a throughput parameter α that corresponds to the exponent of the ﬂooding process tail probability. The reason for this deﬁnition and its connection to the multi-message throughput behavior of network coding becomes apparent in the statement and proof of Theorem 1 below.\nDeﬁnition 4. We say an oblivious network model M on a node set V ﬂoods in time T with throughput α if there exists a prime power q such that for every v ∈ V and every k > 0 we have P [S F (M,1/q,{v}) ≥ T + k] < q −αk .\nTo give a few illustrating examples of ﬂooding times we note that the random phone call network model on n nodes ﬂoods in Θ(log n) time with constant throughput. The uniform gossip model on a connected degree bounded graph G ﬂoods in time Θ(D) and with constant throughput where D is the diameter of G. In many oblivious network models it is easy to give tight bounds on the ﬂooding time and throughput.\nWith this framework for oblivious network models in place we can give a cleaner restatement of Theorem 3 in [12]. We also provide a sketch of the proof since we will later expand on the ideas used therein.\nTheorem 1 (Theorem 3 of [12]). Suppose M is an oblivious network model that ﬂoods in time T with throughput α. Then, for any k, random linear network coding in the network model M spreads k arbitrarily distributed messages to all nodes with probability 1 − after T = T + 1 α (k + log −1 ) rounds.\nProof: The random linear network coding protocol we analyze will use the same ﬁeld size that achieves the parame- ters T and α for M in Deﬁnition 4. We ﬁx a coefﬁcient vector µ ∈ F k q . This vector is initially known to a non-empty subset\nS of nodes. It is easy to check that the probability that a node v does not know µ after it has received a packet from a node that knows µ is at most 1/q. This implies that knowledge of µ spreads through the network exactly as the ﬂooding process F M,1/q,S . Using the assumption, Deﬁnition 4 and the monotonicity of S F (M,1/q,S) in S we get that the probability that a vector µ ∈ F k q is not known to all vectors after T steps is at most q −(k+log −1 ) . A union bound over all q k vectors shows that with probability at least 1 − q − log −1 ≥ 1 − all node will know about all vectors and it is easy to see that this implies that all nodes are able to decode the messages.\nIn this section we give a simple, direct derivation of tight stopping time bounds for gossip with one source and side information at the nodes and gossip with two correlated sources. Our two main results in this section are the following.\nTheorem 2. Suppose M is an oblivious network that ﬂoods in time T with throughput α. For any error probability > 0, any constant δ > 0, l = Ω(log −1 ), any packet size s and any distributions X, {Y v } v∈V suppose the network is initialized with a single message x generated from X and side information y w generated from Y w at every node w. Then, every node v will correctly decode x with probability at least 1 − after T = T + 1 α l s (H(X|Y v ) + δ) + log −1 + 3\nTheorem 3. Suppose M is an oblivious network that ﬂoods in time T with throughput α. For any error probability\n> 0, any constant δ > 0, l = Ω(log −1 ), any packet size s and any distributions X 1 , X 2 suppose the network is initialized with two messages x 1 , x 2 generated from X 1 , X 2 and nodes have no side information. Then, every node v will correctly decode x with probability at least 1 − after T + 1 α l s (H(X 1 , X 2 ) + 2δ) + log −1 + 3 rounds.\nThe idea for proving these theorems is to generalize the observation of [12] that the question of when a node can decode is equivalent to determining when this node knows (see Deﬁnition 1) enough coefﬁcient vectors. The proof of Theorem 1 shows that ﬂooding or spreading of knowledge of vectors can be analyzed using a union bound. This implies that only the number of vectors needed is of importance. In the case with uncorrelated sources and no side information essentially knowledge of all coefﬁcient vectors is necessary. In the correlated scenario, however, we want to relate the number of vectors a node v needs to know to the conditional entropy H(X|Y v ). Lemma 1 helps in this respect. It asserts that in order to decode, a node with side information Y does not necessarily need i = (H(X|Y ) + δ)l speciﬁc bits, but rather, assuming joint typicality decoding, it requires only any sufﬁcient amount of information about the index of the bin in which x resides. This is achieved by any i/s packets containing independent equations on the bin index. We can thus focus on the knowledge a node is required to obtain in order for its coefﬁcient matrix to have rank at least i/s.\nUnfortunately it is possible that a node knows many vectors without having a large rank. In fact, upon reception of the ﬁrst packet a node gets to know all but a 1/q fraction of all vectors. On the other hand, in order to prove faster stopping times we want to argue that the knowledge of only an exponentially small fraction of all vectors sufﬁces for decoding. This is achieve by the following lemma which shows that indeed only q l speciﬁc coefﬁcient vectors sufﬁce to guarantee that at least l independent coefﬁcient vectors were received:\nLemma 2. Let V be a ﬁnite dimensional vector space over F q . For every 0 ≤ h < dim V there exist w = q h + 1 vectors v 1 , . . . , v w ∈ V such that any (subspace) K ⊂ V for which K ⊥ does not contain v i for any i has dimension at least h+1.\nUsing only random binning (Lemma 1) and Lemma 2, it is possible to sketch the proofs of the main results in this section.\nProof Sketch of Theorem 2: We use the ﬁeld size q that achieves the parameters T and α in Deﬁnition 4. We furthermore choose l large enough so that the decoding probability in Lemma 1 is at most /2. By Lemma 1, any node v with access to the side information vector y v and\n(H(X|Y v ) + δ) independent equations on the blocks de- scribing the bin index of x assigned by the random binning procedure, can decode x with probability at least 1 − /2. It thus remains to show that with probability 1 − /2 we have dim S v ≥ l s (H(X|Y v ) + δ) after T rounds. To prove this we apply Lemma 2 and get that there exists a set Z of 2 l s (H(X|Y v )+δ) coefﬁcient vectors such that if v has knowledge of these vectors, it indeed has sufﬁciently many independent equations. Furthermore, we refer to the proof of Theorem 1 for the fact that knowledge of any coefﬁcient vector (in Z) spreads through the network like a ﬂooding process. As before we thus get the fact that in the assumed network model M the probability that any of the coefﬁcient vectors (in Z) is not known after T + 1 α (k + (log −1 + 1)) rounds is smaller than /2 · q −k . Setting k = log |Z| and using a union bound over all coefﬁcient vectors in Z the result follows.\nWhile the proof of Theorem 3 is similar in nature to that of Theorem 2, there is delicate point when considering multiple sources. In a single source scenario, for each terminal there is only one equation governing the rate. That is, r ≥ H(X|Y v )+ δ. Using Lemma 2, this rate constraint is translated into a rank constraint, namely, dim(S v ) ≥ l s (H(X|Y v ) + δ) . For more than one source, however, the rate region is given by multiple rate constraints, and one has to make sure all are satisﬁed. Indeed, for two sources and no side information at the nodes this can be done using a single rank constraint. For more than two sources, or when additional side information is available, a more reﬁned analysis is required. This is the subject of the next section.\nTo date, analysis of gossip schemes focused only on the dissemination time - the number of rounds required to gain the complete knowledge in the network. However, especially when dynamic networks are analyzed, it is interesting to gain a more\naccurate measure of the actual capacities achievable between sets of nodes . Namely, to analyze the capacities induced by the gossip packet exchange process. This is an interesting question in its own right, and, in particular, can give a \u201cblack- box\u201d method to transfer any results of prior works that bound the rates or capacities needed between sources and sinks in the static memory-free setting to stopping times in oblivious network models.\nHerein, we develop such a characterization, and apply it to the results of [4] and [6] to obtain stopping times for gossip protocols with an arbitrary number of correlated sources and side-information, generalizing the results from the last section. We ﬁrst introduce the required notation.\nDeﬁnition 5. Let T > 0, node set V and active edges E 1 , E 2 , . . . , E T be given. A path P from s to d is a sequence of nodes P = (v 0 , v 1 , . . . , v T ) such that v 0 = s, v T = d and for every t ≤ T we have v t−1 = v t or (v t−1 , v t ) ∈ E t .\nDeﬁnition 6. A set of m paths with weights w 1 , . . . , w m is valid if for every t < T and every (u, v) ∈ E t the weights of paths using (u, v) sum to at most one. We say a set of valid weighted paths achieves a capacity of c between two nodes s and d if the weights of paths from s to d sum up to c.\nQuite intuitively, these paths correspond to an information ﬂow through the network from the sources to the sinks; or, alternatively, to a (fractional) network ﬂow in a time-expanded (hyper-)graph or trellis. This intuition was made formal in [18] which proves an explicit equivalence between the algebraic gossip protocol and random linear network coding in the classical memory-free model (e.g., [4]). The directed time- expanded hypergraph G P N C that corresponds to a sequence of active edges can also be found in [18]. We omit the details of this equivalence and instead only recall the facts needed in this paper:\nLemma 3. Let node set V , the active edges E 1 , . . . , E T , destination d ∈ V and sources s 1 , s 2 , . . . , s k ∈ V be given. Algebraic gossip on {E i } T i=1 is equivalent to classical random linear network coding in the transformed hypergraph G P N C described in [18]. In particular, if for some integers c 1 , . . . , c k , it is possible for every s i to transmit c i packets to d, then there exists a sequence of valid paths of weight one and a rate c i between s i and d. Conversely, if for some positive reals c 1 , . . . , c k there is a set of valid weighted paths that achieve a capacity c i between s i and d, then the capacities c i lie in the min-cut region of G P N C .\nGiven this setup, we show the ﬁrst result in this direction: Lemma 4. Let M be a network on a node set V that ﬂoods in time T with throughput α. For any T , > 0, destination d ∈ V and set of k source nodes s 1 , s 2 , . . . , s k ∈ V with integral capacities c 1 , c 2 , . . . , c k ≥ 1, suppose E 1 , . . . , E T is a sequence of active edges on V sampled from M . If T ≥ T + 1 α ( i c i + log −1 ) then with probability at least 1 −\nthere exists a selection of valid weighted paths that achieve the capacity c i between s i and t for every i.\nNote that the above lemma requires the capacities to be integral and thus essentially asks for the time until a certain number of mutually disjoint paths occur. While this is sufﬁ- cient and optimal in the uncorrelated information spreading setting this requirement can be a severe restriction.\nOne setting where this makes a drastic difference is when we have k sources and the total capacities needed sum up to less then one. This corresponds to asking for the time until there is a path from each of the sources to the sink \u2013 without these paths having to be disjoint. If one considers for example the random phone call model with n nodes and k sources it takes in expectation log n+k time until a disjoint path between a node and each source appears while merely log n + log k rounds are sufﬁcient to get this for non-disjoint paths.\nThe following lemma generalizes this observation and strengthens Lemma 4 in this direction to give order optimal bounds for any set of fractional capacities:\nLemma 5. Let M be a network model on a node set V that ﬂoods in time T with throughput α. For any T , any > 0, any sink d ∈ V and any set of k source nodes s 1 , s 2 , . . . , s k ∈ V with rates c 1 , c 2 , . . . , c k > 0, if T ≥ T + 1 α ( i c i + log k + log −1 ) then with probability at least 1 − there exists a selection of valid weighted paths that achieve a capacity of c i between s i and d for every i.\nProof Sketch: The idea is to combine k applications of Lemma 4 using a union bound and capacity sharing. Set the failure probability to /k and in the ith application of Lemma 4 set the c i to \t i c i while setting all other capacities to zero. We get that for every i with probability 1 − /k the number of disjoint paths from s i to d is at least \t i c i . Via a union bound, with probability of 1 − all these paths are there. Yet, while the paths from each source are disjoint, the paths starting at different sources may not be disjoint. We now take the union of these paths while choosing the weight of a paths starting at source s i to be c i\n. This gives capacity of c i between s i and d. Furthermore, since any edge e is used by at most one path going out from each source, we get that the total weight on e summed over all paths is at most i c i\nWe will use Lemma 5 to prove our main result about information dissemination with correlated data in oblivious networks. To state our result we need the following deﬁnition: Deﬁnition 7 (Slepian-Wolf region [1]). A capacity vector c = (c 1 , . . . , c k ) is sufﬁcient for v ∈ V if and only if for every index subset S ∈ [k] we have i∈S c i ≥ H(X S |X S , Y v ).\nPutting together Lemma 5, Lemma 3 and applying the re- sults on network coding with correlated data from [4] and [6], we can now directly state our main result which generalizes and encompasses Theorem 3 and Theorem 2:\nTheorem 4. Suppose M is an oblivious network model that ﬂoods in time T with throughput α. For any constant δ > 0, any error probability > 0, any l = Ω(log −1 ), any joint distribution of X 1 , . . . , X k and the Y v \u2019s, any packet size s > 0, any node v and any capacity vector (c 1 , . . . , c k ) that is\nsufﬁcient for v, node v will correctly decoding x 1 , . . . , x k with probability at least 1 − after T + 1 α ( l s i∈[k] c i + δ + log k + log −1 + δ) rounds.\nIn this work, we designed and analyzed gossip schemes for networks with correlated data. We deﬁned the concepts of oblivious networks, partial knowledge required to decode using side information and network capacities in the context of gossip schemes. This allowed us to give tight bounds on stopping times of these protocols. The suggested solution is based on joint network-source coding [19]. While efﬁcient joint codes exist in certain scenarios [20], [21], the general case is computationally expensive and remains a fascinating future research direction."},"refs":[{"authors":[{"name":"D. Slepian"},{"name":"J. Wolf"}],"title":{"text":"Noiseless coding of correlated information sources"}},{"authors":[{"name":"R. Ahlswede"},{"name":"J. K¨orner"}],"title":{"text":"Source coding with side information and a converse for degraded broadcast channel"}},{"authors":[{"name":"A. Cohen"},{"name":"S. Avestimehr"},{"name":"M. Effros"}],"title":{"text":"On networks with side information"}},{"authors":[{"name":"T. Ho"},{"name":"M. M´edard"},{"name":"R. Koetter"},{"name":"D. R. Karger"},{"name":"M. Effros"},{"name":"J. Shi"},{"name":"B. Leong"}],"title":{"text":"A random linear network coding approach to multicast"}},{"authors":[{"name":"J. Barros"},{"name":"S. D. Servetto"}],"title":{"text":"Network information ﬂow with correlated sources"}},{"authors":[{"name":"M. Bakshi"},{"name":"M. Effros"}],"title":{"text":"On achievable rates for multicast in the presence of side information"}},{"authors":[{"name":"A. Demers"},{"name":"D. Greene"},{"name":"C. Hauser"},{"name":"W. Irish"},{"name":"J. Larson"},{"name":"S. Shenker"},{"name":"H. Sturgis"},{"name":"D. Swinehart"},{"name":"D. Terry"}],"title":{"text":"Epidemic algorithms for replicated database maintenance"}},{"authors":[{"name":"R. Karp"},{"name":"C. Schindelhauer"},{"name":"S. Shenker"},{"name":"B. Vocking"}],"title":{"text":"Randomized rumor spreading"}},{"authors":[{"name":"S. Deb"},{"name":"M. M´edard"},{"name":"C. Choute"}],"title":{"text":"Algebraic gossip: a network coding approach to optimal multiple rumor mongering"}},{"authors":[{"name":"D. Mosk-Aoyamam"},{"name":"D. Shah"}],"title":{"text":"Information dissemination via network coding"}},{"authors":[{"name":"M. Borokhovich"},{"name":"C. Avin"},{"name":"Z. Lotker"}],"title":{"text":"Tight bounds for algebraic gossip on graphs"}},{"authors":[{"name":"B. Haeupler"}],"title":{"text":"Analyzing network coding gossip made easy"}},{"authors":[{"name":"B. Haeupler"},{"name":"D. Karger"}],"title":{"text":"Faster information dissemination in dynamic networks via network coding"}},{"authors":[{"name":"B. Haeupler"},{"name":"M. Medard"}],"title":{"text":"One Packet Sufﬁces - Highly Efﬁcient Packetized Network Coding With Finite Memory"}},{"authors":[{"name":"C. Avin"},{"name":"M. Borokhovich"},{"name":"K. Censor-Hillel"},{"name":"Z. Lotker"}],"title":{"text":"Order optimal information spreading using algebraic gossip"}},{"authors":[{"name":"B. Haeupler"},{"name":"A. Cohen"},{"name":"C. Avin"},{"name":"M. M´edard"}],"title":{"text":"Network coded gossip with correlated data"}},{"authors":[{"name":"A. Clementi"},{"name":"C. Macci"},{"name":"A. Monti"},{"name":"F. Pasquale"},{"name":"R. Silvestri"}],"title":{"text":"Flood- ing time in edge-markovian dynamic graphs"}},{"authors":[{"name":"B. Haeupler"},{"name":"M. Kim"},{"name":"M. Medard"}],"title":{"text":"Optimality of Network Coding with Buffers"}},{"authors":[{"name":"A. Ramamoorthy"},{"name":"K. Jain"},{"name":"P. A. Chou"},{"name":"M. Effros"}],"title":{"text":"Separating distributed source coding from network coding"}},{"authors":[{"name":"G. Maierbacher"},{"name":"J. Barros"},{"name":"M. M´edard"}],"title":{"text":"Practical source-network decoding"}},{"authors":[{"name":"C. Avin"},{"name":"M. Borokhovich"},{"name":"A. Cohen"},{"name":"Z. Lotker"}],"title":{"text":"Efﬁcient Dis- tributed Source Coding for Multiple Receivers Via Matrix Sparsiﬁca- tion"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565139.pdf"},"links":[{"id":"1569565867","weight":15},{"id":"1569559259","weight":5},{"id":"1569566597","weight":3},{"id":"1569564481","weight":5},{"id":"1569566415","weight":3},{"id":"1569558325","weight":7},{"id":"1569565837","weight":9},{"id":"1569566941","weight":3},{"id":"1569564387","weight":3},{"id":"1569566795","weight":9},{"id":"1569561679","weight":9},{"id":"1569566895","weight":5},{"id":"1569565321","weight":3},{"id":"1569566239","weight":3},{"id":"1569561143","weight":3},{"id":"1569565535","weight":15},{"id":"1569561795","weight":3},{"id":"1569559805","weight":3},{"id":"1569559111","weight":3},{"id":"1569566445","weight":18},{"id":"1569565633","weight":3},{"id":"1569565219","weight":3},{"id":"1569566003","weight":16},{"id":"1569565185","weight":7},{"id":"1569566779","weight":7},{"id":"1569557275","weight":3},{"id":"1569565919","weight":3},{"id":"1569566823","weight":3},{"id":"1569566283","weight":3},{"id":"1569566529","weight":7},{"id":"1569563975","weight":5},{"id":"1569564923","weight":5},{"id":"1569566601","weight":7},{"id":"1569565805","weight":3},{"id":"1569557851","weight":13},{"id":"1569567013","weight":5},{"id":"1569565853","weight":11},{"id":"1569566797","weight":9},{"id":"1569566973","weight":9},{"id":"1569565579","weight":7}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S15.T1.3","endtime":"10:50","authors":"Bernhard Haeupler, Asaf Cohen, Chen Avin, Muriel Médard","date":"1341570600000","papertitle":"Network Coded Gossip With Correlated Data","starttime":"10:30","session":"S15.T1: Data Exchange and Gathering","room":"Kresge Rehearsal B (030)","paperid":"1569565139"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
