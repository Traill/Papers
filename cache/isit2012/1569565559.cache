{"id":"1569565559","paper":{"title":{"text":"Compressive Principal Component Pursuit"},"authors":[{"name":"John Wright ∗"},{"name":"Arvind Ganesh \u2020"},{"name":"Kerui Min \u2020"},{"name":"Yi Ma \u2020\u2021"}],"abstr":{"text":"Abstract\u2014We consider the problem of recovering a target matrix that is a superposition of low-rank and sparse compo- nents, from a small set of linear measurements. This problem arises in compressed sensing of structured high-dimensional signals such as videos and hyperspectral images, as well as in the analysis of transformation invariant low-rank recovery. We analyze the performance of the natural convex heuristic for solving this problem, under the assumption that measurements are chosen uniformly at random. We prove that this heuristic exactly recovers low-rank and sparse terms, provided the number of observations exceeds the number of intrinsic degrees of freedom of the component signals by a polylogarithmic factor. Our analysis introduces several ideas that may be of independent interest for the more general problem of compressive sensing of superpositions of structured signals. 1"},"body":{"text":"In recent years, there has been tremendous interest in recovering low-dimensional structure in high-dimensional sig- nal or data spaces. This interest has been fueled by the striking discovery that efﬁcient techniques based on convex programming can accurately recover low-complexity signals such as sparse vectors or low-rank matrices from severely compressive, incomplete, or even corrupted observations.\nOne representative example arises in Robust Principal Com- ponent Analysis (RPCA) . There, the goal is to recover a low- rank matrix L 0 ∈ R m×n from grossly corrupted observations. For example, suppose we observe M = L 0 + S 0 , where S 0 ∈ R m×n is a sparse error. Under mild conditions, the fol- lowing convex program, called Principal Component Pursuit (PCP) [1], [2]:\nprecisely recovers L 0 and S 0 . In (1), · ∗ is the matrix nuclear norm (sum of singular values) and · 1 is the 1 norm (sum of magnitudes). For data analysis applications, this suggests that a low-rank matrix L 0 can be recovered from the observation M despite large-magnitude sparse errors.\nThe conditions under which recovery is known to occur are broad: provided the low-rank term satisﬁes a technical incoherence condition, correct recovery can occur even when rank(L 0 ) is almost proportional n, and the number of nonzero entries in S 0 is proportional to mn [1]. On the other hand, in many applications of interest, the rank may actually be signiﬁcantly smaller than dimension (say 3 [3], or 9 [4]). Moreover, cardinality of the sparse term may also be quite small. In such a situation our number mn of observations\ncould be extravagantly large compared to the number degrees of freedom in the unknowns L 0 , S 0 . Is it possible to recover L 0 and S 0 from smaller sets of linear measurements?\nwhere Q ⊆ R m×n is a linear subspace, and P Q is the projec- tion operator onto Q. The natural convex program becomes\nFollowing [1], in this paper we refer to this convex program as Compressive Principal Component Pursuit (CPCP).\nThe adjective compressive is suggestive of one application of this tool. The low-rank and sparse model captures properties of many signals of interest, including videos [1], [5], [6], structured textures [7], hyperspectral datacubes [8], [9] and more. The ability to recover low-rank and sparse models from small sets of measurements D could be very useful for developing new sensing architectures for such signals [8], [10]. CPCP (3) also arises in other computational problems, including transformation-invariant low-rank recovery [7], [5].\nThe fundamental question is whether we can simultaneously recover the low-rank and sparse components from highly compressive measurements via CPCP? While this question is largely open, there is good reason to believe the answer may be positive. For example, [1], [11] have studied the \u201crobust matrix completion\u201d problem, with P Q = P Ω , where Ω is a small subset of the entries of the matrix. When P Q = P Ω , it is impossible to exactly recover S 0 (many of the entries are simply not observed!), but the low-rank term L 0 can be recovered from near-minimal sets of samples [11]. However, in many applications the sparse term S 0 is actually the quantity of interest: for example, in visual surveillance, S 0 might capture moving foreground objects. To recover both L 0 and S 0 , we must require measurements Q that are incoherent with both the low-rank and the sparse term.\nIn this paper, we investigate the performance of (3) when Q is a randomly chosen subspace. A similar recovery problem was recently considered by [8], with the goal of designing sensing strategies capable of recovering both L 0 and S 0 . We will discuss the results of [8] and other related works in more detail in Section III, after we have stated our main results.\nOur results will show that if the number of measurements q is large enough, (3) will correctly recover L 0 and S 0 with high probability. Clearly, to ensure a unique solution, this q should at least as large as the number of intrinsic degrees of freedom in (L 0 , S 0 ). Since a rank r matrix has (m + n − r)r degrees of freedom, the number of continuous degrees of\nfreedom in the pair (L 0 , S 0 ) is (m + n − r)r + S 0 0 , where · 0 is the number of nonzero entries in a matrix. We will\nshow that when the measurements are Gaussian, (L 0 , S 0 ) can be exactly recovered from a number of measurements that is merely within an O(log 2 m) factor of this lower bound.\nOur analysis actually pertains to a much more general class of problems of decomposing a given observation into multiple incoherent components:\nHere, · (i) are (decomposable) norms that encourage var- ious types of low-complexity structure. Principal Component Pursuit [1], [2], Outlier Pursuit [12], [13] and Morphological Component Analysis [14] are all special cases of this general problem. Our analysis will suggest that if (4) succeeds in recovering all the components {X i } from M , one should also expect to recover them from the highly compressive measurements P Q [M ]. The number of measurements required is again governed by the intrinsic degrees of freedom {X i } times at most a polylog(m) factor. Thus, we believe the results in this paper will be applicable to a broad class of source separation or signal decomposition problems that may arise in signal processing, communications, and pattern recognition.\nWe ﬁrst recall conditions under which M = L 0 +S 0 can be exactly separated into its constituents by PCP. Intuitively, we should not expect to recover all possible low-rank pairs and sparse pairs (L 0 , S 0 ). Indeed, imagine the case when M is rank-one and one-sparse (i.e., M = e i e ∗ j for some i, j). In this situation the answers (L = e i e ∗ j , S = 0) and (L = 0, S = e i e ∗ j ) both seem reasonable \u2013 the problem is ambiguous!\nTo make the problem meaningful, we need conditions that ensure that (i) the low-rank term L 0 does not \u201clook sparse\u201d and (ii) the sparse term S 0 does not \u201clook low-rank.\u201d One popular way formalizing the ﬁrst intuition of doing this is via the notion of incoherence introduced by [15]. If the low- rank matrix L 0 has rank-reduced singular value decomposition L 0 = U ΣV ∗ , then we say that L 0 is µ-incoherent if\n∀ i, U ∗ e i 2 2 ≤ µr/m, ∀ j, V ∗ e j 2 2 ≤ µr/n, (5) U V ∗ ∞ ≤ \t µr/mn. \t (6)\nThese conditions ensure that the singular vectors of L 0 are not too concentrated on only a few coordinates [15].\nAt the same time, we need to ensure that the sparse term does not \u201clook low-rank.\u201d One appealing way of doing this is via a random model: we assume that each (i, j) is an element of supp (S 0 ) independently with probability ρ bounded by some small constant. We assume that the signs of the nonzero entries are independent symmetric ±1 random variables (i.e., Rademacher random variables). In stating our theorems, we call such a distribution an \u201ciid Bernoulli-Rademacher model.\u201d\nWe will give a result for the case when the measurement subspace Q is chosen uniformly at random from the set of all q-dimensional subspaces of R m×n . More precisely, Q is\ndistributed according to the Haar measure on the Grassman- nian G(R m×n , q). This means that Q is equal in distribution to the linear span of a collection of q independent iid N (0, 1) matrices. Under this setting, the following theorem gives a tight bound on the number of (random) measurements required to correctly recover the pair (L 0 , S 0 ) from P Q [M ] via CPCP: Theorem II.1 (Compressive PCP Recovery). Let L 0 , S 0 ∈ R m×n , with m ≥ n, and suppose that L 0 = 0 is a rank-r, µ- incoherent matrix with r ≤ c r n(µ log 2 m) −1 , and sign (S 0 ) is iid Bernoulli-Rademacher with nonzero probability ρ < c ρ . Let Q ⊂ R m×n be a random subspace of dimension\ndistributed according to the Haar measure, probabilistically independent of sign(S 0 ). Then with probability at least 1 − Cm −9 in (sign(S 0 ), Q), the solution to (3) with λ = 1/\nm is unique, and equal to (L 0 , S 0 ). Above, c r , c ρ , C Q , C are positive numerical constants.\nHere, the magnitudes of the nonzeros in S 0 are arbitrary, and no randomness is assumed in L 0 . The randomness is in the sign and support pattern of S 0 and the measurements Q. We note in passing that the randomness in the signs of S 0 can be removed using the techniques of [1] Sections 2.1-2.2.\nWe also note that the bounds on r and ρ essentially match those of [1], possibly with different constants. So, again, r and S 0 0 can be rather large. On the other hand, when these quantities are small, the bound on dim(Q) ensures that the number of measurements needed for accurate recovery is also commensurately small. We will compare our results to other works from the literature in the next section.\nAs mentioned above, in recent years there has been a large amount of work on matrix recovery and decomposition, for example see [1], [2], [16], [17], [12], [13], [18], [19] and references therein. The aforementioned works all pertain to the case when the matrix M is fully observed, and hence are not directly comparable to our result. Our analysis relies on a tool for transforming a certiﬁcate of optimality for the fully-observed problem into a certiﬁcate of optimality for the compressive problem, which might potentially also be applied in conjunction with the above analyses.\nCompared to the fully observed problem, there is much less dedicated work on low-rank and sparse recovery from compressive measurements. Recently, motivated by applica- tions in compressive foreground and background separation and compressive hyperspectral image acquisition, Waters et. al. [8] introduced a greedy algorithm for this problem, which is similar in spirit to the CoSaMP algorithm [20], and performs well on numerical examples. Analyzing its behavior and proving performance guarantees is currently an open problem.\nAs the body of results on speciﬁc problems such as matrix recovery grows, there has been an increasing interest in uni- fying or generalizing the insights obtained. For example, Ne- gahban et. al. [21] give a geometric framework for analyzing\nlow-complexity signal recovery. Agarwal et. al. [18] use this framework to analyze sparse and low-rank recovery, obtaining near-optimal rates for estimation in noise. [18] proceed under weaker assumptions, which preclude exact recovery.\nChandrasekaran et. al. [22] have recently produced a very general analysis of structured signal recovery with Gaussian measurements. That work exploits the geometry of the norm ball, relating the required number of measurements to the Gaussian width of the tangent cone at the desired solution. For our problem, the non-trivial analysis in [1], [2] can be viewed as simply showing that the desired solution lies on the boundary of the norm ball. Estimating the width of the tangent cone seems to entail additional difﬁculty.\nFor Gaussian measurements, Cand`es and Recht [23] also give simple bounds for exact recovery, under the assumption that the regularizer (or norm) is decomposable. To apply similar analysis to our problem, we would need to work with the quotient norm on M : M . = inf L+S=M L ∗ +λ S 1 . This is the inﬁmal convolution of two decomposable norms. Its subdifferential has a number of nice properties, but decom- posability does not appear to be one of them. Nevertheless, our results suggest we should expect the same type of compres- sive sensing results for this class of generalized norms for superpositions of low-complexity components.\nIn this section, we present the technical result used to obtain Theorem II.1 above. As promised, this result will have implications for compressive variants of a large number of conceivable signal decomposition problems. Suppose that the fully observed data M are given as a sum of structured terms:\nwhere each X i satisﬁes a low-complexity model such as sparsity or rank-deﬁciency, possibly also including more exotic types of structured sparsity [24]. For each type of structure, we have a corresponding regularizer · (i) . The natural convex heuristic for decomposing M into its components would solve\nThe goal of this paper is not to study (9) per se, but rather to understand what happens to it when we only observe the projection of M onto a much lower dimensional subspace:\nSuppose we know that (9) correctly decomposes M into X 1 , . . . , X τ . Does this imply that (10) can also recover X 1 , . . . , X τ ? Theorem IV.6 below will imply that this is true under broad circumstances. Provided we have proved optimality for (9), we can move to optimality for (10), as long as the number of measurements dim(Q) is sufﬁciently large. In this sense, our analysis is modular: any technique can be used to perform the analysis of the original decomposition problem, provided it constructs an (approximate) dual certiﬁcate.\nOur result pertains to decomposable norms · (i) [21], [23]. This notion includes sparsity inducing norms such as the 1 norm and nuclear norm, as well as sums of block p norms.\nDeﬁnition IV.1. We say that a norm · is decomposable at X if there exists a subspace T and a matrix S such that\nwhere · ∗ denotes the dual norm of · , and P T ⊥ is nonexpansive with respect to · ∗ .\nFor example, the 1 norm satisﬁes this deﬁnition with T = supp (X) and S = sign (X). This deﬁnition is completely equivalent to that of [23]. It is also related to the deﬁnition of [21], but not strictly equivalent to it. We assume that each\n· (i) is decomposable at the target solution X i, , so per the above deﬁnition we have a sequence of subspaces T i and matrices S i that deﬁne the subdifferentials of each of the reg- ularizers · (i) . We will say that the subspaces T 1 , . . . , T τ are independent if dim(T 1 +· · ·+T τ ) = dim(T 1 )+· · ·+dim(T τ ), and state a simple sufﬁcient optimality condition for (10):\nLemma IV.2. Consider a feasible solution x \t = (X 1, , . . . , X τ, ) to (10). Suppose that each of the norms\n· (i) is decomposable at X i, . If T 1 , . . . , T τ , Q ⊥ are inde- pendent subspaces and there exists Λ satisfying P T i Λ = λ i S i and P T ⊥\nΛ ∗ (i) < λ i for each i, and P Q ⊥ Λ = 0, then x is the unique optimal solution to (10).\nThis condition implies that Λ lies in the subdifferential of λ i · (i) for each i. If we take Q = R m×n in Lemma IV.2, we obtain a sufﬁcient optimality condition for the original decomposition problem (9). The condition given by Lemma IV.2 is not so convenient, because it demands that Λ exactly satisﬁes a set of equality constraints P T i Λ = λ i S i . One very useful device, due to Gross [25], is to trade off between the equality constraints and the dual norm inequality constraints\nΛ ∗ (i) < λ i , tightening the latter while loosening the former. The following deﬁnition gives this idea a name:\nDeﬁnition IV.3. We call Λ an (α, β)-inexact certiﬁcate for a putative solution (X 1, , . . . , X τ, ) to (9) with parameters (λ 1 , . . . , λ τ ) if for each i, P T i Λ − λ i S i F ≤ α, and\nComparing to Lemma IV.2, we can see that this deﬁnition is most meaningful when α is small, and β ≤ 1. Deﬁnition IV.3 pertains to the decomposition problem (9), and does not involve the measurement operator Q in any way. Adding one more constraint, P Q ⊥ Λ = 0, we obtain an inexact certiﬁcate for the compressive problem (10):\nDeﬁnition IV.4. We call Λ an (α, β)-inexact certiﬁcate for a putative solution (X 1, , . . . , X τ, ) to (10) with parameters (λ 1 , . . . , λ τ ) if\n(i) Λ is an (α, β) inexact certiﬁcate for (9), and (ii) P Q ⊥ Λ = 0.\nAs we will see, an inexact certiﬁcate is easier to produce than the \u201cexact\u201d Λ demanded in the optimality condition\nLemma IV.2. Is it still sufﬁcient to certify optimality? The answer is yes, provided α and β are small enough:\nLemma IV.5. Consider a feasible solution x \t = (X 1, , . . . , X τ, ) to the optimization problem (10). Suppose that each of the norms · (i) is decomposable at X i, , and that each of the · (i) majorizes the Frobenius norm. Then if T 1 , . . . , T τ , Q ⊥ are independent subspaces with\nP T i P T j < (τ − 1) −1 ∀ i = j, \t (12) and there exists an (α, β)-inexact certiﬁcate ˆ Λ, with\nThe technical condition that · (i) majorizes the Frobenius norm (i.e., for all X, X (i) ≥ X F ) is immediately satisﬁed by sparsity inducing norms such as the nuclear and\n1 norms. In any case, it can always be ensured by rescaling. Thus, to show that X 1 , . . . , X τ solve the compressive\ndecomposition problem (10), we just have to produce an inexact certiﬁcate Λ following the speciﬁcation of Deﬁnition IV.4 with (α, β) sufﬁciently small. This is fortuitous, since many existing analyses of the original decomposition problem (9) already give certiﬁcates for that problem. To prove that the desired solution remains optimal even when we only see a few measurements Q, we will show that a certiﬁcate for (9) can be \u201cupgraded\u201d to a certiﬁcate for (10), with very high probability in the random Q, and only a small loss in the parameters (α, β). As it turns out, the loss in the dual norm · ∗ (i) depends on the expected dual norm of a standard Gaussian matrix:\nν i . = E G ∗ (i) , \t G ∼ iid N (0, 1) . \t (14) Of course, upgrading should only be possible if the number\nof measurements is sufﬁcient. Interestingly, however, the num- ber of measurements does not need to be too much larger than the number of degrees of freedom in x . More precisely, our theorem will refer to the quantity dim(T 1 + · · · + T τ ). Indeed, for the 1 norm, dim(T i ) is the number of nonzero entries in the solution X i . For the nuclear norm, one can check that dim(T i ) is the number of degrees of freedom a matrix whose rank is equal to that of X i :\nTheorem IV.6 (Certiﬁcate Upgrade). Consider the general decomposition problem (9), and suppose that each of the norms · (i) majorizes the Frobenius norm. Let x = (X 1, , . . . , X τ, ) be feasible for (9), and suppose there exists an (α, β)-inexact certiﬁcate ˆ Λ for x for (9).\ndim(Q) ≥ C Q · dim(T 1 + · · · + T τ ) · log m, \t (15) there exists an (α , β )-inexact certiﬁcate for x for the compressive decomposition problem (10) with\nlog m dim(Q)\nw.p. ≥ 1 − C 2 · τ · m −9 in Q. Above, C Q , C 1 and C 2 are positive numerical constants.\nIn the next two sections, we ﬁrst sketch how Theorem IV.6 implies Theorem II.1, and then sketch a proof of Theorem IV.6. Complete proofs are given in the full version [26].\nProof of Theorem II.1: From Lemma IV.5, to show (L 0 , S 0 ) is the unique solution to (3), it is enough to show that (i) P T P Ω ≤ 1/2, (ii) there exists an (α , 1/2)-inexact CPCP certiﬁcate Λ CPCP with\nm. \t (18) A covering argument shows: Lemma V.1. If S ⊆ R m×n is a ﬁxed subspace, A = γ\nH j H j , · , with (H j ) a sequence of independent i.i.d. N (0, 1/mn) matrices, and R = range(A) ⊆ R m×n , then if γ ≥ C 1 · dim(S), w.p., ≥ 1 − C 2 exp (−cγ),\nWe can use this to show that with high probability, 1 − P Q ⊥ P T ⊕Ω 2 ≥ 15 16 dim(Q) mn ≥ 1 m . So it is enough to show an\n(α , 1/2) CPCP certiﬁcate with α < 1/4m 3/2 . We construct this by taking an inexact PCP certiﬁcate from [1], and then applying Theorem IV.6 to upgrade it to a CPCP certiﬁcate.\nProposition V.2 (Dual Certiﬁcation for PCP [1] 2 ). Un- der the conditions of Theorem II.1, w.p. ≥ 1 − Cm −10 , (i) P Ω P T ≤ 1/2, and (ii) there exists a (m −2 , 1/4)- inexact PCP certiﬁcate Λ PCP for (L 0 , S 0 ), which satisﬁes\ncondition α < 1/4m 3/2 is satiaﬁed. Furthermore, with high probability Λ CPCP 2 F ≤ C(r+ρn). The expected dual norms are ν ≤ C\nlog m for · 1 . Hence, β ≤ β + C (mr + ρmn) log 2 m/dim(Q) 1/2 .\nUnder the stated condition on dim(Q), β ≤ 1/2, and so Lemma IV.5 shows that (L 0 , S 0 ) are optimal for CPCP.\nProof of Theorem IV.6: Let S = T 1 +· · ·+T τ +span( ˆ Λ). Our goal is to generate Λ that is close to ˆ Λ on S, and s.t.,\nSet Λ 0 = 0. Generate inductively a sequence (Λ j ) j=1,...,k for appropriate k, such that with high probability Λ = Λ k is the desired certiﬁcate. Deﬁne the error at step j to be\nBy orthogonal invariance, Q is equal in distribution to the linear span of H 1 , . . . , H dim(Q) , where H j are in- dependent iid N (0, 1/mn) random matrices. Choose from {1, . . . , dim(Q)}, k = 3 log 2 m disjoint subsets I 1 , . . . , I k\nof size γ = dim(Q)/k . Then 2 −k ≤ m −3 . We will require that γ ≥ C 3 · dim(S). Since\nthis is satisﬁed. Let A j : R m×n → R m×n denote the operator that acts via \t A\nApplying Lemma V.1, we can show that for each j, E j F ≤ E j−1 F /2. Using E 0 = − ˆ Λ, this further gives\nFrom the deﬁnition, set α = max i P T i Λ k − λ i S i F . This gives (19), via:\nWe can take β = max i=1,...,τ λ −1 i Λ k ∗ (i) . From (23) and the triangle inequality, we have\nLemma VI.1. Let S be any ﬁxed subspace, and M any ﬁxed matrix. Let A = γ l=1 H l H l , · be a random semideﬁnite operator constructed from a sequence of independent iid N (0, 1/mn) matrices (H l ), · any norm that majorizes the Frobenius norm, and · ∗ its dual norm. Set ν = E [ G ∗ ], with G iid N (0, 1). Then we have\n√ log m √\nlog m dim(Q)"},"refs":[{"authors":[{"name":"E. Cand`es"},{"name":"X. Li"},{"name":"Y. Ma"},{"name":"J. Wright"}],"title":{"text":"Robust principal component analysis?"}},{"authors":[{"name":"V. Chandrasekaran"},{"name":"S. Sanghavi"},{"name":"P. Parillo"},{"name":"A. Wilsky"}],"title":{"text":"Rank- sparsity incoherence for matrix decomposition"}},{"authors":[{"name":"L. Wu"},{"name":"A. Ganesh"},{"name":"B. Shi"},{"name":"Y. Matsushita"},{"name":"Y. Wang"},{"name":"Y. Ma"}],"title":{"text":"Robust photometric stereo via low-rank matrix completion and recovery"}},{"authors":[{"name":"R. Basri"},{"name":"D. Jacobs"}],"title":{"text":"Lambertian reﬂectance and linear subspaces"}},{"authors":[{"name":"A. Ganesh"},{"name":"Y. Peng"},{"name":"W. Xu"},{"name":"J. Wright"},{"name":"Y. Ma"}],"title":{"text":"RASL: Robust alignment via sparse and low-rank decomposition"}},{"authors":[{"name":"X. Shu"},{"name":"N. Ahuja"}],"title":{"text":"Imaging via three-dimensional compressive sampling (3DCS)"}},{"authors":[{"name":"Z. Zhang"},{"name":"A. Ganesh"},{"name":"X. Liang"},{"name":"Y. Ma"}],"title":{"text":"TILT: Transform-invariant low-rank textures"}},{"authors":[{"name":"A. Waters"},{"name":"A. Sankaranarayanan"},{"name":"R. Baraniuk"}],"title":{"text":"Sparcs: Recovering low-rank and sparse matrices from compressive measurements"}},{"authors":[{"name":"M. Golbabaee"},{"name":"P. Vandergheynst"}],"title":{"text":"Hyperspectral image compressed sensing via low-rank and joint sparse matrix recovery"}},{"authors":[{"name":"D. Donoho"}],"title":{"text":"Compressive sensing"}},{"authors":[{"name":"X. Li"}],"title":{"text":"Compressed sensing and matrix completion with constant constant proportion of corruptions"}},{"authors":[{"name":"H. Xu"},{"name":"S. Sanghavi"},{"name":"C. Caramanis"}],"title":{"text":"Robust PCA via outlier pursuit"}},{"authors":[{"name":"M. McCoy"},{"name":"J. Tropp"}],"title":{"text":"Two proposals for robust PCA using semidef- inite programming"}},{"authors":[{"name":"J. Bobin"},{"name":"Y. M. J. L. Starck"},{"name":"M. Elad"}],"title":{"text":"Sparsity and morphological diversity and source separation"}},{"authors":[{"name":"E. Cand`es"},{"name":"B. Recht"}],"title":{"text":"Exact matrix completion via convex opti- mzation"}},{"authors":[{"name":"Z. Zhou"},{"name":"X. Li"},{"name":"J. Wright"},{"name":"E. Candes"},{"name":"Y. Ma"}],"title":{"text":"Stable principal component pursuit"}},{"authors":[{"name":"A. Ganesh"},{"name":"X. Li"},{"name":"J. Wright"},{"name":"E. Candes"},{"name":"Y. Ma"}],"title":{"text":"Dense error correction for low-rank matrices via principal component pursuit"}},{"authors":[{"name":"A. Agarwal"},{"name":"S. Negahban"},{"name":"M. J. Wainwright"}],"title":{"text":"Noisy matrix de- composition via convex relaxation: Optimal rates in high dimensions"}},{"authors":[{"name":"D. Hsu"},{"name":"S. Kakade"},{"name":"T. Zhang"}],"title":{"text":"Robust matrix decomposition with outliers"}},{"authors":[{"name":"D. Needell"},{"name":"J. Tropp"}],"title":{"text":"CoSaMP: Iterative signal recovery from incomplete and inaccurate samples"}},{"authors":[{"name":"S. Negahban"},{"name":"P. Ravikumar"},{"name":"M. Wainwright"},{"name":"B. Yu"}],"title":{"text":"A uniﬁed framework for analyzing m-estimators with decomposible regularizers"}},{"authors":[{"name":"V. Chandrasekaran"},{"name":"B. Recht"},{"name":"P. Parillo"},{"name":"A. Wilsky"}],"title":{"text":"The convex geometry of linear inverse problems"}},{"authors":[{"name":"E. Cand`es"},{"name":"B. Recht"}],"title":{"text":"Simple bounds for low-complexity model reconstruction"}},{"authors":[{"name":"F. Bach"}],"title":{"text":"Structured sparsity-inducing norms through submodular func- tions"}},{"authors":[{"name":"D. Gross"}],"title":{"text":"Recovering low-rank matrices from few coefﬁcients in any basis"}},{"authors":[{"name":"J. Wright"},{"name":"A. Ganesh"},{"name":"K. Min"},{"name":"Y. Ma"}],"title":{"text":"Compressive principal component pursuit"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565559.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T9.1","endtime":"15:00","authors":"John Wright, Arvind Ganesh, Kerui Min, Yi Ma","date":"1341326400000","papertitle":"Compressive Principal Component Pursuit","starttime":"14:40","session":"S7.T9: Compressive Sensing","room":"Stratton West Lounge (201)","paperid":"1569565559"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
