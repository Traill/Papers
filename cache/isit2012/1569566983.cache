{"id":"1569566983","paper":{"title":{"text":"Iterative Estimation of Constrained Rank-One Matrices in Noise"},"authors":[{"name":"Sundeep Rangan"},{"name":"Alyson K. Fletcher"}],"abstr":{"text":"Abstract\u2014We consider the problem of estimating a rank-one matrix in Gaussian noise under a probabilistic model for the left and right factors of the matrix. The probabilistic model can impose constraints on the factors including sparsity and positivity that arise commonly in learning problems. We propose a simple iterative procedure that reduces the problem to a sequence of scalar estimation computations. The method is similar to approximate message passing techniques based on Gaussian approximations of loopy belief propagation that have been used recently in compressed sensing. Leveraging analysis methods by Bayati and Montanari, we show that the asymptotic behavior of the estimates from the proposed iterative procedure is described by a simple scalar equivalent model, where the distribution of the estimates is identical to certain scalar estimates of the variables in Gaussian noise. Moreover, the effective Gaussian noise level is described by a set of state evolution equations. The proposed method thus provides a computationally simple and general method for rank-one estimation problems with a precise analysis in certain high-dimensional settings."},"body":{"text":"We consider the problem of estimating vectors u 0 ∈ R m and v 0 ∈ R n from a matrix A ∈ R m×n of the form\nnormalization factor. The problem can be considered as a rank- one special case of ﬁnding a low-rank matrix in the presence of noise. Such low-rank estimation problems arise in a range of applications including blind channel estimation, antenna array processing, subspace system identiﬁcation, and principal component or factor analysis.\nWhen the noise term W is zero, the vector pair (u 0 , v 0 ) can be recovered exactly, up to a scaling, from the maximal left and right singular vectors of A [1]. However, in the presence of noise, the rank-one matrix can in general only be estimated approximately. In this case, a priori information or constraints on ( u 0 , v 0 ) may improve the estimation. Such constraints arise, for example, in factor analysis in statistics, where one of the factors is often constrained to be either positive or sparse [2]. Similar sparsity constraints arise in the problem of dictionary learning [3]. In digital communications, one of the factors could come from a discrete QAM constellation.\nUnfortunately, optimal estimation with constraints on u 0 or v 0 is often computationally intractable. The chief problem is the bilinear nature of the term u 0 v T 0 . However, the term is linear in u 0 and v 0 separately. Thus, many suboptimal estimation methods are performed in an iterative manner, alternately estimating u 0 and v 0 individually, while holding the estimate of the other factor constant.\nThis paper proposes a variant of such alternating optimiza- tion procedures that we call Iterative Factorization (IterFac), stated in detail in Algorithm 1 below. Through proper selection of relevant parameters, the IterFac algorithm can perform alternating minimizations to optimizations of the form\nHere, X F is the Frobenius norm and c U (u) and c V (v) are cost or regularization functions on the left and right factors. In the case when the cost functions are separable, the IterFac algorithm reduces the vector optimization problem to a sequence of scalar optimization problems on the individual components of u and v; it is thus computationally simple. The IterFac methodology is also genearl since the method can be applied to essentially arbitrary separable cost functions.\nOf course, such iterative algorithms are by no means new; they underlie many existing methods, including the classic alternating power method for ﬁnding maximal singular values [1] and some alternating methods in sparse or non- negative dictionary learning [3]\u2013[7]. More recently, an alter- nating method has been proposed for estimation with matrix uncertainties [8], also using an approximate message passing technique. The IterFac iterations also have some similarities to the updates in gradient descent methods in sparse dictionary learning such as in [9].\nOur main contribution is to show that, under a particular setting of a damping parameter, the IterFac algorithm admits an asymptotically-exact characterization when W is i.i.d. Gaussian noise and the components of the true vectors u 0 and v 0 have limiting empirical distributions. In this scenario, we show that the empirical joint distribution of the components of u 0 and the corresponding estimates from the IterFac algorithm are described by a simple scalar equivalent model where the IterFac component estimates are identically distributed to scalar estimates of the variables corrupted by Gaussian noise. Moreover, the effective Gaussian noise level in this model is described by a simple set of scalar state evolution (SE) equations. From the scalar equivalent model, one can compute the asymptotic value of almost any component- separable metric including mean-squared error or correlation. Thus, in addition to being computationally simple and general, the IterFac algorithm admits a precise analysis in the case of Gaussian noise. Moreover, since ﬁxed points of the IterFac\nalgorithm correspond, under suitable circumstances, to local minima of objectives such as (3), the analysis can be used to characterize the behavior of such minima\u2014even if alternate algorithms to IterFac are used.\nThe main analytical tool is a recently-developed technique by Bayati and Montanari [10] used in the analysis of ap- proximate message passing (AMP) algorithms. AMP methods are Gaussian approximations of loopy belief propagation for estimation of vectors under large random linear measurements. The work [10] applied AMP techniques to compressed sens- ing, and proved that, in the limit for large Gaussian mixing matrices, the behavior of AMP estimates can be described by a scalar equivalent model with effective noise levels deﬁned by certain scalar state evolution (SE) equations. Similar SE analyses have appeared in related contexts [11]\u2013[16]. To prove the SE equations for the IterFac algorithm, we apply a key theorem from [10] with a simple change of variables and a slight modiﬁcation to account for parameter adaptation. A complete version of this paper is available at [17], which contains all the proofs and more details.\nFor a matrix A ∈ R m×n , we consider the following iterative algorithm for estimating the rank-one factors u 0 and v 0 .\nRequire: Matrix A ∈ R m×n and factor selection functions G u (t, p, λ u ) and G v (t, q, λ v ).\nThe output of the algorithm, ( u(t), v(t)), t = 0, 1, . . ., is a sequence of estimates for ( u 0 , v 0 ). The algorithm has several parameters including the initial conditions, the parameters in lines 5 and 9, the termination condition and, most importantly, the functions G u (·) and G v (·). In each iteration, the functions G u (·) and G v (·) are used to generate the estimates of the factors u(t) and v(t). G u (·) and G v (·) will thus be called the factor selection functions.\nTo understand the role of the factor selection functions, suppose that we wish to perform the optimization (2) for some regularization functions c U (·) and c V (·). Consider the factor\nselections given by the minimization G u (t, p, λ u )\n−p T u + c U (u) + λ u 2 u 2 , (4a) G v (t, q, λ v )\nλ u (t) := μ u (t) + v(t) 2 /m, \t (5a) λ v (t) := μ v (t) + u(t+1) 2 /m. \t (5b)\nWith the factor selection functions (4) and non-negative values of the coefﬁcients μ u (t) and μ v (t), it is shown in the full paper [17] that the objective function is monotonically decreasing,\nwhere the coefﬁcients μ u (t) and μ v (t) play a role in damping the steps.\nWe analyze the algorithm under the following assumptions. Assumption 1: Consider a sequence of random realizations\nof the estimation problem in Section I indexed by the dimen- sion n. The matrix A and the parameters in Algorithm 1 satisfy the following:\n(a) For each n, the output dimension m = m(n) is deter- ministic and scales linearly with the input dimension in that\n(b) The matrix A has the form (1) where u 0 ∈ R m and v 0 ∈ R n represent \u201ctrue\u201d left and right factors of a rank one term, and W ∈ R m×n is an i.i.d. Gaussian matrix with components W ij ∼ N (0, τ w ) for some τ w > 0.\n(c) The factor selection functions G u (t, p, λ u ) and G v (t, q, λ v ) in lines 7 and 11 are componentwise separable in that for all component indices i and j,\nG u (t, p, λ u ) i = G u (t, p i , λ u ), \t (8a) G v (t, q, λ v ) j = G v (t, q j , λ v ), \t (8b)\nfor some scalar functions G u (t, p, λ u ) and G v (t, q, λ v ) satisfying certain Lipschitz continuity and differentiabil- ity assumptions, see [17].\n(d) The damping factors are selected by the rules μ u (t+1) = − τ w m n\n(e) The parameters λ u (t) and λ v (t) are are computed via an empirical mean of the functions of ( v 0 , v(t)) and (u 0 , u(t+1) \u2013 see [17] for details.\nθ u (t) = (u 0i , u i (t)), i = 1, . . . , m , (10a) θ v (t) = (v 0j , v j (t)), j = 1, . . . , n . (10b)\nFor t = 0, as n → ∞ the sets θ u (0) and θ v (t) empirically converge with bounded moments of order 2 p − 2 for some p ≥ 1 to some random variable pairs (U 0 , U(0) and ( V 0 , V (0)). See [17] for a precise deﬁnition of the empirical convergence used here.\nA complete discussion of the assumptions can be found in the full paper [17].\nOur main result, Theorem 1 below, will provide a charac- terization of the asymptotic empirical distribution for the sets θ u (t) and θ v (t) in (10). Speciﬁcally, we will show that, for all t, the sets have empirical limits of the form\nθ u (t) d = (U 0 , U(t)), \t (11a) lim\nfor some random variable pairs ( U 0 , U(t) and (V 0 , V (t)). The distributions of the random variables ( U 0 , U(t) and (V 0 , V (t)) can be described recursively in t as follows: For t = 0, (V 0 , V (0)) is the random variable pair in the initial condition Assumption 1(f). Then, for t ≥ 0, (U 0 , U(t+1)) is given by\nU(t+1) = G u (t, P (t), λ u (t)), \t (12a) P (t) = βα v1 (t)U 0 + Z u (t), \t (12b)\nwhere U 0 is the random variable in Assumption 1(f); G u (·) is the scalar function in Assumption 1(d); and Z u (t) is Gaussian noise independent of U 0 . Thus, U(t+1) is distributed identically to the output of the scalar function G u (t, P (t)), where P (t) is a scaled and Gaussian noise-corrupted version of the true variable U 0 . Similarly, for t ≥ 0, the random variable (V 0 , V (t+1)) in (11b) is given by\nV (t+1) = G v (t, Q(t), λ v (t)), \t (13a) Q(t) = α u1 (t+1)V 0 + Z v (t), \t (13b)\nwhere V 0 is the empirical limit in in Assumption 1(f), and Z v (t) is Gaussian noise independent of V 0 .\nThe constants α u0 (t), α u1 (t), α v0 (t), α v1 (t), λ u (t), and λ v (t) in (12) and (13) are deterministic sequences of scalars deﬁned recursively by\nα u0 (t) = E U(t) 2 , α u1 (t) = E [U 0 U(t)] , (14a) α v0 (t) = E V (t) 2 , α v1 (t) = E [V 0 V (t)] , (14b)\nλ u (t) = E [φ λu (V 0 , V (t))] , \t (15a) λ v (t) = E [φ λv (U 0 , U(t+1))] , \t (15b)\nwhich are initialized with the values α v0 (0) and α v1 (0) from the joint distribution of ( V 0 , V (0)) in Assumption 1(f). The subsequent values of α u0 (t), α u1 (t), α v0 (t), α v1 (t), λ u (t) and λ v (t) can then be computed recursively through (14) along with the deﬁnition of the variables in (12) and (13). In line with [10] upon which our analysis is based, we will call the set of recursive equations (12)\u2013(15) the state evolution (SE) equations. From the solution to the SE equations, we get the exact descriptions of the limits in (11).\nTheorem 1: Under Assumption 1, the sets θ u (t) and θ v (t) in (10) converge empirically with bounded moments of order p with the limits in (11).\nThe main contribution of Theorem 1 is that it provides a simple scalar equivalent model for the asymptotic behav- ior of the algorithm. The sets θ u (t) = {(u 0i , u i (t))} and θ v (t) = {(v 0j , v j (t))} in (10) are the components of true vectors u 0 and v 0 and their estimates u(t) and v(t). The theorem shows that empirical distribution of these components are asymptotically equivalent to simple random variable pairs (U 0 , U(t)) and (V 0 , V (t)) given by (12a) and (13a). Following [18], we can thus call the result a single-letter characterization.\nFrom this single-letter characterization, one can exactly compute a large class of performance metrics of the algorithm. Speciﬁcally, the empirical convergence of θ u (t) shows that for any pseudo-Lipschitz function φ(u 0 , u) of order p, the following limit exists almost surely:\nwhere the expectation on the right-hand side is over the vari- ables ( U 0 , U(t)) with U 0 identical to the variable in the limit in Assumption 1(f) and U(t) given by (12a). This expectation can be explicitly evaluated by a simple two-dimensional integral and consequently any component-separable performance met- ric based on a suitably continuous loss function φ(u 0 , u), such as mean-squared error can be exactly computed. The full paper [17] shows, in particular, that one can evaluate the asymptotic correlations\n(t)τ u , (17a) ρ v (t) := lim n→∞ |v(t) T v 0 | 2 v(t) 2 v\nwhich represent the cosines of the angles between the esti- mates u(t) and v(t) and the true vectors u 0 and v 0 .\nAs a ﬁrst simple application of the SE analysis, suppose we use linear selection functions of the form\nwhere the parameters λ u and λ v allow for normalization or other scalings of the outputs. Linear selection functions of the form (18) arise when one selects G u (·) and G v (·) from (4) with zero cost functions, c U (u) = c V (v) = 0. With zero cost functions, the correct solution to the optimization (2) is for ( u, v) to be the (appropriately scaled) left and right maximal singular vectors of A. We will thus call the estimates (u(t), v(t)) of Algorithm 1 and linear selection functions (18) the estimated maximal singular vectors.\nTheorem 2: Consider the state evolution equations (12), (13), and (14) with the linear selection functions (18). Then:\n(a) The asymptotic correlation coefﬁcients (17a) and (17b) satisfy the following recursive rules:\nτ v ρ v (t) + τ w , \t (19a) ρ v (t) = τ u τ v ρ u (t) τ\nτ v ρ u (t) + τ w . \t (19b) (b) For any positive initial condition, ρ v (0) > 0, the asymp-\ntotic correlation coefﬁcients converge to the limits lim\nτ v (τ u τ v + τ w ) , (20b) where [ x] + = max{0, x}.\nThe theorem provides a set of recursive equations for the asymptotic correlation coefﬁcients ρ u (t) and ρ v (t) along with simple expressions for the limiting values as t → ∞. We thus obtain exactly how correlated the estimated maximal singular vectors of a matrix A of the form (1) are to the rank one factors (u 0 , v 0 ). The proof of the theorem also provides expressions for the second-order statistics in (14) to be used in the scalar equivalent model.\nThe ﬁxed point expressions (20) agree with the more general results in [19] that derive the correlations for ranks greater than one and low-rank recovery with missing entries. Similar results can also be found in [20]. An interesting consequence of the expressions in (20) is that unless\nτ v > τ w , the asymptotic correlation coefﬁcients are exactly zero. The ratio τ u τ v /τ w can be interpreted as a scaled SNR.\nNext suppose that the priors on U 0 and V 0 are known. In this case, given (12a) and (13a), a natural choice for the factor selection functions are\nwhich are the MMSE estimates of the variables. In this example, there are no parameters λ u or λ v . We can use the initial condition v j (0) = E[V 0 ] for all j, so that the initial variable in Assumption 1(f) is V (0) = E[V 0 ]. To analyze the algorithms deﬁne\nE u (η u ) := var(U 0 | Y = √ η u U 0 + D), (22a) E v (η v ) := var(V 0 | Y = √ η v V 0 + D), (22b)\nwhere D ∼ N (0, 1) is independent of U 0 and V 0 . That is, E u (η u ) and E v (η v ) are the mean-squared errors of estimating U 0 and V 0 from observations Y with SNRs of η u and η v . The functions E u (·) and E v (·) arise in a range of estimation problems and the analytic and functional properties of these functions can be found in [21], [22].\nTheorem 3: Consider the solutions to the SE equations (12), (13), and (14) under the MMSE selection functions (21) and initial condition V (0) = E[V 0 ]. Then:\n(a) For all t, the asymptotic correlation coefﬁcients (17a) and (17b) satisfy the recursive relationships\nE u (βτ v ρ v (t)/τ w ), (23a) ρ v (t) = 1 − 1 τ\n(b) If, in addition, E u (η u ) and E v (η v ) are continuous, then for any positive initial condition, ρ v (0) > 0, as t → ∞, the asymptotic correlation coefﬁcients ( ρ u (t), ρ v (t)) increase monotonically to ﬁxed points ( ρ ∗ u , ρ ∗ u ) of (23) with ρ ∗ v > 0.\nAgain, we see that we can obtain simple, explicit recursions for the asymptotic correlations.\nTo validate the SE analysis, we consider a simple case where the left factor u 0 ∈ R m is i.i.d. Gaussian, zero mean and v 0 ∈ R n has Bernoulli-Exponential components:\nwhich provides a simple model for a sparse, positive vector. The parameter λ is the fraction of nonzero components and is set in this simulation to λ = 0.1. The dimensions are (m, n) = (1000, 500), and the noise level τ w is set according to the scaled SNR deﬁned as\nEstimating the vector v 0 in this set-up is related to ﬁnding sparse principal vectors of the matrix A T A [2], [23]\u2013[27].\nThe results of the simulation are shown in Fig. 1, which shows the simulated and SE-predicted performance of the IterFac algorithm with both the linear and MMSE selection functions for the priors on u and v. The algorithm is run for t = 10 iterations and the plot shows the median of the ﬁnal correlation coefﬁcient ρ v (t) over 50 Monte Carlo trials at each value of SNR. It can be seen that the performance of the IterFac algorithm for both the linear and MMSE estimates are in excellent agreement with the SE predictions. The correlation coefﬁcient of the linear estimator also matches the correlation of the estimates produced from the maximal singular vectors of A. This is not surprising since, with linear selection functions, the IterFac algorithm is essentially an iterative method to ﬁnd the maximal singular vectors. The ﬁgure also shows the beneﬁt of exploiting the prior on v 0 , which is evident from the\nsuperior performance of the MMSE estimate over the linear reconstruction.\nWe have presented a computationally-efﬁcient method for estimating rank-one matrices in noise. The estimation problem is reduced to a sequence of scalar AWGN estimation problems which can be performed easily for a large class of priors or regularization functions on the coefﬁcients. In the case of Gaussian noise, the asymptotic performance of the algorithm is exactly characterized by a set of scalar state evolution equations which appear to match the performance at moderate dimensions well. Thus, the methodology is computationally simple, general and admits a precise analysis in certain asymptotic, random settings. However, to make the algorithm practical, we need to extend the method to higher ranks and handle the case where the priors are not known. The full paper [17] suggests some methods for addressing these issues in the future.\nThe authors thank Vivek Goyal, Ulugbek Kamilov, Andrea Montanari and Phil Schniter for detailed comments on an earlier draft."},"refs":[{"authors":[{"name":"R. A. Hor"},{"name":"C. R. Johnso"}],"title":{"text":"Matrix Analysis"}},{"authors":[{"name":"J. Cadima"},{"name":"I. T. Jolliffe"}],"title":{"text":"Loadings and correlations in the interpre- tation of principal components"}},{"authors":[{"name":"B. A. Olshausen"},{"name":"D. J. Field"}],"title":{"text":"Natural image statistics and efﬁcient coding"}},{"authors":[],"title":{"text":"Sparse coding with an overcomplete basis set: A strategy employed by v1?"}},{"authors":[{"name":"D. D. Lee"},{"name":"H. S. Seung"}],"title":{"text":"Algorithms for non-negative matrix factorization"}},{"authors":[{"name":"M. S. Lewicki"},{"name":"T. J. Sejnowski"}],"title":{"text":"Learning overcomplete representa- tions"}},{"authors":[{"name":"M. Aharon"},{"name":"M. Elad"},{"name":"A. Bruckstein"}],"title":{"text":"K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation"}},{"authors":[{"name":"J. T. Parker"},{"name":"V. Cevher"},{"name":"P. Schniter"}],"title":{"text":"Compressive sensing under matrix uncertainties: An approximate message passing approach"}},{"authors":[{"name":"J. Mairal"},{"name":"F. Bach"},{"name":"G. Sapiro"},{"name":"J. Ponce"}],"title":{"text":"Online dictionary learning for sparse coding"}},{"authors":[{"name":"M. Bayati"},{"name":"A. Montanari"}],"title":{"text":"The dynamics of message passing on dense graphs, with applications to compressed sensing"}},{"authors":[{"name":"J. Boutros"},{"name":"G. Caire"}],"title":{"text":"Iterative multiuser joint decoding: uniﬁed framework and asymptotic analysis"}},{"authors":[{"name":"A. Montanari"},{"name":"D. Tse"}],"title":{"text":"Analysis of belief propagation for non-linear problems: The example of CDMA (or: How to prove Tanaka\u2019s formula)"}},{"authors":[{"name":"D. Guo"},{"name":"C.-C. Wang"}],"title":{"text":"Asymptotic mean-square optimality of belief propagation for sparse linear systems"}},{"authors":[],"title":{"text":"Random sparse linear systems observed via arbitrary channels: A decoupling principle"}},{"authors":[{"name":"S. Rangan"}],"title":{"text":"Estimation with random linear mixing, belief propagation and compressed sensing"}},{"authors":[],"title":{"text":"Generalized approximate message passing for estimation with random linear mixing"}},{"authors":[{"name":"S. Rangan"},{"name":"A. K. Fletcher"}],"title":{"text":"Iterative Estimation of Constrained Rank-One Matrices in Noise"}},{"authors":[{"name":"D. Guo"},{"name":"D. Baron"},{"name":"S. Shamai"}],"title":{"text":"A single-letter characterization of optimal noisy compressed sensing"}},{"authors":[{"name":"R. Keshavan"},{"name":"A. Montanari"},{"name":"S. Oh"}],"title":{"text":"Matrix completion from a few entries"}},{"authors":[{"name":"M. Capitaine"},{"name":"C. Donati-Martin"},{"name":"D. F´eral"}],"title":{"text":"The largest eigenvalues of ﬁnite rank deformation of large Wigner matrices: Convergence and nonuniversality of the ﬂuctuations"}},{"authors":[{"name":"D. Guo"},{"name":"Y. Wu"},{"name":"S. Shamai"},{"name":"S. Verd´u"}],"title":{"text":"Estimation in Gaussian Noise: Properties of the Minimum Mean-Square Error"}},{"authors":[{"name":"Y. Wu"},{"name":"S. Verd´u"}],"title":{"text":"Functional Properties of Minimum Mean-Square Error and Mutual Information"}},{"authors":[{"name":"I. T. Jolliffe"},{"name":"N. Trendaﬁlov"},{"name":"M. Uddin"}],"title":{"text":"A modiﬁed principal component technique based on the LASSO"}},{"authors":[{"name":"Z. Zhang"},{"name":"H. Zha"},{"name":"H. Simon"}],"title":{"text":"Low rank approximations with sparse factors I: Basic algorithms and error analysis"}},{"authors":[{"name":"H. Zou"},{"name":"T. Hastie"},{"name":"R. Tibshirani"}],"title":{"text":"Sparse principal component analysis"}},{"authors":[{"name":"A. d\u2019Aspremont"},{"name":"L. El Ghaoui"},{"name":"M. I. Jordan"},{"name":"G. R. G. Lanckriet"}],"title":{"text":"A direct formulation for sparse PCA using semideﬁnite programming"}},{"authors":[{"name":"H. Shena"},{"name":"J. Z. Huang"}],"title":{"text":"Sparse principal component analysis"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566983.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T7.4","endtime":"16:00","authors":"Sundeep Rangan, Alyson Fletcher","date":"1341330000000","papertitle":"Iterative Estimation of Constrained Rank-One Matrices in Noise","starttime":"15:40","session":"S7.T7: Approximate Belief Propagation","room":"Stratton (407)","paperid":"1569566983"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
