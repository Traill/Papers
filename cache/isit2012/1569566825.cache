{"id":"1569566825","paper":{"title":{"text":"Adaptive sensing using deterministic partial Hadamard matrices"},"authors":[{"name":"S. Haghighatshoar"},{"name":"E. Abbe"},{"name":"E. Telatar"}],"abstr":{"text":"Abstract\u2014This paper investigates the construction of determin- istic measurement matrices preserving the entropy of a random vector with a given probability distribution. In particular, it is shown that for a random vector with i.i.d. discrete components, this is achieved by selecting a subset of rows of a Hadamard matrix such that (i) the selection is deterministic (ii) the fraction of selected rows is vanishing. In contrast, it is shown that for a random vector with i.i.d. continuous components, no entropy preserving measurement matrix allows dimensionality reduction. These results are in agreement with the results of Wu-Verdu on almost lossless analog compression and provide a low-complexity measurement matrix. The proof technique is based on a polar code martingale argument and on a new entropy power inequality for integer-valued random variables.\nIndex Terms\u2014Entropy-preserving matrices, Analog compres- sion, Compressed sensing, Entropy power inequality."},"body":{"text":"Information theory has extensively studied the lossless and lossy compression of discrete time signals into digi- tal sequences. These problems are motivated by the model of Shannon, where an analog signal is ﬁrst acquired, by sampling it at a high enough rate to preserve all of its information (Nyquist-Shannon sampling theorem), and then compressed. More recently, it was realized that proceeding to \u201cjoint sensing-compression\u201d schemes can be beneﬁcial. In particular, compressed sensing introduces the perspective that sparse signals can be compressively sensed to decrease mea- surement rate. As for joint source-channel coding schemes, one may wonder why this would be useful? Eventually, the signal is represented with the same amount of bits, so why would it be preferable to proceed jointly or separately? In a nutshell, if measurements are expensive (such as for example in certain bio-medical applications), then compressed sensing is beneﬁcial.\nFrom an information-theoretic perspective, compressed sensing can be viewed as a form of analog to analog com- pression, namely, transforming a higher dimensional discrete time signal into a lower-dimensional one over the reals, without \u201closing information\u201d. The key point is that, since measurements are analog, one may as well pack as much information in each measurement (whereas in the compression of discrete signals, a measurement on a larger alphabet is more expensive than a measurement in bits). However, compressing a vector in R n into a vector in R m , m < n, without regularity constraints is not an interesting problem, since R n and R m have the same cardinality.\nRecently, [1] introduced a more reasonable framework to study analog compression from an information-theoretic per- spective. By requiring the encoder to be linear and the decoder to be Lipschitz continuous, the fundamental compression limit is shown to be the R´enyi information dimension. The setting of [1] also raises a new interesting problem: in the same way that coding theory aims at approaching the Shannon limit with low-complexity schemes, it is a challenging problem to devise efﬁcient schemes to reach the R´enyi dimension. For example, in compressed sensing, with O(k log(n/k)) instead of O(k) measurements, k-sparse signals can be reconstructed using l 1 minimization, which is a convex optimization problem, rather than l 0 minimization, which is intractable [6], [7]. Hence, in general, complexity requirements may raise the measurement rate.\nThe scope of this paper is precisely to investigate what measurement rates can be achieved by taking into account the complexity of the sensing matrix, which in turn, inﬂuences the complexity of the reconstruction algorithm. Our goal is to consider signals that are memoryless and drawn from a probability distribution on R, which may be purely atomic, purely continuous or mixed. It is legitimate to attempt reaching this goal by borrowing tools from coding theory, in particular from codes achieving least compression rates in the discrete setting. Our approach is based on using Hadamard matrices for encoding (taking measurements) and developing a counter-part of the polar technique [2], [3] with arithmetic over R (or Z for atomic distributions) rather than F 2 or F q . The proof technique uses a martingale argument as in polar codes and a new form of entropy power inequality for discrete distributions. Rigorous results are obtained and the sensing matrix construction is deterministic. A nested property is also investigated which allows one to adapt the measurement rate to the sparsity level of the signal.\nRecently, spatially-coupled LDPC codes have allowed to achieve rigorous results in coding theory. This approach has also been exploited by [4], [5], which proposes the use of spatially coupled matrices for sensing. In [5], the mixture case is covered and further analysis on the reconstruction algorithm is provided. However, the sensing matrix is still random. It is known that Hadamard matrices truncated randomly afford desirable properties for compressed sensing. We extend this work and show that by knowing signal distribution, Hadamard matrices can be truncated deterministically to achieve a min- imal measurement rate.\nLet X 1 , X 2 , . . . , X N be i.i.d. Bernoulli (p) random vari- ables, where N = 2 n for some n ∈ Z + . We use the notation a j i for the column vector (a i , a i +1 , . . . , a j ) t and set a j i to null if j < i. We also deﬁne [r] = {i ∈ Z : 1 ≤ i ≤ r}. Let G N = 1 1 0 1\nand let Y N 1 = G N X N 1 , with arithmetic over F 2 . Deﬁne H i = H(Y i |Y i− 1 1 ), i ∈ [N], to be the conditional entropy of Y i given Y i− 1 1 . In [3], Arikan shows that for any δ > 0 and for large N , the values H i , i ∈ [N], polarize to 0 or 1. This provides a compression scheme achieving the least compression rate, since for any δ ∈ (0, 1)\nFrom another point of view, every Y i is associated with a speciﬁc row of the matrix G N and (1) indicates that the \u201cmeasurement\u201d rate required to extract the informative components is close to the entropy of the source H(X) for large N .\nIn signal acquisition, measurements are analog. Hence, one could consider Y N 1 = G N X N 1 with arithmetic over the real ﬁeld and investigate if any \u201cpolarization phenomenon\u201d occurs. The difference is that, in this case, the measurement alphabet is unbounded. In particular, the H i values are not bounded from above.\nDeﬁnition 1 (Restricted iso-entropy property). Let X N 1 be discrete i.i.d. random variables with a probability distribution p X supported on a ﬁnite set. The family {Φ N } of measurement matrices, where Φ N has dimension m N × N, is -REP(p X ) with measurement rate ρ if\nIn general, the labeling N can be any subsequence of Z + . We will consider N = 2 n , n ∈ Z + .\nDeﬁnition 2. Let X N 1 be continuous (or mixture) i.i.d. random variables with probability distribution p X . The family of measurement matrices {Φ N } of dimension m N × N is ( , γ)- REP (p X ) with measurement rate ρ if\n1) there exists a single letter quantizer Q : R → Z such that M.M.S.E. of X given Q(X) is less that γ,\n1) Given a probability distribution p X over a ﬁnite set, and > 0, is there a family of measurement matrices that\nis -REP and has measurement rate ρ? What is the set of all possible ( , ρ) pairs? Is it possible to construct a family of truncated Hadamard matrices with a minimal measurement rate? How is the truncation adapted to the distribution p X ?\n2) Is it possible to obtain an asymptotic measurement rate below 1 for continuous distributions?\nRemark 1. The RIP notion, introduced in [6], [7], is useful in compressed sensing, since it guarantees l 2 -stability of the recovery algorithm. We consider truncated Hadamard matrices satisfying -REP condition and since they have a Kronecker structure, we obtain a low-complexity reconstruction algorithm using the maximum likelihood decoding. However, this part is not emphasized in this paper, and we mainly focus on the construction of the truncated Hadamard matrices. Section VI provides numerical simulations of a divide and conquer ML decoding algorithm and illustrates the robustness of the recovery to noise.\nthe family of Hadamard matrices. Suppose X N 1 are discrete i.i.d. random variables with distribution p X over a ﬁnite subset of Z. Let Y N 1 = J N X N 1 and deﬁne H i = H(Y i |Y i− 1 1 ) and m N = # {i ∈ [N] : H i > }. The ( , p X )- truncated Hadamard family { ¯ J N }, is the set of matrices of dimension m N ×N obtained by selecting those rows of J N with H i > . Theorem 1 (Absorption phenomenon). Let X be a discrete random variable with a probability distribution p X supported on a ﬁnite subset of Z. For a ﬁxed > 0, the family of ( , p X )-truncated Hadamard matrices { ¯ J N , N = 2 n , n ∈ Z + } (deﬁned above) are -REP (p X ) with measurement rate 0. In other words,\nRemark 2. Although all of the measurement matrices ¯ J N are constructed by truncating the matrices J N , the order and number of the selected rows, m N , to construct ¯ J N depends on the distribution p X .\nTheorem 2 (An EPI over Z). For any probability distribution p over Z,\nRemark 3. This theorem complements the work in [8] to ob- tain an entropy power inequality for discrete random variables.\nFor continuous distributions, and for any ﬁxed distortion γ, the measurement rate approaches 1 as tends to 0. This result has been shown in [1] in a more general context. We recover this result in our setting for the case of a uniform distribution over [ −1, 1].\nLemma 1. Let p U be the uniform distribution over [ −1, 1] and let Q : [ −1, 1] → {0, 1, . . . , q − 1} be a uniform quantizer for X with M.M.S.E. less than γ. Assume that {Φ N } is a family of full rank measurement matrices of dimension m N × N. If {Φ N } is ( , γ)-REP(p U ), then the measurement rate, ρ, goes to 1 as tends to 0.\nThe entropy power inequality for continuous and indepen- dent random variables X and Y is\nwhere h denotes the differential entropy. If X and Y have the same density p, then (3) becomes\nwhich implies a guaranteed increase of the differential entropy. For this reason, we call (2) an EPI for discrete random variables.\nLemma 2. Let c > 0 and suppose p is a probability measure over Z such that H(p) = c. Then, for any i ∈ Z,\nwhere h 2 (x) = −x log 2 (x) − (1 − x) log 2 (1 − x) is the binary entropy function and p i denotes the probability of i.\nLemma 3. Let c > 0, 0 < α ≤ 1 2 and n ∈ Z. Assume that p is a probability measure on Z such that α ≤ p((−∞, n]) ≤ 1−α and H(p) = c, then\nare scaled restrictions of p to ( −∞, n] and [n + 1, ∞) respectively.\nProof of Theorem 2: Suppose that p is a distribution over Z with H(p) = c. Set y = p ∞ . There is an α ≥ 1−y 2 and an integer n such that α ≤ p((−∞, n]) ≤ 1 − α. Using Lemma 2 and Lemma 5, it results that H(p p) − c ≥ g(c) where\nIt is easy to check that g(c) is a continuous function of c. The monotonicity of g follows from the fact that cy −(1+y)h 2 (y) is an increasing function of c for every y ∈ [0, 1]. For strict positivity, note that (1 − y) 4 is strictly positive for y ∈ [0, 1) and it is 0 when y = 1, but lim y→ 1 cy − (1 + y)h 2 (y) = c. Hence for c > 0, g(c) > 0. If c = 0 then\nand its minimum over [0, 1] is 0. For asymptotic behavior, note that at y = 0, cy − (1 + y)h 2 (y) = 0 and (1−y) 4 8 log(2) = 1 8 log(2) . Hence, from continuity, it results that g(c) ≤ 1 8 log(2) for any c ≥ 0. Also for any > 0 there exists a c 0 such that for any < y ≤ 1, cy − (1 + y)h 2 (y) ≥ 1 8 log(2) . Thus for any\n> 0 there is a c 0 such that for c > c 0 , the outer minimum over y in g(c) is achieved on [0, ]. Hence, for any c > c 0 , g(c) ≥ (1− ) 4 8 log(2) . This implies that for every > 0,\nAssume that X N 1 , N = 2 n , n ∈ Z + , is a set of i.i.d. random variables with probability distribution p X over a ﬁnite subset of Z. Let Y N 1 = J N X N 1 , where J N = 1 1 −1 1\nis the Hadamard matrix of dimension N and let H i = H(Y i |Y i− 1 1 ), i ∈ [N], be the conditional entropy values.\nLemma 6. Let Z N 1 = G N X N 1 where G N is the same as before. Assume that ˜ H i = H(Z i |Z i− 1 1 ), i ∈ [N], then H i =\nLemma 7. Let Z N 1 and ˜ H i , i ∈ [N], be as in Lemma 6. Suppose W N 1 = B N Z N 1 , where B N is the bit shufﬂing matrix introduced in [3] and deﬁne H i = H(W i |W i− 1 1 ), then ˜ H i = H i , i ∈ [N].\nRemark 4. Lemma 6 demonstrates the equivalence of J and G to compute the conditional entropies. However, in application,\nit is preferred to use J because its rows are orthogonal. Lemma 7 shows that shufﬂing Z N 1 by B N does not change the entropy values. For simplicity of the proof, we use the matrix B N G N and relate to the polar code notations in [2], [3].\nNotice that we can represent B N G N in a recursive way. Let us deﬁne two binary operation ⊕ and as follows\n(a, b) = a + b ⊕(a, b) = b,\nwhere + is the usual integer addition. It is easy to see that we can do the multiplication by B N G N in a recursive way. Figure 2 shows a simple case for B 4 G 4 . The − or + sign on an arrow shows that the result for that arrow is obtained by applying a or ⊕ operation to two input operands. If we\noperations on the input random variables which results in Y m . An easy way to ﬁnd this sequence of operations is to write the binary expansion of m − 1. Then, each 0 in this expansion corresponds to a \t operation and each 1 corresponds to a ⊕ operation. Using this binary labeling, we deﬁne a binary stochastic process. Assume that Ω = {0, 1} Z + , and F is the σ-algebra generated by the cylindrical sets\nfor every integer s and i 1 , i 2 , · · · , i s . We also construct the ﬁltration {F n , n ≥ 0}, where F n ⊂ F is the σ-algebra generated by the ﬁrst n coordinates of ω and F 0 = {∅, Ω} is the trivial σ-algebra. We deﬁne the uniform probability measure µ over the cylindrical sets by µ(S (i 1 ,i 2 ,··· ,i n ) ) = 1 2 n . This measure can be uniquely extended to F.\nLet [ω] n = ω 1 ω 2 . . . ω n denote the ﬁrst n coordinates of ω = ω 1 ω 2 . . . , and let Y [ω] n denote the random variable Y i , where the binary expansion of i − 1 is [ω] n . Let us deﬁne Y [ω] n = {Y [η] n : [η] n < [ω] n }. We also deﬁne the random variable I n by\nwhere it is seen that I n (ω) depends on the ﬁrst n coordinates of ω. It is also important to note that if ω n +1 = 0\nwhere ˜ denotes an independent copy of the corresponding random element.\nProof of Theorem 1: Assume that Y N 1 = J N X N 1 , for N = 2 n , n ∈ Z + , and H i = H(Y i |Y i− 1 1 ), i ∈ [N]. Also ﬁx\nK n = {i : i ∈ [N], H i > }, Y [K n ] = {Y j : j ∈ K n }.\nHence, by Deﬁnition 3, |K n | = m N and ¯ J N is obtained from J N by selecting the rows with index in K n . We have\nThis shows that the family { ¯ J N } is -REP. Now we show that the measurement rate of this family is 0. To prove this, we use Lemma 6 and Lemma 7 and construct the martingale I n by (4). I n is a positive martingale and converges to a random variable I ∞ almost surely. Our aim is to show that for any two positive numbers a and b where a < b, µ(I ∞ ∈ (a, b)) = 0, which implies that µ(I ∞ ∈ {0, ∞}) = 1. Since I n is a martingale, E {I n } = E{I 0 } = H(X) < ∞. Using Fatou\u2019s lemma, we obtain\nwhich implies that µ(I ∞ = ∞) = 0. Hence, I n converges almost surely to 0 and it also converges to 0 in probability. In other words, given > 0,\nThis implies that for any > 0 the measurement rate ρ is 0. Hence, it is sufﬁcient to prove that for any two positive numbers a and b, where a < b, µ(I ∞ ∈ (a, b)) = 0. Fix a δ > 0 then for every ω in the convergence set there is an n 0 such that for n > n 0 , |I n +1 (ω) − I n (ω) | < δ. This implies that for n > n 0\nUsing (5) and the entropy power inequality (2), it results that 0 ≤ I n (ω) < ρ(δ) where ρ(δ) can be obtained from the EPI curve in Figure 1. This implies that I n must converge to 0 and this completes the proof.\nFor simulation, we use a binary random variable, where p X (0) = 1 − p for some 0 < p ≤ 1 2 .\nFigure 3 shows the absorption phenomenon for p = 0.05 and N = 256, 512.\nAs illustrated in Figure 4, for N = 512 and different values of p, the conditional entropies at a ﬁxed component are ordered when p varies. We call this the \u201cnested\u201d property. This allows to construct robust decoders when the value of p is not exactly known by using the \u2018worst-case\u2019 value of p.\nFigure 5 shows the stability analysis of the ML recon- struction algorithm to i.i.d. N (0, σ 2 ) measurement noise. For simulation, we used N = 512, p = 0.05 and a 0.01-REP measurement matrix by keeping all of the rows of the matrix J N with indices in the set K. Note that the ML decoder exploits the recursive structure of the Hadamard matrix to perform computations more efﬁciently. We deﬁne the signal to noise ratio at the input and output of the decoder as:\nwhere ˆ X i is the output of the ML decoder. The result shows approximately 4 dB loss in SNR for high SNR regime.\nE.A. would like to thank M. Madiman and A. Barron for stimulating discussions on this problem at Yale University, and P. Vandergheynst for helpful comments."},"refs":[{"authors":[{"name":"Y. Wu"},{"name":"S. Verd´u"}],"title":{"text":"R´enyi information dimension: fundamental limits of almost lossless analog compression"}},{"authors":[{"name":"E. Arikan"}],"title":{"text":"Channel polarization: A method for constructing capacity- achieving codes for symmetric binary-input memoryless channels"}},{"authors":[{"name":"E. Arikan"}],"title":{"text":"Source polarization"}},{"authors":[{"name":"F. Krzakala"},{"name":"M. M´ezard"},{"name":"F. Sausset"},{"name":"Y. Sun"},{"name":"L. Zdeborova"}],"title":{"text":"Statistical physics-based reconstruction in compressed sensing"}},{"authors":[{"name":"D. L. Donoho"},{"name":"A. Javanmard"},{"name":"A. Montanari"}],"title":{"text":"Information-Theoretically optimal compressed sensing via spatial coupling and approximate mes- sage passing"}},{"authors":[{"name":"E. J. Cand`es"},{"name":"T. Tao"}],"title":{"text":"Decoding by linear programming"}},{"authors":[{"name":"E. J. Cand`es"},{"name":"T. Tao"}],"title":{"text":"Near-optimal signal recovery from random projec- tions: universal encoding strategies"}},{"authors":[{"name":"O. Johnson"},{"name":"Y. Yu"}],"title":{"text":"Monotonicity, thinning, and discrete versions of the entropy power inequality"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566825.pdf"},"links":[{"id":"1569565383","weight":5},{"id":"1569565883","weight":5},{"id":"1569565223","weight":5},{"id":"1569566385","weight":11},{"id":"1569564635","weight":5},{"id":"1569565867","weight":5},{"id":"1569559617","weight":5},{"id":"1569566321","weight":5},{"id":"1569566683","weight":11},{"id":"1569566227","weight":5},{"id":"1569565551","weight":5},{"id":"1569552245","weight":5},{"id":"1569565227","weight":5},{"id":"1569567005","weight":5},{"id":"1569566469","weight":5},{"id":"1569565355","weight":11},{"id":"1569564469","weight":11},{"id":"1569551535","weight":5},{"id":"1569565461","weight":5},{"id":"1569564245","weight":5},{"id":"1569564227","weight":5},{"id":"1569565317","weight":5},{"id":"1569565123","weight":11},{"id":"1569566941","weight":5},{"id":"1569565771","weight":11},{"id":"1569566999","weight":5},{"id":"1569564249","weight":5},{"id":"1569565809","weight":5},{"id":"1569566579","weight":5},{"id":"1569566497","weight":5},{"id":"1569566963","weight":5},{"id":"1569564989","weight":5},{"id":"1569565897","weight":5},{"id":"1569566895","weight":5},{"id":"1569564189","weight":5},{"id":"1569564337","weight":5},{"id":"1569566167","weight":16},{"id":"1569566679","weight":5},{"id":"1569563307","weight":11},{"id":"1569558681","weight":5},{"id":"1569555999","weight":5},{"id":"1569566657","weight":5},{"id":"1569566581","weight":5},{"id":"1569566423","weight":5},{"id":"1569566437","weight":5},{"id":"1569565735","weight":22},{"id":"1569553909","weight":5},{"id":"1569559111","weight":11},{"id":"1569565427","weight":5},{"id":"1569552251","weight":5},{"id":"1569564441","weight":5},{"id":"1569554689","weight":5},{"id":"1569566425","weight":5},{"id":"1569554971","weight":11},{"id":"1569566209","weight":5},{"id":"1569562821","weight":5},{"id":"1569566127","weight":5},{"id":"1569565087","weight":5},{"id":"1569564857","weight":5},{"id":"1569566913","weight":5},{"id":"1569566629","weight":5},{"id":"1569554759","weight":11},{"id":"1569566003","weight":5},{"id":"1569566223","weight":5},{"id":"1569566505","weight":5},{"id":"1569565393","weight":5},{"id":"1569562207","weight":11},{"id":"1569567033","weight":5},{"id":"1569566655","weight":5},{"id":"1569565311","weight":5},{"id":"1569566245","weight":11},{"id":"1569560503","weight":5},{"id":"1569565463","weight":5},{"id":"1569562551","weight":5},{"id":"1569551347","weight":5},{"id":"1569566293","weight":11},{"id":"1569565665","weight":5},{"id":"1569557715","weight":5},{"id":"1569566983","weight":16},{"id":"1569565397","weight":5},{"id":"1569566873","weight":16},{"id":"1569565765","weight":11},{"id":"1569565215","weight":11},{"id":"1569565093","weight":5},{"id":"1569566887","weight":5},{"id":"1569566267","weight":5},{"id":"1569552037","weight":5},{"id":"1569566737","weight":11},{"id":"1569566253","weight":5},{"id":"1569565353","weight":11},{"id":"1569564305","weight":11},{"id":"1569566595","weight":5},{"id":"1569552025","weight":16},{"id":"1569565013","weight":5},{"id":"1569566715","weight":5},{"id":"1569566755","weight":16},{"id":"1569566713","weight":5},{"id":"1569566641","weight":11},{"id":"1569565425","weight":5},{"id":"1569564437","weight":5},{"id":"1569551905","weight":5},{"id":"1569564861","weight":5},{"id":"1569564787","weight":5},{"id":"1569565529","weight":27},{"id":"1569566619","weight":5},{"id":"1569566397","weight":5},{"id":"1569565669","weight":5},{"id":"1569566001","weight":5},{"id":"1569567691","weight":11},{"id":"1569565861","weight":5},{"id":"1569565537","weight":5},{"id":"1569562367","weight":5},{"id":"1569566457","weight":5},{"id":"1569566847","weight":5},{"id":"1569565997","weight":38},{"id":"1569559597","weight":5},{"id":"1569565337","weight":5},{"id":"1569566341","weight":5},{"id":"1569565889","weight":5},{"id":"1569566635","weight":5},{"id":"1569566611","weight":11},{"id":"1569566375","weight":5},{"id":"1569565143","weight":5},{"id":"1569558697","weight":5},{"id":"1569566067","weight":16},{"id":"1569564807","weight":5},{"id":"1569566443","weight":33},{"id":"1569566727","weight":11},{"id":"1569565315","weight":11}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T8.4","endtime":"12:50","authors":"Saeid Haghighatshoar, Emmanuel Abbe, Emre Telatar","date":"1341405000000","papertitle":"Adaptive sensing using deterministic partial Hadamard matrices","starttime":"12:30","session":"S10.T8: Group Testing and Detection","room":"Stratton (491)","paperid":"1569566825"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
