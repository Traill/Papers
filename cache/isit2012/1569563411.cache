{"id":"1569563411","paper":{"title":{"text":"Ampliﬁcation of the Hidden Gaussian Channel States"},"authors":[{"name":"Chao Tian"}],"abstr":{"text":"Abstract\u2014We consider the problem of amplifying the channel states in a state-dependent Gaussian channel, where the encoder knows (non-causally) a noisy version of the channel states, i.e., the channel states are hidden under the noise. We provide a complete characterization of the minimum state reconstruction distortion at the decoder under a power constraint at the encoder, and show that a simple analog scheme with power control is optimal. More precisely, if the power available to the encoder is below certain threshold, the analog scheme using full power is optimal, however when the power available to the encoder is above that threshold, analog transmission using only a ﬁxed amount of the available power is optimal. This is in contrast to the state ampliﬁcation problem considered by Sutivong et al., when the channel states are known perfectly at the encoder for which the full power is always used in the optimal scheme. The converse proof of our result relies on a channel decomposition argument which was not necessary for the simpler case when the channel states are known perfectly."},"body":{"text":"The problem of channel state ampliﬁcation was ﬁrst con- sidered by Sutivong et al. in [1], where the encoder, knowing perfectly the channel states non-causally, wishes to minimize the reconstruction distortion of the channel states at the decoder. It was shown that for the Gaussian channel and under the mean squared distortion measure, directly transmitting the channel states analogously is optimal. Furthermore, the problem was extended to simultaneously transmitting a mes- sage and amplifying the channel states, for which a complete characterization of the optimal tradeoff between the message rate and reconstruction distortion was also provided. Kim et al. [2] later considered the general non-Gaussian setting, and by measuring the state reconstruction accuracy using mutual information instead of distortion, they were also able to provide a complete characterization of this tradeoff.\nIn this work, we consider a variation of the Gaussian channel state ampliﬁcation problem, where the encoder only knows a noisy version of the Gaussian channel states, and wishes to amplify these hidden states so that the receiver can most accurately reconstruct the channel states under the mean squared distortion measure; see Fig. 1. Our main result is a complete characterization of the minimum achievable distortion under an encoder power constraint. It is shown that the analog scheme with power control is optimal. More precisely, if the power available to the encoder is below certain threshold, the analog scheme using full power is optimal, however when the power available to the encoder is above\nthat threshold, though the analog scheme is still optimal, only a ﬁxed amount of the available power should be used. The optimality of the analog scheme implies that for this problem, there is no loss when the noisy channel states are only known non-causally symbol-wise instead of block-wise. Though the achievability is relatively straightforward, the converse proof is more involved. A channel decomposition is introduced to provide the necessary bound, which did not appear necessary for the simpler setting of [1].\nThe problem in consideration is applicable when the en- coder is in fact a helper transmitter and the channel states are the analog signal sent by the main transmitter. The problem can also be understood as modeling the original state ampliﬁcation system with certain less than perfect processing components, and in this case, our result is reassuring in the sense that as long as the amount of noise is not excessive, the optimal analog transmission scheme using full power remains optimal even with such noise contamination. The problem is also closely related to the Gaussian CEO problem on the Gaussian multiple access channel considered in [3], and we shall return to this point later with more details.\nThe rest of the paper is organized as follows. The problem deﬁnition is given in Section II, the main theorem is given in Section III, and the proof of the main theorem is given in Section IV. Some discussions of the result are given in Section V, and Section VI concludes the paper.\nwhere X is the channel input, S N(0, Q) is the Gaussian channel state, and Z \t N(0, N ) is the additive Gaussian channel noise; S and Z are mutually independent. We use S i to denote the channel state at time i, and write S 1 , S 2 , . . . , S j\nas S j ; similar notation is used for other random variables. The encoder also has a noisy version of the channel state non-causally (in a block-wise manner)\nwhere U N(0, σ 2 u ) is Gaussian random noise independent of (S, Z). The random vector (S i , Z i , U i ), i = 1, 2, . . . , n, are i.i.d. across time. Since S and V are jointly Gaussian, S can be alternatively written as\nwhere W is a zero-mean Gaussian random variable indepen- dent of V with variance λσ 2 u .\nDeﬁnition 1: An (n, d, P ) state-amplifying code consists of an encoding function\nwhich maps the vector V n into the channel input X n = f (V n ) satisfying an average channel power constraint\n1 n\nwhich maps the channel output Y n into a reconstruction of the channel state vector ˆ S n such that\n1 n\nDeﬁnition 2: A distortion-power pair (D, P ) is said to be achievable if for any > 0 and sufﬁciently large n there exists an (n, D + , P ) state-amplifying code. The collection of all achievable (D, P ) pairs is called the achievable rate-power region , denoted as DP. The distortion-power function is the minimum of all distortion D such that (D, P ) ∈ DP.\nIt is worth noting that in the problem being considered here, the receiver is interested in reconstructing the channel state S. If the receiver were interested instead in reconstructing the noise-contaminated channel state V , the problem would be much simpler. To see this, observe that S = λV + W , and we can alternatively view the channel as Y = X +λV +(Z +W ), where now λV is the \u201creal\u201d channel state known perfectly at the encoder and Z + W is the additive noise in the channel, and the result in [1] would directly apply in this setting.\nThe following theorem provides a complete characterization of the distortion-power function D(P ).\nTheorem 1: The distortion power function D(P ) is given by\n  \n \nAs we shall show in the proof in the next section, the optimal scheme is the pure analog scheme with power control.\nIn this section, we prove Theorem 1. The achievability part is rather simple, which is based on an analog scheme with power control. The converse part is however less straightfor- ward, which involves the separate treatments of two regimes, and in one of the regimes, a novel channel decomposition technique is introduced.\nProposition 1: The distortion power function D(P ) is up- per bounded by D ∗ (P ).\nProof: Consider an analog scheme where the encoder simply sends the noisy version of the channel state V i on the channel after certain scaling, i.e., by letting X i = γ ∗ V i for i = 1, 2, . . . , n, where γ ∗ \t P ∗ Q+σ 2\nfor some P ∗ ∈ [0, P ]. The decoder then estimates S i single-letter-wise using the channel output\nfor i = 1, 2, . . . , n. A linear least mean squared error calcula- tion gives that the following distortion is achievable.\nThis optimization problem can be solved explicitly, and the optimizer is given by\nsubstituting which and the corresponding γ ∗ into (2) gives the desired result.\nThe optimizer P ∗ given above implies that when the available power P is below the given threshold, the optimal strategy is to use full power in the analog scheme; above that threshold, the analog scheme is still optimal, however only a ﬁxed amount in the total available power should be used.\nWe ﬁrst provide a simple lower bound of D(P ), which is tight for the high power regime.\nProof: Consider the system depicted in Fig. 2, and denote the minimum distortion obtained by the estimator in this\nsystem as D. It is clear that D ≤ D(P ), because otherwise we can always form the channel output Y in the original system using the available information at the estimator in this new system, and then use the original decoder g(Y ) to form a better reconstruction. It is easily seen that\n+ Qσ 2 u , which gives the desired result.\nThe next lower bound is more interesting, and before introducing this bound, let us consider a decomposition of the channel noise. Clearly, the channel noise Z can be written as follows\nwhere ¯ Z and ¯ ¯ Z are independent zero-mean Gaussian random variables, with variances ¯ N and ¯ ¯ N = N − ¯ N , respectively. This essentially provides an equivalent channel, as illustrated in Fig. 3. The following proposition provides a set of lower bounds to the minimum achievable distortion.\nProposition 3: Let ¯ N ∈ [0, N ), then the distortion-power function satisﬁes\n)Q + (N + P + Q + 2γQ) ¯ N σ 2 u (Q( ¯ N + σ 2 u ) + ¯ N σ 2 u )(N + P + Q + 2γQ)\nProof: For the system depicted in Fig. 3, let us write by data processing inequality that\nh(Y i ) − n 2\nwhere the last inequality is because the Gaussian distribution maximizes the differential entropy with the same variance. By the fact that (X i , V i ), W i , and Z i are mutually independent,\n= E(X i + λV i + W i + Z i ) 2 ≤ P i + Q + N + 2γ i Q,\n, and without loss of generality, we have assumed EX i = 0; furthermore, the following simple fact is used in the last step\nwith equality holds only when the correlation coefﬁcient between X i and V i is 1. It follows that\nQ N − ¯ N\nwhen α ≥ 0, because it is a composite function of a concave function x + α\nfunction log(·) (see [4] for operations that preserve convexity), and furthermore log(x + α √ y + β) is also monotonically non- decreasing, it follows by Jensen\u2019s inequality that\nsince n i=1 P i ≤ nP , which also implies n i=1 (γ i ) 2 ≤ nγ 2 . The quantity I(V n , S n + ¯ Z n ; ˆ S n ) on the right hand side of\nNote that S i ↔ (V i , S i + ¯ Z i ) ↔ Y n is a Markov string, which implies that S i ↔ (V i , S i + ¯ Z i ) ↔ ˆ S i is also a Markov string. By the distortion constraint n i=1 E(S i − ˆ S i ) 2 ≤ nD,\nand the convexity of the mutual information in the conditional probability distribution, we can write using Jensen\u2019s inequality that\nThe right hand side of (8) is in fact the remote source coding rate distortion function R r (D) (see e.g., [5] or [6]) for source S with noisy observations (V, S + ¯ Z) under the mean squared error distortion measure. For the Gaussian case it has an explicit solution (see. e.g., [7] or [8]), which for our case gives\nCombining (5), (7), (8) and (9) gives the desired result after some simple algebra, and the proof is complete.\nThe minimization problem in the remote rate distortion function R r (D) has a simple and intuitive solution, which can be described as follows (see. e.g., [8]). Since (S, V, S + ¯ Z) are jointly Gaussian, we can write\nwhere W s is a zero-mean Gaussian random variable indepen- dent of (V, S, ¯ Z) with a variance determined by\nThe lower bound in Proposition 2 can also be obtained using a limiting argument from the lower bound given in Proposition 3. However we gave Proposition 2 separately since it is rather straightforward and directly provides insight into the problem.\nIt is clear that in the high power regime, the inner bound and the outer bound indeed match, and it remains to show that this is also the case in the low power regime. We could optimize over ¯ N ∈ [0, N ) using Proposition 3 to derive the strongest lower bound on D(P ), and compare it with the distortion achieved using the analog scheme, which however offers little insight. Instead, an indirect approach is taken next by analyzing the conditions in the low power regime.\nouter bounding steps in the proof of Proposition 3 using the achievable scheme input X i = γV i , in a single-letter manner. Clearly in this case\n= 1 2\nlog P + Q + N + 2γQ N − ¯ N\nthen the inner bound and the outer bound match. Recall the optimal ˆ S in R r (D) discussed in the previous subsection, and it is clear that if γV + (S + ¯ Z) is statistically equivalent to aV + b(S + ¯ Z), then (10) indeed holds. This requirement then reduces to\na b\nThus by choosing ¯ N = ¯ N ∗ , the inner bound and the outer bound match, which can always be done since in the low power regime it is always true that\nReaders less comfortable with the indirect proof approach given above can substitute ¯ N = ¯ N ∗ into the expression in Proposition 3, and it is straightforward to show that it indeed matches the upper bound given in Theorem 1. A typical D(P ) function is given in Fig. 4 for the parameters Q = 10, N = 5\nand σ 2 u = 2; note that the horizontal axis is in log scale, i.e., in dB.\nIn this section, we give a few additional remarks on the converse proof and the connection to other problems.\nIn the low power regime, the converse proof essentially relies on ﬁnding a good cut-set choice between the information sources having information regarding the channel states S and the decoder, as illustrated in Fig. 3. The lower bound of D(P ) given in Proposition 3 is then derived by analyzing the information transfer between the two cut-sets. As the available power at the encoder increases, the cut moves closer to the decoder. This increases the amount of information transfer, i.e., the value of I(S + ¯ Z, V ; Y ), between the two cut-sets, but at the same time, the information that can be directly derived about the channel state S from the two information sources S + ¯ Z and V is in fact decreasing. This trend continues until the available power reaches the given threshold and it moves into the high power regime.\nIn the high power regime, the cut is chosen in such a way that the information transfer between the two cut-sets is no longer constrained (i.e., the case in Fig. 2), however in this case, the overall system is limited by the information that can be derived directly from the information sources S + Z and V for the channel states S, and thus an encoder-decoder pair does not appear in Fig. 2, but only an estimator exists.\nPerhaps not surprisingly, the low power regime can thus be understood as channel capacity-limited, and the high power regime as channel-state-information-limited.\nIn the converse proof, the remote source rate-distortion function plays an instrumental role. The channel output in the optimal scheme is essentially chosen to match the optimal remote source coding solution in a statistical manner, and this observation can be useful in other problems.\nParticularly, it was shown in [3] that the analog scheme using full power is optimal for the complete symmetric case, where the sensors observe the true source S under the same\namount of noise contamination, and all sensor encoders have the same amount of power to use. However this symmetry re- quirement is in fact unnecessarily restrictive. Using an analysis similar to that used in this work, it can be shown that the un- coded scheme is optimal as long as P i σ 4 i (σ 2 s + σ 2 i ) −1 remains constant across all the sensor encoders, where σ 2 s , P i , σ 2 i are the variance of the hidden Gaussian source S, the transmission power constraint and the variance of the sensor noise U i at sensor i, respectively (see Fig. 5). We leave this simple calculation to interested readers.\nWe considered the problem of amplifying the hidden Gaus- sian channel states, and provided a complete solution for the minimum mean squared error distortion achievable under an encoder power constraint. The optimal scheme in this problem is analog transmission of the noisy channel states with power control. This is different from the case when the channel states are known perfectly at the encoder, for which full power is always used in the optimal scheme. The optimality of the analog scheme implies that for this problem, there is no loss when the noisy channel states are known non-causally symbol-wise instead of block-wise. Though the inner bound is straightforward, the outer bound is derived using a novel channel decomposition technique which was not required in the setting when the channel states are known without noise contamination.\nPossible future work includes extension to the problem of simultaneously transmitting a message and amplifying the hidden Gaussian channel states, as well as to the non-Gaussian settings.\nThe author wishes to thank Shlomo Shamai and Bernd Bandemer for reading an early draft and providing feedbacks."},"refs":[{"authors":[{"name":"A. Sutivong"},{"name":"M. Chiang"},{"name":"T. M. Cover"},{"name":"Y.-H. Kim"}],"title":{"text":"Channel capacity and state estimation for state-dependent Gaussian channels"}},{"authors":[{"name":"Y.-H. Kim"},{"name":"A. Sutivong"},{"name":"T. M. Cover"}],"title":{"text":"State ampliﬁcation"}},{"authors":[{"name":"M. Gastpar"}],"title":{"text":"Uncoded transmission is exactly optimal for a simple Gaussian sensor network"}},{"authors":[{"name":"S. Boy"},{"name":"L. Vandenbergh"}],"title":{"text":"Convex optimization , Cambridge University Press, 2004"}},{"authors":[{"name":"T. Berge"}],"title":{"text":"Rate distortion theory: mathematical basis for data compres- sion , Prentice Hall, 1971"}},{"authors":[{"name":"H. Yamamoto"},{"name":"K. Itoh"}],"title":{"text":"Source coding theory for multi-terminal communication systems with a remote source"}},{"authors":[{"name":"M. Gastpar"}],"title":{"text":"A lower bound to the AWGN remote rate-distortion function"}},{"authors":[{"name":"C. Tian"},{"name":"J. Chen"}],"title":{"text":"Remote vector Gaussian source coding with decoder side information under mutual information and distortion con- straints"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569563411.pdf"},"links":[{"id":"1569566381","weight":23},{"id":"1569565383","weight":7},{"id":"1569565223","weight":7},{"id":"1569566725","weight":7},{"id":"1569565377","weight":15},{"id":"1569566385","weight":7},{"id":"1569565067","weight":7},{"id":"1569559617","weight":15},{"id":"1569566683","weight":7},{"id":"1569566697","weight":7},{"id":"1569566943","weight":7},{"id":"1569566591","weight":7},{"id":"1569556029","weight":15},{"id":"1569552245","weight":7},{"id":"1569564481","weight":7},{"id":"1569566415","weight":15},{"id":"1569566373","weight":7},{"id":"1569566765","weight":7},{"id":"1569564233","weight":7},{"id":"1569559541","weight":7},{"id":"1569566941","weight":7},{"id":"1569564203","weight":7},{"id":"1569566751","weight":15},{"id":"1569565771","weight":15},{"id":"1569565859","weight":7},{"id":"1569566579","weight":7},{"id":"1569565347","weight":15},{"id":"1569565455","weight":23},{"id":"1569566709","weight":30},{"id":"1569564989","weight":7},{"id":"1569551763","weight":7},{"id":"1569565953","weight":7},{"id":"1569563981","weight":15},{"id":"1569561085","weight":7},{"id":"1569566419","weight":15},{"id":"1569558681","weight":7},{"id":"1569566511","weight":23},{"id":"1569566531","weight":7},{"id":"1569561143","weight":7},{"id":"1569565833","weight":7},{"id":"1569565667","weight":7},{"id":"1569561795","weight":7},{"id":"1569566423","weight":7},{"id":"1569566851","weight":7},{"id":"1569566687","weight":15},{"id":"1569566939","weight":7},{"id":"1569552251","weight":7},{"id":"1569553519","weight":23},{"id":"1569566231","weight":7},{"id":"1569554971","weight":15},{"id":"1569565655","weight":7},{"id":"1569558985","weight":7},{"id":"1569566809","weight":7},{"id":"1569566257","weight":15},{"id":"1569565033","weight":15},{"id":"1569566447","weight":7},{"id":"1569566357","weight":7},{"id":"1569565887","weight":7},{"id":"1569566721","weight":7},{"id":"1569565219","weight":7},{"id":"1569565095","weight":15},{"id":"1569566553","weight":7},{"id":"1569565029","weight":15},{"id":"1569565357","weight":7},{"id":"1569561245","weight":15},{"id":"1569566191","weight":7},{"id":"1569566603","weight":7},{"id":"1569566673","weight":7},{"id":"1569565441","weight":15},{"id":"1569565311","weight":7},{"id":"1569566233","weight":7},{"id":"1569566501","weight":7},{"id":"1569560503","weight":23},{"id":"1569564339","weight":7},{"id":"1569565439","weight":7},{"id":"1569563395","weight":15},{"id":"1569551347","weight":7},{"id":"1569565415","weight":7},{"id":"1569564175","weight":15},{"id":"1569565397","weight":23},{"id":"1569566129","weight":15},{"id":"1569565385","weight":7},{"id":"1569565181","weight":7},{"id":"1569566711","weight":7},{"id":"1569565661","weight":7},{"id":"1569561221","weight":30},{"id":"1569566651","weight":7},{"id":"1569566823","weight":15},{"id":"1569566137","weight":7},{"id":"1569566237","weight":7},{"id":"1569566283","weight":7},{"id":"1569565375","weight":23},{"id":"1569566755","weight":7},{"id":"1569565541","weight":7},{"id":"1569566813","weight":7},{"id":"1569566641","weight":30},{"id":"1569551905","weight":7},{"id":"1569566487","weight":7},{"id":"1569556759","weight":7},{"id":"1569566619","weight":7},{"id":"1569561185","weight":15},{"id":"1569558779","weight":7},{"id":"1569565233","weight":7},{"id":"1569566817","weight":15},{"id":"1569564923","weight":7},{"id":"1569565367","weight":7},{"id":"1569566299","weight":30},{"id":"1569564769","weight":7},{"id":"1569563919","weight":23},{"id":"1569557851","weight":15},{"id":"1569559251","weight":7},{"id":"1569567013","weight":7},{"id":"1569550425","weight":7},{"id":"1569565165","weight":7},{"id":"1569565635","weight":7},{"id":"1569564931","weight":7},{"id":"1569564141","weight":7},{"id":"1569566973","weight":7},{"id":"1569564755","weight":7},{"id":"1569566839","weight":7},{"id":"1569551751","weight":15},{"id":"1569564419","weight":7}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S17.T4.5","endtime":"16:40","authors":"Chao Tian","date":"1341591600000","papertitle":"Amplification of the Hidden Gaussian Channel States","starttime":"16:20","session":"S17.T4: Communication Models","room":"Stratton 20 Chimneys (306)","paperid":"1569563411"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
