{"id":"1569552037","paper":{"title":{"text":"Bilateral Random Projections"},"authors":[{"name":"Tianyi Zhou"},{"name":"Dacheng Tao"}],"abstr":{"text":"Abstract\u2014Low-rank structure have been profoundly studied in data mining and machine learning. In this paper, we show a dense matrix X\u2019s low-rank approximation can be rapidly built from its left and right random projections Y 1 = XA 1 and Y 2 = X T A 2 , or bilateral random projection (BRP). We then show power scheme can further improve the precision. The deterministic, average and deviation bounds of the proposed method and its power scheme modiﬁcation are proved theoretically. The effectiveness and the efﬁciency of BRP based low-rank approximation is empirically veriﬁed on both artiﬁcial and real datasets."},"body":{"text":"Recent researches about low-rank structure concentrate on developing fast approximation and building meaningful decompositions. Two appealing representatives are the randomized approximate matrix decomposition [1] and column selection [2]. The former proves that a matrix can be well approximated by its projection to the column space of its random projections. This rank-revealing method provides a fast approximation of SVD/PCA. The latter proves that a column subset of a low-rank matrix can span its whole range. The low-rank approximation based on random projections for streaming data is also studied in [3].\nIn this paper, we consider the problem of fast low-rank approx- imation. Given r bilateral random projections (BRP) of an m × n dense matrix X (w.l.o.g, m ≥ n), i.e., Y 1 = XA 1 and Y 2 = X T A 2 , wherein A 1 ∈ R n×r and A 2 ∈ R m×r are random matrices,\nis a fast rank-r approximation of X. The computation of L includes an inverse of an r × r matrix and three matrix multiplications. Thus, for a dense X, 2mnr ﬂoating-point operations (ﬂops) are required to obtain BRP, r 2 (2n + r) + mnr ﬂops are required to obtain L. The computational cost is much less than SVD based approximation. The L in (1) has been proposed in [4] as a recovery of a rank- r matrix X from Y 1 and Y 2 , where A 1 and A 2 are independent Gaussian/SRFT random matrices. However, we propose that L is a tight rank-r approximation of a full rank matrix X, when A 1 and A 2 are correlated random matrices updated from Y 2 and Y 1 , respectively. We then apply power scheme [5] to L for improving the approximation precision, especially when the eigenvalues of X decay slowly.\nTheoretically, we prove the deterministic bound, average bound and deviation bound of the approximation error in BRP based low-rank approximation and its power scheme modiﬁcation. The results show the error of BRP based approximation is close to the error of SVD approximation under mild conditions. Comparing with randomized SVD in [1] that extracts the column space from unilateral random projections, the BRP based method estimates both column and row spaces from bilateral random projections.\nWe give an empirical study of BRP on both artiﬁcial data and face image dataset. The results show its effectiveness and efﬁciency in low-rank approximation and recovery.\nWe ﬁrst introduce the bilateral random projections (BRP) based low-rank approximation and its power scheme modiﬁcation. The approximation error bounds of these two methods are discussed at the end of this section.\nIn order to improve the approximation precision of L in (1) when A 1 and A 2 are standard Gaussian matrices, we use the obtained right random projection Y 1 to build a better left projection matrix A 2 , and use Y 2 to build a better A 1 . In particular, after Y 1 = XA 1 , we update A 2 = Y 1 and calculate the left random projection Y 2 = X T A 2 , then we update A 1 = Y 2 and calculate the right random projection Y 1 = XA 1 . A better low-rank approximation L will be obtained if the new Y 1 and Y 2 are applied to (1). This improvement requires additional ﬂops of mnr in BRP calculation.\nWhen singular values of X decay slowly, (1) may perform poorly. We design a modiﬁcation for this situation based on the power scheme [5]. In the power scheme modiﬁcation, we instead calculate the BRP of a matrix ˜ X = (XX T ) q X, whose singular values decay faster than X. In particular, λ i ( ˜ X) = λ i ( ˜ X) 2q+1 . Both X and ˜ X share the same singular vectors. The BRP of ˜ X is:\nIn order to obtain the approximation of X with rank r, we calculate the QR decomposition of Y 1 and Y 2 , i.e.,\nThe power scheme modiﬁcation (5) requires an inverse of an r × r matrix, an SVD of an r × r matrix and ﬁve matrix multiplications. Therefore, for dense X, 2(2q + 1)mnr ﬂops are required to obtain BRP, r 2 (m + n) ﬂops are required to obtain the QR decompositions, 2r 2 (n+2r)+mnr ﬂops are required to obtain L. The power scheme modiﬁcation reduces the error of (1) by increasing q. When the random matrices A 1 and A 2 are built from Y 1 and Y 2 , mnr additional ﬂops are required in the BRP calculation.\nWe analyze the error bounds of the BRP based low-rank approxi- mation (1) and its power scheme modiﬁcation (5).\nwhere Λ 1 is an r × r diagonal matrix which diagonal elements are the ﬁrst largest r singular values, U 1 and V 1 are the corresponding singular vectors, Λ 2 , U 2 and V 2 forms the rest part of SVD. Assume that r is the target rank, A 1 and A 2 have r + p columns for oversampling. We consider the spectral norm of the approximation error E for (1):\nIn low-rank approximation, the left random projection matrix A 2 is built from the left random projection Y 1 = XA 1 , and then the right random projection matrix A 1 is built from the left random projection Y 2 = X T A 2 . Thus A 2 = Y 1 = XA 1 = U ΛV T A 1 and A 1 = Y 2 =\nX T A 2 = X T XA 1 = V Λ 2 V T A 1 . Hence the approximation error given in (8) has the following form:\nThe following Theorem 1 gives the bound for the spectral norm of the deterministic error X − L .\nTheorem 1. (Deterministic error bound) Given an m × n (m ≥ n) real matrix X with singular value decomposition X = U ΛV T = U 1 Λ 1 V T 1 + U 2 Λ 2 V T 2 , and chosen a target rank r ≤ n − 1 and an n × (r + p) (p ≥ 2) standard Gaussian matrix A 1 , the BRP based low-rank approximation (1) approximates X with the error upper bounded by\nIf the singular values of X decay fast, the ﬁrst term in the deterministic error bound will be very small. The last term is the rank-r SVD approximation error. Therefore, the BRP based low-rank approximation (1) is nearly optimal.\nTheorem 2. (Deterministic error bound, power scheme) Frame the hypotheses of Theorem 1, the power scheme modiﬁcation (5) approximates X with the error upper bounded by\nIf the singular values of X decay slowly, the error produced by the power scheme modiﬁcation (5) is less than the BRP based low-rank approximation (1) and decreasing with the increasing of q.\nThe average error bound of BRP based low-rank approximation is obtained by analyzing the statistical properties of the random matrices that appear in the deterministic error bound in Theorem 1.\nTheorem 3. (Average error bound) Frame the hypotheses of Theorem 1,\nThe average error bound for the power scheme modiﬁcation is then obtained from the result of Theorem 3.\nTheorem 4. (Average error bound, power scheme) Frame the hypotheses of Theorem 1, the power scheme modiﬁcation (5) ap- proximates X with the expected error upper bounded by\nCompared the average error bounds of the BRP based low- rank approximation with its power scheme modiﬁcation, the latter produces less error than the former, and the error can be further decreased by increasing q.\nThe deviation bound for the spectral norm of the approxima- tion error can be obtained by analyzing the deviation bound of\nΛ 2 2 V T 2 A 1 (V T 1 A 1 ) \u2020 Λ −1 1 \t in the deterministic error bound and by applying the concentration inequality for Lipschitz functions of a Gaussian matrix.\nTheorem 5. (Deviation bound) Frame the hypotheses of Theorem 1. Assume that p ≥ 4. For all u, t ≥ 1, it holds that\nr + p p + 1\nThe following lemma and propositions from [1] will be used in the proof.\nLemma 1. Suppose that M 0. For every A, the matrix A T M A 0. In particular,\nProposition 1. Suppose range(N ) ⊂ range(M ). Then, for each matrix A, it holds that P N A ≤ P M A and that (I −P M )A ≤\nProposition 3. We have M ≤ A + C for each partitioned positive semideﬁnite matrix\nProof: Since an orthogonal projector projects a given matrix to the range (column space) of a matrix M is deﬁned as P M = M (M T M ) −1 M T , the deterministic error (9) can be written as\nBy applying Proposition 1 to the error (13), because range(M (V T 1 A 1 ) \u2020 Λ −2 1 ) ⊂ range(M ), we have\nFor the top-left block in (16), Proposition 2 leads to I − I + H T H −1 H T H. For the bottom-right block in (16), Lemma\nT H \t − I + H T H −1 H T −H I + H T H −1 \t I\nΛ T 1 H T HΛ 1 \t −Λ T 1 I + H T H −1 H T Λ 2 −Λ T 2 H I + H T H −1 Λ 1 \t Λ T 2 Λ 2\nAccording to Proposition 3, the spectral norm of Λ(I − P N ) is bounded by\nBy substituting (16) into (14), we obtain the deterministic error bound. This completes the proof.\nProposition 4. Let P be an orthogonal projector, and let A be a matrix. For each nonnegative q,\nProof: The power scheme modiﬁcation (5) applies the BRP based low-rank approximation (1) to ˜ X = (XX T ) q X = U Λ 2q+1 V T rather than X. In this case, the approximation error is\nThe deterministic error bound for the power scheme modiﬁcation is obtained by applying Proposition 4 to (19). This completes the proof.\nProposition 5. Fix matrices S, T , and draw a standard Gaussian matrix G. Then it holds that\nProposition 6. Draw an r × (r + p) standard Gaussian matrix G with p ≥ 2. Then it holds that\nProof: The distribution of a standard Gaussian matrix is rota- tional invariant. Since 1) A 1 is a standard Gaussian matrix and 2) V is an orthogonal matrix, V T A 1 is a standard Gaussian matrix, and its disjoint submatrices V T 1 A 1 and V T 2 A 1 are standard Gaussian matrices as well.\nWe condition on V T 1 A 1 and apply Proposition 5 to bound the expectation w.r.t. V T 2 A 1 , i.e.,\nSince 1) V T 1 A 1 is a standard Gaussian matrix and 2) Λ 1 is a diagonal matrix, each column of Λ 1 V T 1 A 1 follows r- variate Gaussian distribution N r (0, Λ 2 1 ). Thus the random matrix\nΛ 1 V T 1 A 1 Λ 1 V T 1 A 1 T −1 follows the inverted Wishart distri- bution W −1 r (Λ −2 1 , r + p). According to the expectation of inverted Wishart distribution [6], we have\nWe apply Proposition 6 to the standard Gaussian matrix V T 1 A 1 and obtain\nr + p p\nWe apply Theorem 3 to ˜ X and ˜ L and obtain the bound of E ˜ X − ˜ L , noting that λ i ( ˜ X) = λ i (X) 2q+1 .\nBy substituting (29) into (28), we obtain the average error bound of the power scheme modiﬁcation shown in Theorem 4. This completes the proof.\nProposition 8. Let G be a r × (r + p) standard Gaussian matrix where p ≥ 4. For all t ≥ 1,\nProof: According to the deterministic error bound in Theorem 1, we study the deviation of Λ 2 2 V T 2 A 1 V T 1 A 1 \u2020 Λ −1 1 . Consider the Lipschitz function h(X) = Λ 2 2 X V T 1 A 1 \u2020 Λ −1 1 , its Lipschitz constant L can be estimated by using the triangle inequality:\nHence \t the \t Lipschitz \t constant \t satisﬁes \t L \t ≤ Λ 2 2 \t V T 1 A 1 \u2020 Λ −1 1 . We condition on V T 1 A 1 and then\nAccording to Proposition 8, the event T happens except with prob- ability\nApplying Proposition 7 to the function h V T 2 A 1 , given the event T , we have\nAccording to the deﬁnition of the event T and the probability of T , we obtain\nSince \t Theorem \t 1 \t implies \t X − L \t ≤ Λ 2 2 V T 2 A 1 V T 1 A 1 \u2020 Λ −1 1 \t + Λ 2 , we obtain the deviation\nWe ﬁrst evaluate the efﬁciency of the BRP based low-rank approx- imation (1) for exact recovery of low-rank matrices. We consider square matrices of dimension n from 500 to 30000 with rank r from 50 to 500. Each matrix is generated by AB, wherein A and B are both n × r standard Gaussian matrices. Figure 1 shows that the recovery time is linearly increased w.r.t n. This is consistent with the r 2 (2n + r) + mnr ﬂops required by (1). The relative error of each recovery is less than 10 −14 . It also shows that a 30000×30000 matrix with rank 500 can be exactly recovered within 200 CPU seconds. This suggests the advantage of (1) for large-scale applications.\nWe then evaluate the effectiveness of (1) and its power scheme modiﬁcation (5) in low-rank approximation of full rank matrix with slowly decaying singular values. We generate a square matrix with size 1000, whose entries are independently sampled from a standard normal distribution with mean 0 and variance 1, and then apply (1) (q = 0) and (5) with q = 1, 2, 3 to obtain approximations with rank varying from 1 to 600. We show the relative errors in Figure 2 and the relative error of the corresponding SVD approximation as a baseline. The results suggest that our method can obtain a nearly optimal approximation when q is sufﬁciently large (e.g., 2).\nAt last, we evaluate the efﬁciency and effectiveness of BRP on low-rank compression of human face images from dataset FERET [7]. We randomly selected 700 face images of 100 individuals from FERET and built a 700×1600 data matrix, wherein the 1600 features are the 40 × 40 pixels of each image. We then obtain two rank- 60 compressions of the data matrix by using SVD and the power modiﬁcation of BRP based low-rank approximation (5) with q = 1, respectively. The compressed images and the corresponding time costs are shown in Figure 3 and its caption. It indicates that our method is able to produce compression with competitive quality in considerably less time than SVD.\nWe would like to thank the reviewers for their constructive com- ments. This work is supported by the Australian Research Council discovery project (ARC DP-120103730)."},"refs":[{"authors":[{"name":"N. Halko"},{"name":"P. G. Martinsson"},{"name":"J. A. Tropp"}],"title":{"text":"Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions"}},{"authors":[{"name":"A. Deshpande"},{"name":"S. Vempala"}],"title":{"text":"Adaptive sampling and fast low-rank ma- trix approximation"}},{"authors":[{"name":"K. L. Clarkson"},{"name":"D. P. Woodruff"}],"title":{"text":"Numerical linear algebra in the streaming model"}},{"authors":[{"name":"M. Fazel"},{"name":"E. J. Cand`es"},{"name":"B. Recht"},{"name":"P. Parrilo"}],"title":{"text":"Compressed sensing and robust recovery of low rank matrices"}},{"authors":[{"name":"S. Roweis"}],"title":{"text":"Em algorithms for pca and spca"}},{"authors":[{"name":"R. J. Muirhea"}],"title":{"text":"Aspects of multivariate statistical theory"}},{"authors":[{"name":"P. J. Phillips"},{"name":"H. Moon"},{"name":"S. A. Rizvi"},{"name":"P. J. Rauss"}],"title":{"text":"The feret evaluation methodology for face-recognition algorithms"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569552037.pdf"},"links":[{"id":"1569566321","weight":10},{"id":"1569564469","weight":5},{"id":"1569566157","weight":5},{"id":"1569566167","weight":5},{"id":"1569565559","weight":5},{"id":"1569566913","weight":21},{"id":"1569552025","weight":15},{"id":"1569566715","weight":5},{"id":"1569567691","weight":5},{"id":"1569559919","weight":5},{"id":"1569565337","weight":5},{"id":"1569566635","weight":5},{"id":"1569566125","weight":15},{"id":"1569566825","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T9.3","endtime":"15:40","authors":"Tianyi Zhou, Dacheng Tao","date":"1341328800000","papertitle":"Bilateral Random Projections","starttime":"15:20","session":"S7.T9: Compressive Sensing","room":"Stratton West Lounge (201)","paperid":"1569552037"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
