{"id":"1569564469","paper":{"title":{"text":"Non-adaptive Group Testing: Explicit bounds and novel algorithms"},"authors":[{"name":"Chun Lam Chan ∗"},{"name":"Sidharth Jaggi ∗"},{"name":"Venkatesh Saligrama +"},{"name":"Samar Agnihotri ∗"}],"abstr":{"text":"Abstract\u2014 1 . We present computationally efﬁcient and provably correct algorithms with near-optimal sample-complexity for noisy non-adaptive group testing. Group testing involves grouping arbi- trary subsets of items into pools. Each pool is then tested to identify the defective items, which are usually assumed to be sparsely distributed. We consider random non-adaptive pooling where pools are selected randomly and independently of the test outcomes. Our noisy scenario accounts for both false negatives and false positives for the test outcomes. Inspired by compressive sensing algorithms we introduce four novel computationally efﬁcient decoding algo- rithms for group testing, CBP via Linear Programming (CBP-LP), NCBP-LP (Noisy CBP-LP), and the two related algorithms NCBP- SLP+ and NCBP-SLP- (\u201cSimple\u201d NCBP-LP). The ﬁrst of these algorithms deals with the noiseless measurement scenario, and the next three with the noisy measurement scenario. We derive explicit sample-complexity bounds\u2014with all constants made explicit\u2014for these algorithms as a function of the desired error probability; the noise parameters; the number of items; and the size of the defective set (or an upper bound on it). We show that the sample- complexities of our algorithms are near-optimal with respect to known information-theoretic bounds."},"body":{"text":"The goal of group testing is to identify a small unknown subset D of defective items embedded in a much larger set N (usually in the setting where d = |D| is much smaller than n = |N |, i.e., d is o(n)). This problem was ﬁrst considered by Dorfman [1] in scenarios where multiple items in a group can be simultaneously tested, with a binary output depending on whether or not a \u201cdefective\u201d item is present in the group being tested. In general, the goal of group testing algorithms is to identify the defective set with as few measurements as possible. As demonstrated in [1] and later work [2], with judicious grouping and testing, far fewer than the trivial upper bound of n tests may be required to identify D.\nWe consider non-adaptive group testing, where the set of items being tested in each test is required to be independent of the outcome of every other test [2]. This restriction is often useful in practice, since this enables parallelization of the testing process. We describe computationally efﬁcient algorithms with near-optimal performance for noiseless and noisy non-adaptive group testing problems. We describe the different aspects of the paper in some detail next.\n\u201cNoisy\u201d measurements: We also consider the \u201cnoisy\u201d variant of the group testing problem. In this scenario the result of each test may differ from the true result (in an independent and identically distributed manner) with a certain pre-speciﬁed probability q. This leads to both false positives and negatives in the test outcomes. Much of the existing work either considers one-sided noise, namely false positives [3] but no false negatives or a \u201cworst-case\u201d noise [4] wherein the number of false positives and negatives are assumed bounded. 2 Since the measurements are noisy, the problem of estimating the set of defective items is more challenging, and is known to require more tests. 3 The work closest to this work is [7], where explicit upper and lower bounds for the group-testing problem were ﬁrst derived.\nComputationally efﬁcient and near-optimal algorithms: Most algorithms in the literature focus on optimizing the number of measurements required \u2013 in some cases, this leads to algo- rithms that may not be computationally efﬁcient to implement (for e.g. [3]). In this paper we present algorithms that are not only computationally efﬁcient but are also near-optimal in the number of measurements required.\nWe analyze three different types of algorithms (the last one has two ﬂavors), related to those described in the compressive sensing literature (see Section I-A).\nOur algorithms are related to linear programming relaxations used in the compressive sensing (CS) literature. In CS the 0 norm minimization is relaxed to an 1 norm minimization. In the noise-free case this relaxation results in a linear program since the measurements are linear. In contrast, in group testing, the measurements are non-linear and boolean. Furthermore, noise in the group testing scenario is also boolean unlike additive noises encountered in CS. Consequently, we also have to relax boolean measurements. We do so by using a novel combination of inequality and positivity constraints. Our LP formulation and analysis is related to LP decoding of error-correcting codes [8], where one uses a \u201cminimum distance\u201d decoding criteria based\non perturbation analysis of the norm of the error vector. The idea is to decode to a vector pair consisting of defective items, x, and the error vector, η such that the error-vector η is as \u201csmall\u201d as possible by solving a sequence of LPs. We call this algorithm the Noisy Combinatorial Basis Pursuit via LP decoding (NCBP-LP). Using standard concentration results we show that the solution to our LP decoding algorithm recovers the true defective items with high probability. Furthermore, we achieve near-optimal performance in the sense that our sample complexity for NCBP-LP match the lower bounds within a constant factor, where the constant is independent of the number of items n and the defective set size d (but may depend on the noise parameter q, and the error probability ). Based on this analysis, we can directly derive the performance of two other LP-based decoding algorithms. In particular CBP-LP considers the noiseless measurement scenario, and NCBP-SLP+ and NCBP-SLP- consider the noisy measurement scenario, but only use constraints corresponding to positive and negative test outcomes respectively. 4\n\u201cSmall-error\u201d probability: To gain new insights into the con- stants involved in sample-complexity we admit a small but ﬁxed error probability, . Thus our sample complexity bounds have to be interpreted probabilistically, namely, with T measurements, we can identify the true set of defectives with probability at least (1 − ), where the probability is taken over all the sources on randomness. With this new perspective we can derive upper bounds that hold not only in an order-wise sense but also where the constants involved in these order-wise bounds can be made explicit.\nExplicit Sample Complexity Bounds: Our sample complexity bounds are of the form T ≥ β(q, )d log(n). The function β(q, ) is an explicitly computed function of the noise parameter q and admissible error probability . In the literature, order- optimal upper and lower bounds on the number of tests required are known for the problems we consider (for instance [3], [10]). In both the noiseless and noisy variants, the number of measurements required to identify the set of defective items is known to be T = Θ(d log(n)) \u2013 here n = |N | is the total number of items and d = |D| is the size of the defective subset. In fact, if only D, an upper bound on d, is known, then T = Θ(D log(n)) measurements are also known to be necessary and sufﬁcient. In our algorithms we explicitly demonstrate that we require only a knowledge of D rather than the exact value of d. Furthermore, in the noisy variant, we show that the number of tests required is in general at most a constant factor larger than in the noiseless case (where this constant β is independent of both n and d, but may depend on the noise parameter q and the allowable error-probability of the algorithm).\nThis paper is organized as follows. In Section II, we intro- duce the model and corresponding notation, and describe the\nalgorithms analyzed in this work. In Section III, we describe the main results of this work. Section IV contains the analysis of the group-testing algorithms considered.\nCompressive sensing has received signiﬁcant attention over the last decade. We describe the version most related to the topic of this paper [11], [12]. This version considers the following problem. Let x be an exactly d-sparse vector in R n , i.e., a vector with at most d non-zero components (in general in the situations of interest d = o(n)).\nLet z correspond to a noise vector added to the measurement M x. One is given a set of \u201ccompressed noisy measurements\u201d of x as y = M x+z. (The noise is guaranteed to be not \u201ctoo large\u201d (||z|| 2 ≤ c 2 ).) The T ×n matrix M is designed by choosing each entry i.i.d. from a suitable probability distribution (for instance, the set of zero-mean, 1/n variance Gaussian random variables). The decoder must use the resulting noisy measurement vector y ∈ R T and the matrix M to estimate in a computationally efﬁcient manner the underlying vector x. The challenge is to do so with as few measurements as possible, i.e., with the number of rows T of M being as small as possible.\n1) Basis Pursuit: An alternate decoding procedure proceeds by relaxing the compressive sensing problem into the convex optimization problem called Basis Pursuit (BP).\nx = arg min ||x|| 1 \t (1) subject to ||y − M x|| 2 ≤ c 2 \t (2)\nIt can be shown (for instance [11], [12]) that there exist constants c 4 , c 5 and c 6 such that with T = c 4 d log(n), with probability at least 1 − 2 c 5 n , the solution x ∗ to BP satisﬁes ||x ∗ − x|| 2 ≤ c 6 ||z|| 2 .\nA set N contains n items, of which an unknown subset D are said to be \u201cdefective\u201d. 5 The goal of group-testing is to correctly identify the set of defective items via a minimal number of \u201cgroup tests\u201d, as deﬁned below.\nEach row of a T × n binary group-testing matrix M cor- responds to a distinct test, and each column corresponds to a distinct item. Hence the items that comprise the group being tested in the ith test are exactly those corresponding to columns containing a 1 in the ith location. The method of generating such a matrix M is part of the design of the group test. This along with the method of estimating the set D, is described in Section II-B.\nThe length-n binary input vector x represents the set N , and contains 1s exactly in the locations corresponding to the items\nof D. 6 The outcomes of the noiseless tests correspond to the length-T binary noiseless result vector y, with a 1 in the i th location if and only if the ith test contains at least one defective item. The observed vector of test outcomes in the noisy scenario is denoted by the length-T binary noisy result vector ˆ y \u2013 the probability that each entry y i of y differs from the corresponding entry ˆ y i in ˆ y is q, where q is the noise parameter. The locations where the noiseless and the noisy result vectors differ is denoted by the length-T binary noise vector ν, with 1s in the locations where they differ. The estimate of the locations of the defective items is encoded in the length-n binary estimate vector ˆ x, with 1s in the locations where the group-testing algorithms described in Section II-B estimate the defective items to be.\nThe probability of error of any group-testing algorithm is deﬁned as the probability (over the input vector x, group-testing matrix M , and noise vector ν) that the estimated vector differs from the input vector.\nWe now describe our algorithms in both the noiseless and noisy settings. The algorithms are speciﬁed by the choices of encoding matrices and decoding algorithms. The T × n group- testing matrix M is deﬁned by randomly selecting each entry in it in an i.i.d. manner to equal 1 with probability p = 1/D, and 0 otherwise.\nA linear relaxation of the group-testing problem leads nat- urally to NCBP-LP (3-9). In particular, each x i is relaxed to satisfy 0 ≤ x i ≤ 1, ¯ d represents our \u201cguess\u201d for the value of\nd ≤ D, and the non-linear measurements are linearized in (4)- (5). Also, we deﬁne \u201cslack\u201d variables η i for all i ∈ {1, . . . , T } to account for errors in the test outcome. For a particular test i this η i is deﬁned to be zero if a particular test result is correct, and positive (and at least 1) if the test result is incorrect. Of course, the decoder does not know a priori which scenario a particular test outcome falls under, and hence has to also decode η. Nonetheless, as is common in the ﬁeld of error- correction [15], often using a \u201cminimum distance\u201d decoding criteria (decoding to a vector pair (x, η) such that the error- vector η is as \u201csmall\u201d as possible) leads to good decoding performance. Our LP decoder attempts to do so.\nTo be more precise, we deﬁne the pair (ˆ x, ˆ η) to be feasible if ˆ x is a binary vector of weight at most D, and ˆ η ∈ R T is a vector of Hamming weight at most T q(1 + τ ). (The value of τ is a code-design parameter to be speciﬁed later.) We then solve the sequence of LPs in (3\u20139) for each ¯ d ∈ {1, . . . , D} and output the ﬁrst feasible pair (ˆ x( ¯ d), ˆ η( ¯ d)) in the sequence (and output an error if there are none). That is, the decoder sequentially attempts to ﬁnd valid solutions for the above LP for sequentially increasing integers starting from ¯ d = 0. If no feasible solution is found, or if an infeasible solution is found, the decoder increments ¯ d by 1 and continues until it ﬁnds a valid solution for any value of ¯ d ≤ D (if so, the decoder stops and outputs that) or it reaches ¯ d > D (if so, the decoder declares an error). Our analysis demonstrates that this algorithm has a \u201csmall\u201d probability of error.\nCBP-LP, which analyzes the scenario with noiseless measure- ments, is a special case of NCBP-LP with each η j variable set to zero. Hence it reduces to the problem of ﬁnding any feasible point in the constraint set (11-14)\nIn fact, it turns out that simpler LPs still gives essentially the same performance as NCBP-LP. Consider the two LPs given above. The intuition is that if NCBP-LP and works by ﬁnding a η vector with low Hamming weight, then NCBP-SLP+ (respectively NCBP-SLP-) does the same by ﬁnding a η vector with low Hamming weight restricted just to the set of positive (respectively negative) outcomes. Since the noise that converts y to ˆ y is probabilistic, by standard concentration results these two approaches should, with high probability, lead to the same result.\nThe analysis of the constants in the three main theorems are not optimized 7 but the constants are given to demonstrate the functional dependence on δ and q. Our algorithms\u2019 sample complexities are commensurate (up to a constant factor) with the information-theoretic lower bound [7], which we restate it here for the sake of completion.\nTheorem 1 (Lower Bound): Any group-testing algorithm that has measurements that are noisy i.i.d. with probability q and that has a probability of error at most requires at least of\nWe next derive upper bounds based on LP. We observe that when 1 − 2q → 0, the lower and upper bounds have the same functional dependence on (1−2q). We deﬁne Γ as ln(D)/ ln(n) and γ as (2Γ + δ)/(1 + Γ + δ) (note that in the limit of large n, Γ lies in the interval [0, 1) and γ in the interval (δ/(δ + 1), 1].\nTheorem 2: NCBP-LP with error probability at most n −δ requires no more than β LP D log n tests, with β LP deﬁned as\nTheorem 3: CBP-LP with error probability at most n −δ requires no more than 16(1+\nTheorem 4: NCBP-SLP+ and NCBP-SLP- with error prob- ability at most n −δ require no more than β SLP + D log n and β SLP − D log n tests respectively, with β SLP + and β SLP − re- spectively equaling\nWe sketch the proof of Theorem 2 (full details are in [16]), and note that Theorems 3 and 4 are direct corollaries.\nProof sketch of Theorem 2: Without loss of generality, let x be the vector with 1s in the ﬁrst d locations, and 0s in the last n − d locations. 8\nFor the purpose of exposition we break the main ideas into ﬁve steps below. First, we consider the easier case when the exact value of d is known.\n(1) Finite set of Perturbation Vectors: For the known d case we deﬁne a ﬁnite set Φ (that depends on the true x) containing so- called \u201cperturbation vectors\u201d. 9 In particular, Φ = {φ } d(n−d) k =1\nis the set of d(n − d) vectors with a single −1 in the sup- port of x, a single 1 outside the support of x, and zeroes everywhere else. For instance, the ﬁrst φ in the set equals (−1, 0, . . . , 0, 1, 0, . . . , 0), where the 1 is in the (d + 1)th location. We demonstrate that any ¯ x in the feasible set of the constraint set of NCBP-LP can be written as the true x plus a non-negative linear combination of perturbation vectors from this set. The physical intuition behind the proof is that the vectors from Φ correspond to a \u201cmass-conserving\u201d perturbation of x. The property of non-negativity of the linear combinations arises from a physical argument demonstrating that there is a path from x to any point in the feasible set using these perturbations, over which one never has to \u201cback-track\u201d. The linear combination property is important, since this enables us to characterize the directions in which a vector can be perturbed from x to another vector that satisﬁes the constraints of NCBP-LP, in a \u201cﬁnite\u201d manner (instead of having to consider the uncountably inﬁnite number of directions that x could be perturbed to). The non-negativity of the linear combination is also crucial since, as we explain below, this property ensures that the objective function of the LP can only increase when perturbed in a convex combination of the directions in Φ .\n(2) Expected Perturbation Cost: The heart of our argument then lies in the characterization with an exhaustive case-analysis of the expected change (over randomness in the matrix M and noise ν) in the value of each slack variable η i when x is perturbed to some ¯ x by a vector in Φ . In particular, we demonstrate that for each such individual perturbation vector, the expected change in the value of each slack variable η i is strictly positive with high probability. The actual proof follows from a case-analysis similar to that in the example in Table I.\n(3) Concentration & Union Bounding: With slightly careful use of standard concentration inequalities (speciﬁcally, we need to use both the additive and multiplicative forms of the Chernoff bound) we show that the probability distributions derived above concentrate. We then take the union bound over all vectors in Φ (in fact, there are a total of d(n − d) such vectors in Φ ) and show that with high probability the expected change in the value of the objective function (which equals the weighted sum of the changes in the values of the slack variables η i ) for each vector in Φ is also strictly positive.\n(4) Generalization Based on Convexity: We then note that the set of feasible (¯ x, η) of NCBP-LP forms a convex set. Hence if η strictly increases along every direction in Φ , then in fact η strictly increases when the true x is perturbed in any direction (since, as noted before, any vector in the feasible set can be written as x plus a non-negative linear combination of vectors\n(5) Extension to unknown d: Finally, we invoke Theorem 4 from [7] that demonstrates that in fact, with high probability (for appropriate choice of parameters τ and T ) there is exactly one value of ¯ d ≤ D for which a feasible pair (ˆ x, ˆ η( ¯ d)) exists, (and this pair is unique, and the corresponding ˆ x = x), and in fact this ¯ d = d. The proof in [16] is information-theoretic in spirit, and is analogous to \u201cnearest-neighbour decoding\u201d. Hence in our proof we can assume that the sequence of LPs returns infeasible solutions for each D = d, and a feasible (and correct) solution only for the correct D.\nProof of Theorem 3: We substitute q = 0 into Theorem 2 and choose the largest term.\nProof of Theorem 4: The proof is essentially the same as in the case of Theorem 2. Details in [16]."},"refs":[{"authors":[{"name":"R. Dorfman"}],"title":{"text":"The detection of defective members of large populations"}},{"authors":[{"name":"D.-Z. D"},{"name":"F. K. Hwan"}],"title":{"text":"Combinatorial Group Testing and Its Appli- cations , 2nd ed"}},{"authors":[{"name":"G. Atia"},{"name":"V. Saligrama"}],"title":{"text":"Boolean compressed sensing and noisy group testing"}},{"authors":[{"name":"A. J. Macula"}],"title":{"text":"Error-correcting nonadaptive group testing with de-disjunct matrices"}},{"authors":[{"name":"E. N. Gilbert"}],"title":{"text":"A comparison of signaling alphabets"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[{"name":"C. L. Chan"},{"name":"P. H. Che"},{"name":"S. Jaggi"},{"name":"V. Saligrama"}],"title":{"text":"Non-adaptive probabilistic group testing with noisy measurements: Near-optimal bounds with efﬁcient algorithms"}},{"authors":[{"name":"J. Feldman"},{"name":"M. J. Wainwright"},{"name":"D. R. Karger"}],"title":{"text":"Using linear program- ming to decode binary linear codes"}},{"authors":[{"name":"W. Xu"},{"name":"E. Mallada"},{"name":"A. Tang"}],"title":{"text":"Compressive sensing over graphs"}},{"authors":[{"name":"D. Sejdinovic"},{"name":"O. Johnson"}],"title":{"text":"Note on noisy group testing: Asymptotic bounds and belief propagation reconstruction"}},{"authors":[{"name":"E. J. Cand`es"}],"title":{"text":"The restricted isometry property and its implications for compressed sensing"}},{"authors":[{"name":"R. Baraniuk"},{"name":"M. Davenport"},{"name":"R. DeVore"},{"name":"M. Wakin"}],"title":{"text":"A simple proof of the restricted isometry property for random matrices"}},{"authors":[{"name":"A. Macula"}],"title":{"text":"Probabilistic nonadaptive group testing in the presence of errors and dna library screening"}},{"authors":[{"name":"M. Sobel"},{"name":"R. M. Elashoff"}],"title":{"text":"Group testing with a new goal, estimation"}},{"authors":[{"name":"F. J. McWilliam"},{"name":"N. J. A. Sloan"}],"title":{"text":"The Theory of Error-Correcting Codes "}},{"authors":[{"name":"C. L. Chan"},{"name":"S. Jaggi"},{"name":"V. Saligrama"},{"name":"S. Agnihotri"}],"title":{"text":"Non-adaptive group testing: Explicit bounds and novel algorithms"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564469.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T8.3","endtime":"12:30","authors":"Chun Lam Chan, Sidharth Jaggi, Venkatesh Saligrama, Samar Agnihotri","date":"1341403800000","papertitle":"Non-adaptive Group Testing: Explicit bounds and novel algorithms","starttime":"12:10","session":"S10.T8: Group Testing and Detection","room":"Stratton (491)","paperid":"1569564469"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
