{"id":"1569566395","paper":{"title":{"text":"Extremality Properties for Gallager\u2019s Random Coding Exponent"},"authors":[{"name":"Mine Alsan"}],"abstr":{"text":"Abstract\u2014We describe certain extremality properties for Gal- lager\u2019s reliability function E 0 for binary input symmetric DMCs. In particular, we show that amongst such DMC\u2019s whose E 0 (ρ 1 ) has a given value for a given ρ 1 , the BEC and BSC have the largest and smallest value of the derivative of E 0 (ρ 2 ) for any ρ 2 ≥ ρ 1 . As the random coding exponent is obtained by tracing the map ρ → (E 0 (ρ), E 0 (ρ) − ρE 0 (ρ)) this conclusion includes as a special case the results of [1]. Furthermore, we show that amongst channels W with a given value of E 0 (ρ) for a given ρ the BEC and BSC are the most and least polarizing under Arıkan\u2019s polar transformations in the sense that their polar transforms W + and W − has the largest and smallest difference in their E 0 values.\nIndex Terms\u2014Channel reliability function, random coding exponent, channel polarization."},"body":{"text":"While the capacity of a memoryless channel W gives the largest rate that may be communicated reliably across it, the reliability function E(R, W ) provides a ﬁner measure on the quality of the channel: for any rate R less than channel capacity, it is possible to ﬁnd a sequence of codes of increasing blocklength, each of which of rate at least R, and whose block error probability decays exponentially to zero in the blocklength \u2014 E(R, W ) is the largest possible rate of this decay.\nGallager classical treatise [2] gives a lower bound to E(R, W ), the random coding exponent E r (R, W ) in the form E r (R, W ) = max ρ∈[0,1] E 0 (ρ, W ) − ρR. Remarkably, this lower bound is tight for rates above the critical rate E 0 (1, W ). The function E 0 (ρ, W ) that appears as an auxiliary function on the road to deriving E r (R, W ) turns out to be of independent interest in its own right. In particular, E 0 (ρ, W )/ρ is the largest rate for which a sequential decoder can operate while keeping the ρ-th moment of the decoder\u2019s computation effort per symbol bounded.\nIn this paper we investigate the extremal properties of E 0 (ρ, W ) for the class of binary input symmetric channels. We show that among all such channels with a given value of E 0 (ρ 1 , W ) the binary erasure channel (BEC) and the binary symmetric channel (BSC) distinguish themselves in certain ways: they have, respectively, the largest and smallest value of E 0 (ρ 2 , W ) for any ρ 2 ≥ ρ 1 . Among the simple corollaries of this is the conclusion that of all the channels with the same capacity, the BEC and BSC have the largest and smallest value of E r (R, W ), a result reported in [1] last year.\nThe BEC and BSC also exhibit themselves as being ex- tremal for Arıkan\u2019s polarization transforms. In his award winning paper [3], Arıkan describes two synthetic channels W + , and W − which can be obtained from two independent copies of W . It is well known (proved as a corollary to extremes of information combining) that among all channels W with a given symmetric capacity I(W ), the BEC and BSC polarize most and least in the sense of having the largest and smallest difference between I(W + ) and I(W − ). We report a more general conclusion: amongst all channels W with a given value of E 0 (ρ, W ), the BEC and BSC polarize most and least in the sense of having the largest difference between E 0 (ρ, W + ) and E 0 (ρ, W − ).\nDeﬁnition 1: [2] Given a discrete memoryless channel W with input alphabet X and output alphabet Y, ﬁx a distribution Q on its input alphabet.\n(2) The random coding exponent is deﬁned as\nWe restrict our attention to symmetric binary input channels (B-DMCs). In this case the uniform input distribution maxi- mizes Equation (3). Accordingly, we drop the variable Q from the notations. The expression in (2) becomes\nTheorem 5.6.3 in [2] summarizes the properties of E 0 (ρ, W ) with respect to the variable ρ. For ρ ≥ 0, E 0 (ρ, W ) is a positive, concave increasing function in ρ. Moreover, the symmetric capacity I(W ) and the Bhattacharyya parameter Z(W ) of the channel can be derived from E 0 (ρ, W ) by\nThe maximization in the right hand side of Equation (1) over ρ ∈ [0, 1] can be described in terms of the following parametric equations:\nWe show in this section that the binary erasure channel and the binary symmetric channel are extremal among all sym- metric B-DMCs with respect to the random coding exponent. In particular, we show in Theorem 1 a certain extremality property holds even when the quantities appearing in the parametric form of the error exponent are allowed to be evaluated at different values of the parameter.\nTheorem 1: Given a symmetric B-DMC W , for any ﬁxed value of ρ 1 ∈ [0, 1], we deﬁne a binary symmetric channel W BSC , and a binary erasure channel W BEC through the equality\nRemark 1: The erasure probability of W BEC and the crossover probability of W BSC depend both on the channel W and the parameter ρ 1 .\nFor the special case when ρ 1 = ρ 2 , we recover in the next Corollary a result obtained in [4].\nCorollary 1 ([4]): Given a symmetric B-DMC W , for any ﬁxed value of ρ ∈ [0, 1], we deﬁne a binary symmetric channel\nProof: Since E r (ρ, W ) = E 0 (ρ, W ) − ρR(ρ, W ), it sufﬁces to prove the ﬁrst set of inequalities in view of (8). For a ﬁxed value of ρ, we deﬁne another binary erasure channel W BEC through the equality\nBy Theorem 1, we know that R(ρ, W ) ≤ R(ρ, W BEC ). Via (8), we thus get R(ρ, ˜ W BEC ) ≤ R(ρ, W BEC ). As a result,\nThe inequality for the binary symmetric channel can be obtained similarly.\nAnother particular case of Theorem 1 when ρ 1 → 0 recovers the result in [1]: among all symmetric B-DMCs of the same capacity, the binary erasure channel and the binary symmetric channel are extremal with respect to the random coding exponent. This is shown in the next corollary.\nCorollary 2 (Theorem 2.3 [1]): Given a symmetric B- DMC W of capacity I(W ), we deﬁne a binary symmetric channel W BSC , and a binary erasure channel W BEC of the same capacity through the equality\nProof: Same capacity is equivalent to E 0 (ρ 1 , W )\nMoreover, we can reformulate Theorem 1 by replacing the equality condition imposed for the functions E 0 (ρ, W ) in Equation (6), by equality among the functions E 0 (ρ, W ) ρ \t . Since this latter is a continuous function of ρ, we deduce that the result of the theorem is still valid for ρ 1 → 0. As a consequence, for any ρ 2 ∈ [0, 1], we have\n∂ρ E 0 (ρ, W ) = R(ρ, W ), this last inequality implies that E 0 (ρ, W BSC ) ≤ E 0 (ρ, W ) ≤ E 0 (ρ, W BEC )\nwhich, in turn, implies the inequality for the random coding exponent.\nIn this section, we study the behavior of E 0 (ρ, W ) from the aspect of channel polarization. In Theorem 2, we show that the binary erasure channel and the binary symmetric channel are also extremal in the evolution of E 0 (ρ, W ) under the polarization transformations.\nIn [3], a low complexity code construction that achieves the capacity of symmetric B-DMCs is given based on the recursive application of two basic channel transformations. These transforms synthesize two new channels W − : X → Y 2 and W + : X → Y 2 × X by combining two copies of the channel W . The channels are deﬁned by\nArıkan shows that the recursive application of these trans- forms polarize any W in the sense that almost all synthesized channels are either almost perfect or almost very noisy. Intu- itively, one expects that E 0 (ρ)/ρ similarly polarizes. The next proposition provides a simple proof for this argument.\nProposition 1: After the recursive application of the po- larization transformations, the E 0 (ρ)/ρ parameters of the synthesized channels polarize to the extremal values {0, 1} when a long sequence of steps is applied to the channel.\nProof: In [5], it is shown that E 0 (ρ, W )/ρ is a decreasing function in ρ . Therefore, we can squeeze the function between\nThe proof follows based on the fact that the quantities I(W ), and Z(W ) both polarize, i.e., I(W ) → 0 ⇐⇒ Z(W ) → 1, and I(W ) → 1 ⇐⇒ Z(W ) → 0 [3].\nRemark 2: Based on numerical experiments, we conjecture the channels W , W − , and W + satisfy the next relationship:\nRemark 3: [6] For a symmetric W the channels W − , W , and W + are ordered by degradation. Consequently,\nTheorem 2: Given a symmetric B-DMC W , for any ﬁxed value of ρ ∈ [0, 1], we deﬁne (as in Theorem 1) a binary symmetric channel W BSC , and a binary erasure channel W BEC through the equality\nIn Theorem 2, we have shown that among all B-DMC\u2019s W of ﬁxed E 0 (ρ, W ), the binary erasure channel W − transforma- tion results in a lower bound to any E 0 (ρ, W − ) and the binary symmetric channel\u2019s one in an upper bound to any E 0 (ρ, W − ). For the W + transformation, a similar extremality property holds except the difference that the binary erasure channel W + transformation provides an upper bound and the binary symmetric channel\u2019s one a lower bound to any E 0 (ρ, W + ). This shows that the binary erasure and binary symmetric channels appear on reversed sides of the inequalities for E 0 (ρ, W − ) and E 0 (ρ, W + ).\nE 0 (ρ, W + BSC ) − E 0 (ρ, W − BSC ) ≤ E 0 (ρ, W + ) − E 0 (ρ, W − ) ≤ E 0 (ρ, W + BEC ) − E 0 (ρ, W − BEC ).\nRemark 4: Dividing all sides of the inequality above by ρ and taking the limit as ρ → 0, we see that among channels\nof a given symmetric capacity the BEC and BSC are extremal with respect to the polarization transformations, in the sense that\nThese inequalities can also be obtained by the results on the extremes of information combining [7], together with the fact that symmetric capacity is preserved under the polarization transformations [3]. Note also that, when combined with the preservation property, only one of the Equations (13) and (14) would be sufﬁcient to get the result.\nWe have described some extremality properties for binary input channels when the information measure is Gallager\u2019s E 0 . These properties yield in straightforward fashion already known extremality results.\nThe extremality of BEC and BSC for polar transforms can be interpreted in the context of information combining. Theorem 2 shows that even if we change the measure of information from the customary mutual information to E 0 the channels BEC and BSC still remain extremal.\nThe assumed symmetry of the channel W is not a major limitation: as long as we are interested only in the case when the input distribution Q is uniform, the E 0 of a channel W is the same as an associated symmetric channel ˜ W as deﬁned in Lemma 1.4 of [8]. Since the symmetrizing operator W →\n˜ W commutes with the polar transforms, all the conclusions of the paper are valid for arbitrary binary input channels as long as one evaluates all quantities under the uniform input distribution.\nThe author would like to thank Emre Telatar for helpful discussions.\nWe start with a lemma proved in [4] that expresses the function E 0 (ρ, W ) in a more suitable form for the proof.\nLemma 1: [4] Given a symmetric B-DMC W , and a ﬁxed ρ ∈ [0, 1], there exist a random variable Z taking values in the [0, 1] interval such that\ng(ρ, z) = 1 2\n(1 + z) 1 1+ρ + 1 2\nMoreover, the random variable Z BEC of a binary erasure channel is {0, 1} valued. The random variable Z BSC of a binary symmetric channel is a constant z BSC .\nR(ρ, W ) = E [−∂g(ρ, Z)/∂ρ] E [g(ρ, Z)]\nTo prove the theorem we need to introduce a set of lemmas, and corollaries to them.\nLemma 2: For a ﬁxed value of ρ, the function g(ρ, z) deﬁned in Equation (16) is a concave decreasing function in the variable z.\nLemma 3: For ﬁxed values of ρ 1 , ρ 2 ∈ [0, 1] such that ρ 1 ≤ ρ 2 , the function ˜ f ρ 1 ,ρ 2 (t) deﬁned as\nThe convexity analysis is tedious, and we refer the readers to Appendix D of [9] for a proof. We ﬁrst prove Theorem 1 for the case ρ 1 = ρ 2 in the next corollary.\nCorollary 4: Under the same assumptions of Theorem 1, the following extremality property holds:\nProof: By the equality conditions in Equation (6), the denominator in Equation (17) is the same for all the channels. Then, the proof follows directly by the concavity of the function ˜ f ρ 1 (t) in t, and the special structure of the Z random variable corresponding to the channels W BSC , and W BEC . We deﬁne the random variable T = g(ρ, Z). By the two sides of the Jensen\u2019s inequality for concave functions, i.e.\n1 − 2 −ρ 1 \t (E [T ] − 1) ≤ E ˜ f ρ 1 (T ) . ≤ ˜ f ρ 1 (E [T ])\nCorollary 5: Under the same assumptions of Theorem 1, we have for ρ 2 ≥ ρ 1\nProof: By the continuity of E 0 (ρ, W ) in the channel, it sufﬁces to show that\nE 0 (ρ, W BSC ) − E 0 (ρ, W ). Noting that R(ρ) = ∂ ∂ρ E 0 (ρ), the corollary is implied by the following statement:\nBut this is true by elementary considerations on differential equations.\nNow, we proceed with the proof of the theorem for the case ρ 2 > ρ 1 . By Lemma 3, the function ˜ f ρ 1 ,ρ 2 (t) is concave in t. So, we can apply the two sides of Jensen\u2019s inequality to obtain\nOn the other hand, as E 0 (ρ, W ) = − log E [g(ρ, Z)] by Lemma 1, we know by Corollary 5 that\nTaking the ratios of the last two set of inequalities proves the extremality property stated in Equation (7).\nAs we did in Appendix A, we start with two lemmas to express the functions E 0 (ρ, W + ), and E 0 (ρ, W − ) in a similar form as in Lemma 1.\nLemma 4: Given a B-DMC W and a ﬁxed ρ ∈ [0, 1], there exist i.i.d. random variables Z 1 and Z 2 taking values in the [0, 1] interval such that\nLemma 5: Given a B-DMC W and a ﬁxed ρ ∈ [0, 1], there exist i.i.d. random variables Z 1 and Z 2 taking values in the [0, 1] interval such that\nThe proof of the extremality property for the W − transfor- mation, given in Equation (13), relies on the convexity of the function deﬁned in the next lemma.\nLemma 6: We deﬁne the function F z,ρ (t) : [2 −ρ , 1] → [g(ρ, z), 1] as\nwhere both z, ρ ∈ [0, 1]. For ﬁxed values of ρ and z, the function F z,ρ (t) is convex with respect to the variable t.\nThe convexity analysis of this function is tedious. We refer the readers to Appendix B of [9] for a proof. By Lemmas 1 and 4, we have\nwhere Z 1 and Z 2 are independent random variables. Moreover by Lemma 1, we know Z BSC = z BSC and Z BEC ∈ {0, 1}. Hence,\nLet be the erasure probability of W BEC . Then, we have P (Z BEC = 0) = , and\nSince, it is known that the channel W − is a BEC with erasure probability 2 − 2 , we get\nexp{−E 0 (ρ, W − )} = E Z 1 [E Z 2 [F z 1 ,ρ (g(ρ, Z 2 )) | Z 1 = z 1 ]] ≥ E Z 1 [F Z 1 ,ρ (E Z 2 [g(ρ, Z 2 )])] = E Z 1 [F Z 1 ,ρ (g(ρ, z BSC ))]\nZ 1 [F z BSC ,ρ (g(ρ, Z 1 ))] ≥ F z BSC ,ρ (E Z 1 [g(ρ, Z 1 )]) = exp{−E 0 (ρ, W − BSC )}\nwhere (1) follows by symmetry of the variables Z 1 and z BSC . Similarly, given E 0 (ρ, W ) = E 0 (ρ, W BEC ), we have\nexp{−E 0 (ρ, W − )} \t (21) = E Z 1 [E Z 2 [F z 1 ,ρ (g(ρ, Z 2 )) | Z 1 = z 1 ]]\n−ρ ) + 2 −ρ − 1] 2 2 −ρ − 1\n= 2P (Z BEC = 0) − P (Z BEC = 0) 2 (1 − 2 −ρ ) + 2 −ρ = exp{−E 0 (ρ, W − BEC )}.\nNow, we sketch the proof of the extremality property for the W + transformation, given in Equation (14).\n1 − z 1 z 2 ) (22) where ρ, z 1 , z 2 ∈ [0, 1]. Note that h ρ (z 1 , z 2 ) is symmetric in the variables z 1 , and z 2 .\nLemma 7: Let the function H z,ρ (t) : [2 −ρ , 1] → [2 −ρ , g(ρ, z)] be deﬁned as\nwhere both z, ρ ∈ [0, 1]. For ﬁxed values of ρ and z, the function H z,ρ (t) is concave with respect to the variable t. We refer the readers to Appendix F of [9] for a proof.\nThe proof of the theorem for the W + transformation can be completed following similar steps to the W − case. By Lemma 5, we have E [h ρ (Z 1 , Z 2 )] = exp{−E 0 (ρ, W + )}. We deﬁne T 1 = g(ρ, Z 1 ), and T 2 = g(ρ, Z 2 ). Then, using the concavity of the function H z,ρ (t) with respect to t, and symmetry of Z 1 and Z 2 , we obtain"},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566395.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T8.3","endtime":"12:30","authors":"Mine Alsan","date":"1341576600000","papertitle":"Extremality Properties for Gallager's Random Coding Exponent","starttime":"12:10","session":"S16.T8: Error Exponents","room":"Stratton (491)","paperid":"1569566395"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
