{"id":"1569565669","paper":{"title":{"text":"The Capacity of the Multi-MMSE Constrained Gaussian Channel"},"authors":[{"name":"Ronit Bustin"},{"name":"Shlomo Shamai (Shitz)"}],"abstr":{"text":"Abstract\u2014We examine codes, over the additive Gaussian noise channel, designed for reliable communication at some speciﬁc signal-to-noise ratio (snr) and constrained by the permitted MMSE at K lower snrs. Extending the result of the single MMSE constrained code, we show that K-layers superposition codes attain the constrained capacity. Moreover, we prove that given a reliable code attaining the multi-MMSE constrained capacity, its MMSE and mutual information, as functions of snr, are completely deﬁned. Thus, no other multi-MMSE constrained capacity achieving code attains better MMSE performance at unconstrained snrs."},"body":{"text":"In this work we examine the additive Gaussian noise chan- nel, and derive the capacity region under K minimum-mean square error (MMSE) constraints, at lower signal-to-noise ratios (snrs). In [1] it was shown that the mutual information and thus also the MMSE of \u201cgood\u201d (capacity achieving [2]) point-to-point codes is known exactly, no matter the speciﬁc structure of the code. Constraining the MMSE may only reduce the maximum possible rate. In [3], [4] the authors investigate \u201cbad\u201d point-to-point codes (non-capacity achiev- ing) [3], [4], heavily used in many multi-terminal wireless networks. It was shown in [4] that \u201cbad\u201d codes can obtain lower MMSE at low snrs. However, the exact tradeoff between rate and MMSE was still an open question. In [5] this tradeoff was exactly depicted for a single MMSE constraint at some lower snr. This work extends the description of the tradeoff to an arbitrary number of MMSE constraints. In both cases it is shown that Gaussian superposition codebooks attain the MMSE constrained capacity.\nThe advantage of a \u201cbad\u201d point-to-point code, constrained in MMSE at some lower snrs, is meaningless in point-to- point communication, where all that matters is the perfor- mance at the receiver. However, in multi-terminal wireless networks, such as a cellular network, the case is different. In such networks the transmission of one user interferes with neighboring, unintended, receivers. A lower MMSE implies better possible interference cancelation, and thus improved rates for the interfered user. For comparison, the performance of optimal point-to-point codes in the interference setting was investigated in [6].\nThe best known achievable region for the two-user interfer- ence channel is given by the Han and Kobayashi (HK) scheme [7]. This scheme uses partial decoding of the interfering message at the receiver. Rate splitting (that is, superposition coding) is a special case of the HK scheme, and is also point- to-point \u201cbad\u201d (see [3, Appendix VIII-C]). It was shown in [8] that these codes are close to optimal for the two-user Gaussian interference channel, and in fact are within one bit from capacity. On the other hand, as explained in [9], for K-users interference channels these codes might very well be sub-optimal. This does not contradict the result show here, that these codes are in fact optimal MMSE-wise for any number of unintended receivers (see concluding remarks in section IV).\nA similar question has been raised in the work of Bandemer and El Gamal [10], where they provide the rate-disturbance region: for any given rate that can be transmitted reliably to the intended receiver, what is the minimum possible disturbance that can be attained at some interfered user. In [10] the authors measure the disturbance using the mutual information between the codeword and the output at the interfered user, rather than the minimum possible MMSE, as done here. We further discuss and compare the two measures in our concluding remarks (section IV).\nIn this work we are looking at the transmission of code- words, of length n, through a discrete memoryless standard Gaussian channel:\nwhere N is standard additive Gaussian noise. The codewords are constrained by the standard average power constraint:\nwhere C n stands for a code of n-dimensional codewords. We examine codebooks designed for reliable transmission at γ = snr K (reliable decoding of the codeword from Y (γ = snr K )). Our main interest will be in examining non-optimal codes, alternatively known as \u201cbad\u201d codes [4], deﬁned using code- sequences, as follows:\nDeﬁnition 1. A non-optimal code-sequence C = {C n } ∞ n=1 , for a channel with capacity C, is a code-sequence with vanishing\n1 n\nwhere E X (γ) is the MMSE matrix when estimating the codeword X from the output of the channel Y =\nSurely, for any such codes, the error probability for any γ > snr K is zero, when n → ∞, since reliable transmission is guaranteed at snr K . As a result MMSE c (γ) for these snrs is also zero. On the other hand, for γ < snr K the value of the error probability is not guaranteed to be any speciﬁc value. For an optimal code, it was shown in [1], that I(γ) for γ < snr K follows that of the Gaussian i.i.d. input and thus MMSE c (γ) is also known exactly and descends gradually according to 1 1+γ .\nOur goal is to understand the tradeoff between the rate at snr K and the MMSE at lower snrs. In other words, what is the maximum rate, assuming we require limited MMSE at lower snrs, or equivalently, given a reduced rate at snr K what is the minimum possible MMSE at lower snrs. In [5] we have shown that given a reliable code of reduced rate, designed for some snr 2 , we can obtain a lower bound on the MMSE of that code at some snr 1 < snr 2 . Moreover, this lower bound is attained by the optimal superposition codebook for (snr 1 , snr 2 ). Equivalently, this result can be stated as the maximum rate at snr 2 assuming the code is limited in the MMSE at snr 1 . This result is given in the next theorem, in the later form.\nTheorem 1 ([5]). Assume snr 1 < snr 2 . The solution of the following optimization problem,\n1 + βsnr 1 \t (8) for some β ∈ [0, 1], is the following\nI(snr 2 ) = 1 2\nlog (1 + βsnr 2 ) + 1 2\n1 + βsnr 1 \t (9) and is attainable when using the optimal Gaussian superpo- sition codebook designed for (snr 1 , snr 2 ) with a rate-splitting coefﬁcient β.\nIn [11] this result has been extended to the case of two MMSE constraints, where we have shown that a two-layer superposition code attains the maximum rate. In this work we further extend the problem to arbitrary K constraints, showing that K-layers superposition codes attain the optimal rate. Moreover, we examine the behavior of optimal codes complying with all K constraints, and show that their MMSE and mutual information is completely deﬁned, and, of course, is that of the K-layers superposition code.\nThe approach used in order to provide insight into the above mentioned problem is the I-MMSE approach, this to say that we make use of the fundamental relationship between the mutual information and the MMSE in the Gaussian channel and its generalizations [12], [13]. Even though we are exam- ining a scalar Gaussian channel, the n-dimensional version of this relationship is required since we are looking at the transmission of n-dimensional codewords through the channel. In our setting the relationship is as follows:\nI(snr) = 1 2\nThe main property of the I-MMSE used for these proofs is an n-dimensional \u201csingle crossing point\u201d property derived in [14] given here for completeness. This property is an extension of the scalar \u201csingle crossing point\u201d property shown in [15]. In [14] the following function is deﬁned for an arbitrary random vector X:\nwhere A is some n × n general weighting matrix. The following theorem is proved in [14],\nTheorem 2 ([14]). Let A ∈ S n + be a positive semideﬁnite matrix. Then, the function γ → q A (X, σ 2 , γ), deﬁned in (12), has no nonnegative-to-negative zero crossings and, at most, a single negative-to-nonnegative zero crossing in the range γ ∈ [0, ∞). Moreover, let snr 0 ∈ [0, ∞) be that negative-to- nonnegative crossing point. Then,\n2) q A (X, σ 2 , γ) is a strictly increasing function in the range γ ∈ [0, snr 0 ).\nIn this work, the matrix A can be set to the identity matrix. The above property is valid for all natural n, thus we may also take n → ∞.\nAn important family of non-optimal codes, that is, a family of codes that do not attain the point-to-point capacity at snr K ,\nis that of Gaussian superposition codes which are optimal for a degraded Gaussian BC [16]. As will be shown in the sequel these codes are optimal MMSE-wise. The analysis of this family was done by Merhav et. al. in [17, section 5.3] from a statistical physics perspective. As noted in [17], the MMSE of this family of codebooks undergoes phase transitions, that is, it is a discontinuous function of γ. The mutual information, I(γ), and MMSE c (γ) of this family of codebooks is known exactly and given in the next theorem. An example of a 3- layers superposition code is depicted in Figure 1.\nTheorem 3 (extension of [17] section V.C). A superposition codebook designed for (snr 0 , snr 1 , · · · , snr K ) with the rate- splitting coefﬁcients β 0 > · · · > β K−1 has the following I(γ):\n       \n      \n+ 1 2 log (1 + β i γ) , if snr i ≤ γ ≤ snr i+1\n \n, snr i ≤ γ ≤ snr i+1 0, \t snr K < γ\nWe refer to this codebook as the optimal Gaussian K-layers superposition codebook.\nTheorem 4. Assume snr 0 < snr 1 < · · · < snr K ( K ≥ 1 is some natural number). The solution of the following optimiza-\n1 2\nand is attainable when using the optimal K-layers Gaussian superposition codebook designed for (snr 0 , snr 1 , · · · , snr K ) with rate-splitting coefﬁcients (β 0 , · · · , β K−1 ).\nfor snr i−1 ≤ snr ≤ snr i when β ≥ β i−1 , do not affect the above result.\nProof: It is simple to verify that the optimal Gaussian K- layers superposition codebook (Theorem 3) complies with the above MMSE constraints and attains the maximum rate. Thus, we need to derive a tight upper bound on the rate. Deriving the upper bound begins with the usage of Theorem 1. Due to the constraint at snr 0 :\n1 + β 0 snr 0 \t (16) we have the following upper bound\nThe other constraints, for i ∈ {1, 2, . . . , K −1}, can be written as follows,\nwhere mmse G i (snr i ) denotes the MMSE of the estimation of a Gaussian random variable, X G i , with zero mean and variance β i , from Y = √ snr i X G i + N , where N ∼ N (0, 1). Thus,\nAccording to Theorem 2 the function q I (X, β i , γ) has no nonnegative-to-negative zero crossings, thus we may conclude that,\n= 1 2\n+ 1 2\n· · · 1 + β K−2 snr K−1 1 + β K−2 snr K−2\nThis allows us to provide a tight upper bound on the following difference:\nUsing (17) and (21) we can bound (22) as shown in (23) at the top of this page.\nNow, according to (20) we have that any additional con- straint, MMSE c (snr ) ≤ \t β 1+β snr for snr i−1 ≤ snr ≤ snr i when β ≥ β i−1 , is already complied with, since\nTheorem 4 states that K-layers superposition codes attain the maximum possible rate at snr K under a set of K MMSE constraints at lower snrs. However, there might be a different codebook with this property, which also has some other desirable properties. In the next theorem we prove that the MMSE and mutual information as a function of the snr, for any code attaining the maximum rate under the set of MMSE constraints, are completely deﬁned for all snr, and are those of K-layers superposition codes. Thus, no other code can outperform superposition codes in this sense.\nTheorem 5. The MMSE c (γ) (and thus also I(γ)) of any code attaining the maximum rate at snr K , under the MMSE constraints, deﬁned in Theorem 4, is completely deﬁned for all 0 ≤ γ, and is that of the K-layers superposition codebook.\nProof: Due to the set of K constraint and following the steps that lead to (20) in the proof of Theorem 4 we can\nfor i ∈ {0, 1, 2, . . . , K − 1}, where mmse G i (snr i ) denotes the MMSE of the estimation of a Gaussian random variable, X G i , with zero mean and variance β i , from Y =\nIn the proof of Theorem 4, equation (21), we have seen that the above property can be used to construct the following upper bounds\n= 1 2\nlog 1 + β i snr i+1 1 + β i snr i\nI(snr K ) − I(snr 0 ) = 1 2\n1 2\n1 2\n1 2\nlog 1 + β i snr i+1 1 + β i snr i\nwhere we used both the assumption that the code attains the maximum rate at snr K , under the MMSE constraints (Theorem 4), and the maximum entropy theory to obtain the maximum\nfor any code attaining the maximum rate at snr K under the MMSE constraints, given in Theorem 4. Looking at the upper bound (27), this equality can be attained only if\n1 2\nfor all i ∈ {0, 1, . . . , K − 1}. Due to (25) this is equivalent to MMSE c (γ) = mmse G i (γ) = β i 1+β\nfor all snr i ≤ γ < snr i+1 . Thus, we deﬁned the function MMSE c (γ) for all γ ∈ [snr 0 , snr K ]. Surely since this is a reliable code designed for snr K , we also have that MMSE c (γ) = 0 for all γ ≥ snr K . The only region that remains to be determined is γ ∈ [0, snr 0 ]. Since the lower bound, (28), is attained with equality we have I(snr 0 ) = 1 2 log (1 + snr 0 ) which guarantees that MMSE c (γ) = 1 1+γ for all γ ∈ [0, snr 0 ].\nThe model presented here is not an interference channel, but rather a model with a single transmitter and K unintended neighboring receivers. We have shown that the K-layers superposition code is optimal MMSE-wise for this setting. The relation of this model to the two-user interference channel is clear: each receiver is interfered by a single transmitter (K = 1 in our setting). Although the two-user interference channel capacity is still, in general, an open problem, intuition suggests that evaluating the affect of the interference using its MMSE is a reasonable approach. In this sense, our results for K = 1 support the good performance of the HK scheme [8], as explained in [5]. On the other hand, in the K + 1-user interference channel, each receiver is interfered by K trans- mitters. The total effect of the interference on each receiver, is a function of all of these transmissions. Thus, the extension presented here is only one of the K building blocks of the total interference in the K +1-user interference channel. This settles with the fact that coding schemes that deal directly with the interference rather than with each interferer separately, such as interference alignment and structure codes, achieve larger number of degrees of freedom than simple HK schemes for the Gaussian noise channel (see [9] and reference therein). The extension shown here does provide further insight to the design and performance of the novel decoding approach presented in [18] for the fully connected K-user Gaussian interference channel.\nAs stated in the introduction, another possible quantity to measure the effect of the interference (or \u201cdisturbance\u201d) is the mutual information at the unintended receiver. This is the approach investigated, for the general discrete mem- oryless channel, in [10]. In [11] we have shown that this approach is conceptually different than the one suggested here. More speciﬁcally, [10, Corollary 2], which derives the rate- disturbance region for the two-user Gaussian case, and can also\nbe derived directly from the I-MMSE formulation, does not indicate a superposition coding scheme, but rather a Gaussian code with reduced power, and does not attain the minimum MMSE at the lower snr. On the other hand, Bandemer and El Gamal extended their result to the two-user MIMO Gaussian case where the result does suggest rate-splitting. Finally, ex- tending to any number of unintended receivers, in the Gaussian regime, using the mutual information disturbance measure, is trivial, as only the most constraining one determines the result."},"refs":[{"authors":[{"name":"M. Peleg"},{"name":"A. Sanderovich"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"On extrinsic in- formation of good codes operating over Gaussian channels"}},{"authors":[{"name":"S. Shamai (Shitz)"},{"name":"S. Verd´ u"}],"title":{"text":"The empirical distiribution of good codes"}},{"authors":[{"name":"A. Bennatan"},{"name":"S. Shamai (Shitz)"},{"name":"A. R. Calderbank"}],"title":{"text":"In prais of bad codes for multi-terminal communications"}},{"authors":[{"name":"A. Bennatan"},{"name":"A. R. Calderbank"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"Bounds on the mmse of \u201cbad\u201d LPDC codes at rates above capacity"}},{"authors":[{"name":"R. Bustin"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"Properties of MMSE of \u201cbad\u201d codes"}},{"authors":[{"name":"F. Baccelli"},{"name":"A. El Gamal"},{"name":"D. Tse"}],"title":{"text":"Interference networks with point- to-point codes"}},{"authors":[{"name":"T. S. Han"},{"name":"K. Kobayashi"}],"title":{"text":"A new achievable rate region for the interference channel"}},{"authors":[{"name":"R. Etkin"},{"name":"D. Tse"},{"name":"H. Wang"}],"title":{"text":"Gaussian interference capacity to within one bit"}},{"authors":[{"name":"D. Tuninetti"}],"title":{"text":"K-user interference channels: General outer bounds and sum-capacity for certain Gaussian channels"}},{"authors":[{"name":"B. Bandemer"},{"name":"A. El Gamal"}],"title":{"text":"Communication with disurbance constraints"}},{"authors":[{"name":"R. Bustin"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"On Gaussian channels with MMSE interference"}},{"authors":[{"name":"D. Guo"},{"name":"S. Shamai (Shitz)"},{"name":"S. Verd´ u"}],"title":{"text":"Mutual information and minimum mean-square error in Gaussian channels"}},{"authors":[{"name":"D. P. Palomar"},{"name":"S. Verd´ u"}],"title":{"text":"Gradient of mutual information in linear vector Gaussian channels"}},{"authors":[{"name":"R. Bustin"},{"name":"M. Payar´o"},{"name":"D. P. Palomar"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"On MMSE properties and I-MMSE implications in parallel MIMO Gaussian channels"}},{"authors":[{"name":"D. Guo"},{"name":"Y. Wu"},{"name":"S. Shamai (Shitz)"},{"name":"S. Verd´ u"}],"title":{"text":"Estimation in Gaussian noise: Properties of the minimum mean-square error"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements in Information Theory"}},{"authors":[{"name":"N. Merhav"},{"name":"D. Guo"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"Statistical physics of signal estimation in Gaussian noise: Theory and examples of phase transitions"}},{"authors":[{"name":"C. Gong"},{"name":"A. Tajer"},{"name":"X. Wang"}],"title":{"text":"Interference channel with constrained partial group decoding"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565669.pdf"},"links":[{"id":"1569566381","weight":10},{"id":"1569566527","weight":3},{"id":"1569566485","weight":10},{"id":"1569564889","weight":3},{"id":"1569565663","weight":7},{"id":"1569565377","weight":7},{"id":"1569566385","weight":7},{"id":"1569567049","weight":7},{"id":"1569564635","weight":3},{"id":"1569565067","weight":17},{"id":"1569559665","weight":3},{"id":"1569565691","weight":3},{"id":"1569566875","weight":3},{"id":"1569564605","weight":3},{"id":"1569566981","weight":7},{"id":"1569566683","weight":3},{"id":"1569559259","weight":3},{"id":"1569556029","weight":7},{"id":"1569564481","weight":7},{"id":"1569566415","weight":3},{"id":"1569566081","weight":3},{"id":"1569565355","weight":17},{"id":"1569565931","weight":3},{"id":"1569551535","weight":3},{"id":"1569566765","weight":7},{"id":"1569565461","weight":7},{"id":"1569565171","weight":3},{"id":"1569564227","weight":10},{"id":"1569565837","weight":3},{"id":"1569566303","weight":7},{"id":"1569566119","weight":3},{"id":"1569566459","weight":3},{"id":"1569565317","weight":3},{"id":"1569566319","weight":3},{"id":"1569566941","weight":7},{"id":"1569558459","weight":7},{"id":"1569565291","weight":3},{"id":"1569564203","weight":7},{"id":"1569556713","weight":3},{"id":"1569566751","weight":3},{"id":"1569565771","weight":10},{"id":"1569560613","weight":17},{"id":"1569566903","weight":3},{"id":"1569566999","weight":14},{"id":"1569564249","weight":3},{"id":"1569565809","weight":3},{"id":"1569566843","weight":7},{"id":"1569566579","weight":7},{"id":"1569558483","weight":10},{"id":"1569565455","weight":3},{"id":"1569566963","weight":7},{"id":"1569566709","weight":10},{"id":"1569565897","weight":3},{"id":"1569551763","weight":3},{"id":"1569566895","weight":3},{"id":"1569566865","weight":3},{"id":"1569566095","weight":3},{"id":"1569565907","weight":3},{"id":"1569566239","weight":7},{"id":"1569566679","weight":3},{"id":"1569563981","weight":3},{"id":"1569566905","weight":7},{"id":"1569566733","weight":7},{"id":"1569566753","weight":7},{"id":"1569566063","weight":3},{"id":"1569558681","weight":3},{"id":"1569566759","weight":3},{"id":"1569559995","weight":3},{"id":"1569566643","weight":3},{"id":"1569566511","weight":7},{"id":"1569565841","weight":3},{"id":"1569566369","weight":3},{"id":"1569566531","weight":3},{"id":"1569567665","weight":3},{"id":"1569561143","weight":7},{"id":"1569565833","weight":10},{"id":"1569564611","weight":17},{"id":"1569566325","weight":7},{"id":"1569566423","weight":3},{"id":"1569567015","weight":3},{"id":"1569566811","weight":14},{"id":"1569553909","weight":3},{"id":"1569566687","weight":7},{"id":"1569562285","weight":3},{"id":"1569553537","weight":7},{"id":"1569565427","weight":3},{"id":"1569566403","weight":7},{"id":"1569553519","weight":3},{"id":"1569567051","weight":3},{"id":"1569566231","weight":10},{"id":"1569554881","weight":3},{"id":"1569554971","weight":3},{"id":"1569566209","weight":10},{"id":"1569562821","weight":3},{"id":"1569566649","weight":3},{"id":"1569565655","weight":7},{"id":"1569566127","weight":7},{"id":"1569558985","weight":10},{"id":"1569565087","weight":7},{"id":"1569566473","weight":10},{"id":"1569564857","weight":3},{"id":"1569564333","weight":3},{"id":"1569565033","weight":7},{"id":"1569563897","weight":3},{"id":"1569565929","weight":3},{"id":"1569566721","weight":7},{"id":"1569565633","weight":7},{"id":"1569555879","weight":14},{"id":"1569565219","weight":10},{"id":"1569558509","weight":3},{"id":"1569566003","weight":3},{"id":"1569556671","weight":7},{"id":"1569566223","weight":7},{"id":"1569566553","weight":10},{"id":"1569566593","weight":7},{"id":"1569566043","weight":10},{"id":"1569565029","weight":3},{"id":"1569565357","weight":3},{"id":"1569565393","weight":3},{"id":"1569562207","weight":3},{"id":"1569566191","weight":14},{"id":"1569565527","weight":7},{"id":"1569567029","weight":3},{"id":"1569565363","weight":3},{"id":"1569566695","weight":3},{"id":"1569566051","weight":7},{"id":"1569555787","weight":7},{"id":"1569565441","weight":3},{"id":"1569565311","weight":3},{"id":"1569566233","weight":3},{"id":"1569566667","weight":7},{"id":"1569566407","weight":3},{"id":"1569560349","weight":7},{"id":"1569566501","weight":3},{"id":"1569565741","weight":3},{"id":"1569566481","weight":3},{"id":"1569565463","weight":3},{"id":"1569566229","weight":3},{"id":"1569566133","weight":7},{"id":"1569562551","weight":7},{"id":"1569563395","weight":14},{"id":"1569551347","weight":7},{"id":"1569566383","weight":7},{"id":"1569566929","weight":3},{"id":"1569565665","weight":3},{"id":"1569565549","weight":7},{"id":"1569565611","weight":10},{"id":"1569566983","weight":10},{"id":"1569566097","weight":3},{"id":"1569556361","weight":3},{"id":"1569565397","weight":10},{"id":"1569565765","weight":7},{"id":"1569565435","weight":7},{"id":"1569557275","weight":3},{"id":"1569566129","weight":10},{"id":"1569566887","weight":3},{"id":"1569566267","weight":7},{"id":"1569564131","weight":10},{"id":"1569566253","weight":7},{"id":"1569566691","weight":10},{"id":"1569565421","weight":3},{"id":"1569566651","weight":7},{"id":"1569566823","weight":3},{"id":"1569566137","weight":3},{"id":"1569565013","weight":7},{"id":"1569566237","weight":7},{"id":"1569566283","weight":7},{"id":"1569566755","weight":3},{"id":"1569566713","weight":10},{"id":"1569565293","weight":7},{"id":"1569566771","weight":7},{"id":"1569566641","weight":7},{"id":"1569559035","weight":3},{"id":"1569564247","weight":3},{"id":"1569551905","weight":14},{"id":"1569564861","weight":3},{"id":"1569556759","weight":14},{"id":"1569566619","weight":3},{"id":"1569565271","weight":3},{"id":"1569561185","weight":10},{"id":"1569566301","weight":3},{"id":"1569565233","weight":3},{"id":"1569563721","weight":7},{"id":"1569560235","weight":3},{"id":"1569566817","weight":3},{"id":"1569564157","weight":3},{"id":"1569566389","weight":3},{"id":"1569566911","weight":14},{"id":"1569564923","weight":7},{"id":"1569566299","weight":3},{"id":"1569564769","weight":10},{"id":"1569566933","weight":7},{"id":"1569563919","weight":3},{"id":"1569557851","weight":7},{"id":"1569565389","weight":7},{"id":"1569559919","weight":3},{"id":"1569565537","weight":3},{"id":"1569555891","weight":3},{"id":"1569559251","weight":3},{"id":"1569567013","weight":7},{"id":"1569566583","weight":3},{"id":"1569560459","weight":10},{"id":"1569565853","weight":7},{"id":"1569550425","weight":3},{"id":"1569564505","weight":7},{"id":"1569565165","weight":7},{"id":"1569561397","weight":3},{"id":"1569565113","weight":3},{"id":"1569564257","weight":3},{"id":"1569564141","weight":10},{"id":"1569566987","weight":25},{"id":"1569551541","weight":7},{"id":"1569551751","weight":3},{"id":"1569558697","weight":7},{"id":"1569564419","weight":21},{"id":"1569566067","weight":3},{"id":"1569566825","weight":3},{"id":"1569564807","weight":3},{"id":"1569566609","weight":3},{"id":"1569566113","weight":7},{"id":"1569566443","weight":10}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S1.T7.2","endtime":"10:30","authors":"Ronit Bustin, Shlomo (Shitz) Shamai","date":"1341223800000","papertitle":"The Capacity of the Multi-MMSE Constrained Gaussian Channel","starttime":"10:10","session":"S1.T7: Gaussian Channels","room":"Stratton (407)","paperid":"1569565669"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
