{"id":"1569565611","paper":{"title":{"text":"A New Outer Bound on the Capacity Region of Gaussian Interference Channels"},"authors":[{"name":"Junyoung Nam"},{"name":"Giuseppe Caire"}],"abstr":{"text":"Abstract\u2014The best known outer bound on the capacity region of the two-user Gaussian interference channel is given as the intersection of regions of genie-aided outer bounds, in which a genie provides extra side information to the receivers. In this paper, we present a new outer bound that does not resort to a genie-aided channel but make use of auxiliary random variables. In order to obtain such bound, we introduce a conditional version of the worst additive noise lemma. The new bound is shown to be tighter than genie-aided bounds for certain range of parameters."},"body":{"text":"In the past few years, the Gaussian interference channel (IC) received great attention as a primitive model for a wireless network with mutually interfering links. While the IC has been studied over 30 years, the capacity region of the simplest two- user Gaussian IC is not fully characterized yet. We know the capacity region only in some special cases such as the very strong and strong interferences.\nThe capacity region for the more generally encountered weak interference situation in wireless networks is still an open problem. Since the works of Kramer [1] and Etkin, Tse, and Wang [2], however, there has been a signiﬁcant progress to the characterization of the sum capacity of the two-user Gaussian IC in the weak interference regime. Following that work, the sum capacity of the Gaussian IC in the very weak interference regime was obtained independently and simultaneously by [3], [4], [5], in which treating interference as noise with Gaussian inputs was shown to be optimal. The sum-rate upper bounds (converses) in all these works are based on a genie-aided channel, in which a genie provides some side information to either one or both receivers in order to cast the original interference channel into a mathematically more tractable one.\nThe characterization of the entire capacity region in the weak interference regime is less known. In the past decade, one successful approach was to use a genie-aided channel to obtain a tight outer bound to the capacity region. Kramer [1] deﬁned a general genie signal and selected a genie that provides the exact information of the interference to one of the receivers such that the resulting genie-aided channel becomes the one- sided Gaussian IC, for which an outer bound is known. Etkin et al. [2] adopted a different genie signal that provides some noisy observation of the intended signals to the receivers and derived an outer bound (so-called \u201cETW outer bound\u201d) tighter than the Kramer\u2019s one for certain ranges of parameters. By\nusing a more general genie signal, the successive works in [3], [4], [5] achieved an enhancement of the ETW bound, which will be called \u201cenhanced ETW outer bound\u201d in this paper. Since the enhanced ETW bound contains the Kramer and ETW outer bounds as special cases, the best known outer bound is given by the enhanced ETW outer bound whose sum- rate upper bound was further tightened later by Etkin [6].\nIn this paper, we turn our attention to a classical approach for the proof of converse based on auxiliary random variables in order to achieve a new outer bound on the capacity region of the two-user Gaussian IC in the weak interference regime. In particular, we do not construct a genie-aided channel to outer-bound the capacity region and instead introduce appro- priate auxiliary random variables and a conditional version of the worst additive noise lemma [7]. Thus, we can obtain a potentially tighter outer bound than those with the aid of a genie (explicit side information to the receivers). One major difﬁculty in upper-bounding the differences of conditional entropies is the fact that the interfering signal terms are no longer Gaussian distributed but just with average power constraints. The role of the auxiliary random variables is to replace arbitrarily distributed interfering signal with i.i.d. Gaussian random sequences to avoid some difﬁcult terms. Then, we can apply the conditional worst additive noise lemma to some differences of conditional entropies to achieve a new outer bound. Finally, we show by numerical examples that the new outer bound is tighter than the previously known outer bounds for certain range of parameters.\nNotations: Cov(X) is the variance of X and Cov(X) is the covariance matrix of X.\nwhere the inputs X 1 and X 2 have the average power con- straints of P 1 and P 2 , respectively, and Z 1 and Z 2 are Gaussian random variables of unit variance that are i.i.d in time and independent of inputs.\nSuppose a sequence of (2 nR 1 , 2 nR 2 ) codes with lim n→∞ P (n) e \t = 0. Let M 1 and M 2 be independent, uniformly distributed messages over [1 : 2 nR 1 ] and [1 : 2 nR 2 ],\nand let X n 1 ∈ X 1 , X n 2 ∈ X 2 , Y n 1 ∈ Y 1 , Y n 2 ∈ Y 2 be the random sequences induced by encoders enc i : [1 : 2 nR i ] → X i , i = 1, 2, and the channel, respectively.\nThe main contribution of the work by Etkin, Tse, and Wang [2] is to devise a genie in a sophisticated manner to get a tighter outer bound on the channel than that in [1]. The authors designed the genie signal such that\nso as to obtain the useful properties, h(Y n 1 |S n 1 , X n 1 ) = h(S n 2 ) and h(Y n 2 |S n 2 , X n 2 ) = h(S n 1 ), where X n i denotes the sequence of length n of the input X i and S i denotes side information to receiver i for i = 1, 2. Using the genie signal, the authors provided a meaningful outer bound in [2, Thm. 3] and showed that a simpliﬁed Han-Kobayashi scheme can achieve a rate region within one bit to the capacity region of the two-user Gaussian IC.\nThroughout this paper, let X ig ∼ N (0, P i ) and Y ig = X ig + Z i for all i and the corresponding i.i.d. random sequences are denoted by X n ig and Y n ig , respectively. The known outer bounds on the capacity region of the IC deﬁned by (1) in the weak interference regime (i.e., h 12 ≤ 1 and h 21 ≤ 1) are as follows.\nTheorem 1 (ETW bound [2]): The capacity region of a two-user Gaussian IC in the weak interference regime is contained in the set of rate pairs (R 1 , R 2 ) satisfying\nR 1 ≤ I(X 1g ; Y 1g |X 2g ) \t (3a) R 2 ≤ I(X 2g ; Y 2g |X 1g ) \t (3b)\nR 1 + R 2 ≤ I(X 1g ; Y 1g |X 2g ) + I(X 2g ; Y 2g ) \t (3c) R 1 + R 2 ≤ I(X 1g ; Y 1g ) + I(X 2g ; Y 2g |X 1g ) \t (3d) R 1 + R 2 ≤ I(X 1g ; Y 1g , S 1g ) + I(X 2g ; Y 2g , S 2g ) (3e)\n+ I(X 2g ; Y 2g , S 2g ) \t (3f) R 1 + 2R 2 ≤ I(X 1g ; Y 1g , S 1g ) + I(X 2g ; Y 2g |X 1g )\nThe ETW outer bound was improved by choosing a genie more elaborately in some subsequent works [3], [4], [5], which used the following natural generalization of the genie signal in (2):\nwhere W i is the zero mean Gaussian random variable with variance σ 2 W\n, independent of (X 1 , X 2 ). Here we allow W i to be correlated to Z i with correlation coefﬁcient ρ i , for i = 1, 2.\nTheorem 2 (Enhanced ETW bound [3], [5]): The capacity region of a two-user Gaussian IC in the weak interference regime is outer-bounded by the set of rate pairs (R 1 , R 2 ) satisfying (3) in which (3e), (3f), and (3g) are tightened by using the genie signal (4) and the extremal inequality for [3] (or the entropy power inequality (EPI) for [5]).\nIn this section, we provide a new outer bound on the capacity region of the two-user Gaussian IC. To do so, we propose a different approach using auxiliary random variables, without the aid of a genie.\nGiven the channel inputs X n 1 and X n 2 in the previous section, we introduce auxiliary random variables (U 1 , U 2 ) deﬁned by\nwhere W i is a zero mean i.i.d. Gaussian random sequence with variance σ 2 W\n, independent of (X n 1 , X n 2 ) and correlated to Z i with correlation coefﬁcient ρ i , for i = 1, 2. The main role of the corresponding auxiliary random sequence U n 1 is to replace the arbitrary random sequence h 12 X n 2 (interference with an average power constraint) by the Gaussian random sequence h 12 W n 1 whose components are i.i.d., and so U n 2 is.\nWe provide several lemmas here to make this paper self- contained, some of which have appeared in different forms. The following lemma is a generalization of the well-known result by Thomas [8] that Gaussian distribution maximizes the conditional distribution for a given covariance constraint.\nLemma 1: Let X n i , i = 1, 2, · · · , m, denote a random sequence, let S be a subset of [1 : m] and ¯ S be its complement and let X n S = (X n S(1) , X n S(2) , · · · , X n S(s) ), X n ¯ S = (X n ¯ S(1) , X n ¯ S(2) , · · · , X n ¯ S(m−s) ), where S(i) and ¯ S(i) are the ith elements of S and ¯ S, respectively, and |S| = s. The random sequences {X n i : i = 1, · · · , m} are mutually, arbitrarily correlated for each time j with covariance matrix given by K K K j = Cov(X S(1)j , · · · , X S(s)j , X ¯ S(1)j , · · · , X ¯ S(m−s)j ), such that 1 n n j=1 K K K j \t K K K X . Also let (X Sg , X ¯ Sg ) be a jointly Gaussian random vector with covariance K K K X , where X Sg = (X S(1)g , X S(2)g , · · · , X S(s)g ) and X ¯ Sg = (X ¯ S(1)g , X ¯ S(2)g , · · · , X ¯ S(m−s)g ). Then, we have\nProof: We use the standard time sharing argument. With- out loss of generality, we assume that 1 n n j=1 K K K j = K K K X . Let Q ∼ Unif[1 : n] be independent of X n i for all i. Then, we have\n= nh(X S(1)Q , · · · , X S(s)Q |X ¯ S(1)Q , · · · , X ¯ S(m−s)Q , Q) ≤ nh(X S(1)Q , · · · , X S(s)Q |X ¯ S(1)Q , · · · , X ¯ S(m−s)Q )\nwhere (a) follows from a permutation of the ran- dom sequence X n S into the random vector sequence (X S(1) , · · · , X S(s) ) n and (b) follows from [8, Lem.1], given the covariance constraint on the random vector (X S(1)Q , · · · , X S(s)Q , X ¯ S(1)Q , · · · , X ¯ S(m−s)Q ).\nThe inequality (6) may be also viewed as a generalization of [5, Lem. 1], for which m = 2 and Y n i are not arbitrary. Moreover, our lemma upper-bounds the conditional entropy of a random sequence with that of a Gaussian random vector, given the particular covariance constraint that should differ from [5, Lem. 1] to apply Lemma 1 in our main results.\nThe following two lemmas are with respect to the worst additive noise lemma in [7]. The ﬁrst one is a rather straight- forward generalization and the second is a conditional version of the well-known worst additive noise lemma. The latter is a key inequality in deriving a new outer bound based on auxiliary random variables in this paper.\nLemma 2: Let X n denote a random sequence with an average power constraint, W n and Z n be i.i.d. N (0, σ 2 W ) and N (0, σ 2 Z ), respectively, correlated with each other but independent of X n . If σ 2 Z − σ 2 W ≥ 0, then\nh(X n + W n ) − h(X n + Z n ) ≤ nh(X g + W ) − nh(X g + Z) (8)\nThe proof of Lemma 2 can be given either by the worst additive noise lemma or by an extremal inequality in [10, Corrolary 4].\nLet X n denote a random sequence with an average power constraint, Z n be i.i.d. N (0, σ 2 Z ) independent of X n and let U n denote another random sequence with an average power constraint, correlated with X n but independent of Z n . Suppose that a corresponding random vector sequence (Z, X + Z, U ) n has an average covariance constraint such that\nCov(Z j , X j + Z j , U j ) K K K. Also let (Z, X g + Z, U g ) be a zero-mean Gaussian random vector with covariance K K K. Then, we have\nProof: Without loss of generality, we assume as before that 1 n n j=1 Cov(Z j , X j + Z j , U j ) = K K K. Similar to the proof in [5, Lem. 4], we have\nwhere (a) follows from the fact that Z n is independent of U n and from Lemma 1.\nNote that the above lemma has a different covariance con- straint from [5, Lem. 4] and it is analogous to the conditional extremal inequality in [10, Thm. 8]. While the conditional extremal inequality is conditioned on a scalar random variable U since its proof relies on the classical conditional EPI [9], our inequality (9) is on a random sequence U n .\nBy replacing the arbitrarily distributed interfering signals h 12 X n 2 and h 21 X n 1 in (1) with the i.i.d. Gaussian sequences W n 1 and W n 2 , respectively, and then using Lemma 3, the following inequalities are readily given.\nLemma 4: For X n i , Y n i , Z n i , and U n i deﬁned in (1) and (5), respectively, let V n i be i.i.d. N (0, σ 2 V\n) independent of both X n k and U n k , where k = 1 if i = 2, otherwise k = 2 and\nnh(Y 1g |U 1g ) − nh(h 21 Y 1g + ˜ V 2 |U 1g ) \t (12a) h(Y n 2 |U n 2 ) − h(h 12 X n 2 + V n 1 ) ≤\nProof: It sufﬁces to prove (12a) since we can immediately obtain (12b) by swapping the user indices of (12a). We ﬁrst observe that the average power constraints on X n 1 and X n 2 can be naturally translated into an average covariance constraint on a random vector sequence (h 21 Y 1 , ˜ V 2 , U 1 ) n by construction.\n= h(h 21 X n 1 − h 21 W n 1 + h 21 Z n 1 |U n 1 ) − h(h 21 X n 1 + V n 2 ) − n log |h 21 |\nwhere (a) follows from the fact that the condition in (11a) satisﬁes 1 − σ −2 V\nh 2 21 Cov(W 1 − Z 1 ) ≥ 0, (b) follows from the fact that conditioning reduces entropy, and (c) follows by using the fact that U n 1 is independent of V n 2 and by applying\nLemma 3 for a given average covariance constraint on the random vector sequence (h 21 Y 1 , ˜ V 2 , U 1 ) n .\nUsing the above lemmas, we present the following theorem to upper-bound the sum rate R 1 + R 2 .\nTheorem 3: The capacity region of a two-user Gaussian IC in the weak interference regime is contained in the following region:\n+ h(Y 2g ) − h(Y 2g |X 2g ) + h(U 2g ) − h(h 21 Y 1g + ˜ V 2 |U 1g ) + h(Y 2g |U 2g ) − h(W 2 − Z 2 ) \t (13)\n≤ 1. Proof: Using the auxiliary random variables in (5) and\n≤ I(U n 1 ; Y n 1 |X n 1 ) + h(Y n 1 |U n 1 ) − h(Y n 1 |X n 1 ) = h(U n 1 ) − h(h 12 X n 2 + W n 1 |h 12 X n 2 + Z n 1 )\n) − h(h 12 X n 2 + V n 1 ) − nh(W 1 − Z 1 ) + h(Y n 1 |U n 1 ) (14)\nwhere n → 0 as n → ∞, (a) follows from the well-known fact that if p(x, y, z) = p(x)p(z)p(y|x, z), then I(X; Y |Z) ≥ I(X; Y ) and (b) follows from the identity\n= h(W n 1 − Z n 1 |h 12 X n 2 + Z n 1 ) + h(h 12 X n 2 + Z n 1 ) = h(h 12 X n 2 + Z n 1 |W n 1 ) + h(W n 1 − Z n 1 ) = h(h 12 X n 2 + V n 1 ) + nh(W 1 − Z 1 )\nwhere V n 1 is the minimum mean-squared error (MMSE) estimation error of Z n 1 given W n 1 and hence V n 1 is i.i.d. N (0, Cov(Z 1 |W 1 − Z 1 )) independent of (W n 1 − Z n 1 ) as well as X n 1 . Likewise, we have\nn(2R 1 + 2R 2 − 4 n ) ≤ nh(Y 1g ) − h(Y n 1 |X n 1 ) + h(U n 1 ) − h(h 12 X n 2 + V n 1 ) + h(Y n 1 |U n 1 ) − nh(W 1 − Z 1 )\n+ nh(Y 2g ) − h(Y n 2 |X n 2 ) + h(U n 2 ) − h(h 21 X n 1 + V n 2 ) + h(Y n 2 |U n 2 ) − nh(W 2 − Z 2 )\n− h(h 12 X n 2 + V n 1 ) − nh(W 1 − Z 1 ) + nh(Y 2g ) − nh(Y 2g |X 2g ) + nh(U 2g ) + h(Y n 1 |U n 1 ) − h(h 21 X n 1 + V n 2 ) − nh(W 2 − Z 2 )\nwhere (a) follows from applying Lemma 2 to −h(Y n 1 |X n 1 ) + h(U n 1 ) = −h(h 12 X n 2 + Z n 1 ) + h(h 12 X n 2 + W n 1 ) and −h(Y n 2 |X n 2 )+h(U n 2 ) = −h(h 21 X n 1 +Z n 2 )+h(h 21 X n 1 +W n 2 ), respectively, since σ 2 W i ≤ σ 2 Z i = 1 for i = 1, 2. Then, (13) immediately follows from Lemma 4.\nh(Y 1g ) − h(Y 1g |X 1g ) + h(U 1g ) − h(h 12 Y 2g + ˜ V 1 |U 2g ) + h(Y 1g |U 1g ) − h(W 1 − Z 1 )\n \n \n \n \nUsing a known bounding technique by Kramer [1], we can also obtain upper bounds for both R 1 + 2R 2 and 2R 1 + R 2 .\nTheorem 4: The capacity region of a two-user Gaussian IC in the weak interference regime is contained in the following region:\nR 1 ≤ h(U 1g ) − h(h 12 Y 2g + ˜ V 1 |U 2g ) + h(Y 1g |U 1g ) − h(W 1 − Z 1 ) − h(h 21 Y 1g + ˜ V 2 |U 1g ) + h(Y 2g |U 2g ) − h(W 2 − Z 2 )\nR 2 ≤ h(U 2g ) − h(h 21 Y 1g + ˜ V 2 |U 1g ) + h(Y 2g |U 2g ) − h(W 2 − Z 2 ) − h(h 12 Y 2g + ˜ V 1 |U 2g ) + h(Y 1g |U 1g ) − h(W 1 − Z 1 )\n+ 1 2 log 2πe (P 1 + h 2 12 P 2 + 1)2 −2R 1 − 1 + σ 2 W 1 − R 1 (20)\nR 1 + R 2 ≤ h(U 1g ) − h(h 12 Y 2g + ˜ V 1 |U 2g ) + h(Y 1g |U 1g ) − h(W 1 − Z 1 ) + 1 n h(U n 2 ) − h(h 21 Y 1g + ˜ V 2 |U 1g )\nFollowing the EPI-based bounding technique in [1, Thm. 2] (see also [5, Lem. 11]) based on known outer bound results for the degraded Gaussian IC [11] and the one-sided Gaussian IC [12], we have\n≥ n 2\nwhere (a) follows from EPI and (b) follows from (17). Substituting (22) into (21) gives the upper bound (19) on R 1 + 2R 2 . Interchanging the user indices, we have (20) for 2R 1 + R 2 .\nFig. 1 and Fig. 2 show the tightness of the new outer bound in Section III on the capacity region of the two-user symmetric Gaussian IC with two channel parameter sets (P = 15, h 2 = 0.2) and (P = 10, h 2 = 0.3), over the best known genie-aided outer bound. To compare with the outer bounds that we have discussed here, we also plot a time division inner bound and a simpliﬁed case of the HK inner bound that does not consider time sharing and is limited to only Gaussian distributions for the private and common messages. Furthermore, compared to the sum-rate upper bound given in [6] and all other bounds\ntherein, our sum-rate upper bound in Theorem 3 is shown to be tighter in a moderately weak interference regime. For example, this is the case when 0.42 ≤ h 2 < 0.52 for the same parameters of Fig. 1 in [6], where P = 6. Moreover, such range of h 2 is shown to be gradually widened as P increases."},"refs":[{"authors":[{"name":"G. Kramer"}],"title":{"text":"Outer bounds on the capacity region of Gaussian interference channels"}},{"authors":[{"name":"R. H. Etkin"},{"name":"D. N. C. Tse"},{"name":"H. Wang"}],"title":{"text":"Gaussian interference channel capacity to within one bit"}},{"authors":[{"name":"A. S. Motahar"},{"name":"A. K. Khandan"}],"title":{"text":"Capacity bounds for the Gaussian interference channel, IEEE Trans"}},{"authors":[{"name":"X. Shan"},{"name":"G. Krame"},{"name":"B. Che"}],"title":{"text":"A new outer bound and the noisy- interference sum-rate capacity for the Gaussian interference channels, IEEE Trans"}},{"authors":[{"name":"V. S. Annapureddy"},{"name":"V. V. Veeravalli"}],"title":{"text":"Gaussian interference networks: Sum capacity in the low-interference regime and new outer bounds on the capacity region"}},{"authors":[{"name":"R. H. Etkin"}],"title":{"text":"New sum-rate upper bound for the two-user Gaussian interference channel"}},{"authors":[{"name":"S. Diggav"},{"name":"T. M. Cove"}],"title":{"text":"Worst additive noise under covariance constraints, IEEE Trans"}},{"authors":[{"name":"J. A. Thoma"}],"title":{"text":"Feedback can at most double Gaussian multiple access channel capacity, IEEE Trans"}},{"authors":[{"name":"P. P. Bergman"}],"title":{"text":"A simple converse for broadcast channels with additive white Gaussian noise"}},{"authors":[{"name":"T. Li"},{"name":"P. Viswanat"}],"title":{"text":"An extremal inequality motivated by multiter- minal information theoretic problems, IEEE Trans"}},{"authors":[{"name":"H. Sato"}],"title":{"text":"On degraded Gaussian two-user channels"}},{"authors":[{"name":"M. H. M. Costa"}],"title":{"text":"On the Gaussian interference channel"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565611.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T2.5","endtime":"13:10","authors":"Junyoung Nam, Giuseppe Caire","date":"1341579000000","papertitle":"A New Outer Bound on the Capacity Region of Gaussian Interference Channels","starttime":"12:50","session":"S16.T2: Gaussian Interference Channels","room":"Kresge Auditorium (109)","paperid":"1569565611"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
