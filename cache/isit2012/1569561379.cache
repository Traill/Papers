{"id":"1569561379","paper":{"title":{"text":"An Information-Spectrum Approach to the Capacity Region of General Interference Channel"},"authors":[{"name":"Lei Lin ∗"},{"name":"Xiao Ma \u2020"},{"name":"Xiujie Huang \u2020"},{"name":"Baoming Bai \u2021 ∗"}],"abstr":{"text":"Abstract\u2014This paper is concerned with general interference channels characterized by a sequence of transition (conditional) probabilities. We present a general formula for the capacity region of the interference channel with two pairs of users. The formula shows that the capacity region is the union of a family of rectangles, where each rectangle is determined by a pair of spectral inf-mutual information rates. Although the presented formula is usually difﬁcult to compute, it provides us useful insights into the interference channels. For example, the formula suggests us that the simplest inner bounds (obtained by treating the interference as noise) could be improved by taking into account the structure of the interference processes. This is veriﬁed numerically by computing the mutual information rates for Gaussian interference channels with embedded convolutional codes."},"body":{"text":"The interference channel (IC) is a communication model with multiple pairs of senders and receivers, in which each sender has an independent message intended only for the corresponding receiver. This model was ﬁrst mentioned by Shannon [1] in 1961 and further studied by Ahlswede [2] in 1974. A basic problem for the IC is to determine the capacity region, which is currently one of long-standing open problems in information theory. Only in some special cases, the capacity regions are known, such as strong interference channels and deterministic interference channels [3\u20136]. On the other hand, some inner and outer bounds of the capacity region are obtained, for example, see [4, 7, 8]. In recent years, Etkin, Tse and Wang [8] introduced the idea of approximation to show that Han and Kobayashi region (HK region) [4] is within one bit of the capacity region for Gaussian interference channel (GIFC).\nIn [9], the authors proposed a new computational model for the two-user GIFC, in which one pair of users (called primary users) are constrained to use a ﬁxed encoder and the other pair of users (called secondary users) are allowed to optimize their codes. The maximum rate at which the secondary users can communicate reliably without degrading the performance of the primary users is called the accessible capacity of the secondary users. Since the structure of the interference from the primary link has been taken into account in the computation, the accessible capacity is usually higher than the maximum rate when treating the interference as noise, as is consistent with the spirit of [10][11]. However,\nto compute the accessible capacity [9], the primary link is allowed to have a non-neglected error probability. This makes the model unattractive when the capacity region is considered. For this reason, we will remove the ﬁxed-code constraints on the primary users in this paper. 1 In other words, we will compute a pair of transmission rates at which both links can be asymptotically error-free.\nTo justify the computational results, we consider a more general interference channel which is characterized by a sequence of transition probabilities W = {W n } ∞ n=1 . Similar to [12], we utilize the information spectrum approach [13][14]. The capacity region can be described in terms of the spectral inf-mutual information rates.\nThe rest of the paper is structured as follows. Sec. II introduces the main deﬁnitions, including general IC and spectral inf-mutual information rate. In Sec. III, a formula is proved for the capacity region of the general IC. In Sec. IV we present numerical results for GIFC with binary-phase shift- keying (BPSK) modulation. Sec. V concludes this paper.\nLet X 1 , X 2 be two ﬁnite input alphabets and Y 1 , Y 2 be two ﬁnite output alphabets. A general interference channel W (see Fig. 1) is characterized by a sequence W = {W n ( ·, ·|·, ·)} ∞ n=1 , where W n : X n 1 × X n 2 → Y n 1 × Y n 2 is a probability transition matrix. That is, for all n,\nThe marginal distributions W n 1 , W n 2 of the W n are given by W n 1 (y 1 |x 1 , x 2 ) =\nDeﬁnition 1: An (n, M (1) n , M (2) n , ε (1) n , ε (2) n ) code for the interference channel W consists of the following essentials,\nM (1) n = {1, 2, . . . , M (1) n }, for sender 1 M (2) n = {1, 2, . . . , M (2) n }, for sender 2\n{x 1 (1), x 1 (2), . . . , x 1 (M (1) n ) } ⊂ X n 1 , for encoder 1 {x 2 (1), x 2 (2), . . . , x 2 (M (2) n ) } ⊂ X n 2 , for encoder 2\nFor sender 1 to transmit message i, encoder 1 outputs the codeword x 1 (i). Similarly, for sender 2 to transmit message i, encoder 2 outputs the codeword x 2 (j).\n∅ for i ̸= i \u2032 and B 2j ∩ B 2j \u2032 = ∅ for j ̸= j \u2032 . After receiving y 1 , decoder 1 outputs ˆi whenever y 1 ∈ B 1ˆ i . Similarly, after receiving y 2 , decoder 2 outputs ˆ j whenever y 2 ∈ B\nwhere \u201cc\u201d denotes the complement of a set. Here we have assumed that each message of i ∈ M (1) n and j ∈ M (2) n is produced with equal probability.\nRemark : The optimal decoding to minimize the probability of errors is deﬁning the decoding sets B 1i and B 2j according to the the maximum likelihood decoding [15]. That is, the two receivers choose, respectively,\nDeﬁnition 2: A rate pair (R 1 , R 2 ) is achievable if there exists a sequence of (n, M (1) n , M (2) n , ε (1) n , ε (2) n ) codes such that\nDeﬁnition 3: The set of all achievable rates is called the capacity region of the interference channel W, which is denoted by C(W).\nB. Preliminaries of Information-Spectrum Approach The following notions can be found in [14].\nDeﬁnition 4 (liminf in probability): For a sequence of ran- dom variables {Z n } ∞ n=1 ,\nDeﬁnition 5: If two random variables sequences X 1 = {X n 1 } ∞ n=1 and X 2 = {X n 2 } ∞ n=1 satisfy that\nX n 2 (x 1 , x 2 ) = P X n 1 (x 1 )P X n 2 (x 2 ) \t (3) for all x 1 ∈ X n 1 , x 2 ∈ X n 2 and n, they are called independent and denoted by X 1 ⊥X 2 .\nDeﬁnition 6: Set S I △ = {(X 1 , X 2 ) |X 1 ⊥X 2 }. Given an (X 1 , X 2 ) ∈ S I , for the interference channel W, we deﬁne the spectral inf-mutual information rate by\nIn this section, we derive a formula for the capacity region C(W) of the general IC.\nTheorem 1: The capacity region C(W) of the interference channel W is given by\nwhere R W (X 1 , X 2 ) is deﬁned as the collection of all (R 1 , R 2 ) satisfying that\nTo prove Theorem 1, we need the following lemmas. Lemma 1: Let\nbe any channel input such that (X 1 , X 2 ) ∈ S I . The corresponding output via an interference channel W = {W n } is denoted by (Y 1 = {Y n 1 } ∞ n=1 , Y 2 = {Y n 2 } ∞ n=1 ). Then, for any ﬁxed M (1) n and M (2) n , there exists an (n, M (1) n , M (2) n , ε (1) n , ε (2) n ) code satisfying that\nProof of Lemma 1: The proof is similar to that of [13, Lemma 3].\nCodebook generation . Generate M (1) n independent code- words x 1 (1), ..., x 1 (M (1) n ) ∈ X n 1 subject to the probability distribution P X n 1 . Similarly, generate M (2) n independent code- words x 2 (1), ..., x 2 (M (2) n ) ∈ X n 2 subject to the probability distribution P X n 2 .\nEncoding. To send message i, sender 1 sends the codeword x 1 (i). Similarly, to send message j, sender 2 sends x 2 (j).\nDecoding. The receiver 1 chooses the i such that (x 1 (i), y 1 ) ∈ T n (1) if such i exists and is unique. Similarly, the receiver 2 chooses the j such that (x 2 (j), y 2 ) ∈ T n (2) if such j exists and is unique. Otherwise, an error is declared.\nAnalysis of the error probability. By the symmetry of the random code construction, we can assume that (1, 1) was sent. Deﬁne\nFor receiver 1, an error occurs if (x 1 (1), y 1 ) / ∈ T n (1) or (x 1 (i), y 1 ) ∈ T n (1) for some i ̸= 1. Similarly, for receiver 2, an error occurs if (x 2 (1), y 2 ) / ∈ T n (2) or (x 2 (j), y 2 ) ∈ T n (2) for some j ̸= 1. So the ensemble average of the error probabilities of decoder 1 and decoder 2 can be upper-bounded as follows:\nwhere \u201ca\u201d follows from the independence of x 1 (i) (i ̸= 1) and y 1 and \u201cb\u201d follows from the deﬁnition of T n (1). Similarly, we obtain\nCombining all inequalities above, we can see that there must exist at least one (n, M (1) n , M (2) n , ε (1) n , ε (2) n ) code satis- fying (11).\nLemma 2: For all n, any (n, M (1) n , M (2) n , ε (1) n , ε (2) n ) code satisﬁes that\n≤ 1 n log M (2) n − γ} − e −nγ , (13)\nfor every γ > 0, where X n 1 (resp., X n 2 ) places probability mass 1/M (1) n (resp., 1/M (2) n ) on each codeword for encoder 1 (resp., encoder 2) and (3), (6), (7) hold.\nProof of Lemma 2: The proof is similar to that of [13, Lemma 4]. By using the relation\n|X n 1 (y 1 |x 1 ) P Y n 1 (y 1 )\n|Y n 1 (x 1 |y 1 ) P X n 1 (x 1 )\n, we can rewrite the ﬁrst term on the right-hand side of the ﬁrst inequality of (13) as\n|Y n 1 (X n 1 |Y n 1 ) ≤ e −nγ }. By setting\n|Y n 1 (x 1 |y 1 ) ≤ e −nγ }, the ﬁrst inequality of (13) can be expressed as\n(15) where B 1i is the decoding region corresponding to codeword x 1 (i) and \u201ca\u201d follows from the deﬁnition of A i . Therefore, the ﬁrst inequality of (13) is proved. Similarly, we can obtain the second inequality of (13).\nNow we prove Theorem 1. Proof of Theorem 1:\n1) To prove that an arbitrary (R 1 , R 2 ) satisfying (9) and (10) is achievable, we deﬁne\nfor an arbitrarily small constant γ > 0. Then, Lemma 1 guarantees the existence of an (n, M (1) n , M (2) n , ε (1) n , ε (2) n ) code satisfying\n(16) From the deﬁnition of the spectral inf-mutual information rate, we have\n2) Suppose that a rate pair (R 1 , R 2 ) is achievable. Then, for any constant γ > 0, there exists an (n, M (1) n , M (2) n , ε (1) n , ε (2) n ) code satisfying\n(18) Taking the limits as n → ∞ on both sides, we have\nimplying by deﬁnition that R 1 − 2γ ≤ I(X 1 ; Y 1 ) and R 2 − 2γ ≤ I(X 2 ; Y 2 ). This completes the proof since γ is arbitrary.\nWe have obtained a formula of the capacity region for the general IC, which shows that any pair of independent input processes deﬁne a pair of achievable rates. Although it is not computable in general, the formula provides us useful insights\ninto the interference channels, as illustrated by the following example.\nThe considered example has the model as shown in Fig. 2, where the channel inputs x 1 (i) and x 2 (j) are BPSK sequences with power constraints P 1 and P 2 , respectively; the additive noise n 1 and n 2 are sequences of independent and identically distributed (i.i.d.) Gaussian random variables of variance one; the channel outputs y 1 and y 2 are\n2 (j) + n 1 , \t (20) y 2 = x 2 (j) +\n1 (i) + n 2 . \t (21) For any two arbitrary input processes x 1 and x 2 , the deﬁned pair of rates given in Theorem 1 are infeasible to compute. Therefore, we assume that x 1 and x 2 are the outputs from two (possibly different) generalized trellis encoders driven by independent uniformly distributed (i.u.d.) input sequences, as proposed in [9]. In this case, both x 1 and x 2 are block-wise stationary, and (hence)\nSince x 1 , x 2 and y 1 , y 2 can be viewed as the Markov chains and the hidden Markov chains, respectively, the right-hand sides of (22) and (23) can be estimated by performing the BCJR algorithm 2 [16][17].\nWe consider two schemes, UnBPSK and CcBPSK, where UnBPSK means that x 1 (resp. x 2 ) is an i.u.d. BPSK sequence and CcBPSK means that x 1 (resp. x 2 ) is an output from the convolutional encoder with the generator matrix\nFig. 3 shows the trellis representation of the signal model when sender 1 uses CcBPSK and sender 2 uses UnBPSK. Fig. 4 shows the numerical results. The point \u201cA\u201d can be achieved by a coding scheme, in which sender 1 uses a binary linear (coset) code concatenated with the convolutional code and sender 2 uses a binary linear code, and the point \u201cB\u201d can be achieved similarly; while the points on the line \u201cAB\u201d can be achieved by time-sharing scheme. The point \u201cC\u201d represents the limits when the two users use binary linear codes but regard the interference as an i.u.d. additive (BPSK) noise. It can be seen that knowing the structure of the interference can be used to improve potentially the bandwidth-efﬁciency.\nIn this paper, we have proved that the capacity region of the two-user interference channel is the union of a family of rectangles, each of which is deﬁned by a pair of spectral inf-mutual information rates associated with two independent input processes. For special cases, the deﬁned pair of rates can be computed, which provide us useful insights into the interference channels.\nThis work was supported by the 973 Pro- gram (No.2012CB316100) and the NSF under Grants 61172082 and 60972046 of China.\n[1] C. E. Shannon, \u201cTwo-way communication channels,\u201d in Forth Berkeley Symp. on Math. Statist. and Prob., J. Neyman, Ed., vol. 1. Statist. Lab. of the University of California, Berkely: University of California Press, Jun. 20 - Jul. 30 1961, pp. 611\u2013644.\n[2] R. Ahlswede, \u201cThe capacity region of a channel with two senders and two receivers,\u201d The Annals of Probability, vol. 2, no. 5, pp. 805\u2013814, Oct. 1974.\nreduce capacity,\u201d IEEE Trans. Inform. Theory, vol. 21, no. 5, pp. 569\u2013570, 1975.\n[4] T. S. Han and K. Kobayashi, \u201cA new achievable rate region for the interference channel,\u201d IEEE Trans. Inform. Theory, vol. IT-27, no. 1, pp. 49\u201360, Jan. 1981.\n[5] A. A. El Gamal and M. H. M. Costa, \u201cThe capacity region of a class of determinsitic interference channels,\u201d IEEE Trans. Inform. Theory, vol. IT-28, no. 2, pp. 343\u2013 346, Mar. 1982.\n[6] M. H. M. Costa and A. A. El Gamal, \u201cThe capacity region of the discrete memoryless interference channel with strong interference.\u201d IEEE Trans. Inform. Theory, vol. IT-33, no. 5, pp. 710\u2013711, Sep. 1987.\n[7] G. Kramer, \u201cOuter bounds on the capacity of Gaus- sian interference channels,\u201d IEEE Trans. Inform. Theory, vol. 50, no. 3, pp. 581\u2013586, Mar. 2004.\n[8] R. H. Etkin, D. N. C. Tse, and H. Wang, \u201cGaussian interference channel capacity to within one bit,\u201d IEEE Trans. Inform. Theory, vol. 54, no. 12, pp. 5534\u20135562, Dec. 2008.\n[9] X. Ma, X. Huang, L. Lin, and B. Bai, \u201cAccessible capacity of secondary users,\u201d Dec. 2010, submitted to IEEE Trans. Inform. Theory, available on the web http://arxiv.org/abs/1012.5197, also presented in ISIT, St. Petersburg, Russia, Aug.2011, PP.814-818.\n[10] F. Baccelli, A. A. El Gamal, and D. Tse, \u201cInterference networks with point-to-point codes,\u201d IEEE Trans. Inform. theory, vol. 57, no. 5, pp. 2582\u20132596, May 2011.\n[11] K. Moshksar, A. Ghasemi, and A. K. Khandani, \u201cAn al- ternative to decoding interference or treating interference as gaussian noise,\u201d in IEEE International Symposium on Information Theory,, St. Petersburg, Russia, Aug. 2011, pp. 1176\u20131180.\n[12] T. S. Han, \u201cAn information-spetrum approach to capacity theorems for the general multiple-access channel,\u201d IEEE Trans. Inform. Theory, vol. 44, no. 7, pp. 2773\u20132795, Nov. 1998.\n[13] T. S. Han and S. Verd´u, \u201cApproximation theory of output statistics,\u201d IEEE Trans. Inform. Theory, vol. 39, no. 3, pp. 752\u2013772, May 1993.\n[14] T. S. Han, Information-spectrum methods in information theory. New York: Springer-Verlag Berlin Heidelberg, 2003.\n[15] R. H. Etkin, N. Merhav, and E. Ordentlich, \u201cError expo- nents of optimum decoding for the interference channel,\u201d IEEE Trans. Inform. Theory, vol. 56, no. 1, pp. 40\u201356, Jan. 2010.\n[16] D. M. Arnold, H.-A. Loeliger, P. O. Vontobel, A. Kavˇci´c, and W. Zeng, \u201cSimulation-based computation of infor- mation rates for channels with memory,\u201d IEEE Trans. Inform. Theory, vol. 52, no. 8, pp. 3498\u20133508, Aug. 2006.\n[17] L. R. Bahl, J. Cocke, F. Jelinek, and J. Raviv, \u201cOptimal decoding of linear codes for minimizing symbol error rate,\u201d IEEE Trans. Inform. Theory, vol. IT-20, no. 2, pp. 284\u2013287, Mar. 1974."},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569561379.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T2.2","endtime":"15:20","authors":"Lei Lin, Xiao Ma, Xiujie Huang, Bao-Ming Bai","date":"1341500400000","papertitle":"An Information-Spectrum Approach to the Capacity Region of General Interference Channel","starttime":"15:00","session":"S13.T2: Interference Channels","room":"Kresge Auditorium (109)","paperid":"1569561379"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
