{"id":"1569566737","paper":{"title":{"text":"Universal Bounds on the Scaling Behavior of Polar Codes"},"authors":[{"name":"Ali Goli"},{"name":"S. Hamed Hassani"},{"name":"R¨udiger Urbanke"}],"abstr":{"text":"Abstract\u2014We consider the problem of determining the trade- off between the rate and the block-length of polar codes for a given block error probability when we use the successive cancellation decoder. We take the sum of the Bhattacharyya parameters as a proxy for the block error probability, and show that there exists a universal parameter µ such that for any binary memoryless symmetric channel W with capacity I(W ), reliable communication requires rates that satisfy R < I(W ) − αN − 1 µ , where α is a positive constant and N is the block-length. We provide lower bounds on µ, namely µ ≥ 3.553, and we conjecture that indeed µ = 3.627, the parameter for the binary erasure channel."},"body":{"text":"Polar coding schemes provably achieve the capacity of a wide array of channels including binary memoryless symmet- ric (BMS) channels. Let W be a BMS channel with capacity I(W ). In [1], Arıkan showed that for any rate R < I(W ) the block error probability of the successive cancellation (SC) decoder is upper bounded by N −1/4 for block-length N large enough. In [2], Arıkan and Telatar signiﬁcantly tightened this result. They showed that for any rate R < I(W ) and any β < 1 2 , the block error probability is upper bounded by 2 −N β for N large enough. Later in [3], these bounds were reﬁned to be dependent on R and it was shown that similar asymptotic lower bounds are valid when we perform MAP decoding. Hence, SC and MAP decoders share the same asymptotic performance in this sense. Such an exponential decay suggests that error ﬂoors should not be a problem for polar codes even at moderate block lengths (e.g. N > 10 4 ).\nAnother problem of interest in the area of polar codes is to determine the trade-off between the rate and the block- length for a given error probability when we use the successive cancellation (SC) decoder. In other words, in order to have reliable transmission with block error probability at most , how does the maximum possible rate R scale in terms of the block-length N ? This problem has been previously considered in [4] and [5] mainly for the family of Binary Erasure Channels (BEC). In both [4] and [5], the authors provide strong evidence (both numerically and analytically) that for polar codes with the SC decoder, reliable communication over the BEC requires rates N − 1 µ below capacity, where µ ≈ 3.627.\nIn this paper, we provide rigorous lower bounds on the value of µ, such that for any BMS channel W , reliable transmission\n(in the sense that the sum of the Bhattacharyya parameters is small) requires rates at least N − 1 µ below capacity. We begin by giving the notation and the general problem set-up.\nLet W : X → Y be a BMS channel, with input alphabet X = {0, 1}, output alphabet Y, and the transition probabilities {W (y | x) : x ∈ X , y ∈ Y}. We consider the following three parameters for the channel W ,\nThe parameter H(W ) is equal to the entropy of the output of W given its input when we assume uniform distribution on the inputs, i.e., H(W ) = H(X | Y ). Hence, we call the parameter H(W ) the entropy of the channel W . Also note that the capacity of W , which we denote by I(W ), is given by I(W ) = 1 − H(W ). The parameter Z(W ) is called the Bhattacharyya parameter of W and E(W ) is called the error probability of W . It can be shown that E(W ) is equal to the error probability in estimating the channel input x on the basis of the channel output y via the maximum-likelihood decoding of W (y|x) (with the further assumption that the input has uniform distribution). It can be shown that the following relations hold between these parameters (see for e.g., [1] and [6, Chapter 4]):\n0 ≤ 2E(W ) ≤ H(W ) ≤ Z(W ) ≤ 1, \t (4) H(W ) ≤ h 2 (E(W )), \t (5) Z(W ) ≤ 1 − (1 − H(W )) 2 , \t (6)\nLet W denote the set of all the BMS channels and consider a transform W → (W − , W + ) that maps W to W 2 in the following manner. Having the channel W : {0, 1} → Y, the\nchannels W − : {0, 1} → Y 2 and W + : {0, 1} → {0, 1} × Y 2 are deﬁned as\nA direct consequence of the chain rule of entropy yields H(W + ) + H(W − )\nH(W ) ≤ H(W − ) ≤ 1 − (1 − H(W )) 2 , \t (10) H(W ) 2 ≤ H(W + ) ≤ H(W ). \t (11)\nLet {B n , n ≥ 1} be a sequence of iid Bernoulli( 1 2 ) random variables. Denote by (F , Ω, P) the probability space generated by this sequence and let (F n , Ω n , P n ) be the probability space generated by (B 1 , · · · , B n ). For a BMS channel W , deﬁne a random sequence of channels W n , n ∈ N {0, 1, 2, · · · }, as W 0 = W and\nW − n−1 If B n = 0, \t (12) where the channels on the right side are given by the transform W n−1 → (W − n−1 , W + n−1 ). Let us also deﬁne the random pro- cesses {H n } n∈N , {I n } n∈N and {Z n } n∈N as H n = H(W n ), I n = I(W n ) = 1 − H(W n ) and Z n = Z(W n ). From (9) one can easily observe that H n (and I n ) is a martingale with E[H n ] = H(W ). It is further known from [1] that the processes H n and Z n converge almost surely to limit random variables H ∞ and Z ∞ and furthermore, these limit random variables take their values in the set {0, 1} with Pr(H ∞ = 0) = Pr(Z ∞ = 0) = H(W ).\nGiven the rate R < I(W ), polar coding is based on choosing a set of 2 n R rows of the matrix G n = 1 0 1 1 ⊗n to form a 2 n R × 2 n matrix which is used as the generator matrix in the encoding procedure 1 . The way this set is chosen is dependent on the channel W and is brieﬂy explained as follows: At time n ∈ N, consider a speciﬁc realization of the sequence (B 1 , · · · , B n ), and denote it by (b 1 , · · · , b n ). The random variable W n outputs a BMS channel, according to the procedure (12), which we can naturally denote by W (b 1 ,··· ,b n ) . Let us now identify a sequence (b 1 , · · · , b n ) by an integer i in the set {1, · · · , N } such that the binary expansion of i − 1 is equal to the sequence (b 1 , · · · , b n ), with b 1 as the least signiﬁcant bit. As an example for n = 3, we identify (b 1 , b 2 , b 3 ) = (0, 0, 1) with 5 and (b 1 , b 2 , b 3 ) = (1, 0, 0) with 2. To simplify notation, we use W (i) n to denote W (b 1 ,··· ,b n ) . Given the rate R, the indices of the matrix G n are chosen as\nfollows: Choose a subset of size N R from the set of channels {W (i) N } 1≤i≤N that have the least possible error probability (given in (3)) and choose the rows G n with the same indices as these channels. E.g., if the channel W (j) N is chosen, then the j-th row of G n is selected. In the following, given N , we call the set of indices of N R channels with the least error probability, the set of good indices and denote it by I N,R .\nIt is proved in [1] that the block error probability of such polar coding scheme under SC decoding, denoted by P e (N, R), is bounded from both sides by 2\nConsider a BMS channel W and let us assume that a polar code with block-error probability at most a given value > 0, is required. One way to accomplish this is to ensure that the right side of (13) is less than . However, this is only a sufﬁcient condition that might not be necessary. Hence, we call the right side of (13) the strong reliability condition. Based on this measure of the block-error probability, we provide bounds on how the rate R scales in terms of the block-length N .\nTheorem 1: For any BMS channel W with capacity I(W ) ∈ (0, 1), there exist constants , α > 0, which depend only on I(W ), such that\nwhere µ is a universal parameter lower bounded by 3.553. A few comments are in order:\n1) As we will see in the sequel, we can obtain an increasing sequence of lower bounds, call this sequence {µ m } m∈N , for the universal parameter µ. For each m, in order to show the validity of the lower bound we need to verify the concavity of a certain polynomial (deﬁned in (20)) in [0, 1]. For small values of m concavity can be proved directly using pen and paper. For larger values of m we can automate this process: each polynomial has rational coefﬁcients. Hence also its second derivative has rational coefﬁcients. To show concavity it sufﬁces to show that there are no roots of this second derivative in [0, 1]. This task can be accomplished exactly by computing so-called Sturm chains (see Sturm\u2019s Theorem [8]). Computing Sturm chains is equivalent to running Euclid\u2019s algorithm starting with the second and third derivative of the original polynomial. The lower bound for µ stated in Theorem 1 is the one corresponding to m = 8, an arbitrary choice. If we increase m we get e.g., µ 16 = 3.614. We conjecture that the sequence µ m converges to µ = 3.627, the parameter for the BEC.\n2) Let , α, µ be as in Theorem 1. If we require the block- error probability to be less than (in the sense that the condition (14) is fulﬁlled), then the block-length N should be at least\n3) It is well known that the value of µ for the random linear ensemble is µ = 2, which is the optimal value since the variations of the channel itself require µ ≥ 2. Thus, given a block-length N , reliable transmission by polar codes requires a larger gap to the channel capacity than the optimal value.\nThe rest of the paper is devoted to proving Theorem 1. In Section II, we provide universal lower bounds on how fast the process H n converges to its limit H ∞ . We then use these bounds to prove Theorem 1 in Section III. Finally, Section IV concludes the paper with stating the related open questions.\nConsider a channel W with its entropy process H n = H(W n ). Since the bounded process H n converges al- most surely to a 0 − 1 valued random variable, we have lim n→∞ E[H n (1 − H n )] = 0. In this section, we provide universal lower bounds on the speed with which the quantity E[H n (1−H n )] decays to 0. We ﬁrst derive such lower bounds for the family of Binary Erasure Channels (BEC) and then extend them to other BMS channels.\nConsider a binary erasure channel with erasure probability h ∈ [0, 1] which we denote by BEC(h). One can show that (see [6, Chapter 4] ) for such a channel we have\nboth proved in [1]. Hence, the processes H n and Z n for BEC(h) are equal and have a simple closed form expression as the following: Let H 0 = h and 3\nLet us now deﬁne the sequence of functions {f n (h)} n∈N as f n : [0, 1] → [0, 1] and for h ∈ [0, 1],\nHere, note that for h ∈ [0, 1] the value of f n (h) is a deterministic value that is dependent on the process H n with the starting value H 0 = h. By using the recursive relation (18), one can easily deduce that\n2 ) 2\nRemark 2: One can compute the value of a m by ﬁnding the extreme points of the function f m+1 f\n(i.e., ﬁnding the roots of the polynomial g m = f m+1 f m − f m+1 f m ) and checking which one gives the global minimum. Again, for small values e.g., m = 0, 1, pen and paper sufﬁce. For higher values of m we can again automatize the process: all these polynomials have rational coefﬁcients and therefore it is possible to determine the number of real roots exactly and to determine their value to any desired precision (by computing Sturm chains as mentioned earlier). Hence, we can ﬁnd the value of a m to any desired precision. Table I contains the numerical value of a m up to precision 10 −4 for m ≤ 8. As the table shows, the values a m are increasing (see Lemma 3), and we conjecture that they converge to 2 − 1 3.62713 = 0.8260, the corresponding value for the channel BEC. \t ♦ We now show that each of the values a m is a lower bound on the speed of decay of the sequence f n .\nLemma 3: Fix m ∈ N. For all n ≥ m and h ∈ [0, 1], we have\nProof: The proof goes by induction on n−m. For n−m = 0 the result is trivial. Now, assume that the relation (22) holds for a n − m = k, i.e., for h ∈ [0, 1] we have\nWe show that (22) is indeed true for k + 1 and h ∈ [0, 1]. We have\n2 ) 2\nf m (h) f m (h) = (a m ) k+1 f m (h).\nHere, (a) follows from (20) and (b) follows from the left side inequality in (23), and hence the lemma is proved via induction.\nFor a BMS channel W , there is no simple 1-dimensional recursion for process H n as for BEC. However, by using (10) and (11), one can give bounds on how H n evolves. In this section, we use the functions {f n } n∈N deﬁned in (20) to provide universal lower bounds on the quantity E[H n (1−H n )]. We start by introducing one further technical condition given as follows.\nDeﬁnition 4: We call an integer m ∈ N suitable if the function f m (h), deﬁned in (20), is concave on [0, 1].\nRemark 5: For small values of m, i.e., m ≤ 2, it is easy to verify by hand that the function f m is concave. As discussed previously, for larger values of m we can use Sturm\u2019s theorem [8] and a computer algebra system to verify this claim. Note that the polynomials 2 m f m have integer coefﬁcients. Hence, all the required computations can be done exactly. Unfortunately, the degree of f m is 2 m+1 . We have checked up to m = 8 that f m is concave and we conjecture that in fact this is true for all m ∈ N. \t ♦ In the rest of this section, we show that for any BMS channel W , the value of a m is a lower bound on the speed of decay of H n provided that m is a suitable integer.\nLemma 6: Let m ∈ N be a suitable integer and W a BMS channel. We have for n ≥ m\nProof: We use induction on n − m: For n − m = 0 there is nothing to prove. Now, assume that the result of the lemma is correct for n − m = k. Hence, for any BMS channel W with H n = H(W n ) we have\nE[H m+k (1 − H m+k )] ≥ (a m ) k f m (H(W )). \t (25) We now prove the lemma for m − n = k + 1. For the BMS channel W , let us recall that the the transform (W → (W − , W + )) yields two channels W − and W + such that the relation (9) holds. Deﬁne the process {(W − ) n , n ∈ N} as the channel process that starts with (W − ) 0 = W − and evolves as in (12) similarly deﬁne {(W + ) n , n ∈ N} similar with (W + ) 0 = W + . Let us also deﬁne the two processes H − n = H((W − ) n ) and H + n = H((W + ) n ). We have,\n(1 − H − m+k )] + E[H + m+k (1 − H + m+k )] 2\n+ )) 2\n2 ) 2\nIn the above chain of inequalities, relation (a) follows from the fact that W m has 2 m possible outputs among which half of them are branched out from W + and the other half are branched out from W − . Relation (b) follows from the induction hypothesis given in (25). Relation (c) follows from (10), (11) and the fact that the function f m is concave. More precisely, since f m is concave on [0, 1], we have the following inequality for any sequence of numbers 0 ≤ x ≤ x ≤ y ≤ y ≤ 1 that satisfy x+y 2 = x +y 2 :\nIn particular, we set x = H(W ) 2 , x = H(W + ), y = H(W − ), y = 1 − (1 − H(W )) 2 and we know from (10) and (11) that 0 ≤ x ≤ x ≤ y ≤ y ≤ 1. Hence, by (26) we obtain (c). Relation (d) follows from the recursive deﬁnition of f m given in (20). Finally, relation (e) follows from the deﬁnition of a m given in (21).\nTo ﬁt the bounds of Section II into the framework of Theorem 1, let us ﬁrst introduce the sequence {µ m } m∈N as\nwhere a m is deﬁned in (21). In the last section, we proved that for a suitable m, the speed with which the quantity E[H n (1 − H n )] decays is lower bounded by a m = 2 − 1 µm , i.e. for n ≥ m we have E[H n (1 − H n )] ≥ 2 − (n−m) µm f m (H(W )). To relate the strong reliability condition in (14) to the rate bound in (15), we need the following lemma.\nLemma 7: Consider a BMS channel W and assume that there exist positive real numbers γ, θ and m ∈ N such that E[H n (1 − H n )] ≥ γ2 −nθ for n ≥ m. Let α, β ≥ 0 be such that 2α + β = γ, we have for n ≥ m\nProof: The proof is by contradiction. Let us assume the contrary, i.e., we assume there exists n ≥ m s.t.,\nIn the following, we show that with such an assumption we reach to a contradiction. We have\nE[H n (1−H n ) | H n > α2 −nθ ]Pr(H n > α2 −nθ ) ≥ 2 −nθ (γ−α). (31)\n+ E[1 − H n | H n > α2 −nθ ]Pr(H n > α2 −nθ ), (32)\nand by noting the fact that H n ≥ H n (1 − H n ) we can plug (31) in (32) to obtain\nE[(1 − H n )] > (I(W ) − β2 −nθ )(1 − α2 −nθ ) + 2 −nθ (γ − α) ≥ I(W ) + 2 −nθ (γ − α(1 + I(W )) − β),\nand since 2α + β = γ, we get E[1 − H n ] > I(W ). This is a contradiction since H n is a martingale and E[1−H n ] = I(W ).\nLet us now use the result of Lemma 7 to conclude the proof of Theorem 1. By Lemma 6, we have for n ≥ m\nThus, if we now let γ = 2 m µm f m (H(W )) and 2α = β = γ 2 , then by using Lemma 7 we obtain\nLet I N,R be the set of indices chosen for such a rate R, i.e., I N,R includes the 2 n R indices of the sub-channels with the least value of error probability. Deﬁne the set A as\nwhere the last step follows from the fact that for x ∈ [0, 1 √ 2 ], we have h −1 2 (x) ≥ x 8 log( 1\n. Thus, having a block-length N = 2 n , in order to have error probability (measured by (13)) less\n, the rate can be at most I(W ) − γ 4 2 − n µm . Finally, if we let m = 8 (by the discussion in Remark 5,\n= 3.553 and choosing\n< 1 2 ) and furthermore, to have block-error probability less that the rate should be less than R given in (35).\nThe results of this paper can be extended in the following ways.\n1) In this paper, we take the right side of (13) as a proxy for the block error probability and hence our results are with respect to the strong reliability condition (14). A signiﬁcant step in this regard would be to prove equivalent bounds for the block error probability.\n2) An other way to improve the results of this paper is to provide better values of the universal parameter µ. Based on numerical experiments, we conjecture that the value of µ can be increased up to the scaling parameter of the channel BEC. That is, the right value of µ to plug in (15) is equal to µ = 3.62713. Thus, the ultimate goal would be to show that for the channel BEC, the polarization phenomenon takes place faster than all the other BMS channels. One way to do this, is to prove that the functions f n deﬁned in (20) are concave on the interval [0, 1].\n3) The result of Theorem 1 suggests that in terms of ﬁnite- length performance, polar codes are far from optimal. How- ever, we might get different results if we consider extended polar codes with × kernels ([7]). It is not very hard to prove that at least for the BEC, as grows large, for almost all the × kernels the ﬁnite-length performance of polar codes improves towards the optimal one (i.e., µ → 2). However, this is at the cost of an increase in complexity proportional to 2 . This suggests that there might still exist kernels with reasonable size with superior ﬁnite-length properties than the original 2 × 2 kernel. Hence, an interesting open problem is the ﬁnite-length analysis of polar codes that are constructed from × kernels and how relate such analysis to ﬁnding kernels with better ﬁnite-length properties.\nThe authors wish to thank anonymous reviewers for their valuable comments on an earlier version of this manuscript. The work of Hamed Hassani was supported by Swiss National Science Foundation Grant no 200021-121903."},"refs":[{"authors":[{"name":"E. Arıkan"}],"title":{"text":"Channel polarization: A method for constructing capacity- achieving codes for symmetric binary-input memoryless channels"}},{"authors":[{"name":"E. Arıkan"},{"name":"E. Telatar"}],"title":{"text":"On the rate of channel polarization"}},{"authors":[{"name":"S. H. Hassani"},{"name":"R. Mori"},{"name":"T. Tanaka"},{"name":"R. Urbanke"}],"title":{"text":"Rate dependent analysis of the asymptotic behavior of channel polarization"}},{"authors":[{"name":"S. B. Korada"},{"name":"A. Montanari"},{"name":"E. Telatar"},{"name":"R. Urbanke "}],"title":{"text":"An emprical scaling law for polar codes"}},{"authors":[{"name":"S. H. Hassani"},{"name":"K. Alishahi"},{"name":"R. Urbanke"}],"title":{"text":"On the scaling of Polar codes: II. The behavior of un-polarized channels"}},{"authors":[{"name":"T. Richardso"},{"name":"R. Urbank"}],"title":{"text":"Modern Coding Theory"}},{"authors":[{"name":"S. B. Korada"},{"name":"E. S¸as¸o˘glu"},{"name":"R. Urbanke"}],"title":{"text":"Polar Codes: Characteri- zation of Exponent, Bounds, and Constructions"}},{"authors":[],"title":{"text":"See http://en"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566737.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T5.1","endtime":"10:10","authors":"Ali Goli, S. Hamed Hassani, Ruediger L Urbanke","date":"1341481800000","papertitle":"Universal Bounds on the Scaling Behavior of Polar Codes","starttime":"09:50","session":"S11.T5: Polar Codes:  Theory and Practice","room":"Kresge Little Theatre (035)","paperid":"1569566737"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
