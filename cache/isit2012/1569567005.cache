{"id":"1569567005","paper":{"title":{"text":"Applications of the Shannon-Hartley Theorem to Data Streams and Sparse Recovery"},"authors":[{"name":"Eric Price"},{"name":"David P. Woodruff"}],"abstr":{"text":"Abstract\u2014The Shannon-Hartley theorem bounds the maximum rate at which information can be transmitted over a Gaussian channel in terms of the ratio of the signal to noise power. We show two unexpected applications of this theorem in computer science: (1) we give a much simpler proof of an Ω(n 1−2/p ) bound on the number of linear measurements required to approximate the p-th frequency moment in a data stream, and show a new distribution which is hard for this problem, (2) we show that the number of measurements needed to solve the k-sparse recovery problem on an n-dimensional vector x with the C-approximate 2 / 2 guarantee is Ω(k log(n/k)/ log C). We complement this result with an almost matching O(k log ∗ k log(n/k)/ log C) upper bound."},"body":{"text":"Let S be a real-valued random variable with E[S 2 ] = τ 2 . Consider the random variable S + T , where T ∼ N (0, σ 2 ) is additive white Gaussian noise of variance σ 2 . The Shannon-Hartley theorem states that\nwhere I(X; Y ) = h(X) − h(X|Y ) is the mu- tual information between X and Y , and h(X) = − X f (x) log f (x)dx is the differential entropy of a random variable X with probability density function f .\nWe show two unexpected applications of the Shannon- Hartley theorem in computer science, the ﬁrst to estimat- ing frequency moments in a data stream, and the second to approximating a vector by a sparse vector.\nIn the data stream literature, a line of work has consid- ered the problem of estimating the frequency moments F p (x) = x p p = n i=1 |x i | p , where x ∈ R n and p ≥ 2. One usually wants a linear sketch, that is, we choose a random matrix A ∈ R m×n from a certain distribution, for m \t n, and compute Ax, from which one can output a constant-factor approximation to F p (x) with high probability. Linearity is crucial for distributed com- putation, formalized in the MUD (Massive Unordered Distributed) model [9]. In this model the vector x is split into pieces x 1 , . . . , x r , each of which is handled by a different machine. The machines individually compute Ax 1 , . . . , Ax r , and an aggregation function combines these to compute Ax and estimate F p (x). Linearity\nis also needed for network aggregation, which usually follows a bottom-up approach [18]: given a routing tree where the nodes represent sensors, starting from the leaves the aggregation propagates upwards to the root. We refer to the rows of A as measurements.\nAlon, Matias, and Szegedy [2] initiated the line of work on frequency moments. There is a long line of upper bounds on the number of linear measurements; we refer the reader to the most recent works [3], [11] and the references therein. Similarly, we refer the reader to the most recent lower bounds [16], [22] and the references therein. The best upper and lower bounds for obtaining a (1 + )-approximation with probability at least 1 − δ have the form n 1−2/p · poly( −1 log(nδ −1 ).\nThe existing lower bounds are rather involved, using the direct sum paradigm for information complexity [5]. Moreover, they apply to the number of bits rather than the number of linear measurements, and typically do not provide an explicit distribution which is hard. These issues can be resolved using techniques from [4], [20] and [15]. The resulting hard distribution is: choose x ∈ {−1, 0, 1} n uniformly at random, and then with probability 1/2, replace a random coordinate x i of x with a value in Θ(n 1/p ). F p (x) changes by a constant factor in the two cases, and so the approximation algo- rithm must determine which case we are in.\nWe instead consider the following continuous dis- tribution: choose x to be a random N (0, I n ) vector, i.e., a vector whose coordinates are independent stan- dard normal random variables. With probability 1/2, replace a random coordinate x i of x with a value in Θ(n 1/p ). The use of Gaussians instead of signs allows us to derive our lower bound almost immedi- ately from the Shannon-Hartley theorem. We obtain an Ω(n 1−2/p ) bound on the number of linear measurements required for estimating F p , matching known bounds up to poly( −1 log(M nδ −1 ) factors. Our proof is much simpler than previous proofs.\nOur new hard distribution may also more accurately model those signals x arising in practice, since it corre- sponds to a signal with support 1 which is corrupted by independent Gaussian noise in each coordinate. Identify- ing natural hard distributions has been studied for other data stream problems, see, e.g., [19] and [17].\nIn the ﬁeld of compressed sensing, a standard problem is that of stable sparse recovery: we want a distribution A of matrices A ∈ R m×n such that, for any x ∈ R n and with probability 1 − δ > 2/3 over A ∈ A, there is an algorithm to recover ˆ x from Ax with\nfor some > 0 and norm p. We call this a (1 + )- approximate p / p recovery scheme with failure proba- bility δ. We will focus on the popular case of p = 2.\nFor any constant δ > 0 and any satisfying = O(1) and = Ω(n −1/2 ), the optimal number of measurements is Θ(k log(n/k)/ ). The upper bound is in [12], and the lower bound is given by [1], [6], [14], [20]; see [20] for a comparison of these works.\nOne question is if the number of measurements can be improved when the approximation factor C = 1 +\nis very large (i.e. ω(1)). In the limiting case of C = ∞, corresponding to sparse recovery in the absence of noise, it is known that O(k) measurements are sufﬁcient [7]. However, the intermediate regime has not been well studied.\nUsing the Shannon-Hartley theorem, we prove an Ω(k log(n/k)/ log C) lower bound on the number of measurements. We complement this with a novel sparse recovery algorithm, which builds upon [12] and [13], but is the ﬁrst to obtain an improved bound for C > 1. Our bound is O(k + k log(n/k) log ∗ k/ log C), which matches our lower bound up to a log ∗ k factor. Because log(1 + ) ≈ , these results match the Θ( 1 k log(n/k)) results for \t 1.\nRelated work. Related lower bounds have appeared in a number of recent works, including [6], [14], [1], [21], and [10]. See [20] for a comparison.\nThis section is devoted to proving the following the- orem:\nTheorem 1: Any sketching algorithm for F p up to a factor of (1 ± ) for < 1/2, which succeeds with probability 1 − δ for a sufﬁciently small constant δ > 0, requires m = Ω(n 1−2/p ).\nLet G p = E[|X| p ] where X ∼ N (0, 1). For constant p, G p is Θ(1).\nConsider the following communication game between two players, Alice and Bob. Alice chooses a random\n∈ [n] and associated standard unit vector e = (0, . . . , 0, 1, 0, . . . , 0) ∈ R n . She also chooses w ∼ N (0, I n ). Then she chooses Z ∈ {0, 1} uniformly at random. If Z = 0, then Alice sets x = w. If Z = 1, then Alice sets x = (4G p ) 1/p · n 1/p e + w. She sets y = Ax, where A is the random matrix used for estimating F p . She sends y to Bob, who runs the estimation procedure associated with A to recover an estimate r to F p (x). If\nr ≥ 2G p n, then Bob sets Z = 1, else Bob sets Z = 0. We thus have a Markov chain , Z → x → y → Z .\nIf A works for any x with probability 1 − δ, as a distribution over A, then there is a speciﬁc A and random seed such that A, together with the associated estimation procedure, succeeds with probability 1 − δ over x drawn from the distribution described above. Let us ﬁx this choice of A and associated random seed, so that Alice and Bob run deterministic algorithms. Let m be the number of rows of A. We can assume the rows of A are orthonormal since this can be done in post-processing.\nProof: Let the rows of A be denoted v 1 , . . . , v m . Then we have that\nwhere w i ∼ N (0, 1). Deﬁne z i = (4G p ) 1/p n 1/p · v i , e Z so y i = z i + w i . Then\n= 1 2\n= 1 2\nHence, y i = z i + w i is a Gaussian channel with power constraint E[z 2 i ] = Θ(1/n 1−2/p ) and noise variance E[(w i ) 2 ] = 1. By the Shannon-Hartley theorem,\nBy the data processing inequality for Markov chains and the chain rule for entropy,\nI(Z; Z ) ≤ I(z; y) = h(y) − h(y|z) = h(y) − h(y − z|z) = h(y) −\nProof of Theorem 1: If Z = 1, then x p p ≥ 4G p ·n, and so any (1 ± )-approximation is at least 2G p n for\n< 1/2. On the other hand, if Z = 0, then E[ x p p ] = G p · n, and since the |x i | p are i.i.d. (as we range over i) with bounded variance, by Bernstein\u2019s inequality, with probability at least 1−1/n, x p p ≤ 4 3 ·G p ·n. Hence, any (1 ± )-approximation is less than 2G p n for < 1/2. So if the algorithm succeeds with probability at least 1 − δ, then Z = Z with probability at least 1 − δ − 1/n.\nBy Fano\u2019s inequality and the fact that Z, Z ∈ {0, 1}, if q = Pr[Z = Z] then we have H(Z|Z ) ≤ H(q) + q. Hence,\nif q is less than a sufﬁciently small constant, which follows from δ being a sufﬁciently small constant. But by Lemma 2, I(Z; Z ) = O(m/n 1−2/p ). Hence m =\nFor C = 1 + a lower bound of Ω(k log(n/k)/ ) was shown in [1], [6], [14], [20] for any constant δ > 0 and\nsatisfying = O(1) and = Ω(n −1/2 ). As in the lower bound of [20], ours uses the Shannon-Hartley theorem, but this proof is simpler because it can use that C is large. We explain the approach and our modiﬁcation, and refer the reader to [20] for more details.\nTheorem 3: Any C-approximate 2 / 2 recovery scheme with failure probability δ < 1/2 requires m = Ω(k log(n/k)/ log C).\nAs in [20], let F ⊂ {S ⊂ [n] | |S| = k} be a family of k-sparse supports such that:\n\u2022 Pr S∈F [i ∈ S] = k/n for all i ∈ [n], and \u2022 log |F | = Ω(k log(n/k)).\nA random linear code on [n/k] k with relative distance 1/2 has these properties (see discussion in [20]).\nLet X = {x ∈ {0, ±1} n | supp(x) ∈ F }. Let w ∼ N (0, α k n I n ) be i.i.d. normal with variance αk/n in each coordinate. Consider the following process.\nAlice chooses S ∈ F uniformly at random, then x ∈ X uniformly at random subject to supp(x) = S, then w ∼ N (0, α k n I n ). She sets y = A(x + w) and sends y to Bob. Bob performs sparse recovery on y to recover x ≈ x, rounds to X by ˆ x = arg min ˆ x∈X ˆ x − x 2 , and sets S = supp(ˆ x). This gives a Markov chain S → x → y → x → S .\nIf sparse recovery works for x + w with probability 1 − δ over A, then there is a ﬁxed A and random seed such that sparse recovery works with probability 1 − δ over x+w; choose this A and random seed, so that Alice and Bob run deterministic algorithms on their inputs.\nLemma 4: (4.1 of [20]) I(S, S ) = O(m log(1 + 1 α )). We modify Lemma 4.3 of [20] to obtain our main\nlemma and theorem. It is simpler than [20] since when C is large, the recovery algorithm cannot try to output many of the Gaussian coordinates in lieu of ﬁnding x.\nLemma 5: I(S, S ) = Ω(k log(n/k)) if α = Ω(1/C). Proof: The claim is that with probability at least\n1/2, ˆ x = x, and so S = S . By Fano\u2019s inequality we will then have H(S|S ) ≤ 1 + Pr[S = S] log |F |, and\nTo show the claim, we condition on successful sparse recovery, which happens with probability 1 − δ ≥ 2/3. Let z = x + w be the transmitted signal. We also condition on w 2 ∞ ≤ O( αk n log n) and w 2 2 /(αk) ≤ 2, which happen with probability at least 1 − o(1). So both events occur with probability at least 2/3 − o(1) > 1/2. Given this conditioning and that α = Ω(1/C), the best k-sparse approximation to z is x + w S , where w S is the restriction of w to coordinates in S.\nSuppose ˆ x = x, so ˆ x − x 2 ≤ x − x 2 . Then because sparse recovery was successful, z − x 2 ≤ C w − w S 2 ≤ C w 2 . Hence\nˆ x − x 2 ≤ ˆ x − x 2 + x − x 2 ≤ 2 x − x 2\n≤ 2( x − z 2 + z − x 2 ) ≤ 2(C + 1) w 2 ≤ 2(C + 1)\nk for appropriate α = Ω(1/C). This is a contradiction, and so ˆ x = x, as desired.\nProof of Theorem 3: Combining Lemma 4 with Lemma 5, Ω(k log(n/k)) = I(S, S ) = O(m log C), from which m = Ω(k log(n/k)/ log C).\nWe ﬁrst focus on recovery of a single heavy coordi- nate. We then study recovery of 90% of the heavy hitters for general k. We conclude with recovery of all the heavy hitters.\n1) k=1: We observe 2r measurements, for some r = O(log C n). Let D = C/16. For i ∈ [r], we choose pairwise independent hash functions h i : [n] → [D] and s i : [n] → {±1}. We then observe\nc j ← |{i ∈ [r] | h i (j) = α i }| for j ∈ [n]. S ← {j ∈ [n] | c j > 5r/8}. if |S| = 1 then\nreturn j ∈ S else\nreturn ⊥ end if\nLemma 6: Suppose there exists a j ∗ ∈ [n] such that |x j ∗ | ≥ C x −j ∗ 2 . Then if C is a sufﬁciently large constant, we can choose r = O(log C n + log 1/δ) and\nD = C/16 so that I DENTIFY S INGLE returns j ∗ with probability 1 − δ.\nStraightforward concentration inequalities then give the result. To get (1), deﬁne the \u201cnoise\u201d β i =\nThus with probability at least 1 − 2/9 > 3/4, γ i ≤ 3 x −j ∗ 2 and β i ≤ 3D x −j ∗ 2 . But then\n∗ )/C 1 − 3/C\nso if D = C/16 < (C − 2)/12, as happens for sufﬁciently large C, this is less than 1/2 so α i =\nThen by a Chernoff bound, Pr[j ∗ / ∈ S] = Pr[c j ∗ < 5r/8] = e −Ω(r) < δ/2 for r = Ω(log(1/δ)). Suppose that j ∗ ∈ S. In order for any j = j ∗ to lie in S, it must have h i (j) = h i (j ∗ ) for at least r/4 different i (because both match α for 5r/8 coordinates). But Pr[h i (j) = h i (j ∗ )] = 1/D independently over i, so\nas long as C is larger than a ﬁxed constant. But for r = O(log C (n/δ)) this gives Pr[j ∈ S] < δ/(2n), so a union bound gives that S = {j} with probability 1 − δ.\n2) General k, ﬁnding most coordinates: For general k, we identify a set L of O(k) coordinates by partitioning the coordinates into O(k) sets of size Θ(n/k) and applying I DENTIFY S INGLE . To be speciﬁc, we use a pairwise independent hash function h : [n] → [l] to partition into l sets.\nLemma 7: With O(k log C (n/k)) measurements, this algorithm returns a set L of size O(k) such that each j ∈ S has j ∈ L with probability at least 3/4.\nProof: For each coordinate j ∈ S, Lemma 6 shows it will be recovered as long as three events hold: none of the other elements of the top k coordinates hash to the same value as j, the 2 2 norm of the mass that hashes to the same value as j is no more than a constant factor\ntimes its expectation Err 2 (x, k)/k, and the algorithm in Lemma 6 does not fail. All these occur with constant probability if l = O(k), giving the result.\nCorollary 8: With O(k log C (n/k) log(1/δ)) mea- surements, I DENTIFY M OST returns a set L of size O(k) such that each j ∈ S has j ∈ L with probability at least 1 − δ.\nProof: We repeat the method of Lemma 7 O(log(1/δ)) times, and take all coordinates that are listed in more than half the sets L i . This at most doubles the output size, and by a Chernoff bound each j ∈ S lies in the output with probability at least 1 − δ.\nCorollary 8 gives a good method for ﬁnding the heavy hitters, but we also need to estimate them.\n3) Estimating coordinates: We estimate using Count- Sketch [8], with R = O(log(1/δ)) hash tables of size O(k/ ).\nc j ← |{r | j ∈ L r }| for j ∈ [n]. L ← {j | c j > R/2} return x L\nx j ← median r x (r) j return x L\nLemma 9: Suppose |L| ≤ O(k). With O( 1 k log( 1 f δ )) measurements, E STIMATE M OST returns x L so that for any j, with probability 1 − δ we have\nThus with probability 1 − δ, at most f |L| = O(f k) of the j have |x j − x j | 2 > k Err 2 (x, k). Rescaling f and gives the result.\nLemma 10: The result x L of I DENTIFY M OST fol- lowed by E STIMATE M OST satisﬁes\nwith probability 1−δ, and uses O(k log C (n/k) log( 1 f δ )) measurements.\nProof: Let T contain the largest k coordinates of x. By Corollary 8, each j ∈ S has j ∈ L with probability\nk ← k, δ ← 1/16, x (0) ← 0 for r ← [R] do\nDecrease k , , δ per Theorem 11 end for\n1 − δf , so with probability 1 − δ we have |S \\ L| ≤ f k. Then\n≤ ( + 1 + C 2 )Err 2 (x, k) ≤ 2C 2 Err 2 (x, k)\nwith probability 1 − δ by Lemma 9. Rescale f , δ, and C to get the result.\n(n/k)) measurements and 3/4 success probability.\nProof: We will achieve D O(log ∗ k) -approximate re- covery using O(k log D (n/k)) measurements. Substitut- ing log C = log D log ∗ k gives the result.\nDeﬁne δ i = 1 8·2 i . Let f 0 = 1/16 and f i+1 = 2 −1/(4 i f i ) . Let k i = k j<i f j . Then for R = O(log ∗ k), k R < 1.\nWe set x (0) = 0, and iterate I DENTIFY M OST and E STIMATE M OST on x − x (r) in each round r with δ r , f r , k r , D as parameters, getting update v (r) and set- ting x (r+1) = x (r) + v (r) .\nso x − x (R) 2 2 ≤ D 2R Err 2 (x, k), which is D O(log ∗ k) - approximate recovery.\nE. Price is supported in part by an NSF Graduate Research Fellowship."},"refs":[{"authors":[{"name":"S. Aero"},{"name":"V. Saligram"},{"name":"M. Zhao"}],"title":{"text":"Information theoretic bounds for compressed sensing"}},{"authors":[],"title":{"text":"Noga Alon, Yossi Matias, and Mario Szegedy"}},{"authors":[],"title":{"text":"Alexandr Andoni, Robert Krauthgamer, and Krzysztof Onak"}},{"authors":[{"name":"P. Woodruff"}],"title":{"text":"Khanh Do Ba, Piotr Indyk, Eric Price, and David  Lower bounds for sparse recovery"}},{"authors":[{"name":"T. S. Jayra"},{"name":"D. Sivakumar"}],"title":{"text":"Ziv Bar-Yossef,  Ravi Kumar, and  An information statistics approach to data stream and communication complexity"}},{"authors":[{"name":"J. Cand`e"},{"name":"A. Davenport"}],"title":{"text":"E"}},{"authors":[{"name":"J. Cand`e"},{"name":"J. Romber"},{"name":"T. Tao"}],"title":{"text":"E"}},{"authors":[],"title":{"text":"Moses Charikar, Kevin Chen, and Martin Farach-Colton"}},{"authors":[{"name":"S. Muthukrishna"}],"title":{"text":"Jon Feldman,  Anastasios Sidiropoulos, Clif- ford Stein, and Zoya Svitkina"}},{"authors":[{"name":"K. Fletche"},{"name":"K. Goyal"}],"title":{"text":"Alyson  Sundeep Rangan, and Vivek  Necessary and sufﬁcient conditions for sparsity pattern recovery"}},{"authors":[],"title":{"text":"Sumit Ganguly"}},{"authors":[{"name":"C. Gilber"},{"name":"J. Strauss"}],"title":{"text":"Anna  Yi Li, Ely Porat, and Martin  Ap- proximate sparse recovery: optimizing time and measurements"}},{"authors":[{"name":"P. Woodruff"}],"title":{"text":"Piotr Indyk, Eric Price, and David  On the power of adaptivity in sparse recovery"}},{"authors":[],"title":{"text":"MA Iwen and AH Tewﬁk"}},{"authors":[{"name":"T. S. Jayram"}],"title":{"text":"Unpublished manuscript"}},{"authors":[{"name":"T. S. Jayram"}],"title":{"text":"Hellinger strikes back: A note on the multi-party information complexity of and"}},{"authors":[],"title":{"text":"Ravi Kumar and Rina Panigrahy"}},{"authors":[{"name":"J. Frankli"},{"name":"M. Hellerstei"}],"title":{"text":"Samuel Madden, Michael  Joseph  and Wei Hong"}},{"authors":[],"title":{"text":"Rajeev Motwani and Sergei Vassilvitskii"}},{"authors":[{"name":"P. Woodruff"}],"title":{"text":"Eric Price and David  (1 + eps)-approximate sparse recovery"}},{"authors":[{"name":"J. Wainwright"}],"title":{"text":"Martin  Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting"}},{"authors":[{"name":"P. Woodruf"}],"title":{"text":"David  Qin Zhang"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569567005.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T9.5","endtime":"16:20","authors":"Eric Price, David Woodruff","date":"1341504000000","papertitle":"Applications of the Shannon-Hartley Theorem to Data Streams and Sparse Recovery","starttime":"16:00","session":"S13.T9: Fourier Subsampling","room":"Stratton West Lounge (201)","paperid":"1569567005"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
