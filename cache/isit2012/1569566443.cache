{"id":"1569566443","paper":{"title":{"text":"Optimal Phase Transitions in Compressed Sensing with Noisy Measurements"},"authors":[{"name":"Yihong Wu"},{"name":"Sergio Verd´u"}],"abstr":{"text":"Abstract\u2014Compressed sensing deals with efﬁcient recovery of analog signals from linear encodings. This paper presents a statistical study of compressed sensing by modeling the input signal as an i.i.d. random process. Three classes of encoders are considered, namely, optimal nonlinear, optimal linear and random linear encoders. Focusing on optimal decoders, we investigate the fundamental tradeoff between measurement rate and reconstruction ﬁdelity gauged by the noise sensitivity. The optimal phase-transition threshold is determined as a functional of the input distribution and compared to suboptimal thresholds achieved by popular reconstruction algorithms. In particular, we show that Gaussian sensing matrices incur no penalty on the phase-transition threshold with respect to optimal nonlinear encoding. Our results also provide a rigorous justiﬁcation of previous results based on replica heuristics in the weak-noise regime."},"body":{"text":"Compressed sensing [1], [2] is a signal processing tech- nique that compresses analog vectors by means of a linear transformation. By leveraging prior knowledge of the signal structure (e.g., sparsity) and by designing efﬁcient nonlinear reconstruction algorithms, effective compression is achieved by taking a much smaller number of measurements than the dimension of the original signal.\nMost of the compressed sensing literature focuses on the setup where\na) performance is measured on a worst-case basis with respect to the n-dimensional input vector.\nb) the encoder is constrained to be a linear mapping character- ized by a k × n matrix, called the sensing or measurement matrix, which is usually assumed to be random, and known at the decoder.\nc) the decoder is a low-complexity algorithm which is robust with respect to observation noise, for example, decoders based on convex optimizations such as 1 -minimization [3] and 1 -penalized least-squares (i.e. LASSO) [4], greedy algorithms such as matching pursuit [5], graph-based itera- tive decoders such as approximate message passing (AMP) [6].\nIn contrast, in this paper we formulate an information- theoretic fundamental limit in the following setup:\na) the input vector is random with a known distribution and performance is measured on an average basis. Similar Bayesian modeling is followed in some of the compressed sensing literature, for example, [6], [7], [8], [9].\nThe overarching goal is to investigate the fundamental tradeoff between reconstruction ﬁdelity and measurement rate\nas n → ∞, as a functional of the signal and noise statistics. When the measurements are noiseless, the goal is to re-\nconstruct the original signal in the sense of driving the error probability to zero as the ambient dimension, n, grows. For many input processes (e.g., i.i.d. ones), it turns out that there exists a threshold for the measurement rate, above which it is possible to achieve a vanishing error probability and below which the error probability will eventually approach one for any sequence of encoder-decoder pairs. Such a phe- nomenon is known as phase transition in statistical physics. In information-theoretic parlance, we say that the strong converse holds.\nWe introduced the framework of almost-lossless analog compression in [7] as a Shannon-theoretic formulation of noiseless compressed sensing. Under regularity conditions such as linearity of the encoder and Lipschitz continuity of the decoder, [7] derives various coding theorems for the minimal measurement rate involving the R´enyi information dimension of the input distribution [10]. See also [11, Section III] for a non-asymptotic exposition.\nThe focus of this paper is the case where the measurements are corrupted by additive noise. Exact analog signal recovery is obviously impossible and we gauge reconstruction ﬁdelity by the noise sensitivity, deﬁned as the ratio between the mean- square reconstruction error and the noise variance. Similar to the behavior of error probability in the noiseless case, there exists a phase-transition threshold of measurement rate, which only depends on the input statistics, above which the noise sensitivity is bounded for all noise variances, and below which the noise sensitivity blows up as the noise variance tends to zero.\nIn Section II we consider three formulations of noise sen- sitivity: optimal nonlinear, optimal linear and random linear (with i.i.d. entries) encoder and the associated optimal decoder. For memoryless sources, we show that for any input distri- bution, the phase-transition threshold for optimal encoding is given by the input information dimension. Moreover, this result also holds for discrete-continuous mixtures with optimal\nlinear encoders and Gaussian random measurement matrices. The fact that randomly chosen sensing matrices turn out to incur no penalty in phase-transition threshold with respect to optimal nonlinear encoders lends further importance to the conventional compressed sensing setup. In addition, we compare the optimal phase-transition threshold to the subop- timal threshold of several practical reconstruction algorithms under various input distributions. In particular, in Section III we demonstrate that the thresholds achieved by the LASSO decoder and the AMP decoder [8] lie far from the optimal boundary, especially in the highly sparse regime, which is most relevant to compressed sensing applications.\nInvoking the results in [12], in Section II-F we show that the calculation of the reconstruction error with random measurement matrices based on heuristic replica methods in [9] predicts the correct phase-transition threshold. These results also serve as a rigorous veriﬁcation of the replica calculations in [9] in the high-SNR regime (up to o(σ 2 ) as the noise variance σ 2 vanishes).\nThe basic setup of noisy compressed sensing is the joint source-channel coding problem shown in Fig. 1, where we\n\u2022 The source X n consists of i.i.d. copies of a real-valued random variable X with unit variance.\n\u2022 The channel is memoryless with additive Gaussian noise σN k where N k ∼ N (0, I k ).\n\u2022 The measurement rate, i.e., the dimensionality compres- sion ratio, is given by R = k n .\n\u2022 The reconstruction error is gauged by the per-symbol MSE distortion: d(x n , ˆ x n ) = 1 n ˆx n − x n 2 2 .\nIn this setup, the fundamental question is: For a given noise variance and measurement rate, what is the lowest reconstruc- tion error? For a given encoder f , the corresponding optimal decoder g is the MMSE estimator of the input X n given the channel output ˆ Y k = f(X n ) + σN k . Therefore the optimal distortion achieved by encoder f is mmse(X n |f(X n )+σN k ), where mmse(U|V ) E U − E [U|V ] 2 2 .\nSparse vectors, supported on a subspace with dimension smaller than n, play an important role in signal processing and\nstatistical models. A stochastic model that captures sparsity is the following mixture distribution [7], [6], [9]:\nwhere δ 0 denotes a unit mass at 0, P c is an absolutely continuous (with respect to Lebesgue measure) distribution from which the non-zero entries are drawn, and 0 ≤ γ ≤ 1 parametrizes the signal sparsity. This model corresponds to the regime of proportional (or linear) sparsity, because X n drawn independently from (1) has approximately γn non-zeros.\nGeneralizing (1), we henceforth consider discrete- continuous mixed distributions:\nwhere P d is a discrete probability measure and P c is an abso- lutely continuous probability measure. In addition to sparsity, there are other signal structures that have been previously explored in the compressed sensing literature, which ﬁt the model in (2). For example, the so-called simple signal in infrared absorption spectroscopy [13, Example 3, p. 914] is such that each entry of of the signal vector is constrained to lie in the unit interval, with most of the entries saturated at the boundaries (0 or 1) (see Section III for its probabilistic model). Although most of the results in the present paper hold for arbitrary input distributions, with no practical loss of generality, we focus on discrete-continuous mixtures because of their relevance to compressed sensing applications.\nFor a ﬁxed noise variance, we deﬁne three distortion-rate functions that correspond to optimal encoding, optimal linear encoding and random linear encoding respectively.\n1) Optimal encoder: The minimal distortion achieved by the optimal encoding scheme is given by:\n1 n\nThe asymptotic optimization problem in (3) can be solved by applying Shannon\u2019s joint source-channel coding separation theorem for memoryless channels and sources [14, Section XI], which states that the lowest rate, R, that achieves distor- tion D is given by\nwhere R X (·) is the rate-distortion function of X under the mean-square error distortion and C(σ 2 ) = 1 2 log(1 + σ −2 ) is the AWGN channel capacity. By the monotonicity of the rate-distortion function, we have\nIn general, optimal joint source-channel encoders are nonlin- ear.\n2) Optimal linear encoder: To analyze the fundamental limit of conventional noisy compressed sensing, we restrict the encoder f in (3) to be a linear mapping represented by a matrix H ∈ R k ×n , and denote the left-hand side of (3) by D ∗ L (X, R, σ 2 ). Since X n are i.i.d. with zero mean and unit variance, the input power constraint in (3) simpliﬁes to\n3) Random linear encoder: We consider the ensemble per- formance of a sequence of k ×n random measurement matrices A n with i.i.d. entries of zero mean and variance 1 n and deﬁne D L (X, R, σ 2 ) as in (3) with f replaced by A n . Note that the power constraint holds on average: E[ A n 2 F ] = k. By deﬁnition, we have 0 ≤ D ∗ ≤ D ∗ L ≤ D L ≤ 1.\nA key objective of compressed sensing with noisy ob- servations is to achieve robust reconstruction, obtaining a reconstruction error proportional to the noise variance. To quantify robustness, we analyze the noise sensitivity, namely, the ratio between the mean-square error and the noise variance, as a function of R and σ 2 . As a succinct characterization of robustness, we focus particular attention on the worst-case noise sensitivity:\nDeﬁnition 1. The worst-case noise sensitivity of optimal encoding is deﬁned as\nσ 2 \t . \t (6) For linear encoding, ζ ∗ L and ζ L are analogously deﬁned with D ∗ replaced by D ∗ L and D L , respectively.\nThe phase-transition threshold of the noise sensitivity is deﬁned as the minimal measurement rate R such that the noise sensitivity is bounded for all σ 2 :\nFor linear encoding, R ∗ L (X) and R L (X) are analogously deﬁned with ζ ∗ in (7) replaced by ζ ∗ L and ζ L , respectively.\nAlternatively, we can consider the asymptotic noise sensitiv- ity by replacing the supremum in (6) with the limit as σ 2 → 0, denoted by ξ ∗ , ξ ∗ L and ξ L respectively, which are ﬁnite if and only if the corresponding suprema are ﬁnite. Therefore, the phase-transition thresholds can also be deﬁned as the minimum measurement rate for which the reconstruction error vanishes according to O(σ 2 ) as σ 2 → 0.\nBy deﬁnition, the phase-transition thresholds are ordered naturally as 0 ≤ R ∗ (X) ≤ R ∗ L (X) ≤ R L (X) ≤ 1. It can be shown that Gaussian input X G ∼ N (0, 1) maximizes D ∗ , D ∗ L and D L simultaneously under the variance constraint with explicit formulae given in [11], which yield the following phase-transition threshold:\nThe equality of the three phase-transition thresholds turns out to hold well beyond the Gaussian signal model. In the next subsection, we formulate and prove the existence of the phase thresholds for all three distortion-rate functions and discrete-continuous mixtures, which turn out to be equal to the information dimension of the input distribution.\nIn this subsection, we give bounds and exact characteri- zations of the three phase-transition thresholds introduced in Deﬁnition 2 in terms of the input information [10] and MMSE dimension [12], deﬁned as follows.\nDeﬁnition 3. Let X be a real-valued random variable. Let m ∈ N. The information dimension of X is deﬁned as\nThe lim inf and lim sup in (9) (resp. (10)) are called lower and upper information (resp. MMSE) dimensions of X, denoted by d(X) and d(X) (resp. D(X) and D(X)).\nIt is shown in [12, Theorem 8] that the information dimen- sions are sandwiched between the MMSE dimensions:\nFor discrete-continuous mixtures (2), the MMSE dimension coincides with the information dimension [12, Theorem 15]:\nIt is possible that the MMSE dimension does not exist and the inequalities in (11) are strict (e.g., Cantor distributed X [12, Theorem 16]).\nThe phase-transition threshold for optimal encoding is given by the upper information dimension of the input: Theorem 1. For any X, R ∗ (X) = d(X).\nThe next result shows that random linear encoders with i.i.d. Gaussian coefﬁcients also achieve information dimension for any discrete-continuous mixtures, which, in view of Theorem 1, implies that, at least asymptotically, (random) linear en- coding sufﬁces for robust reconstruction as long as the input distribution contains no singular component.\nTheorem 2. Assume that X has a discrete-continuous mixed distribution as in (2) with H(P d ) < ∞. Then\nMoreover, (13) holds for any noise distribution with ﬁnite non- Gaussianness (deﬁned as its relative entropy with respect to a Gaussian distribution with the same mean and variance).\nBased on the statistical-physics approach in [15], [16], the decoupling principle results in [16] were imported into the compressed sensing setting in [9] to postulate the following formula for D L (X, R, σ 2 ). Note that this result is based on replica heuristics currently lacking a rigorous justiﬁcation.\nwhere 0 < η < 1 satisﬁes η −1 = 1 + σ −2 mmse(X, ηR σ −2 ). In the case of multiple solutions, η is chosen to minimize the free energy I(X; ηRσ −2 X + N ) + R 2 (η − 1 − log η).\nAssuming the validity of the replica symmetry postulate, it can be shown that the phase-transition threshold for random linear encoding is always sandwiched between the lower and the upper MMSE dimensions of the input. The relationship between the MMSE dimension and the information dimension in (11) plays a key role in analyzing the minimizer of the free energy.\nTheorem 3. Assume that the replica symmetry postulate holds for X. Then for any i.i.d. random measurement matrix whose entries have zero mean and variance 1 n ,\nThe general result in Theorem 3 holds for any input distribu- tion but relies on the conjectured validity of the replica sym- metry postulate. For the special case of discrete-continuous mixtures in (2), in view of (12), Theorem 3 predicts (with the caveat of the validity of the replica symmetry postulate) that the phase-transition threshold for random linear encoding is γ, which agrees with the rigorously proven result in Theorem 2. Therefore, the only added beneﬁt of Theorem 3 is to allow singular components in the input distribution.\nThe achievability proof of R L (X) in Theorem 3 is a direct application of the noiseless result in [11], where it is shown that there exists a sequence of Lipschitz decompressors with bounded Lipschitz constants and vanishing block error prob- ability. The noiseless Lipschitz decompressor as a suboptimal estimator achieves ﬁnite noise sensitivity. This achievability strategy applies to any noise with ﬁnite variance, without requiring that the noise be additive, memoryless or that it have a density. In contrast, replica-based results rely crucially on the fact that the additive noise is memoryless Gaussian.\nIn this section, we compare the phase transition thresholds of LASSO and AMP achieved in the Bayesian setting to the optimal thresholds for the following families of input distributions considered in [6, p. 18915], indexed by χ = ±, + and respectively, which all belong to the mixture form (2):\n+ sparse non-negative signals (1) with the continuous component P c supported on R + .\nsimple signals (see Section II-B): P X = (1 − γ) 1 2 δ 0 + 1 2 δ 1 + γ P c , where P c is some absolutely continuous distribution supported on the unit interval.\nWe consider the AMP decoder [8] and the LASSO decoder: ˜g λ (y) = argmin\n2 y − Ax 2 2 + λ x 1 , \t (17) where λ > 0 is a regularization parameter. For Gaussian sens- ing matrices and Gaussian observation noise, the asymptotic mean-square error achieved by LASSO can be determined as a function of (P X , λ, σ 2 ) by applying [17, Corollary 1.6]. For sparse X distributed according to (1), the following minimax expression for the worst-case (or asymptotic) noise sensitivity of LASSO under the least favorable prior with optimized λ is proposed in [8, Proposition 3.1(1.a)] and proved by [17]:\n(19) and c = 2. Analogously, the LASSO decoder (17) can be adapted to other signal structures (see for example [8, Sec. VI-A]), resulting in the phase-transition threshold R + (γ) for sparse positive signals given by (19) with c = 1 and R (γ) =\n2 for simple signals. Furthermore, (18) also applies to the AMP algorithm [8]. The suboptimal thresholds are plotted in Fig. 2 along with the optimal threshold obtained from Theorem 2 which is γ. In the gray area below the diagonal in the (γ, R)-phase diagram, the noise sensitivity blows up for any sequence of sensing matrices and decoders. Moreover, we observe that the LASSO and AMP decoders are severely suboptimal unless γ is close to one. In the highly sparse regime which is most relevant to compressed sensing problems, it follows from [13, Theorem 3] that for sparse signals (χ = ± or +), R χ (γ) = 2γ log e 1 γ (1+o(1)), as γ → 0, which implies that R χ has inﬁnite slope at γ = 0. Therefore when γ 1, the LASSO and AMP decoders require on the order of 2s log e n s measurements to successfully recover the unknown vector with s nonzero components. In contrast, s measurements sufﬁce when using an optimal decoder (or 0 -minimization decoder). The LASSO or AMP decoders are also highly suboptimal for simple signals, since R (γ) converges to 1 2 instead of zero as γ → 0. Interestingly, the phase-transition thresholds of block error probability in the noiseless case are identical to Fig. 2, as observed in [11, Sec. 5.1].\nFor sparse signals of the form (1) with γ = 0.1, Fig. 3 compares the asymptotic noise sensitivity of the LASSO (and AMP) decoder to the optimal noise sensitivity predicted by Theorem 3 based on replica heuristics. Note that the phase- transition threshold of LASSO is approximately 3.3 times the optimal.\nγ R\nFig. 2: Suboptimal thresholds obtained with LASSO and AMP v.s. optimal threshold for the three signal models in Section III.\nFig. 3: Asymptotic noise sensitivity of the optimal decoder and the LASSO decoder exhibiting phase transitions: sparse signal model (1) with γ = 0.1.\nAs opposed to a worst-case (Hamming) approach, in this paper we adopt a statistical (Shannon) framework for com- pressed sensing by modeling input signals as random processes rather than individual sequences. As customary in information theory, it is advisable to initiate the study of fundamental limits assuming independent identically distributed informa- tion sources with known distributions. Naturally, this entails substantial loss of practical relevance, so generalization to sources with memory is left for future work. We have obtained the phase-transition thresholds (minimum measurement rate) of normalized MMSE with noisy observations achievable by optimal nonlinear, optimal linear, and random linear en- coders combined with the corresponding optimal decoders. For discrete-continuous mixtures, the optimal phase-transition threshold is shown to be the information dimension of the input. The phase-transition thresholds of popular decoding algorithms (e.g., LASSO or AMP decoders) turn out to be far from the optimal boundary. In a recent preprint [18], it is shown that using sensing matrices constructed from spatially coupled error-correcting codes and the corresponding AMP decoder, information dimension and MMSE dimension are respectively achievable in both noiseless and noisy cases under mild conditions, which are optimal in view of the results in\nOne of our main ﬁndings is Theorem 2 which shows that Gaussian sensing matrices achieve the same phase-transition threshold as optimal nonlinear encoding, for any discrete- continuous mixture. This result is universal in the sense that it holds for arbitrary noise distributions with ﬁnite non- Gaussianness. Moreover, the fundamental limit depends on the input statistics only through the weight of the analog component, regardless of the speciﬁc discrete and continu- ous components. The universal optimality of random sensing matrices with non-Gaussian i.i.d. entries in terms of phase- transition thresholds remains open."},"refs":[{"authors":[{"name":"E. Cand´es"},{"name":"J. Romberg"},{"name":"T. Tao"}],"title":{"text":"Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information"}},{"authors":[{"name":"D. L. Donoho"}],"title":{"text":"Compressed sensing"}},{"authors":[{"name":"S. S. Chen"},{"name":"D. L. Donoho"},{"name":"M. A. Saunders"}],"title":{"text":"Atomic decomposition by basis pursuit"}},{"authors":[{"name":"R. Tibshirani"}],"title":{"text":"Regression shrinkage and selection via the LASSO"}},{"authors":[{"name":"S. G. Mallat"},{"name":"Z. Zhang"}],"title":{"text":"Matching pursuits with time-frequency dictionaries"}},{"authors":[{"name":"D. L. Donoho"},{"name":"A. Maleki"},{"name":"A. Montanari"}],"title":{"text":"Message-passing algo- rithms for compressed sensing"}},{"authors":[{"name":"Y. Wu"},{"name":"S. Verd´u"}],"title":{"text":"R´enyi information dimension: Fundamental limits of almost lossless analog compression"}},{"authors":[{"name":"D. L. Donoho"},{"name":"A. Maleki"},{"name":"A. Montanari"}],"title":{"text":"The noise-sensitivity phase transition in compressed sensing"}},{"authors":[{"name":"D. Guo"},{"name":"D. Baron"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"A single-letter character- ization of optimal noisy compressed sensing"}},{"authors":[{"name":"A. R´enyi"}],"title":{"text":"On the dimension and entropy of probability distributions"}},{"authors":[{"name":"Y. Wu"},{"name":"S. Verd´u"}],"title":{"text":"Optimal phase transitions in compressed sensing"}},{"authors":[],"title":{"text":"MMSE dimension"}},{"authors":[{"name":"D. L. Donoho"},{"name":"J. Tanner"}],"title":{"text":"Precise undersampling theorems"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"Communication in the presence of noise"}},{"authors":[{"name":"T. Tanaka"}],"title":{"text":"A statistical-mechanics approach to large-system analysis of CDMA multiuser detectors"}},{"authors":[{"name":"D. Guo"},{"name":"S. Verd´u"}],"title":{"text":"Randomly spread CDMA: Asymptotics via statistical physics"}},{"authors":[{"name":"M. Bayati"},{"name":"A. Montanari"}],"title":{"text":"The LASSO risk for Gaussian matrices"}},{"authors":[{"name":"D. L. Donoho"},{"name":"A. Javanmard"},{"name":"A. Montanari"}],"title":{"text":"Information- theoretically optimal compressed sensing via spatial coupling and approximate message passing"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566443.pdf"},"links":[{"id":"1569565383","weight":4},{"id":"1569565883","weight":8},{"id":"1569565223","weight":4},{"id":"1569565663","weight":4},{"id":"1569566385","weight":8},{"id":"1569564635","weight":4},{"id":"1569565867","weight":4},{"id":"1569559617","weight":8},{"id":"1569566981","weight":12},{"id":"1569566321","weight":4},{"id":"1569566605","weight":4},{"id":"1569566683","weight":4},{"id":"1569566227","weight":4},{"id":"1569565551","weight":8},{"id":"1569556029","weight":4},{"id":"1569552245","weight":12},{"id":"1569565227","weight":16},{"id":"1569567005","weight":4},{"id":"1569566469","weight":8},{"id":"1569566081","weight":4},{"id":"1569565355","weight":16},{"id":"1569564469","weight":4},{"id":"1569565931","weight":4},{"id":"1569565461","weight":4},{"id":"1569564245","weight":4},{"id":"1569564227","weight":4},{"id":"1569565837","weight":4},{"id":"1569565317","weight":4},{"id":"1569565123","weight":8},{"id":"1569566941","weight":8},{"id":"1569558459","weight":4},{"id":"1569556713","weight":4},{"id":"1569566467","weight":4},{"id":"1569565771","weight":12},{"id":"1569560613","weight":8},{"id":"1569566999","weight":12},{"id":"1569564249","weight":4},{"id":"1569565809","weight":4},{"id":"1569566843","weight":4},{"id":"1569566579","weight":4},{"id":"1569558483","weight":4},{"id":"1569565455","weight":4},{"id":"1569566497","weight":8},{"id":"1569566795","weight":4},{"id":"1569566963","weight":4},{"id":"1569564989","weight":4},{"id":"1569565897","weight":4},{"id":"1569566895","weight":4},{"id":"1569566095","weight":4},{"id":"1569564337","weight":8},{"id":"1569566239","weight":4},{"id":"1569566167","weight":25},{"id":"1569566679","weight":8},{"id":"1569563981","weight":4},{"id":"1569559565","weight":4},{"id":"1569566733","weight":4},{"id":"1569566753","weight":4},{"id":"1569563307","weight":4},{"id":"1569558681","weight":4},{"id":"1569555999","weight":4},{"id":"1569566759","weight":4},{"id":"1569566657","weight":8},{"id":"1569565213","weight":4},{"id":"1569564611","weight":4},{"id":"1569566423","weight":4},{"id":"1569567015","weight":4},{"id":"1569566437","weight":12},{"id":"1569559111","weight":4},{"id":"1569565427","weight":4},{"id":"1569565915","weight":4},{"id":"1569552251","weight":8},{"id":"1569564441","weight":4},{"id":"1569566425","weight":4},{"id":"1569554881","weight":4},{"id":"1569554971","weight":12},{"id":"1569566209","weight":8},{"id":"1569562821","weight":4},{"id":"1569565559","weight":4},{"id":"1569566127","weight":4},{"id":"1569558985","weight":4},{"id":"1569565087","weight":4},{"id":"1569564857","weight":4},{"id":"1569564333","weight":4},{"id":"1569566913","weight":8},{"id":"1569566809","weight":4},{"id":"1569566629","weight":4},{"id":"1569565033","weight":4},{"id":"1569565055","weight":4},{"id":"1569555879","weight":4},{"id":"1569565219","weight":4},{"id":"1569566003","weight":4},{"id":"1569566223","weight":4},{"id":"1569566553","weight":4},{"id":"1569565357","weight":4},{"id":"1569566505","weight":4},{"id":"1569565393","weight":4},{"id":"1569562207","weight":4},{"id":"1569566191","weight":4},{"id":"1569567033","weight":4},{"id":"1569565527","weight":4},{"id":"1569566603","weight":4},{"id":"1569565311","weight":4},{"id":"1569566297","weight":4},{"id":"1569566245","weight":16},{"id":"1569560503","weight":4},{"id":"1569565463","weight":4},{"id":"1569565439","weight":8},{"id":"1569562551","weight":8},{"id":"1569563395","weight":4},{"id":"1569566901","weight":4},{"id":"1569551347","weight":4},{"id":"1569566383","weight":4},{"id":"1569565571","weight":4},{"id":"1569565885","weight":4},{"id":"1569564411","weight":4},{"id":"1569565665","weight":12},{"id":"1569557715","weight":4},{"id":"1569566983","weight":33},{"id":"1569565397","weight":4},{"id":"1569566873","weight":20},{"id":"1569565765","weight":4},{"id":"1569565435","weight":4},{"id":"1569565093","weight":4},{"id":"1569566887","weight":4},{"id":"1569566267","weight":4},{"id":"1569566737","weight":8},{"id":"1569566253","weight":4},{"id":"1569565353","weight":16},{"id":"1569564305","weight":4},{"id":"1569566651","weight":4},{"id":"1569566595","weight":4},{"id":"1569552025","weight":8},{"id":"1569565013","weight":8},{"id":"1569566715","weight":4},{"id":"1569566755","weight":50},{"id":"1569566813","weight":4},{"id":"1569566641","weight":12},{"id":"1569565425","weight":4},{"id":"1569564437","weight":4},{"id":"1569563975","weight":4},{"id":"1569551905","weight":8},{"id":"1569564861","weight":4},{"id":"1569565529","weight":20},{"id":"1569556759","weight":4},{"id":"1569566619","weight":8},{"id":"1569565271","weight":4},{"id":"1569561185","weight":12},{"id":"1569566397","weight":8},{"id":"1569558779","weight":4},{"id":"1569565669","weight":12},{"id":"1569566001","weight":16},{"id":"1569566817","weight":4},{"id":"1569564923","weight":4},{"id":"1569565805","weight":4},{"id":"1569563919","weight":8},{"id":"1569557851","weight":4},{"id":"1569567691","weight":25},{"id":"1569565389","weight":4},{"id":"1569559919","weight":4},{"id":"1569565861","weight":4},{"id":"1569562367","weight":4},{"id":"1569565997","weight":12},{"id":"1569559597","weight":4},{"id":"1569564961","weight":4},{"id":"1569565337","weight":4},{"id":"1569564253","weight":4},{"id":"1569565853","weight":4},{"id":"1569566341","weight":4},{"id":"1569565889","weight":4},{"id":"1569566635","weight":8},{"id":"1569566611","weight":12},{"id":"1569564505","weight":4},{"id":"1569565165","weight":4},{"id":"1569565707","weight":4},{"id":"1569565113","weight":4},{"id":"1569566375","weight":8},{"id":"1569564257","weight":8},{"id":"1569564509","weight":4},{"id":"1569551541","weight":4},{"id":"1569558697","weight":4},{"id":"1569566067","weight":8},{"id":"1569566825","weight":25},{"id":"1569564807","weight":4},{"id":"1569566727","weight":8},{"id":"1569565315","weight":4},{"id":"1569560581","weight":8}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T9.1","endtime":"10:10","authors":"Yihong Wu, Sergio Verdú","date":"1341395400000","papertitle":"Optimal Phase Transitions in Compressed Sensing with Noisy Measurements","starttime":"09:50","session":"S9.T9: Compressive Sensing and Phase Transitions","room":"Stratton West Lounge (201)","paperid":"1569566443"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
