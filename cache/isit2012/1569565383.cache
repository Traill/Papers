{"id":"1569565383","paper":{"title":{"text":"Pointwise Relations between Information and Estimation in Gaussian Noise"},"authors":[{"name":"Kartik Venkat"},{"name":"Tsachy Weissman"}],"abstr":{"text":"Abstract\u2014Many of the classical and recent relations between information and estimation in the presence of Gaussian noise can be viewed as identities between expectations of random quantities. These include the I-MMSE relationship of Guo et al.; the relative entropy and mismatched estimation relationship of Verd ´u; the relationship between causal estimation and mutual information of Duncan, and its extension to the presence of feedback by Kadota et al.; the relationship between causal and non-casual estimation of Guo et al., and its mismatched version of Weissman. We dispense with the expectations and explore the nature of the pointwise relations between the respective random quantities. The pointwise relations that we ﬁnd are as succinctly stated as - and give considerable insight into - the original expectation identities.\nAs an illustration of our results, consider Duncan\u2019s 1970 dis- covery that the mutual information is equal to the causal MMSE in the AWGN channel, which can equivalently be expressed saying that the difference between the input-output information density and half the causal estimation error is a zero mean random variable (regardless of the distribution of the channel input). We characterize this random variable explicitly, rather than merely its expectation. Classical estimation and information theoretic quantities emerge with new and surprising roles. For example, the variance of this random variable turns out to be given by the causal MMSE (which, in turn, is equal to the mutual information by Duncan\u2019s result)."},"body":{"text":"The literature abounds with results that relate classical quantities in information and estimation theory. Of particu- lar elegance are relations that have been established in the presence of additive Gaussian noise. In this work, we reﬁne and deepen our understanding of these relations by exploring their \u2018pointwise\u2019 properties.\nDuncan, in [1], showed that for the continuous-time additive white Gaussian noise channel, the minimum mean squared ﬁltering (causal estimation) error is twice the input-output mutual information for any underlying signal distribution. Another discovery was made by Guo et al. in [2], where the derivative of the mutual information was found to equal half the minimum mean squared error in non-causal estimation. By combining these results, the authors of [2] also establish the remarkable equality of the causal mean squared error (at some \u2018signal to noise\u2019 level snr) and the non-causal error averaged over \u2018signal to noise\u2019 ratio uniformly distributed between 0 and snr. There have been extensions of these results to the presence of mismatch. In this case, the relative entropy and\nthe difference of the mismatched and matched mean squared errors are bridged together: Mismatched estimation in the scalar Gaussian channel was considered by Verd´u in [3]. In [4], a generalization of Duncan\u2019s result to incorporate mismatch in the generality of continuous time processes is provided. In [5], Kadota et al. generalize Duncan\u2019s theorem to the presence of feedback. These, and similar relations between information and estimation are quite intriguing, and merit further study of their inner workings, which is the goal of this paper.\nThe basic information-estimation identities, such as the ones mentioned above, can be formulated as expectation identities. We explicitly characterize the random quantities involved in a pointwise sense, and in the process ﬁnd new connections between information and estimation for the Gaussian channel. Girsanov theory and Itˆo calculus provide us with tools to understand the pointwise behavior of these random quantities, and to explore their properties.\nThe paper is organized as follows. In Section II, we present and discuss our main results. In Section III, we present further results and observations for scalar random variables. We refer the reader to [6] for proofs of the stated results as well as a more elaborate discussion of their implications. We conclude in Section IV with a summary of our main ﬁndings.\n1) Matched Case: We begin by describing the problem setting. We are looking at mean square estimation in the presence of additive white Gaussian noise. This problem is characterized by an underlying clean signal X (which follows a law P X ) and its AWGN corrupted version Y γ measured at a given \u2018signal-to-noise ratio\u2019 γ.\nIn a communication setting, we are interested in the mutual information between the input X and the output Y γ . In an estimation setting, one would be interested in using the observed output to estimate the underlying input signal while minimizing a given loss function. Deﬁne mmse(γ) to be the minimum mean square error at \u2018signal-to-noise ratio\u2019 γ\nRecall now that we can express the mutual information be- tween two random variables as\nwhere the quantity in the brackets denotes the log Radon- Nikodym derivative of the measure induced by the conditional law of Y |X with respect to the measure induced by the law of Y . This quantity is referred to in some parts of the literature as the input-output information density i(X, Y ) (cf. [7]). In particular, let us look at the following additive Gaussian channel at \u2018signal-to-noise ratio\u2019 γ,\nfor γ ∈ [0, snr], where W · is a standard Brownian motion, independent of X.\nNow that (X, Y snr 0 ) are on the same probability space (where throughout Y snr 0 is shorthand for {Y γ , 0 ≤ γ ≤ snr}), it is meaningful to exchange the expectation and integration in the right hand side of (2) to yield the following equivalent representation of the I-MMSE result\nIn other words, the I-MMSE relationship can be restated succinctly as:\ndenotes the \u201ctracking error between the information density and half the squared error integrated over snr\u201d. But what can we say about the random variable Z itself, beyond the fact that it has zero mean? The answer lies in the following Proposition.\nProposition 1: Assume X has ﬁnite variance. Z, as deﬁned in (7), satisﬁes\nwhere the integral on the right hand side of (8) denotes the Itˆo integral with respect to W · .\nIn particular, the above characterization implies that Z is a martingale, and (by virtue of having zero expectation) directly implies the I-MMSE relationship in (6) (which is equivalent to (2)). Another immediate consequence of Proposition 1 is the following:\nThus we observe a remarkable and simple characterization of the second moment of the tracking error, in terms of classical estimation and information quantities. The relationship in (9) tells us how far apart the information density and the estimation error typically are, two quantities that we know to\nhave equal expectations. That the variance of their difference can be described directly in terms of the original estimation error is striking.\n2) Mismatched Case: In the scenario of mismatched scalar estimation, [3] presents a relationship between the relative entropy of the true and mismatched output laws, and the difference between the mismatched and matched estimation losses\nD(P ∗ N (0, 1/snr)||Q ∗ N (0, 1/snr)) = \t (10) 1 2\nTowards deriving a pointwise extension of (11), we note that it can be recast, assuming again the observation model in (4), as the expectation identity\nLet Z M denote the difference between the random quantities appearing in the above expression, i.e.\nIn the following, we provide an explicit characterization of this random variable.\nProposition 2: Assuming X has ﬁnite variance under both P and Q, Z M deﬁned in (14), satisﬁes\nWe observe that the above Itˆo integral is a martingale and consequently has zero expectation E[Z M ] = 0, recovering (13), i.e., Verd´u\u2019s relation from [3]. A further implication that can be read off of Proposition 2 rather immediately is the following:\nTheorem 2: Assuming X has ﬁnite variance under both P and Q, Z M deﬁned in (14), satisﬁes\nSimilarly as in the non-mismatched case, we observe that the variance of the difference between the information and estima- tion theoretic random variables whose expectations comprise the respective two sides of Verd´u\u2019s mismatch relationship has a distribution independent characterization in terms of the matched and mismatched estimation errors and, consequently, in terms of the relative entropy between the output distribu- tions. In the following subsection we extend this line of inquiry and results from the scalar case to that where the channel input is a continuous-time process.\nWe now turn to the continuous-time Gaussian channel. Let X T 0 be the underlying noise-free process (with ﬁnite power) to be estimated. The continuous time channel is characterized by the following relationship between the input and output processes,\nwhere {W t } t≥0 is a standard Brownian motion, independent of X T 0 .\n1) \u201cPointwise Duncan\u201d: In [1], Duncan proved the equality between input-output mutual information and the ﬁltering squared error, of a ﬁnite power continuous time signal X t , corrupted according to (18) to yield the noise corrupted process Y t . The signal is observed for a time duration [0, T ]. Denoting the time averaged ﬁltering squared error,\nand letting I(X T ; Y T ) denote the input-output mutual infor- mation, Duncan\u2019s theorem then states that,\nIn [5], Kadota et al. extend this result to communication over channels with feedback, and in the recent [8] this result is extended to more general scenarios involving the presence of feedback, and it is shown that (20) remains true in these more general cases upon replacing the mutual information on the left hand side with directed information. In [9], several properties of likelihood ratios and their relationships with estimation error are studied. We now proceed to describe a pointwise characterization of Duncan\u2019s theorem.\nWe now present an explicit formula for D(T ) in the following Proposition.\nNote that on the the right side of (23) is a stochastic integral with respect to the Brownian motion W · driving the noise in the channel. With this representation, Duncan\u2019s theorem follows from the mere fact that this stochastic integral is a martingale and, in particular, has zero expectation.\nOn applying another basic property of the stochastic integral we get the following characterization of the variance of D(T ).\nTheorem 3: For a continuous-time signal X T 0 with ﬁnite power, D(T ) as deﬁned in (21) satisﬁes\nIn conjunction with Duncan\u2019s theorem (20), we get the fol- lowing triangular relationship,\nwhich parallels our discovery for scalar random variables, in (9). Thus, we ﬁnd that the pointwise tracking error satisﬁes this intriguing distribution independent property, in the full generality of continuous time inputs for the Gaussian Channel. That the estimation error and mutual information emerge in the characterization of the variance of this pointwise object is striking.\n2) Pointwise Mismatch: We now consider the setting in [4], where a continuous time signal X t , distributed according to a law P is observed through additive Gaussian noise, and is estimated by an estimator that would have been optimal if the signal had followed the law Q. In this general setting , the main result in [4] shows that the relative entropy between the laws of the output for the two different underlying distributions (P and Q), is exactly half the difference between the mismatched and matched ﬁltering errors. Let Y t be the continuous time AWGN corrupted version of X t as given by (18). Let P Y T\nbe the output distributions when the underlying signal X T 0 has law P and Q, respectively. As before, T denotes the time duration for which the process is observed. We denote the mismatched causal mean squared error,\nIn this setting, [4] established that the relative entropy between the output distributions is half the difference between the mismatched and matched ﬁltering errors, i.e.\nDeﬁne the pointwise difference between the log Radon- Nikodym derivative and half the mismatched causal squared error difference,\nThe following Proposition explicitly characterizes M (T ): Proposition 4:\nwhere M (T ) is as deﬁned in (28), and X t is assumed to have ﬁnite power under the laws P and Q.\nWe note that relation (27) is implied immediately by Propo- sition 4 due to the \u2018zero mean\u2019 property of the martingale M (T ). Analogous to the mismatched scalar setting, we also have the following result.\nVar(M (T )) = cmse P,Q (T ) − cmse P,P (T ) (30) = 2D(P Y T\nThus, the variance of M (T ) is exactly the difference between the causal mismatched and matched squared errors. And further, from [4] we know that it is equal to twice the relative entropy between the output distributions according to laws P and Q. Thus, the remarkable equivalence between the estimation error and the variance of the pointwise tracking error emerges in this setting as well.\nRemark 1: One can note that for the DC signal X t ≡ X in the interval [0, T ], we can employ the continuous time results, to recover the corresponding scalar estimation results in Section II-A. A discussion on the relation between the ﬁltering error in continuous-time for a DC input, and the scalar MMSE can be found in [2, Section III.E].\n3) Extensions and Generalizations: The pointwise charac- terizations discussed above lead to an understanding of well known information-estimation results, which emerge as direct corollaries of our characterization of the tracking errors as stochastic integrals. Similar techniques can be applied to yield pointwise characterizations of other well known identities in the information-estimation arena. Examples include: the extension of Duncan\u2019s result for channels with feedback, the I-MMSE relationship for processes, as well as the relation between causal and anticausal squared error. We refer to [6] for a detailed presentation of these and related results.\nReturning to the scalar channel setting of Subsection II-A, we introduce notation as follows:\n1 2\nwhere Z, as in the previous section, is informally referred to as the \u201ctracking error\u201d.\nWe studied the example of the scalar Gaussian channel corrupted by additive Gaussian noise where the additive noise components for the different SNR levels were coupled via a standard Brownian motion, as in (4). We characterized explicitly, in Proposition 1, the tracking error Z between the information density and half the estimation error.\nWe shall now look at the pointwise scalar estimation problem in a new light. Recall that in moving from (2) to (5), we put all the random variables (X, Y snr 0 ) on the same probability space, via a standard Brownian motion, as in (4). Note, however, that the only assumption for the original results that hold in expectation is that, for each γ > 0,\nwhere N (µ, σ 2 ) denotes the Gaussian distribution with mean µ and variance σ 2 . Taking the channel noise variables for\nthe various SNR levels to be the components of a Brownian motion, as in (4), is but one possibility for a coupling that respects (35).\nAs mentioned, however, there are several ways in which we can couple the input X and outputs {Y snr 0 } together so that they satisfy (35). The I-MMSE relationship implies that for all such couplings we have E[Z] = 0. Before exploring some other possible couplings and their properties, let us note a reﬁnement of this zero-mean property pertaining to the random variable Z, which holds regardless of the coupling.\nProposition 5: Suppose X has ﬁnite variance and that Z is deﬁned as in Deﬁnition 1, under a joint distribution on (X, Y snr 0 ) satisfying (35). Then,\nThus, not only is the tracking error a zero-mean random variable, but even its conditional expectation E[Z|X] is zero.\nHaving brieﬂy touched upon the idea of ways other than the channel in (4), in which we can comply with the marginal channel requirements in (35), let us look at some concrete examples and draw a comparison between them.\nAn alternative coupling between X and Y γ that respects (35), is achieved by using a scaled standard Gaussian random variable as additive noise, instead of the Brownian motion considered in the previous setting. The channel is described by letting, for γ ∈ [0, snr],\nwhere N ∼ N (0, 1) is independent of X. A pointwise char- acterization of the \u2018tracking error\u2019 for this setting is presented in [6, Lemma 11].\nUnlike the previous two examples of Sections III-A and III-B, respectively, our third example looks at the limiting behavior of a family of couplings achieved by the following construction.\nLet ∆ > 0. Deﬁne ∆ = snr M for M natural. Let I i ≡ [(i − 1)∆, i∆) for i ∈ {1, 2, 3 . . . M }. Let N i be independent standard Gaussian random variables ∼ N (0, 1). Now we deﬁne the following process\nγ , \t (38) where N γ = N i for γ ∈ I i . Note that this is a coupling of the channel noise components at different SNR\u2019s that adheres to (35). Let Z ∆ be the \u2018tracking error\u2019 (34) for this process. We now consider the limiting process as ∆ → 0 + . We observe that, under mild regularity conditions on the distribution of X,\nFig. 1: Variance of Tracking Error Z vs. snr for the examples of sections III-A, III-B and III-C.\nPreviously, in III-A, III-B and III-C we have considered different couplings that are consistent with (35) and give rise to different pointwise relations between X and Y snr 0 . For the special case when X ∼ N (0, 1) we explicitly compute and plot the variances of the tracking errors in Fig. 1, for each of the three couplings. This comparison effectively tells us which particular relationship results in a better pointwise tracking of the information density and the actual squared error of the MMSE estimators. We observe that in this example of a Gaussian input, the coupling III-C results in the lowest variance of the tracking error, while that of III-B in the highest variance. We conjecture that to be the case in general, i.e., for any distribution of X with ﬁnite power.\nWe consider the scenario of mean square estimation of a signal observed through additive white Gaussian noise. We formulate classical information and estimation relationships in these contexts as expectation identities. We explicitly char- acterize the input-output information density for both scalar and continuous time Gaussian channels. Using this character- ization, which relies on Girsanov theory, we obtain pointwise representations of these identities with the expectations re- moved and discover that these random quantities themselves are intimately connected to the classical quantities of informa- tion and estimation. In particular, information and estimation appear to be bridged by the second moment of the pointwise tracking error between the information density and the scaled ﬁltering error. In this manner, we present pointwise relations for Duncan\u2019s theorem, mismatched estimation, channels with feedback, the I-MMSE relationship, as well as the causal vs. non-causal and causal vs. anticausal errors. A special treatment for scalar estimation is also provided where we present and discuss alternative ways of coupling the channel outputs across the different values of SNR.\nIn future work, we would like to explore higher order moments of the tracking error. In addition, it would be inter- esting to investigate whether pointwise relationships similar to those presented here, hold also for the Poissonian channel, where relations between estimation and information have been recently uncovered in [10] for a natural loss function.\nThe authors thank Rami Atar for valuable discussions. This work has been supported under a Stanford Graduate Fellow- ship, NSF grant CCF-0729195, and the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370."},"refs":[{"authors":[{"name":"T. E. Dunca"},{"name":"J. Appl"}],"title":{"text":"On the calculation of Mutual Information,\u201d SIAM  Math"}},{"authors":[{"name":"D. Gu"},{"name":"S. Shama"},{"name":"S. Verd´"}],"title":{"text":"Mutual Information and minimum mean-square error in Gaussian channels\u201d, IEEE Trans"}},{"authors":[{"name":"S. Verd´u"}],"title":{"text":"Mismatched Estimation and relative Entropy"}},{"authors":[{"name":"T. Weissman"}],"title":{"text":"The Relationship Between Causal and Noncausal Mis- matched Estimation in Continuous-Time AWGN Channels"}},{"authors":[{"name":"T. Kadota"},{"name":"M. Zakai"},{"name":"J. Ziv"}],"title":{"text":"Mutual Information of the White. Gaussian Channel With and Without Feedback"}},{"authors":[{"name":"K. Venkat"},{"name":"T. Weissman"}],"title":{"text":"Pointwise Relations between Information and Estimation in Gaussian Noise"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"H. V. Poor"},{"name":"S. Verd´u"}],"title":{"text":"New Channel Coding Achievability Bounds"}},{"authors":[{"name":"T. Weissman"},{"name":"Y.-H. Kim"},{"name":"H. H. Permuter"}],"title":{"text":"Directed Information, Causal Estimation, and Communication in Continuous Time"}},{"authors":[{"name":"M. Zakai"}],"title":{"text":"On Mutual Information, Likelihood Ratios, and Estimation Error for the Additive Gaussian Channel"}},{"authors":[{"name":"R. Atar"},{"name":"T. Weissman"}],"title":{"text":"Mutual Information, Relative Entropy, and Estimation in the Poisson Channel"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565383.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T8.1","endtime":"17:00","authors":"Kartik Venkat, Tsachy Weissman","date":"1341247200000","papertitle":"Pointwise Relations between Information and Estimation in Gaussian Noise","starttime":"16:40","session":"S4.T8: Information and Estimation","room":"Stratton (491)","paperid":"1569565383"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
