{"id":"1569566873","paper":{"title":{"text":"Minimum Complexity Pursuit: Stability Analysis"},"authors":[{"name":"Shirin Jalali"},{"name":"Arian Maleki"},{"name":"Richard Baraniuk"}],"abstr":{"text":"Abstract\u2014 A host of problems involve the recovery of struc- tured signals from a dimensionality reduced representation such as a random projection; examples include sparse signals (compressive sensing) and low-rank matrices (matrix comple- tion). Given the wide range of different recovery algorithms developed to date, it is natural to ask whether there exist \u201cuniversal\u201d algorithms for recovering \u201cstructured\u201d signals from their linear projections. We recently answered this question in the afﬁrmative in the noise-free setting. In this paper, we extend our results to the case of noisy measurements."},"body":{"text":"Data compressors are ubiquitous in the digital world. They are built based on the premise that text, images, videos, etc. all are highly structured objects, and hence exploiting those structures can dramatically reduce the number of bits required for their storage. In recent years, a parallel trend has been developing for sampling analog signals. There too, the idea is that many analog signals of interest have some kind of structure that enables considerably lowering the sampling rate from the Shannon-Nyquist rate.\nThe ﬁrst structure that was extensively studied in this context is sparsity. It has been observed that many nat- ural signals have sparse representations in some domain. The term compressed sensing (CS) refers to the process of undersampling a high-dimensional sparse signal through linear measurements and recovering it from those measure- ments using efﬁcient algorithms [1], [2]. Low-rankedness [3], model-based compressed sensing [4]\u2013[8], and ﬁnite rate of innovation [9] are examples of some other structures that have already been explored in the literature.\nWhile in the original source coding problem introduced by Shannon [10], the assumption was that the source distribution is known both to the encoder and to the decoder, and hence is used in the code design, it was later shown that this information is not essential. In fact, universal compression algorithms are able to code stationary ergodic processes at their entropy rates, without knowing the source distribution [11]. In other words, there exists a family of compression codes that are able to code any stationary ergodic process at its entropy rate asymptotically [11]. The same result holds for universal lossy compression.\nOne can ask similar questions for the problem of un- dersampling \u201cstructured\u201d signals: How to deﬁne the class of \u201cstructured\u201d signals? Are there sampling and recovery algorithms for the reconstruction of \u201cstructured\u201d signals from\ntheir linear measurements without having the knowledge of the underlying structure? Does this ignorance incur a cost in the sampling rate?\nIn algorithmic information theory, Kolmogorov complex- ity , introduced by Solomonoff [12], Kolmogorov [13], and Chaitin [14], deﬁnes a universal notion of complexity for ﬁnite-alphabet sequences. Given a ﬁnite-alphabet sequence x, the Kolmogorov complexity of x, K(x), is deﬁned as the length of the shortest computer program that prints x and halts. In [15], extending the notion of Kolmogorov complex- ity to real-valued signals 1 by their proper quantization, we addressed some of the above questions. We introduced the minimum complexity pursuit (MCP) algorithm for recover- ing \u201cstructured\u201d signals from their linear measurements. We showed that ﬁnding the \u201csimplest\u201d solution satisfying the linear measurements recovers the signal using many fewer measurements than its ambient dimension.\nIn this paper, we extend the results of [15] to the case where the measurements are noisy. We ﬁrst propose an updated version of MCP that takes into account that the measurements are a linear transformation of the signal plus Gaussian noise. We prove that the proposed algorithm is stable with respect to the noise and derive bounds on its reconstruction error in terms of the sampling rate and the variance of the noise.\nThe organization of this paper is as follows. Section II deﬁnes the notation used throughout the paper. Section II-B deﬁnes Kolmogorov information dimension of a real-valued signal. Section III formally deﬁnes the MCP algorithm and reviews and extends some of the related results proved in [15]. Section IV considers the case of noisy measurements and proves that MCP is stable. Section V mentions some of the related work in the literature, and Section VI concludes the paper. Appendix A presents two useful lemmas used in the proofs.\nCalligraphic letters such as A and B denote sets. For a set A, |A| and A c denote its size and its complement, respectively. For a sample space Ω and event set A ⊆ Ω,\n1 A denotes the indicator function of the event A. Bold- faced lower case letters denote vectors. For a vector x = (x 1 , x 2 , . . . , x n ) ∈ R n , its p and ∞ norms are deﬁned as\nx p p \t n i=1 |x i | p and x ∞ max i |x i |, respectively. For integer n, let I n denote the n × n identity matrix.\nFor x ∈ [0, 1], let ((x) 1 , (x) 2 , . . .), (x) i ∈ {0, 1}, denote the binary expansion of x, i.e., x = ∞ i=1 2 −i (x) i . The m-bit approximation of x, [x] m , is deﬁned as [x] m\n2 −i (x) i . Similarly, for a vector (x 1 , . . . , x n ) ∈ [0, 1] n , [x n ] m ([x 1 ] m , . . . , [x n ] m ).\nThe Kolmogorov complexity of a ﬁnite-alphabet sequence x with respect to a universal Turing machine U is deﬁned as the length of the shortest program on U that prints x and halts. 2 Let K(x) denote the Kolmogorov complexity of binary string x ∈ {0, 1} ∗ ∪ n≥1 {0, 1} n .\nDeﬁnition 1: For real-valued x = (x 1 , x 2 , . . . , x n ) ∈ [0, 1] n , deﬁne the Kolmogorov complexity of x at resolution m as\nDeﬁnition 2: The Kolmogorov information dimension of vector (x 1 , x 2 , . . . , x n ) ∈ [0, 1] n at resolution m is deﬁned as\nLemma 1: For (x 1 , x 2 , . . .) ∈ [0, 1] ∞ and any resolution sequence {m n }, we have\nTherefore, by Lemma 1, we call a signal compressible, if lim sup n→∞ n −1 κ m,n < 1. As stated in the following proposition, Lemma 1\u2019s upper bound on κ m,n is achievable.\nProposition 1: Let {X i } ∞ i=1 iid ∼ Unif[0, 1]. Then, 1\nConsider the problem of reconstructing a vector x n o ∈ [0, 1] n from d < n random linear measurements y d o = Ax n o . The MCP algorithm proposed in [15] reconstructs x n o from its linear measurements y d o by solving the following optimization problem:\nLet the elements of A ∈ R d×n , A ij , be i.i.d. N (0, 1). 3 Let ˆ x n o = ˆ x n o (y d o , A) denote the output of (1) to inputs y d o = Ax n o and A. Theorem 1 stated below is a generalization of Theorem 2 proved in [15].\nTheorem 1: Assume that x o = (x o,1 , x o,2 , . . .) ∈ [0, 1] ∞ . For integers m and n, let κ m,n denote the Kolmogorov information dimension of x n o at resolution m. Then, for any τ n < 1 and t > 0, we have\nTheorem 1 can be proved following the steps used in the proof of Theorem 2 in [15]. To interpret this theorem, in the following we consider several interesting corollaries that follow from Theorem 1. Note that in all of the results, the logarithms are to the base of Euler\u2019s number e.\nCorollary 1: Assume that x o = (x o,1 , x o,2 , . . .) ∈ [0, 1] ∞ and m = m n = log n . Let κ n κ m n ,n . Then if d n =\nκ n log n , for any > 0, we have P ( x n o − ˆ x n o 2 > ) → 0, as n → ∞.\nHence, ﬁxing t > 0 and setting τ n = τ = 0.1, for any > 0 and large enough values of n we have\nAccording to Corollary 1, if the complexity of the signal is less than κ, then the number of linear measurements needed for asymptotically perfect recovery is, roughly speaking, at the order of κ log n. In other words, the number of measure- ments is proportional to the complexity of the signal and only logarithmically proportional to its ambient dimension.\nCorollary 2: Assume that x o = (x o,1 , x o,2 , . . .) ∈ [0, 1] ∞ and m = m n = log n . Let κ n \t κ m n ,n . Then, if d = d n = 3κ n , we have\nProof: Setting τ n = n −0.5 , m = m n = log n , and d = d n = 3κ n in Theorem 1, it follows that\nSince 1.5 − 2 log 2 > 0, for any > 0 and n large enough, we have\nIn other words, if we are interested in the normalized mean square error, or per element squared distance, then 3κ n measurements are sufﬁcient.\nIn the previous section we considered the case where the signal is exactly of low complexity and the measurements are also noise-free. In this section, we extend the results to noisy measurements, where y d o = Ax n o + w d , with w d ∼ N (0, σ 2 I d ). Assuming that the complexity of the signal is known at the reconstruction stage, we consider the following reconstruction algorithm:\nNote that κ m,n m is an upper bound on the Kolmogorov complexity of x o at resolution m. The major issue of this section is to calculate the number of measurements required for robust recovery in noise.\nTheorem 2: Assume that x o = (x o,1 , x o,2 , . . .) ∈ [0, 1] ∞ . For integers m and n, let κ m,n denote the information dimension of x n o at resolution m. If m = m n = log n\nand d = 8rκ m,n m, where r > 1, then for any > 0, we have\nAccording Theorem 2, as long as d > 8rκ n log n the algorithm is stable in the sense that the reconstruction error is proportional to the variance of the input noise. By increasing the number of measurements one may reduce the reconstruction error.\nProof: Since by deﬁnition K [·] m (x n o ) = k m,n m n , x n o is also a feasible point in (4). But, by assumption, ˆ x n o is the solution of (4). Therefore,\nExpanding Aˆ x n o − y d o 2 2 = Aˆ x n o − Ax n o − w d 2 2 in (6), it follows that\nLet e n m x n o − [x n o ] m and ˆ e n m ˆ x n o − [ˆ x n o ] m denote the quantization errors of the original and the reconstructed sig- nals, respectively. Using these deﬁnitions, and the Cauchy- Schwartz inequality, we ﬁnd a lower bound for A(ˆ x n o −\n= A([ˆ x n o ] m − [x n o ] m ) + A(ˆ e n m − e n m ) 2 2 ≥ A([ˆ x n o ] m − [x n o ] m ) 2 2\n− 2 (ˆ e n m − e n m ) T A T A ([ˆ x n o ] m − [x n o ] m ) ≥ A([ˆ x n o ] m − [x n o ] m ) 2 2\nOn the other hand, again using our deﬁnitions plus the Cauchy-Schwartz inequality, we ﬁnd an upper bound on |(w d ) T A(ˆ x n o − x n o )| as\n(9) For any x ∈ [0, 1], 0 ≤ x − [x] m < 2 −m . Therefore,\nLet set S be the set of all vectors of length n that can be written as the difference of two vectors with complexity less than k m,n m; that is,\nFor any ﬁxed h n , Ah n is an i.i.d. zero-mean Gaussian vector of length d and variance h n 2 2 . Assuming that\nHence, by the union bound and the fact that |S| ≤ 2 2κ m,n m [11], we have\nDeﬁne the event E (n) 3 as E (n) 3 \t {∀ h n ∈ S : Ah n 2 2 > (1 − t 4 )d h n 2 2 }. By the union bound and Lemma 2, it follows that\nGiven w d , A T w d is an n dimensional i.i.d. Gaussian random vector with variance w d 2 2 . Hence, by Lemma 2,\nChoosing t 6 , t 7 , t 8 > 0 such that t 6 < t 7 and 1 + t 6 = (1 − t 8 )(1 + t 7 ), it follows that\nCombining (8) and (9) and the upper and lower bounds derived for the corresponding terms of (8) and (9), and choosing t 1 = 2σ d(1 + t 2 )(2κ m,n m), with probability P(E 1 ∩ E 2 ∩ E 3 ∩ E 4 ∩ E 5 ), the following inequality holds:\nn)) ∆ m 2 − 2 σ\nInequality (17) involves a quadratic equation of ∆ m 2 . Finding the roots of this quadratic equation, using\n1 + t 5 (1 − t 4 ) −1 , γ 3 =\n1 + t 6 (1−t 4 ) −1 . On the other hand, by the union bound,\nP ((E 1 ∩ E 2 ∩ E 3 ∩ E 4 ∩ E 5 ) c ) = P(E c 1 ∪ E c 2 ∪ E c 3 ∪ E c 4 ∪ E c 5 ) ≤ P(E c 1 ) + P(E c 2 ) + P(E c 3 ) + P(E c 4 ) + P(E c 5 ). \t (19)\nr and ﬁxing t 1 , t 3 , t 5 , . . . , t 8 at appropriate ﬁxed small numbers, (12), (13), (14), (15) and (16) guarantee that (19) goes to zero, as n → ∞. Moreover, for chosen parameters, γ 3 < √\nr −1 ). Finally, for any > 0, for n large enough, (γ 1\nd −1 γ 4 < . This concludes the proof.\nThe MCP algorithm proposed in [15] is mainly inspired by [18] and [19]. Consider the universal denoising problem where θ is corrupted by additive white Gaussian noise as X n = θ + Z n . The denoiser\u2019s goal is to recover θ from the noisy observation X n . The minimum Kolmogorov complexity estimation (MKCE) approach proposed in [18] suggests a denoiser that looks for the sequence ˆ θ with minimum Kolmogorov complexity among all the vectors that are within some distance of the observation vector X n . [18] shows that if θ i are i.i.d., then under certain conditions, the average marginal conditional distribution of ˆ θ i given X i tends to the actual posterior distribution of θ 1 given X 1 .\nIn [18], the authors consider the problem of recovering a low-complexity sequence from its linear measurements. Let S(k 0 ) {x n ∈ [0, 1] n : K(x n ) ≤ k 0 }. Consider measuring x n o ∈ S(k 0 ) using a d×n binary matrix A. Let y d o = Ax n o . To recover x n o from measurements y d o , [18] suggests ﬁnding ˆ x n as ˆ x n (y d o , A) arg min x n : y d\nK(x n ), and proves that if d ≥ 2k 0 , then this algorithm is able to ﬁnd x n o with high probability. Clearly assuming that a real-valued sequence has a low complexity is very restrictive, and hence S(k 0 ) does not include any of the classes that has been studied in CS literature. For instance most of the one sparse signals have inﬁnite Kolmogorov complexity, and hence the result of [18] does not imply useful information.\nIn a recent and independent work, [20] and [21] consider a scheme similar to MCP. For a stationary and ergodic source, they propose an algorithm to approximate MCP. While the empirical results are promising, no theoretical guarantees are provided on either the performance of MCP or their ﬁnal algorithm.\nThe notion of sparsity has already been generalized in the literature in several different directions [3], [4], [9], [22]. More recently, [22] introduced the class of simple functions and atomic norm as a framework that uniﬁes some\nof the above observations and extends them to some other signal classes. While all these models can be considered as subclasses of the general model considered in this paper, it is worth noting that even though the recovery approach proposed here is universal, given the incomputibility of Kolmogorov complexity, it is not useful for practical pur- poses. Finding practical algorithms with provable perfor- mance guarantees is left for future research.\nIn this paper, we have focused on deterministic signal models. For the case of random signals, [23] considers the problem of recovering a memoryless process from its under- sampled linear measurements and establishes a connection between the required number of measurements and the Renyi entropy of the source. Also, our work is in the same spirit as the minimum entropy decoder proposed by Csiszar in [24], which is a universal decoder, for reconstructing an i.i.d. signal from its linear measurements.\nIn this paper, we have considered the problem of recov- ering structured signals from their random linear measure- ments. We have investigated the minimum complexity pursuit (MCP) scheme. Our results conﬁrm that if the Kolmogorov complexity of the signal is upper bounded by κ, then MCP recovers the signal accurately from O(κ log n) random linear measurements, which is much smaller than the ambient dimension. In this paper, we have speciﬁcally proved that MCP is stable, such that the 2 -norm of the reconstruction error is proportional to the standard deviation of the noise.\nThe following two lemmas are frequently used in our proofs.\nLemma 2 ( χ-square concentration): Fix τ > 0, and let Z i ∼ N (0, 1), i = 1, 2, . . . , d. Then, P( d i=1 Z 2 i < d(1 −\nLemma 3: Let X n and Y n denote two independent Gaus- sian random vectors with i.i.d. elements. Further assume that for i = 1, . . . , n, X i ∼ N (0, 1) and Y i ∼ N (0, 1). Then the distribution of (X n ) T y n = n i=1 X i Y i is the same as the distribution of X n 2 G, where G ∼ N (0, 1) is independent of X n 2 .\nProof: We need to show that (X n ) T Y n / X n 2 is dis- tributed as N (0, 1) and is independent of X n 2 . To prove the ﬁrst claim, note that\nY i given X n / X n 2 = a n is independent of a n ,\nTo prove the independence, note that X n / X n 2 and Y n are both independent of X n 2 ."},"refs":[{"authors":[{"name":"D. L. Donoho"}],"title":{"text":"Compressed sensing"}},{"authors":[{"name":"E. Cand`e"},{"name":"J. Romber"},{"name":"T. Tao"}],"title":{"text":"Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency infor- mation"}},{"authors":[{"name":"B. Rech"},{"name":"M. Faze"},{"name":"P. A. Parrilo"}],"title":{"text":"Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization"}},{"authors":[{"name":"R. G. Baraniu"},{"name":"V. Cevhe"},{"name":"M. F. Duart"},{"name":"C. Hegde"}],"title":{"text":"Model-based compressive sensing"}},{"authors":[{"name":"Y. C. Elda"},{"name":"P. Kuppinge"},{"name":"H. Bolcskei"}],"title":{"text":"Block-sparse signals: Uncertainty relations and efﬁcient recovery"}},{"authors":[{"name":"M. Stojni"},{"name":"F. Parvares"},{"name":"B. Hassibi"}],"title":{"text":"On the reconstruction of block-sparse signals with an optimal number of measurements"}},{"authors":[{"name":"M. Stojnic"}],"title":{"text":"Block-length dependent thresholds in block-sparse com- pressed sensing"}},{"authors":[{"name":"D. Maliouto"},{"name":"M. Ceti"},{"name":"S. Willsky"}],"title":{"text":"A"}},{"authors":[{"name":"M. Vetterl"},{"name":"P. Marzilian"},{"name":"T. Blu"}],"title":{"text":"Sampling signals with ﬁnite rate of innovation"}},{"authors":[{"name":"C. E. Shannon"},{"name":"I. Bell Syst"}],"title":{"text":"A mathematical theory of communication: Parts I and I Tech"}},{"authors":[{"name":"T. Cove"},{"name":"J. Thomas"}],"title":{"text":"Elements of Information Theory"}},{"authors":[{"name":"R. J. Solomonoff"}],"title":{"text":"A formal theory of inductive inference"}},{"authors":[{"name":"A. N. Kolmogorov"}],"title":{"text":"Logical basis for information theory and proba- bility theory"}},{"authors":[{"name":"G. J. Chaitin"},{"name":"J. Assoc"}],"title":{"text":"On the length of program for computing binary sequences"}},{"authors":[{"name":"S. Jalal"},{"name":"A. Maleki"}],"title":{"text":"Minimum complexity pursuit"}},{"authors":[{"name":"L. Staiger"}],"title":{"text":"The Kolmogorov complexity of real numbers"}},{"authors":[{"name":"E. Cand`e"},{"name":"J. Romber"},{"name":"T. Tao"}],"title":{"text":"Decoding by linear programming"}},{"authors":[{"name":"D. L. Donoho"}],"title":{"text":"Kolmogorov sampler"}},{"authors":[{"name":"D. L. Donoh"},{"name":"H. Kakavan"},{"name":"J. Mammen"}],"title":{"text":"The simplest solution to an underdetermined system of linear equations"}},{"authors":[{"name":"D. Baro"},{"name":"M. Duarte"}],"title":{"text":"Universal MAP estimation in compressed sensing"}},{"authors":[{"name":"D. Baro"},{"name":"M. Duarte"}],"title":{"text":"Signal recovery in compressed sensing via universal priors"}},{"authors":[{"name":"V. Chandrasekara"},{"name":"B. Rech"},{"name":"P. A. Parril"},{"name":"A. Willsky"}],"title":{"text":"The convex geometry of linear inverse problems"}},{"authors":[{"name":"Y. W"},{"name":"S. Verd´u"}],"title":{"text":"Renyi information dimension: Fundamental limits of almost lossless analog compression"}},{"authors":[{"name":"I. Csiszar"}],"title":{"text":"Linear codes for sources and source networks: Error exponents, universal coding"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566873.pdf"},"links":[{"id":"1569565383","weight":2},{"id":"1569565223","weight":2},{"id":"1569565663","weight":2},{"id":"1569566385","weight":2},{"id":"1569564635","weight":2},{"id":"1569565867","weight":2},{"id":"1569566799","weight":2},{"id":"1569559665","weight":2},{"id":"1569559617","weight":6},{"id":"1569566981","weight":2},{"id":"1569566321","weight":6},{"id":"1569566683","weight":2},{"id":"1569566855","weight":2},{"id":"1569566227","weight":2},{"id":"1569566697","weight":2},{"id":"1569565551","weight":2},{"id":"1569566943","weight":2},{"id":"1569552245","weight":4},{"id":"1569565227","weight":8},{"id":"1569567005","weight":4},{"id":"1569566469","weight":4},{"id":"1569566081","weight":2},{"id":"1569565355","weight":4},{"id":"1569564469","weight":2},{"id":"1569565461","weight":2},{"id":"1569564245","weight":6},{"id":"1569564227","weight":2},{"id":"1569566119","weight":2},{"id":"1569565123","weight":6},{"id":"1569566941","weight":2},{"id":"1569556713","weight":4},{"id":"1569562685","weight":2},{"id":"1569566467","weight":2},{"id":"1569565771","weight":2},{"id":"1569566903","weight":2},{"id":"1569566999","weight":2},{"id":"1569566843","weight":2},{"id":"1569558483","weight":2},{"id":"1569556091","weight":2},{"id":"1569565347","weight":2},{"id":"1569565455","weight":2},{"id":"1569566497","weight":2},{"id":"1569566963","weight":2},{"id":"1569564989","weight":2},{"id":"1569565897","weight":2},{"id":"1569565953","weight":4},{"id":"1569567009","weight":2},{"id":"1569566095","weight":2},{"id":"1569564337","weight":2},{"id":"1569566167","weight":4},{"id":"1569559565","weight":2},{"id":"1569566753","weight":2},{"id":"1569566063","weight":2},{"id":"1569558681","weight":2},{"id":"1569559995","weight":2},{"id":"1569565213","weight":2},{"id":"1569566643","weight":2},{"id":"1569566531","weight":2},{"id":"1569567665","weight":2},{"id":"1569565667","weight":2},{"id":"1569567015","weight":2},{"id":"1569566437","weight":4},{"id":"1569565427","weight":2},{"id":"1569566403","weight":2},{"id":"1569565915","weight":4},{"id":"1569552251","weight":2},{"id":"1569554881","weight":2},{"id":"1569554971","weight":2},{"id":"1569566209","weight":2},{"id":"1569562821","weight":2},{"id":"1569565655","weight":2},{"id":"1569566913","weight":4},{"id":"1569566629","weight":2},{"id":"1569566447","weight":2},{"id":"1569566141","weight":2},{"id":"1569565055","weight":4},{"id":"1569565633","weight":2},{"id":"1569555879","weight":2},{"id":"1569565219","weight":2},{"id":"1569566037","weight":2},{"id":"1569565095","weight":2},{"id":"1569566553","weight":2},{"id":"1569566505","weight":4},{"id":"1569565393","weight":2},{"id":"1569562207","weight":2},{"id":"1569567033","weight":2},{"id":"1569566603","weight":2},{"id":"1569566051","weight":2},{"id":"1569565311","weight":2},{"id":"1569560997","weight":2},{"id":"1569566245","weight":4},{"id":"1569560503","weight":2},{"id":"1569565463","weight":2},{"id":"1569565439","weight":4},{"id":"1569562551","weight":4},{"id":"1569563395","weight":2},{"id":"1569566901","weight":2},{"id":"1569551347","weight":2},{"id":"1569566383","weight":2},{"id":"1569565571","weight":2},{"id":"1569565885","weight":2},{"id":"1569564411","weight":2},{"id":"1569565665","weight":2},{"id":"1569557715","weight":2},{"id":"1569566983","weight":2},{"id":"1569565397","weight":2},{"id":"1569565435","weight":2},{"id":"1569566129","weight":2},{"id":"1569566267","weight":2},{"id":"1569565511","weight":2},{"id":"1569566917","weight":2},{"id":"1569566253","weight":2},{"id":"1569565353","weight":6},{"id":"1569566595","weight":4},{"id":"1569552025","weight":4},{"id":"1569565013","weight":2},{"id":"1569566715","weight":2},{"id":"1569566639","weight":2},{"id":"1569566755","weight":8},{"id":"1569566813","weight":2},{"id":"1569566641","weight":2},{"id":"1569565425","weight":8},{"id":"1569563975","weight":2},{"id":"1569551905","weight":2},{"id":"1569565529","weight":10},{"id":"1569566619","weight":4},{"id":"1569565271","weight":2},{"id":"1569566397","weight":2},{"id":"1569566001","weight":2},{"id":"1569564769","weight":4},{"id":"1569565805","weight":2},{"id":"1569563919","weight":2},{"id":"1569557851","weight":2},{"id":"1569567691","weight":10},{"id":"1569565389","weight":2},{"id":"1569562367","weight":2},{"id":"1569566847","weight":2},{"id":"1569559597","weight":2},{"id":"1569567013","weight":2},{"id":"1569565337","weight":2},{"id":"1569565853","weight":2},{"id":"1569566341","weight":2},{"id":"1569565889","weight":2},{"id":"1569566635","weight":2},{"id":"1569566611","weight":4},{"id":"1569565165","weight":2},{"id":"1569565635","weight":2},{"id":"1569566125","weight":2},{"id":"1569565113","weight":2},{"id":"1569566375","weight":4},{"id":"1569565143","weight":2},{"id":"1569564257","weight":2},{"id":"1569566973","weight":2},{"id":"1569566449","weight":2},{"id":"1569564755","weight":2},{"id":"1569564509","weight":2},{"id":"1569565895","weight":2},{"id":"1569564419","weight":2},{"id":"1569566067","weight":4},{"id":"1569566825","weight":6},{"id":"1569566443","weight":10},{"id":"1569566417","weight":2},{"id":"1569560581","weight":2}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T9.2","endtime":"12:10","authors":"Shirin Jalali, Arian Maleki, Richard Baraniuk","date":"1341402600000","papertitle":"Minimum Complexity Pursuit: Stability Analysis","starttime":"11:50","session":"S10.T9: Compressive Sensing and Algorithms","room":"Stratton West Lounge (201)","paperid":"1569566873"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
