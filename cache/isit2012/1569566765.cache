{"id":"1569566765","paper":{"title":{"text":"Capacity Lower Bound of MIMO Channels with Output Quantization and Correlated Noise"},"authors":[{"name":"Amine Mezghani"},{"name":"Josef A. Nossek"}],"abstr":{"text":"Abstract\u2014 We investigate the effect of quantization on the performance of multiple-input multiple-output (MIMO) channels in terms of achievable communication rates. To this end, we derive a lower bound on the channel capacity of quantized MIMO channels based on the Bussgang decomposition. The work is of interest in the case when low resolution A/D-converters (ADCs) have to be used to enable higher sampling rate and to simplify the hardware. An essential aspect of our derivation is that we take into account possible noise correlation. The low signal-to- noise ratio (SNR) analysis of the special case of 1-bit quantization reveals that noise correlation might reduce the capacity loss due to quantization when compared to the uncorrelated noise case, where it is well known that the capacity decreases by the factor 2/𝜋 after the 1-bit quantization at low SNR."},"body":{"text":"Index Terms: Quantization, MIMO channel, Noise Corre- lation, Capacity Lower Bound, Low SNR analysis.\nIn future multiple-input multiple-output (MIMO) commu- nication systems, low power and low cost are becoming key requirements. Among other things, it is desirable to reduce the analog-to-digital converter (ADC) resolution in order to save power and chip area [1]. In fact, in high speed systems the sampling/conversion power may reach values in the order of the processing power. Therefore, coarse ADCs, acting as low- resolution scalar quantizers, may be a cost-effective solution for such applications, especially when the array size becomes very large or when the sampling rate becomes very high (in the GHz range) [2]. Unfortunately, most of the contributions on MIMO communications assume that the receiver has access to the channel data with inﬁnite precision. In [3], [4], the effects of 1-bit quantization are studied from an information theoretical point of view for MIMO systems, where the channel is perfectly known at the receiver and the noise is uncorrelated. It turns out that the well known reduction of low SNR channel capacity by factor 2/𝜋 due to 1-bit quantization holds also for the general MIMO case with uncorrelated noise. In [5], the non-coherent MIMO channel was studied in detail and a similar conclusion has been made. On the other hand, [6] and [7] showed that the channel capacity loss due to the 1-bit quantization of the single-input single-output (SISO) AWGN channel can be reduced at low SNR by using a 1-bit asymmetric quantizer or by oversampling, while [8] studied the SISO capacity problem with multi-bit quantization.\nIn this work, we provided a lower bound on the channel capacity based on the Bussgang decomposition [9] of the quantized output for the general case of correlated noise. Although this simple method, which neglects the deterministic\nstructure of the quantization process, usually leads to a loose lower bound especially in the high SNR regime, it provides a good approximation in the low SNR regime. In fact many previous results (such as the 2/𝜋 result) can be re-proved in a substantially easier and more elegant way. Moreover, we extensively handle the 1-bit case with noise correlation and show that depending on the correlation structure of the noise, the low SNR capacity loss can be higher or smaller than 2/𝜋 compared to the unquantized case. Surprisingly, we observed that even for simple SIMO channels with 1-bit output, the mutual information achieved by QPSK can be exceeded.\nOur paper is organized as follows. Section II describes the general system model. In Sections III and IV, an (approximate) lower bound of the mutual information with Gaussian input is derived based on the Bussgang decomposition. Finally, we handle the 1-bit symmetric quantizer case in more detail in Section V.\nNotation: Vectors and matrices are denoted by lower and upper case italic bold letters. The operators (∙) T , (∙) H and tr (∙) stand for transpose, Hermitian transpose and trace of a matrix, respectively. 1 𝑀 denote the ( 𝑀 × 𝑀) identity matrix. 𝑎 𝑖 denotes the 𝑖-th element of the vector 𝒂 and 𝑎 𝑖,𝑐 with 𝑐 ∈ {𝑅, 𝐼} is the real or imaginary part of 𝑎 𝑖 . The operator E [∙] stands for expectation with respect to the random variables. Finally, diag (𝑨) denotes a diagonal matrix containing only the diagonal elements of 𝑨 and nondiag(𝑨) = 𝑨 − diag(𝑨).\nWe start from a general system model of a MIMO channel with 𝑀 transmit antennas and 𝑁 receive antennas shown in Fig. 1 and described by\n𝒓 = 𝑄(𝒚), with \t (1) 𝒚 = 𝑯𝒙 + 𝜼, \t (2)\nwhere 𝒚 is the unquantized receive vector of dimension 𝑁, 𝑯 ∈ ℂ 𝑁×𝑀 is the channel matrix and 𝒙 is the unknown data vector, while 𝜼 is an i.i.d. Gaussian noise with covariance ma- trix 𝑹 𝜂𝜂 . The operator 𝑄(⋅) represents the scalar quantization process, where the real parts 𝑦 𝑖,𝑅 and the imaginary parts 𝑦 𝑖,𝐼 of each signal component 𝑦 𝑖 is mapped to a quantized value from a ﬁnite set of code words as follows\n𝑟 𝑖,𝑐 = 𝑄(𝑦 𝑖,𝑐 ), if 𝑦 𝑖,𝑐 ∈ [𝑟 lo 𝑖,𝑐 , 𝑟 up 𝑖,𝑐 ), 𝑐 ∈ {𝑅, 𝐼}, 1 ≤ 𝑖 ≤ 𝑁. (3)\nThereby, 𝑟 lo 𝑖,𝑐 and 𝑟 up 𝑖,𝑐 are the lower value and the upper limits associated to the quantized value 𝑟 𝑖,𝑐 . The restriction to scalar\nquantization is motivated by the fact that vector quantization is rather uncommon for ADC implementations.\nOur goal is to derive a lower bound on the achievable rates, where we assume for analytical tractability a Gaussian input distribution. Evidently, this distribution is not necessarily the capacity achieving distribution and just provides a lower bound for it. The analysis that follows considers the case where perfect channel state ( 𝑯 and 𝑹 𝜂𝜂 ) is available at the receiver, which is principally possible even with coarse quantization [10]. However, the presented framework can be applied independently of the kind of channel state information at the transmitter.\nThe Bussgang theorem implies that one can decompose the output of the nonlinear quantizer 𝒓 = 𝑄(𝒚) into a desired signal component and an uncorrelated distortion 𝒆\nwhere 𝑭 can be obtained from the linear MMSE estimation of 𝒓 from 𝒚\nand the distortion error 𝒆 has the following correlation matrix 𝑹 𝑒𝑒 = E[(𝒓 − 𝑭 𝒚)(𝒓 − 𝑭 𝒚) H ]\nBased on this decomposition, the channel output 𝒓 can be written as function of the channel input in the following form\n= 𝑭 𝑯𝒙 + 𝑭 𝜼 + 𝒆 = 𝑯 \u2032 𝒙 + 𝜼 \u2032 ,\nwhere we introduced the effective channel 𝑯 \u2032 = 𝑭 𝑯\nand the non-Gaussian effective noise 𝜼 \u2032 with the covariance matrix\nNext, we introduce a new MIMO Gaussian channel that is described by the same effective channel matrix 𝑯 \u2032 and the same effective noise covariance matrix\nbut differs from the original channel by the fact, that the noise vector 𝜼 𝐺 is Gaussian distributed with the same covariance matrix 𝑹 𝜂 \u2032 𝜂 \u2032 = E[𝜼 𝐺 𝜼 H 𝐺 ]. In [11], it was shown that, for a given noise covariance matrix (known at the receiver), the Gaussian distributed noise minimizes the mutual information, which leads to the following lower bound\n𝐼(𝒙; 𝒓) ≥ 𝐼(𝒙; 𝒓 𝐺 ). \t (11) 𝐼(𝒙; 𝒓 𝐺 ) corresponds to the mutual information of the MIMO Gaussian channel, that reads as\nwhen the channel input 𝒙 is Gaussian distributed with the covariance matrix 𝑹 𝑥𝑥 . Again, 𝑯 \u2032 and 𝑹 𝜂 \u2032 𝜂 \u2032 are given in (8) and (9) respectively.\nNow, the main difﬁculty consists of deriving the covariance matrices 𝑹 𝑟𝑟 and 𝑹 𝑟𝑦 assuming a Gaussian input. For the 1-bit quantizer, these matrices can be found in a closed form (see Section V). However, for general scalar quantizers, we get an approximate evaluation, which will be described in the following section.\nEach quantization process can be given a distortion factor 𝜌 (𝑖,𝑐) 𝑞 to indicate the relative amount of quantization noise generated, which is deﬁned as follows\nwhere 𝑞 𝑖,𝑐 = 𝑟 𝑖,𝑐 −𝑦 𝑖,𝑐 is the quantization error and 𝑟 𝑦 𝑖,𝑐 𝑦 𝑖,𝑐 = E[𝑦 2 𝑖,𝑐 ] is the variance of 𝑦 𝑖,𝑐 . The distortion factor 𝜌 (𝑖,𝑐) 𝑞\ndepends on the quantizer characteristics and the probability density function of 𝑦 𝑖,𝑐 . Note that the signal-to-quantization noise ratio (SQR) has an inverse relationship with regard to the distortion factor\nFor a symmetric input probability density function and a symmetric quantizer, we can assume without loss of generality that the following properties holds for all 0 ≤ 𝑖 ≤ 𝑁 𝑐 ∈ {𝑅, 𝐼}: 1\nE[𝑞 𝑖,𝑐 ] = 0 \t (15) E[𝑟 𝑖,𝑐 𝑞 𝑖,𝑐 ] = 0 \t (16)\nwhich are for instance valid for a uniform or non-uniform quantizer minimizing the mean square error (distortion) be- tween the input 𝑦 𝑖,𝑐 and the output 𝑟 𝑖,𝑐 . Obviously, Eq. (17) follows from Eqs (13) and (16). Assuming now that the channel input is Gaussian then the quantizer input signals 𝑦 𝑖,𝑐 are Gaussian distributed and thus, they undergo the same distortion factor 𝜌 𝑞 , i.e., 𝜌 (𝑖,𝑐) 𝑞 = 𝜌 𝑞 ∀𝑖∀𝑐. Furthermore, the optimal parameters of the uniform as well as the non-uniform quantizer and the resulting distortion factor 𝜌 𝑞 for Gaussian distributed signals are tabulated in [12] for different bit res- olutions 𝑏. Recent research work on the optimal quantization of a Gaussian source can be found in [13], [14], [15]. Now, let 𝑞 𝑖 = 𝑞 𝑖,𝑅 + 𝑗𝑞 𝑖,𝐼 be the complex quantization error. Under the assumption of uncorrelated real and imaginary part of 𝑦 𝑖 , we obtain: \t 𝑟\nFor the uniform quantizer case, it was shown in [15], that the optimal quantization step Δ for a Gaussian source decreases as\nOn the other hand, the optimal non-uniform quantizer achieves, under high-resolution assumption, approximately the following distortion [16]\n2 2 −2𝑏 . \t (19) Based on these considerations, we aim at approximating the required correlation matrices 𝑹 𝑦𝑟 and 𝑹 𝑟𝑟 based on the scalar 𝜌 𝑞 . In fact, we have\nWe now derive all required covariance matrices by using the fact that the quantization error 𝑞 𝑖 , conditioned on 𝑦 𝑖 , is statistically independent of all other random variables of the system.\nFirst we calculate 𝑟 𝑦 𝑖 𝑞 𝑗 = E[𝑦 𝑖 𝑞 ∗ 𝑗 ] for 𝑖 ∕= 𝑗: E[𝑦 𝑖 𝑞 ∗ 𝑗 ] = E 𝑦 𝑗 [ E[𝑦 𝑖 𝑞 ∗ 𝑗 ∣𝑦 𝑗 ] ]\n= E 𝑦 𝑗 [ E[𝑦 𝑖 ∣𝑦 𝑗 ]E[𝑞 ∗ 𝑗 ∣𝑦 𝑗 ] ] = E 𝑦 𝑗\nIn (22), the Bayesian estimator E[𝑦 𝑖 ∣𝑦 𝑗 ] corresponds to the linear estimator 𝑟 𝑦 𝑖 𝑦 𝑗 𝑟 −1 𝑦 𝑗 𝑦 𝑗 𝑦 𝑗 since the vector 𝒚 is jointly Gaussian distributed. Eq. (23) follows from (18).\nIn a similar way, we evaluate 𝑟 𝑞 𝑖 𝑞 𝑗 for 𝑖 ∕= 𝑗: E[𝑞 𝑖 𝑞 ∗ 𝑗 ] = E 𝑦 𝑗 [ E[𝑞 𝑖 𝑞 ∗ 𝑗 ∣𝑦 𝑗 ] ]\nwhere we used Eq. (24) and (18) and we approximated the Bayesian estimator E[𝑞 𝑖 ∣𝑦 𝑗 ] with the linear estimator. From (26) and (18) we deduce the covariance matrix of the quantization error:\n= 𝜌 𝑞 𝑹 𝑦𝑦 − (1 − 𝜌 𝑞 )𝜌 𝑞 nondiag (𝑹 𝑦𝑦 ). (27) Inserting the expressions (24) and (27) into Eq. (21), we obtain:\n= (1 − 𝜌 𝑞 ) ((1 − 𝜌 𝑞 )𝑹 𝑦𝑦 + 𝜌 𝑞 diag (𝑹 𝑦𝑦 )) . (28) In summary, using (25), the effective channel from (8) be- comes\nwhile the effective noise covariance matrix from (9) can be obtained by means of (25) and (28)\n= (1 − 𝜌 𝑞 ) ((1 − 𝜌 𝑞 )𝑹 𝜂𝜂 + 𝜌 𝑞 diag (𝑹 𝑦𝑦 )) . (30) Finally, we obtain the approximate lower bound on the mutual information as 2\nlog 2 1+(1−𝜌 𝑞 ) ((1−𝜌 𝑞 )𝑹 𝜂𝜂 +𝜌 𝑞 diag (𝑹 𝑦𝑦 )) −1 𝑯𝑹 𝑥𝑥 𝑯 H , (31)\nWith these result, we can study the asymptotic of the mutual information at low and high SNR. In fact, for the low SNR regime we have\nand thus we get a ﬁrst order approximate lower bound 𝐼(𝒙; 𝒓 𝐺 )∣ low SNR ≈\nOn the other hand, at the high SNR regime, the approximate lower bound converges to the value\nIn this section, we deal with the special case of symmetric 1-bit quantization, where, fortunately, the required correlation matrices can be obtained in an exact way. In fact, it is known from the classical arcsine law [17] that the output of a decision device 𝑟 𝑖,𝑐 = sign[𝑦 𝑖,𝑐 ] ∈ {−1, 1} has the following correlation matrix\nwhere the arcsine function is applied element-wise to its matrix argument. Additionally, the correlation matrix between the input and the output of the 1-bit quantizer can be obtained as [17]\n𝜋 diag (𝑹 𝑦𝑦 ) − 1 2 𝑹 𝑦𝑦 . \t (37) Then, we get the effective channel from (8) as\n𝜋 diag (𝑹 𝑦𝑦 ) − 1 2 𝑯, \t (38) while the effective noise covariance in (9) becomes\n𝑹 𝜂 \u2032 𝜂 \u2032 = 2 𝜋 [ arcsin ( diag (𝑹 𝑦𝑦 ) − 1 2 𝑹 𝑦𝑦 diag (𝑹 𝑦𝑦 ) − 1 2 )] − 2 𝜋 diag (𝑹 𝑦𝑦 ) − 1 2 𝑹 𝑦𝑦 diag (𝑹 𝑦𝑦 ) − 1 2 + 2\nFinally, the lower bound on the mutual information of the 1- bit quantized MIMO channel with Gaussian input is computed from (12).\nAgain, we can consider the low SNR regime where 𝑹 𝑦𝑦 ≈ 𝑹 𝜂𝜂 to obtain the following asymptotic lower bound of the mutual information\n𝑯𝑹 𝑥𝑥 𝑯 H diag (𝑹 𝜂𝜂 ) − 1 2 [\n) .\n(40) For the case that the noise covariance matrix is diagonal, this ﬁrst order asymptotic of the mutual information simpliﬁes to\nwhich coincides with the result that has been shown in [3], stating that the MIMO achievable rate under 1-bit quantization reduces by the factor 2/𝜋 at the low SNR regime and therefore the derived lower bound (41) is asymptotically tight, when the noise is uncorrelated.\nWe now consider a simple example, where interesting effects of noise correlation and quantization can be observed. It consists of a 1×2 single-input multiple-output (SIMO) channel with the following channel vector\n[ 1 1\n[ 1 𝑟 𝑟 1\nparametrized by the noise correlation factor 𝑟 fulﬁlling ∣𝑟∣ ≤ 1. Further, the power of the scalar input is denoted by 𝑃 = E[∣𝑥∣ 2 ]. For the case 𝑟 = −1 (the two noise compo- nents are oppositely correlated), and under 1-bit quantization, the obtained lower bound on the mutual information with Gaussian input is shown in Fig. 2 as function of the input power. Interestingly, the lower bound exhibits a non-monotonic behavior with respect to the signal power (i.e. the SNR), which leads to the conclusion that noise might be favorable for the mutual information of quantized output channel. For comparison, the achievable rate of the same channel with QPSK input is depicted. It can be shown that this corresponds to the capacity of two parallel erasure channels with an erasure probability of erfc(\nWe can observe that for certain range of the input power, the Gaussian symbol alphabet can achieve higher mutual information than the QPSK schemes despite of the 1-bit output quantization. Other numerical experiments shows that this interesting phenomenon usually occurs in quantized MIMO channels, where the number of outputs exceed the number of inputs.\nNext, Fig. 3 shows, for the same channel, the ratio of the low SNR mutual information with 1-bit output to the one without quantization as function of the noise correlation coefﬁcient 𝑟. Thereby, we make use of the asymptotic value of the mutual information from (41) as well as the approximation given in (34) with 𝜌 𝑞 = 1 − 2/𝜋. First of all, the delivered approx- imation seems to be accurate when the correlation factor is not near to 1. Then, for 𝑟 > 0 the rate ratio is obviously higher that the value 2/𝜋 and reaches its maximum around 𝑟 = 0.7, whereas for 𝑟 < 0 the loss in terms of achievable information rate becomes more pronounced. Consequently, if the signal subspace 𝒉𝒉 H and the noise subspace 𝑹 𝜂𝜂 exhibits a certain similarity then the channel capacity reduction due to quantization is quite small. However, if they are nearly orthogonal then the loss becomes substantial.\nWe studied the mutual information of MIMO channels with output quantization and correlated noise. Based on the Bussgang decomposition, a lower bound on the achievable rate has been derived, which is useful when analyzing the low SNR behavior of the mutual information. We dealt also with the 1-bit quantizer case and showed that the reduction of the mutual information becomes smaller than 2/𝜋 under certain conditions for the channel matrix and the noise covariance matrix. Additionally, we observed that the additive noise might, at certain level, be favorable when the observation are quantized, since the lower bound on the mutual information curves is not necessary monotonic with the SNR. Although,\nwe focused on the single user scenario, similar analysis can be carried out for the multi-user broadcast channel."},"refs":[{"authors":[{"name":"R. Schreier"},{"name":"G. C. Temes"}],"title":{"text":"Understanding Delta-Sigma Data Converters"}},{"authors":[{"name":"D. D. Wentzloff"},{"name":"R. Bl´azquez"},{"name":"F. S. Lee"},{"name":"B. P. Ginsburg"},{"name":"J. Powell"},{"name":"A. P. Chandrakasan"}],"title":{"text":"System design considerations for ultra-wideband communication"}},{"authors":[{"name":"A. Mezghani"},{"name":"J. A. Nossek"}],"title":{"text":"On Ultra-Wideband MIMO Systems with 1-bit Quantized Outputs: Performance Analysis and Input Opti- mization"}},{"authors":[{"name":"J. A. Nossek"},{"name":"M. T. Ivrlaˇc"}],"title":{"text":"Capacity and coding for quantized MIMO systems"}},{"authors":[{"name":"A. Mezghani"},{"name":"J. A. Nossek"}],"title":{"text":"Analysis of 1-bit Output Noncoherent Fading Channels in the Low SNR Regime"}},{"authors":[{"name":"T. Koch"},{"name":"A. Lapidoth"}],"title":{"text":"Asymmetric Quantizers are Better At Low SNR"}},{"authors":[{"name":"T. Koch"},{"name":"A. Lapidoth"}],"title":{"text":"Increased Capacity per Unit-Cost by Oversampling"}},{"authors":[{"name":"J. Singh"},{"name":"O. Dabeer"},{"name":"U. Madhow"}],"title":{"text":"On the limits of communication with low-precision analog-to-digital conversion at the receiver"}},{"authors":[{"name":"J. J. Bussgang"}],"title":{"text":"Crosscorrelation functions of amplitude-distorted Gaussian signals"}},{"authors":[{"name":"A. Mezghani"},{"name":"F. Antreich"},{"name":"J. A. Nossek"}],"title":{"text":"Multiple Parameter Estimation With Quantized Channel Output"}},{"authors":[{"name":"S. N. Diggavi"},{"name":"T. M. Cover"}],"title":{"text":"Worst additive noise under covariance constraints"}},{"authors":[{"name":"J. Max"}],"title":{"text":"Quantizing for Minimum Distortion"}},{"authors":[{"name":"N. Al-Dhahir"},{"name":"J. M. Ciofﬁ"}],"title":{"text":"On the Uniform ADC Bit Precision and Clip Level Computation for a Gaussian Signal"}},{"authors":[{"name":"L. Neuhoff"}],"title":{"text":"On the Support of MSE-Optimal, Fixed-Rate, Scalar Quantizers"}},{"authors":[{"name":"D. Hui"},{"name":"D. L. Neuhoff"}],"title":{"text":"Asymptotic Analysis of Optimal Fixed-Rate Uniform Scalar Quantization"}},{"authors":[{"name":"A. Gersh"},{"name":"R. M. Gra"}],"title":{"text":"Vector Quantization and Signal Compression, Kluwer Academic Publishers, Dordrecht, Niederlande, ﬁrst edition, 1992"}},{"authors":[{"name":"A. Papouli"}],"title":{"text":"Probability, random variables, and stochastic processes, McGraw-Hill, fourth edition, 2002"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, John Wiley and Son, New York, 1991"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566765.pdf"},"links":[{"id":"1569566381","weight":9},{"id":"1569565223","weight":3},{"id":"1569566725","weight":6},{"id":"1569565663","weight":9},{"id":"1569565377","weight":6},{"id":"1569566385","weight":3},{"id":"1569565867","weight":6},{"id":"1569564669","weight":3},{"id":"1569559259","weight":3},{"id":"1569565607","weight":3},{"id":"1569565355","weight":9},{"id":"1569551535","weight":6},{"id":"1569565461","weight":6},{"id":"1569564227","weight":6},{"id":"1569558325","weight":3},{"id":"1569566303","weight":15},{"id":"1569564233","weight":3},{"id":"1569563411","weight":3},{"id":"1569559541","weight":3},{"id":"1569556713","weight":3},{"id":"1569560613","weight":6},{"id":"1569566903","weight":3},{"id":"1569566999","weight":6},{"id":"1569566579","weight":3},{"id":"1569558483","weight":6},{"id":"1569565455","weight":6},{"id":"1569566963","weight":6},{"id":"1569566709","weight":12},{"id":"1569564989","weight":3},{"id":"1569563981","weight":6},{"id":"1569566905","weight":6},{"id":"1569565213","weight":3},{"id":"1569566511","weight":3},{"id":"1569561143","weight":6},{"id":"1569565833","weight":3},{"id":"1569566325","weight":3},{"id":"1569566851","weight":6},{"id":"1569553909","weight":6},{"id":"1569566687","weight":6},{"id":"1569566939","weight":3},{"id":"1569558985","weight":12},{"id":"1569566473","weight":6},{"id":"1569566257","weight":3},{"id":"1569565033","weight":6},{"id":"1569566447","weight":3},{"id":"1569565887","weight":3},{"id":"1569566721","weight":9},{"id":"1569555879","weight":6},{"id":"1569565219","weight":6},{"id":"1569556671","weight":6},{"id":"1569566223","weight":6},{"id":"1569565029","weight":3},{"id":"1569566191","weight":6},{"id":"1569565527","weight":3},{"id":"1569566051","weight":3},{"id":"1569565467","weight":3},{"id":"1569566655","weight":3},{"id":"1569565311","weight":3},{"id":"1569560349","weight":3},{"id":"1569566387","weight":3},{"id":"1569560503","weight":3},{"id":"1569566229","weight":3},{"id":"1569566133","weight":6},{"id":"1569563395","weight":9},{"id":"1569551347","weight":6},{"id":"1569555367","weight":3},{"id":"1569566383","weight":3},{"id":"1569565885","weight":3},{"id":"1569565549","weight":6},{"id":"1569565611","weight":6},{"id":"1569566479","weight":3},{"id":"1569565397","weight":9},{"id":"1569565765","weight":6},{"id":"1569565435","weight":6},{"id":"1569566129","weight":3},{"id":"1569565385","weight":3},{"id":"1569565661","weight":6},{"id":"1569561221","weight":3},{"id":"1569566253","weight":6},{"id":"1569566691","weight":3},{"id":"1569566823","weight":3},{"id":"1569565013","weight":3},{"id":"1569566237","weight":9},{"id":"1569566283","weight":3},{"id":"1569565375","weight":3},{"id":"1569566771","weight":6},{"id":"1569564247","weight":3},{"id":"1569551905","weight":6},{"id":"1569556759","weight":12},{"id":"1569561185","weight":3},{"id":"1569565669","weight":6},{"id":"1569563721","weight":9},{"id":"1569560235","weight":3},{"id":"1569564923","weight":6},{"id":"1569566299","weight":9},{"id":"1569564769","weight":9},{"id":"1569566933","weight":6},{"id":"1569563919","weight":3},{"id":"1569557851","weight":6},{"id":"1569565389","weight":9},{"id":"1569559251","weight":3},{"id":"1569567013","weight":3},{"id":"1569566583","weight":3},{"id":"1569560459","weight":6},{"id":"1569565853","weight":6},{"id":"1569565889","weight":3},{"id":"1569564505","weight":6},{"id":"1569565165","weight":6},{"id":"1569565635","weight":3},{"id":"1569564931","weight":3},{"id":"1569564141","weight":6},{"id":"1569566987","weight":3},{"id":"1569551751","weight":3},{"id":"1569564419","weight":3},{"id":"1569564807","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T3.5","endtime":"13:10","authors":"Amine Mezghani, Josef A. Nossek","date":"1341406200000","papertitle":"Capacity Lower Bound of MIMO Channels with Output Quantization and Correlated Noise","starttime":"12:50","session":"S10.T3: MIMO Capacity","room":"Stratton S. de P. Rico (202)","paperid":"1569566765"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
