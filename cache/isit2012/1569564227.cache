{"id":"1569564227","paper":{"title":{"text":"A General Formula of Rate-Distortion Functions for Source Coding with Side Information at Many Decoders"},"authors":[{"name":"Tetsunao Matsuta ∗"},{"name":"Tomohiko Uyematsu \u2020"}],"abstr":{"text":"Abstract\u2014Heegard and Berger introduced the model of lossy source coding in which side information is available at many decoders. For this model, their showed an upper bound of the rate-distortion function in the case where the source is stationary memoryless. In this paper, we extend their model to the case where the source may be nonstationary and/or nonergodic, and clarify the rate-distortion function for this model. This result is based on the information-spectrum method introduced by Han and Verd ´ u. We also show some special cases of the rate- distortion function, and a single-letterized upper bound of the rate-distortion function in the case where the source is stationary memoryless."},"body":{"text":"In multi-terminal information theory, various models of source coding have been considered and analyzed by many researchers [1], [2]. One of the most important models is Wyner-Ziv source coding introduced by Wyner and Ziv [3]. This is a model of lossy source coding in the case where side information is available at the decoder. For Wyner-Ziv source coding, Wyner and Ziv clariﬁed the rate-distortion function for stationary memoryless sources, where the rate-distortion function is the inﬁmum of rates of codes in which the distortion between the source sequence and the reproduction sequence satisﬁes a certain ﬁdelity criterion. Later, Heegard and Berger [4] extended Wyner-Ziv source coding to the case where there are many decoders and side information is available at decoders. This model is called source coding with side information at many decoders. This is illustrated in Fig.1. In this model, there are one encoder and many decoders.\nEncoder encodes a source sequence, and sends a codeword to decoders. Each decoders independently reproduces the source sequence with a certain distortion from the codeword and\nside information. For this model, Heegard and Berger [4] showed the single-letterized upper bound of the rate-distortion function in the case where the source is stationary memoryless. Although their bound is optimal for some cases, it is unknown whether the bound is optimal in general. Hence, the problem of clarifying the rate-distortion function is still open.\nRecently, in order to deal with general sources and channels which are not required to satisfy ergodicity and stationarity, Han and Verd´u developed information-spectrum methods (see [5], [6]). By using these methods, Iwata and Muramatsu [7] generalized the rate-distortion theory for Wyner-Ziv source coding to the case where the source may be nonstationary and/or nonergodic. They showed the general formula of the rate-distortion function for Wyner-Ziv source coding with the maximum distortion criterion. In this paper, we extend their re- sult to source coding with side information at many decoders, and clarify the rate-distortion function for this source coding with the maximum distortion criterion. The proof is based on information-spectrum methods, and Iwata\u2019s and Muramatsu\u2019s methods used in [7]. The reason that we can clarify the rate- distortion function for this open problem is due to the use of multi-letterized formula. We also show some special cases of the rate-distortion function, and a single-letterized upper bound of the rate-distortion function in the case where source is stationary memoryless. Our bound differs from the bound derived by Heegard and Berger. However, the bound is optimal in a certain case.\nIn this section, we provide a precise deﬁnition of source coding with side information at many decoders.\nWe will denote random variables (RVs) by capital letters X ,Y, Z, ···, the values they can take by lowercase letters x, y, z, ···, and the set of these values by calligraphic letters X ,Y ,Z ,···. The probability distribution of a RV X taking values in X will be denoted by P X . In a similar manner, the probability distribution of a pair of RVs (X ,Y ) taking values in X × Y will be denoted by P XY . The conditional distribution for X given Y will be denoted by P X |Y . In what follows, all logarithms and exponentials are taken to the base of natural logarithm.\nLet X , Y 1 , Y 2 , ˆ X 1 and ˆ X 2 be arbitrary ﬁnite sets. Let (X, Y 1 , Y 2 ) denote a correlated general source [5] which is characterized by {(X n ,Y n 1 ,Y n 2 ) } ∞ n=1 , where (X n ,Y n 1 ,Y n 2 ) denotes a triple of RVs with the joint probability distribution P X n Y n 1 Y n 2\nover X n ×Y n 1 ×Y n 2 . When a source is stationary memoryless, we simply express a source (X, Y 1 , Y 2 ) as (X ,Y 1 ,Y 2 ), where (X ,Y 1 ,Y 2 ) denotes a RV with the joint probability distribu- tion P XY 1 Y 2 over X × Y 1 × Y 2 . X = {X n } ∞ n=1 represents the source sequence encoded by the encoder. Y 1 = {Y n 1 } ∞ n=1 and Y 2 = {Y n 2 } ∞ n=1 represent side information which are available at decoder 1 and decoder 2, respectively. Then, we can deﬁne the encoder as\nf n : X n → M n = {1,2,··· ,M n } and deﬁne the rate R n of the encoder as\nR n 1 n\nlog M n . Decoder k can be deﬁned as\nFor each decoder, the distortion between the source sequence X n and the reproduction sequence ˆ X n k = ϕ (k) n (Y n k , f n (X n )) is measured by a distortion measure\nThen, we call (R, D 1 , D 2 ) is f m-achievable when there exists a sequence of codes {( f n , ϕ (1) n , ϕ (2) n ) } such that\nwhere \u201cp- lim sup\u201d is a notion deﬁned in the following deﬁni- tion [5].\nDeﬁnition 1 (Limit superior and inferior in probability). For an arbitrary sequence of real-valued RVs {Z n } ∞ n=1 , we deﬁne\nWe now deﬁne f m-rate distortion function as R f m (D 1 , D 2 |X,Y 1 , Y 2 )\nOur purposed is to clarify the f m-rate distortion function. To this end, we introduce following notations [5].\nDeﬁnition 2. For a given sequence of RVs (X, Y) = {(X n ,Y n ) } ∞ n=1 , we deﬁne\n1 n\nIn this section, we show a general formula of f m-rate distortion function, and show some special cases.\nTheorem 1. For a correlated source (X, Y 1 , Y 2 ), D 1 ≥ 0 and D 2 ≥ 0,\nwhere S is a set of sequences of RVs Z = {Z n } ∞ n=1 satisfying the following conditions:\n2) For some functions ψ (k) n : Y n k × Z n → ˆ X n k , D(X, ˆ X k ) ≤ D k ∀ k = 1, 2,\nSource coding with side information at many decoders includes some special cases. One of special cases is source coding when side information may be absent [4], [8]. This is a model of lossy source coding in which the encoder encodes a sequence of source X = {X n } ∞ n=1 in preparation for the absence of side information Y = {Y n } ∞ n=1 . Then, the decoder reproduces the source sequence with distortion D 1 when side information is present, and reproduces the source sequence with distortion D 2 when side information is absent. Just as in Section II, we can deﬁne the f m-rate distortion function R HBK f m (D 1 , D 2 |X,Y) for this source coding with obvious modiﬁcation. Then, according to Theorem 1, we have the next corollary.\nCorollary 1. For a correlated source (X, Y), D 1 ≥ 0 and D 2 ≥ 0,\nwhere S HBK is a set of sequences of RVs Z = {Z n } ∞ n=1 satisfying the following conditions:\n2) For some functions ψ (1) n : Y n × Z n → ˆ X n 1 and ψ (2) n : Z n → ˆ X n 2 ,\nˆ X 1 = { ψ (1) n (Y n , Z n ) } ∞ n=1 , ˆ X 2 = { ψ (2) n (Z n ) } ∞ n=1 and d (k) n : X n × ˆ X n k → [0,∞) is a distortion measure.\nProof: Source coding when side information may be absent can be seen as a special case of source coding with side information at many decoders via the following associations: Y 1 = Y and Y 2 = constant. Then, by applying Theorem 1, we have\nAnother special case is Wyner-Ziv source coding. In Wyner- Ziv source coding, an encoder encodes a sequence of source X = {X n } ∞ n=1 and sends a codeword to a decoder. The decoder\nreproduces the source sequence with distortion D by using side information Y = {Y n } ∞ n=1 . Then, by deﬁning the f m-rate distortion function R WZ f m (D |X,Y) for Wyner-Ziv source coding just as in Section II, we have the following corollary.\nCorollary 2 (Theorem 1 in [7]). For a correlated source (X, Y) and D ≥ 0,\nwhere S WZ is a set of sequences of RVs Z = {Z n } ∞ n=1 satisfying the following conditions:\n2) For some functions ψ n : Y n × Z n → ˆ X n , D(X, ˆ X) ≤ D,\nd n (X n , ˆ X n ), \t ˆ X = { ψ n (Y n , Z n ) } ∞ n=1 and d n : X n × ˆ X n → [0,∞) is a distortion measure.\nProof: For source coding with side information at many decoders, we consider the following associations: Y 1 = Y, Y 2 = X, d (1) n = d n , D 1 = D and D 2 = ∞. Then, the f m- rate distortion function for this case is equal to the f m-rate distortion function for Wyner-Ziv source coding. Thus, by applying Theorem 1, we have\nwhere the last equality comes form the data processing the- orem [6, Theorem 9]. Hence, by noting that S = S WZ , we have the corollary.\nIn this section, we prove Theorem 1. Since the proof of the converse part is similar to that of Theorem 1 in [7], we only show the direct part.\nLet us ﬁx Z and ψ (k) n which satisfy conditions in Theorem 1. For an arbitrarily constant γ > 0, we deﬁne φ n : X n ×Y n 1 × Y n 2 × Z n → {0,1} as\n0, else. where\nOn the other hand, we have Pr {(X n ,Y n k , Z n ) / ∈ A (k) n } → 0 and Pr {(Y n k , Z n ) / ∈ B (k) n } → 0 as n → ∞. Thus, we have Pr {(X n ,Y n k , Z n ) / ∈ T (k) n } = 0 as n → ∞. This implies that Pr { φ n (X n ,Y n 1 ,Y n 2 , Z n ) = 1 } → 0 as n → ∞. According to this fact, i.e., Pr { φ n (X n ,Y n 1 ,Y n 2 , Z n ) = 1 } → 0 as n → ∞, we can show the existence of a sequence {g n } ∞ n=1 of functions g n : X n → {z i } ˜ M n i=1 ⊆ Z n such that\nPr { φ n (X n ,Y n 1 ,Y n 2 , g n (X n )) = 1 } = 0. \t (3) The existence of this function can be proved by using the next lemma.\nLemma 1 (Lemma 1 in [7]). Let U n , V n and W n be RV which take values in ﬁnite sets U n , V n and W n , respectively, and satisfy a Markov condition U n ↔ V n ↔ W n for each n. Now, let { φ n } ∞ n=1 be a sequence of arbitrary functions such that\nAccording to this lemma, by letting U = X ×Y 1 ×Y 2 , V = X , and W = Z , we can show the existence of a sequence {g n } ∞ n=1 of functions g n : X n → {z i } ˜ M n i=1 ⊆ Z n satisfying (2) and (3).\nNow, we construct an encoder and decoders. These are constructed by using the above function g n as follows:\n˜ C n {g n (x n ) ∈ Z n : x n ∈ X n }, and set M n as\nThen, we make M n bins, and randomly and independently assign all z n ∈ ˜ C n into M n bins according to the uniform probability distribution over {1,2,··· ,M n }. We denote by b n : Z n → {1,2··· ,M n } the bin function, where b n (z n ) denotes the index of the bin to which z n belongs.\nEncoding: Observing a source sequence x n ∈ X n , encoder sends the index b n (g n (x n )) as the codeword of x n . Hence, the encoder is deﬁned as\nDecoding: Observing side information y n k and the codeword f n (x n ), decoder k (k = 1, 2) ﬁnds a sequence z n ∈ ˜ C n such that b n (z n ) = f n (x n ) and (y n k , z n ) ∈ B (k) . Then, decoder k declares\nIf there is more than one such z n or there is no such z n , decoder k declares ϕ n (y n k , f n (x n )) = ˆ x n k , where ˆ x n k ∈ ˆ X n k is a certain ﬁxed sequence.\nWe consider sequences (x n , y n 1 , y n 2 ) satisfying the following conditions:\n1) There is no ˜z n ∈ ˜ C n such that ˜z n = g n (x n ), (y n 1 , ˜z n ) ∈ B (1) n and b n (˜z n ) = b n (g n (x n )).\n2) There is no ˜z n ∈ ˜ C n such that ˜z n = g n (x n ), (y n 2 , ˜z n ) ∈ B (2) n and b n (˜z n ) = b n (g n (x n )).\nThen, g n (x n ) is a unique element in ˜ C n such that b n (g n (x n )) = f n (x n ) and (y n k , g n (x n )) ∈ B (k) n , ∀ k = 1, 2. This is because that g n (x n ) satisﬁes (y n k , g n (x n )) ∈ B (k) n from condition 3), and there is no ˜z n ∈ ˜ C n except for g n (x n ) such that (y n k , ˜z n ) ∈ B (k) n\nand b n (˜z n ) = f n (x n ) from condition 1) and 2). Thus, when (x n , y n 1 , y n 2 ) satisﬁes the above conditions, decoders declare\nϕ n (y n k , f n (x n )) = ψ (k) n (y n k , g n (x n )), ∀ k = 1, 2. Then, we have\nd (k) n (x n , ϕ (k) n (y n k , f n (x n ))) ≤ D(X, ˆX k ) + γ , ∀ k = 1, 2, where S 1 (b n ), S 2 (b n ) and S 3 is deﬁned by\n(x n , y n 1 , y n 2 ) satisﬁes the condition k) }, k = 1,2, S 3 {(x n , y n 1 , y n 2 ) ∈ X n × Y n 1 × Y n 2 :\nand hence S k (b n ) depend on b n . We deﬁne P (n) e (b n ) as P (n) e (b n ) Pr\nγ , ∃ k ∈ {1,2} } ,\nwhere the code ( f n , ϕ (1) n , ϕ (2) n ) is deﬁned by the bin function b n as the above code construction. We consider the average probability of P (n) e (b n ) with respect to the random bin selec- tion, i.e.,\nwhere the RV B n denotes the random bin function. Then, according to properties of sequences (x n , y n 1 , y n 2 ) in S 1 (b n ) ∩ S 2 (b n ) ∩ S 3 , we have\nwhere [ ·] c denotes the complement of a set. [ S k (B n )] c can be described as\nPr {(X n ,Y n 1 ,Y n 2 ) ∈ [S k (B n )] c } → 0, n → ∞, ∀ k = 1, 2. On the other hand, we have\nPr {(X n ,Y n 1 ,Y n 2 ) ∈ [S 3 ] c } = Pr{ φ (X n ,Y n 1 ,Y n 2 , g n (X n )) = 1 }. Since Pr { φ (X n ,Y n 1 ,Y n 2 , g n (X n )) = 1 } = 0 as n → ∞, we have\nHence, there exists a sequence of bin functions {b n } ∞ n=1 such that P (n) e (b n ) → 0 as n → ∞, i.e., there exists a sequence of codes {( f n , ϕ (1) n , ϕ (2) n ) } ∞ n=1 such that\nγ , ∃ k ∈ {1,2} } → 0, n → ∞.\nThis implies that this sequence of codes also satisﬁes Pr\nThus, for any ﬁxed γ > 0, the sequence of codes {( f n , ϕ (1) n , ϕ (2) n ) } ∞ n=1 satisﬁes\n≤ D k + γ , ∀ k = 1, 2. Furthermore, it trivially holds that\nNow, we choose a positive sequence { γ k } ∞ k=1 satisfying γ 1 > γ 2 > ··· > 0 and γ k → 0 as n → ∞, and repeat the same argument in the order of γ = γ 1 , γ = γ 2 , ···. Then, by using the diagonal method [5], we can show the existence of a sequence of codes {( f n , ϕ (1) n , ϕ (2) n ) } ∞ n=1 such that\nfor a given Z ∈ S . Therefore, we conclude that R f m (D 1 , D 2 |X,Y 1 , Y 2 ) ≤ inf\nIn this section, we show an upper bound of the rate- distortion function in the case where a source is stationary memoryless.\n1 n\nwhere d (k) : X × ˆ X k → [0,∞) is a bounded function. Then, we have the following theorem.\nTheorem 2. For a stationary memoryless source (X ,Y 1 ,Y 2 ), D 1 ≥ 0 and D 2 ≥ 0, the rate-distortion function is upper bounded as\nwhere S ml is a set of RVs Z satisfying the following condi- tions:\n2) For some functions ψ (k) : Y k × Z → ˆ X k , E[d (k) (X , ˆ X k )] ≤ D k ∀ k = 1, 2,\nRemark 1. In this theorem, we consider the following as- sociations: Y 1 = Y , Y 2 = X , d (k) = d, D 1 = D and D 2 = ∞, where (X ,Y ) is an RV, d : X × ˆ X → [0,∞) is a bounded function and D is a positive constant. Then, in the similar way to the proof of Corollary 2, we obtain the upper bound of the rate-distortion function of Wyner-Ziv source coding for the stationary memoryless source (X ,Y ) as\nwhere S WZ ml is a set of RVs Z satisfying the following conditions:\n2) For some functions ψ : Y × Z → ˆ X , E[d(X , ˆ X )] ≤ D,\nAccording to Theorem 1 in [3], this upper bound is optimal. Proof: For a stationary memoryless source (X ,Y 1 ,Y 2 ), we\nconsider a set of sequences of RVs Z = {Z n } ∞ n=1 satisfying the following conditions:\n1) {(X i ,Y 1i ,Y 2i , Z i ) } n i=1 are i.i.d. according to a probability distribution P XY 1 Y 2 Z , ∀ n = 1, 2, ···, where the marginal distribution P XY 1 Y 2 is equal to the distribution of the source (X ,Y 1 ,Y 2 ).\n3) For some functions ψ (k) : Y k × Z → ˆ X k , D(X, ˆ X k ) ≤ D k ∀ k = 1, 2,\nˆ X k = { ψ (k) n (Y n k , Z n ) } ∞ n=1 \t and ψ (k) n (Y n k , Z n ) = ( ψ (k) (Y k1 , Z 1 ), ··· , ψ (k) (Y kn , Z n )).\nWe denote this set by S . Since the set S is a subset of the set S in Theorem 1, we have\nAccording to the condition 1) of the set S , we have I(X; Z) = I(X ; Z),\nwhere (X ,Y 1 ,Y 2 , Z) is the RV with the probability distribution P XY 1 Y 2 Z . Hence, according to (4), we have\nwhere (a) comes from the fact that (Y 1 ,Y 2 ) ↔ X ↔ Z. On the other hand, in the condition 3), we have\nwhere ˆ X k = ψ (k) (Y k , Z). This implies that, for any Z ∈ S , there exists a ¯ Z ∈ S ml such that\nConversely, for any ¯ Z ∈ S ml , there exists a Z ∈ S satisfying (6). Thus, we have"},"refs":[{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"},{"name":"J. Wile"},{"name":"W. InterScienc"}],"title":{"text":"Elements of Information Theory"}},{"authors":[{"name":"A. Gamal"},{"name":"Y. Kim"}],"title":{"text":"Lecture notes on network information theory"}},{"authors":[{"name":"A. Wyner"},{"name":"J. Ziv"}],"title":{"text":"The rate-distortion function for source coding with side information at the decoder"}},{"authors":[{"name":"C. Heegard"},{"name":"T. Berger"}],"title":{"text":"Rate distortion when side information may be absent"}},{"authors":[{"name":"T. S. Ha"}],"title":{"text":"Information-Spectrum Methods in Information Theory"}},{"authors":[{"name":"S. Verd´u"},{"name":"T. S. Han"}],"title":{"text":"A general formula for channel capacity"}},{"authors":[{"name":"K. Iwata"},{"name":"J. Muramatsu"}],"title":{"text":"An information-spectrum approach to rate- distortion function with side information"}},{"authors":[{"name":"A. Kaspi"}],"title":{"text":"Rate-distortion function when side-information may be present at the decoder"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564227.pdf"},"links":[{"id":"1569566381","weight":11},{"id":"1569565383","weight":11},{"id":"1569565223","weight":5},{"id":"1569565663","weight":11},{"id":"1569565377","weight":11},{"id":"1569566385","weight":5},{"id":"1569567049","weight":5},{"id":"1569564635","weight":11},{"id":"1569565067","weight":5},{"id":"1569559617","weight":5},{"id":"1569566981","weight":11},{"id":"1569566683","weight":16},{"id":"1569566227","weight":5},{"id":"1569559259","weight":5},{"id":"1569566597","weight":5},{"id":"1569566943","weight":11},{"id":"1569552245","weight":5},{"id":"1569564481","weight":5},{"id":"1569560833","weight":5},{"id":"1569566415","weight":5},{"id":"1569566469","weight":5},{"id":"1569565355","weight":16},{"id":"1569566765","weight":11},{"id":"1569565461","weight":16},{"id":"1569564245","weight":5},{"id":"1569566303","weight":11},{"id":"1569564233","weight":5},{"id":"1569565123","weight":11},{"id":"1569564203","weight":5},{"id":"1569565771","weight":5},{"id":"1569560613","weight":11},{"id":"1569566999","weight":16},{"id":"1569564249","weight":5},{"id":"1569558483","weight":16},{"id":"1569566089","weight":5},{"id":"1569566963","weight":16},{"id":"1569566709","weight":16},{"id":"1569564989","weight":11},{"id":"1569566523","weight":11},{"id":"1569565897","weight":11},{"id":"1569551763","weight":5},{"id":"1569566865","weight":5},{"id":"1569566095","weight":5},{"id":"1569563981","weight":5},{"id":"1569566905","weight":11},{"id":"1569566753","weight":5},{"id":"1569558681","weight":11},{"id":"1569565841","weight":5},{"id":"1569561143","weight":16},{"id":"1569564611","weight":5},{"id":"1569565667","weight":5},{"id":"1569561795","weight":11},{"id":"1569566423","weight":11},{"id":"1569566437","weight":5},{"id":"1569566851","weight":5},{"id":"1569553909","weight":11},{"id":"1569566687","weight":11},{"id":"1569565427","weight":5},{"id":"1569552251","weight":11},{"id":"1569553519","weight":5},{"id":"1569554971","weight":5},{"id":"1569566209","weight":5},{"id":"1569562821","weight":5},{"id":"1569565655","weight":5},{"id":"1569558985","weight":11},{"id":"1569566473","weight":5},{"id":"1569566629","weight":5},{"id":"1569565033","weight":5},{"id":"1569566721","weight":11},{"id":"1569565633","weight":11},{"id":"1569555879","weight":22},{"id":"1569565219","weight":11},{"id":"1569556671","weight":11},{"id":"1569566223","weight":11},{"id":"1569566593","weight":5},{"id":"1569566043","weight":5},{"id":"1569565029","weight":5},{"id":"1569565357","weight":5},{"id":"1569566505","weight":11},{"id":"1569565393","weight":16},{"id":"1569562207","weight":5},{"id":"1569566191","weight":11},{"id":"1569567033","weight":5},{"id":"1569566051","weight":5},{"id":"1569566673","weight":11},{"id":"1569566233","weight":11},{"id":"1569566317","weight":5},{"id":"1569565463","weight":11},{"id":"1569566229","weight":5},{"id":"1569566133","weight":11},{"id":"1569562551","weight":16},{"id":"1569563395","weight":16},{"id":"1569551347","weight":16},{"id":"1569565415","weight":5},{"id":"1569555367","weight":5},{"id":"1569565571","weight":5},{"id":"1569565549","weight":11},{"id":"1569565611","weight":11},{"id":"1569566983","weight":5},{"id":"1569565397","weight":22},{"id":"1569566873","weight":5},{"id":"1569565765","weight":16},{"id":"1569565435","weight":11},{"id":"1569565919","weight":5},{"id":"1569566267","weight":11},{"id":"1569564131","weight":11},{"id":"1569566035","weight":5},{"id":"1569566253","weight":11},{"id":"1569566547","weight":5},{"id":"1569565013","weight":5},{"id":"1569566237","weight":11},{"id":"1569565375","weight":5},{"id":"1569565293","weight":5},{"id":"1569566771","weight":11},{"id":"1569566641","weight":5},{"id":"1569564247","weight":5},{"id":"1569551905","weight":22},{"id":"1569565529","weight":5},{"id":"1569556759","weight":11},{"id":"1569566619","weight":11},{"id":"1569561185","weight":5},{"id":"1569558779","weight":5},{"id":"1569565669","weight":16},{"id":"1569563721","weight":11},{"id":"1569566817","weight":5},{"id":"1569566911","weight":5},{"id":"1569564923","weight":11},{"id":"1569566299","weight":5},{"id":"1569564769","weight":11},{"id":"1569565805","weight":5},{"id":"1569566933","weight":11},{"id":"1569563919","weight":5},{"id":"1569557851","weight":5},{"id":"1569565389","weight":11},{"id":"1569559597","weight":5},{"id":"1569559251","weight":5},{"id":"1569565337","weight":5},{"id":"1569565737","weight":5},{"id":"1569560459","weight":11},{"id":"1569565853","weight":11},{"id":"1569550425","weight":11},{"id":"1569566341","weight":5},{"id":"1569565889","weight":5},{"id":"1569563725","weight":5},{"id":"1569564505","weight":11},{"id":"1569565165","weight":11},{"id":"1569565635","weight":5},{"id":"1569566375","weight":5},{"id":"1569564257","weight":5},{"id":"1569564141","weight":16},{"id":"1569566987","weight":5},{"id":"1569551541","weight":5},{"id":"1569564419","weight":5},{"id":"1569566067","weight":16},{"id":"1569566825","weight":5},{"id":"1569566443","weight":5}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S2.T1.1","endtime":"11:50","authors":"Tetsunao Matsuta, Tomohiko Uyematsu","date":"1341228600000","papertitle":"A General Formula of Rate-Distortion Functions for Source Coding with Side Information at Many Decoders","starttime":"11:30","session":"S2.T1: Network Source Coding with Side Information","room":"Kresge Rehearsal B (030)","paperid":"1569564227"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
