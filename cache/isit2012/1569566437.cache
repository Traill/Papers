{"id":"1569566437","paper":{"title":{"text":"Lossy joint source-channel coding in the ﬁnite blocklength regime"},"authors":[{"name":"Victoria Kostina"},{"name":"Sergio Verd´u"}],"abstr":{"text":"Abstract\u2014This paper shows new tight ﬁnite-blocklength bounds for the best achievable lossy joint source-channel code rate, and demonstrates that joint source-channel code design brings considerable performance advantage over a separate one in the non-asymptotic regime. A joint source-channel code maps a block of k source symbols onto a length−n channel codeword, and the ﬁdelity of reproduction at the receiver end is measured by the probability that the distortion exceeds a given threshold d . For memoryless sources and channels, it is demonstrated that the parameters of the best joint source-channel code must satisfy nC − kR (d) ≈ pnV + kV(d)Q −1 ( ), where C and V are the channel capacity and dispersion, respectively; R(d) and V(d) are the source rate-distortion and rate-dispersion functions; and Q is the standard Gaussian complementary cdf.\nIndex Terms\u2014Achievability, converse, ﬁnite blocklength regime, joint source-channel coding, lossy source coding, memo- ryless sources, rate-distortion theory, Shannon theory."},"body":{"text":"For a large class of sources and channels, in the limit of large blocklength, the maximum achievable joint source- channel coding (JSCC) rate compatible with vanishing excess distortion probability is characterized by the ratio C R (d) [1], attainable by separate source-channel coding (SSCC). How- ever, at ﬁnite blocklengths not only is the fundamental limit\nno longer achievable but separation ceases to be optimal. Quantifying, as a function of blocklength and excess distortion probability, the required backoff from C R (d) as well as the increase in the rate of source symbols per channel use afforded by an optimal joint design bears great practical, as well as conceptual, interest.\nPrior research in this direction includes the work of Csisz´ar [2], [3] who demonstrated that the error exponent of joint source-channel coding outperforms that of separate source- channel coding. For discrete source-channel pairs with average distortion criterion, Pilc\u2019s achievability bound [4], [5] applies. For the transmission of a Gaussian source over a discrete channel under the average mean square error constraint, Wyner\u2019s achievability bound [6], [7] applies. Most recently, Tauste Campo et al. [8] showed a number of ﬁnite-blocklength random-coding bounds applicable to the almost-lossless JSCC setup. For sources and channels with ﬁnite alphabets, Wang et al. [9] recently found the dispersion of JSCC. In this paper, we show new nonasymptotically tight converse bounds, generalize\nthe achievability bound in [8] to lossy compression, and extend the dispersion result of [9] to sources with abstract alphabets and Gaussian channels.\nThe rest of the paper is organized as follows. Section II summarizes basic deﬁnitions and notation. Section III and IV introduce the new converse and achievability bounds to the maximum achievable coding rate, respectively. A Gaussian approximation analysis of the new bounds is presented in Section V. The evaluation of the bounds and the approximation is performed for two important special cases: the transmission of a binary memoryless source (BMS) over a binary symmetric channel (BSC) with bit error rate distortion (Section VI) and the transmission of a Gaussian memoryless source (GMS) with mean-square error distortion over an AWGN channel with a total power constraint (Section VII).\nA lossy source-channel code is a (possibly randomized) pair of mappings f : M → X and c: Y → ˆ M. A distortion measure d : M × ˆ M → [0, +∞) is used to quantify the performance of a lossy code.\nDeﬁnition 1. A (d, ) code for {M, X , Y, ˆ M, P S , P Y |X , d } is a source-channel code with P [d (S, c(Y )) > d] ≤ where f(S) = X.\nIf the channel has input cost constraints, the additional requirement in Deﬁnition 1 will be f (S) ∈ F, where F is the set of the allowable channel inputs, F ⊆ X . The special case d = 0 and d(s, z) = 1 {s = z} corresponds to almost-lossless compression. If, in addition, P S is equiprobable on an alphabet of cardinality M , a (0, ) code in Deﬁnition 1 corresponds to an (M, ) channel code (i.e. a code with M codewords and average error probability ). On the other hand, if the channel is an identity mapping on an alphabet of cardinality M , a (d, ) code in Deﬁnition 1 corresponds to an (M, d, ) lossy compression code (as e.g. deﬁned in [10]).\nDeﬁnition 2. In the conventional ﬁxed-to-ﬁxed (or block) setting in which X and Y are the n−fold Cartesian products of alphabets A and B, M and ˆ M are the k−fold Cartesian products of alphabets S and ˆ S, and d k : S k × ˆ S k → [0, +∞), a (d, ) code for {S n , A n , B n , ˆ S n , P S k , P Y n |X n , d k } is called a (k, n, d, ) code.\nDeﬁnition 3. Fix , d and the channel blocklength n. The max- imum achievable source blocklength and coding rate (source\nR (n, d, ) = k (n, d, ) n \t (2) Denote, for a given P Y |X ,\nand, for a given P S and the distortion measure d : M × ˆ M → [0, +∞),\nWe impose the following basic restrictions on P Y |X , P S and the distortion measure.\nd min = inf {d: R S (d) < ∞} \t (5) (b) The inﬁmum in (4) is achieved by a unique P Z |S . (c) The supremum in (3) is achieved by a unique P X .\nThe dispersion, which serves to quantify the penalty on the rate of the best JSCC code induced by the ﬁnite blocklength, is deﬁned as follows.\nDeﬁnition 4. Fix d ≥ d min . The rate-dispersion function of joint source-channel coding (source samples squared per channel use) is deﬁned as\n− R(n, d, ) Q −1 ( )\nwhere C and R(d) are the channel capacity and source rate- distortion function, respectively. 1\nDeﬁnition 5 (d−tilted information [10]). For d > d min , the d −tilted information in s is deﬁned as 2\nwhere the expectation is with respect to the unconditional distribution of Z , and\nThe following properties of d−tilted information, proven in [11], are used in the sequel.\n S (s, d) = ı S ;Z (s; z) + λ d(s, z) − λ d \t (9) E [exp {λ d − λ d(S, z) +  S (S, d)}] ≤ 1 \t (10)\nwhere (9) holds for P Z -almost every z, while (10) holds for all z ∈ ˆ M, and\ndenotes the information density of the joint distribution P SZ at (s, z). We can deﬁne the right side of (11) for a given (P Z |S , P Z ) even if there is no P S such that the marginal of P S P Z |S is P Z . We use the same notation ı S ;Z for that more general function. To extend Deﬁnition 5 to the lossless case, for discrete random variables we deﬁne 0-tilted information as\n S (s, 0) = ı S (s) \t (12) where\n(13) is the information in outcome s ∈ M.\nTheorem 1 (Converse). Fix a positive integer T and a partition {X t , t = 1, . . . , T } of the set of the allowable channel inputs F ⊆ X . Deﬁne the function T : X → [1, . . . , T ], T (x) = t if x ∈ X t . Any (d, ) code for source S and channel P Y |X must satisfy\n(14) where the conditional probability is with respect to Y dis- tributed according to P Y |X=x (independent of S), and\nand the supremum is over γ > 0 and all collections {P Y 1 , . . . , P Y T } of probability distributions on the channel output alphabet Y parameterized by the partition index t.\nWhen we apply Theorem 1 to ﬁnd the dispersion of JSCC, we will let T be the number of channel input types. For symmetric channels (in a sense to be formalized) T = 1 already yields a tight bound. Moreover, symmetry signiﬁcantly simpliﬁes the computation of (14), as the following result details.\nTheorem 2 (Converse). If there exists a P ¯ Y such that the distribution of\n(according to P Y |X=x ) does not depend on the choice of x ∈ F, then any (d, ) code for source S and channel P Y |X with subset F ⊆ X of allowable channel inputs must satisfy\nProof: In Theorem 1, let T = 1 and observe that under our channel symmetry assumption, the conditional probability in (14) is the same regardless of the choice of x ∈ F.\nRemark 1. With the substitution (12), Theorems 1 and 2 still hold in the case d = 0 and d(x, y) = 1 {x = y}, which corresponds to almost-lossless data compression.\nRemark 2. Our converse for lossy source coding in [10, Theorem 7] can be viewed as a particular case of the result in Theorem 2. Indeed, if X = Y = {1, . . . , M} and P Y |X (m|m) = 1, P ¯ Y (1) = . . . = P ¯ Y (M ) = 1 M , then (17)\nTo show the converses in this section, we use the equivalent list decoding setting, the idea originally put forward by Csisz´ar [3]. In list decoding, the encoder is the random transformation P X |S , where X takes values on the subset of allowable channel inputs, F ⊆ X , and the decoder is the random transformation P ˜ S |Y , where ˜ S ∈ M (L) , and M (L) consists of all lists containing no more than L elements drawn from M. The error probability with list decoding is the probability that the source output S is not on the decoder output list for Y :\nWhile traditionally list decoding has only been considered in the context of ﬁnite alphabet sources, we generalize the setting to sources with abstract alphabets.\nDeﬁnition 6. (list decoder) The (L, Q S ) list decoder is the random transformation P ˜ S |Y , where ˜ S takes values on Q S - measurable sets with Q S -measure not exceeding L:\nDeﬁnition 7. (list code) An ( , L, Q S ) list code is a pair of random transformations (P X |S , P ˜ S |Y ) such that (20) holds and the list error probability does not exceed .\nOf course, letting Q S = U S , where U S is the counting measure on S, we recover the conventional list decoder deﬁnition. The traditional almost-lossless JSCC is described by L = 1, Q S = U S . If the source alphabet is the real line, it is reasonable to measure the list size in terms of its Lebesgue measure.\nTo draw a connection with lossy joint source-channel cod- ing, observe that any (d, ) lossy code can be converted to a list code with list error probability not exceeding by feeding the lossy decoder output to a function that outputs the list of all source outcomes s within distortion d from the output z ∈ ˆ M of the original lossy decoder. Therefore, the set of all\n(d, ) lossy codes is included in the set of all list codes with list error probability ≤ and list size\nVia (21), any converse for list coding implies a converse for lossy coding. Keeping this in mind, we state our converses in this section for list decoding.\nthe optimal performance achievable among all randomized tests P W |X : X → {0, 1} between probability distributions P and Q on X is denoted by (1 indicates that the test chooses P ) 3 . Note that Q need not be a probability measure, it just needs to be σ-ﬁnite in order for the Neyman-Pearson lemma and related results to hold.\nThe hypothesis testing converse for channel coding [12, Theorem 27] can be generalized to joint source-channel coding with list decoding as follows.\nTheorem 3 (Converse). Any ( , L, Q S ) list code, where Q S is σ-ﬁnite, must satisfy\nwhere the supremum is over all probability measures P ¯ Y deﬁned on the channel output alphabet Y.\nUnfortunately, computing the inﬁmum over all encoders in (23) in general is not feasible. However, if the chan- nel is symmetric (in a sense formalized in Theorem 4), β 1− (P S P X |S P Y |X , U S P X |S P ¯ Y ) is independent of P X |S , and the following result holds.\nTheorem 4 (Converse). Fix a probability measure P Y . Assume that the distribution of ı X ; ¯ Y (x, Y ) according to both P Y |X=x and P ¯ Y does not depend on x ∈ F, where F ⊆ X is the subset of allowable channel inputs. Then, any ( , L, Q S ) list code must satisfy\nRemark 3. In the case of ﬁnite channel input and output alphabets, the channel symmetry assumption of Theorem 4 holds, in particular, if the rows of the channel\u2019s transi- tion probability matrix are permutations of each other, and P ¯ Y n is the equiprobable distribution on the (n-dimensional) channel output alphabet, which, coincidentally, is also the capacity-achieving output distribution. For Gaussian channels with equal power constraint, which corresponds to F =\nx : |x| 2 = nP for some P > 0, any spherically-symmetric P ¯ Y n satisﬁes the assumption of Theorem 4.\nWe generalize the recent achievability bound by Tauste Campo et al. [8] applicable to almost-lossless JSCC to the case with nonzero tolerable distortion d.\nB d (s) = {z ∈ ˆ M: d(s, z) ≤ d} \t (25) Theorem 5 (Achievability). There exists a (d, ) source- channel code with\n(26) where the expectations are with respect to P S P X P Y |X P Z , and\nm \t (27) is the M −th harmonic number.\nGiven an (M, ζ) channel code and an (M, d, η) source code, we may concatenate them to obtain a separate source-channel code. The performance of such codes, optimized over M , is easily evaluated using the recent results in [10], [12], as outlined in the following Theorem.\nTheorem 6 (Achievability, SSCC). Fix P Y |X , d and P S . Denote by (M ) the minimum achievable maximal error probability among all transmission codes of size M , and the minimum achievable probability of exceeding distortion d with a source code of size M by ∗ (M, d).\nIn addition to the basic conditions (a)-(c) of Section II, in this section we impose the following restrictions.\n(i) The channel is stationary and memoryless, P Y n |X n = P Y |X × . . . × P Y |X , either discrete with ﬁnite input alphabet, or Gaussian with a maximal power constraint.\n(ii) The source is stationary and memoryless, P S k = P S × . . . × P S , and the distortion measure is separable, d (s k , z k ) = 1 k k i =1 d (s i , z i ).\n(iii) The distortion level satisﬁes d min < d < d max , where d min is deﬁned in (5), and d max = inf z ∈ ˆ C E [d(S, z)], where the average is with respect to the unconditional distribution of S. The excess-distortion probability satis- ﬁes 0 < < 1.\n(iv) E d 9 (S, Z ) < ∞ where the average is with respect to P S ×P Z and P Z is the output distribution corresponding to the minimizer in (4).\nConditions (i) and (ii) are standard in the memoryless joint source-channel coding problem setup. The technical condition (iv) ensures applicability of the Gaussian approximation in Theorem 7 below.\nTheorem 7 (Gaussian approximation). Under restrictions (i)\u2013 (iv), the optimal (k, n, d, ) code satisﬁes\n(29) V(d) = Var [ S (S, d)] \t (30)\nX , Y are the capacity-achieving input and output random variables.\nRemark 4. Using Theorem 7, the rate-dispersion function of JSCC is found as (recall Deﬁnition 4),\nR 3 (d) \t (32) Remark 5. If the channel and the data compression codes are designed separately, we can invoke channel coding [12] and lossy compression [10] results to show that,\n+ O (log(n + k)) \t (33) Comparing (33) to (29), observe that if either the channel or the source (or both) have zero dispersion, the joint source- channel coding dispersion can be achieved by separate coding. In that special case, the asymptotics kick in fast enough to neutralize the performance gain achieved by joint code design.\nIn this section we particularize the bounds in Sections III, IV and the approximation in Section V to the transmission of a BMS with bias p over a BMS with crossover probability δ. The distortion measure is bit error rate. The source and the channel dispersions are given by [10], [12]:\nV = δ(1 − δ) log 2 1 − δ δ \t (34) V(d) = p(1 − p) log 2 1 − p p \t (35)\nwhere note that (35) does not depend on d. A source of fair coin ﬂips has zero dispersion, and as anticipated in Remark 5, JSSC does not bring much of a gain in the ﬁnite blocklength regime (Fig. 1). The situation is different if the source is biased, with JSCC showing signiﬁcant gain over SSCC (Fig. 2).\nConsider the transmission of a GMS with variance σ 2 S over an AWGN channel with signal to noise ratio P and the mean- square error distortion measure. The source and the channel dispersions are given by [10], [12]:\nWhile the bounds are not as tight as in Section VI, they sufﬁce to show that JSCC noticeably outperforms SSCC in the displayed region of blocklengths (Fig. 3).\nTo estimate the maximum source coding rate sustainable as a function of channel blocklength and the probability of exceeding a given distortion level, we have shown a new converse bound (Theorem 1) and a new achievability bound (Theorem 5) applicable in full generality. As evidenced by the numerical results, the converse in Theorem 4, which applies to those channels satisfying a certain symmetry condition, can outperform the general converse in Theorem 1. The asymptotic analysis of the new bounds leads to the Gaussian approximation in Theorem 7, which holds for both discrete and Gaussian channels. Unless either the source or the channel has\nzero dispersion, joint code design offers signiﬁcant advantage in the ﬁnite blocklength regime."},"refs":[{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"Coding theorems for a discrete source with a ﬁdelity criterion"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"Joint source-channel error exponent"}},{"authors":[],"title":{"text":"On the error exponent of source-channel transmission with a distortion threshold"}},{"authors":[{"name":"R. Pilc"}],"title":{"text":"Coding theorems for discrete source-channel pairs"}},{"authors":[],"title":{"text":"The transmission distortion of a discrete source as a function of the encoding block length"}},{"authors":[{"name":"A. D. Wyner"}],"title":{"text":"Communication of analog data from a Gaussian source over a noisy channel"}},{"authors":[],"title":{"text":"On the transmission of correlated Gaussian data over a noisy channel with ﬁnite encoding block length"}},{"authors":[{"name":"A. Tauste Campo"},{"name":"G. Vazquez-Vilar"},{"name":"A. Guill´en i F`abregas"},{"name":"A. Mar- tinez"}],"title":{"text":"Random-coding joint source-channel bounds"}},{"authors":[{"name":"D. Wang"},{"name":"A. Ingber"},{"name":"Y. Kochman"}],"title":{"text":"The dispersion of joint source- channel coding"}},{"authors":[{"name":"V. Kostina"},{"name":"S. Verd´u"}],"title":{"text":"Fixed-length lossy compression in the ﬁnite blocklength regime"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"On an extremum problem of information theory"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"H. V. Poor"},{"name":"S. Verd´u"}],"title":{"text":"Channel coding rate in ﬁnite blocklength regime"}},{"authors":[{"name":"H. Poo"}],"title":{"text":"An introduction to signal detection and estimation"}},{"authors":[{"name":"W. Felle"}],"title":{"text":"An introduction to probability theory and its applications, 2nd ed"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566437.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T4.3","endtime":"10:50","authors":"Victoria Kostina, Sergio Verdú","date":"1341397800000","papertitle":"Lossy joint source-channel coding in the finite blocklength regime","starttime":"10:30","session":"S9.T4: Joint Source-Channel Codes","room":"Stratton 20 Chimneys (306)","paperid":"1569566437"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
