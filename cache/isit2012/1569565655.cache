{"id":"1569565655","paper":{"title":{"text":"Achievability Proof via Output Statistics of Random Binning"},"authors":[{"name":"Mohammad Hossein Yassaee"},{"name":"Mohammad Reza Aref"},{"name":"Amin Gohari"}],"abstr":{"text":"Abstract\u2014This paper presents a new and ubiquitous frame- work for establishing achievability results in network information theory (NIT) problems. The framework is used to prove various new results. To express the main tool, consider a set of discrete memoryless correlated sources (DMCS). Assume that each source (except one, Z n ) is randomly binned at a ﬁnite rate. We ﬁnd sufﬁcient conditions on these rates such that the bin indices are nearly mutually independent of each other and of Z n . This is used in conjunction with the Slepian-Wolf (S-W) result to set up the framework. We begin by illustrating this method via examples from channel coding and rate-distortion (or covering problems). Next, we use the framework to prove a new result on the lossy transmission of a source over a broadcast channel. We also prove a new lower bound to a three receiver wiretap broadcast channel under a strong secrecy criterion. We observe that we can directly prove the strong notion of secrecy without resorting to the common techniques, e.g., the leftover hash lemma. We have also used our technique to solve the problem of two-node interactive channel simulation and the problem of coordination via a relay."},"body":{"text":"Random coding and random binning are widely used in the achievability proofs of the network information theory (NIT) problems. Random coding is a channel coding technique that allows one to prove the existence of a good codebook (which is a subset of the product set X n [1:T ] := T i=1 X n i ), while random binning is a source coding technique that partitions the product set into bins with desired properties.\nExisting achievability proofs for source coding and chan- nel coding problems are mostly based on repeated use of these techniques. Moreover some connections between cer- tain source coding and channel coding problems has been observed. Slepian and Wolf, in their seminal paper on the lossless source coding [2], interpreted the achievability of the rate R = H(X|Y ) for compressing the source X n at rate R to the destination with access to the source Y n , through a channel coding problem. In contrast, Csiszar and Korner, obtained an achievability proof for multiple access channels (MAC) through the distributed source coding problem of Slepian and Wolf, [3]. In a recent work [4], Renes and Renner showed the achievability of the channel capacity via a combination of Slepian-Wolf (S-W) coding and privacy ampliﬁcation. The main theme in these works is that the set of sequences mapped to the same index through S-W coding constitutes a good channel code, and hence we have a decomposition of the product set into the channel codebooks. Similarly our\nframework treats source coding and channel coding problems on the same footing.\nConsider the problem of sending a message M over the channel q(y|x). Traditional random coding considers an en- coder X n (M, F ) and a decoder ˆ M (Y n , F ) where F is a common randomness, independent of M , available to both the transmitter and receiver. R.v. F represents the random nature of codebook generation. Since the probability of error is the average of that over all realizations of F , one can ﬁnd f such that X n (M, F = f ) and ˆ M (Y n , F = f ) form appropriate encoder and decoder. We depart from this by ﬁrst generating n i.i.d. copies of X n and Y n . Then we take both F and M to be functions of X n such that F is nearly independent of M . Note that we still have the property that p(y n |x n , F = f ) = p(y n |x n ) and p(m|F = f ) ≈ p(m) meaning that X n (M, F = f ) and ˆ M (Y n , F = f ) are legitimate choices as stochastic encoder and decoder. We construct F and M as random partitions (binnings) of X n . The question then arises that under what conditions two random bin indices are independent (as in the case of F and M ), and what is the sufﬁcient condition for recovering X n from Y n and a bin index F .\nTo answer these questions in a more general framework, in Section II, we prove two main theorems on approximating the joint pmf of the bin indices in a distributed random bin- ning. We study properties of random binning in two extreme regimes, namely, when the binning rates are low and high. In the ﬁrst case, we observe that if the rates of a distributed ran- dom binning are sufﬁciently small, the bin indices are nearly jointly independent, uniformly distributed and independent of a non-binned source Z n . This result generalizes the one for the channel intrinsic randomness [5]. The second case is the S-W region, which shows that if the rates of distributed binning are sufﬁciently large, the outputs of random binning are enough to recover the sources.\nWe argue that not only the new framework has a simple structure (it uses only random binning), but it can solve some problems much easier than the traditional techniques; see [11], [12] for two examples. These examples consider the problems of channel simulation and coordination. Furthermore, we be- lieve that the framework leads to more rigorous and simpler proofs for problems in secrecy (See the full version of this paper [10]).\nby demonstrating our approach for some primitive problems of NIT, i.e. channel coding and lossy source coding problems, before getting into our new results. Then we apply our approach to two complicated scenarios, i.e., lossy coding over broadcast channels and a three receiver wiretap broadcast channel under a strong secrecy constraint. In Section IV, we discuss connections between our framework and the covering and packing lemmas in a multivariate setup. We show that the set of typical sequences can be decomposed into covering and packing codebooks. Finally in Section V, we give a summary of our framework.\nNotation: In this paper, we use X S to denote (X j : j ∈ S), p U A to denote the uniform distribution over the set A and p(x n ) to denote the the i.i.d. pmf n i=1 p(x i ), unless otherwise stated. The total variation between two pmf\u2019s p and q on the same alphabet X , is deﬁned by p(x) − q(x) 1 :=\n|p(x) − q(x)|. When a pmf itself is random, we use capital letter, e.g. P X . For simplicity of notation, we write P X ≈ Q X if E P X − Q X 1 < , where P X and Q X are random pmf. The notation p X ≈ q X is deﬁned, similarly.\nLet (X [1:T ] , Z) be a DMCS distributed according to a joint pmf p X [1:T ] ,Z on a countably inﬁnite set T i=1 X i × Z. A distributed random binning consists of a set of random mappings B i : X n i → [1 : 2 nR i ], i ∈ [1 : T ], in which B i maps each sequence of X n i uniformly and independently to the set [1 : 2 nR i ]. We denote the random variable B t (X n t ) by B t . A random distributed binning induces the following random pmf on the set X n [1:T ] × Z n × T t=1 [1 : 2 nR t ],\nOne can easily verify that (B 1 , · · · , B T ) are uniformly distributed and mutually independent of Z n in the mean, that is\nEP (z n , b [1:T ] ) = 2 −n T t=1 R t p(z n ) = p(z n ) T t=1 p U [1:2 nRt ] (b t ). The following theorem ﬁnds constraints on the rate-tuple\n(R 1 , · · · , R T ), such that the preceding observation about the mean holds for almost any distributed binning. See [10] for the proof.\nTheorem 1: If for each S ⊆ [1 : T ], the following constraint holds\nRemark 1: In [5], the channel intrinsic randomness was deﬁned \u201cas the maximum random bit rate that can be extracted from a channel output independently of an input with known statistics\u201d. One can generalize this deﬁnition to the broadcast channel p X [1:T ] |Z , in the sense of ﬁnding random bit rates (R 1 , · · · , R T ) that can be extracted individually from the\nchannel outputs independently of the channel input and the other random bits. Theorem 1 gives an achievable rate region for this scenario and implies that random binning is sufﬁcient to prove the achievability. 1\nTheorem 1 enables us to approximate the pmf P (z n , b 1:T ). We now consider another region for which we can approximate a speciﬁed pmf. This region is the Slepian-Wolf region for reconstructing X n [1:T ] in the presence of (B 1:T , Z n ) at the decoder. As in the achievability proof of the [7, Lemma 7.2.1], we can deﬁne a decoder with respect to any ﬁxed distributed binning. We denote the decoder by the random conditional pmf P SW (ˆ x n [1:T ] |z n , b [1:T ] ) (note that since the decoder is a function, this pmf takes only two values, 0 and 1). Now we write the Slepian-Wolf theorem in the following equivalent form. See [10] for details.\nLemma 1: If for each S ⊆ [1 : T ], the following constraint holds\nIn this section, we illustrate the use of Theorem 1 and Slepian-Wolf binning (Lemma 1) through some examples. Before going through these examples, we state a useful lemma on total variation of arbitrary (random) pmfs. Its proof can be found in [10].\nConsider a channel with the conditional pmf p(y|x). We want to prove that for any p(x), R < I(X; Y ) is an achievable rate for the channel. Let M = [1 : 2 nR ] be the message set. The joint pmf of (M, X n , Y n ) is equal to p U M (m)p(x n |m)p(y n |x n ), where p(x n |m) can be non- deterministic (although it is deterministic in the standard deﬁnition) and M has uniform distribution due to the deﬁnition of channel coding. Here we want to determine p(x n |m) and a decoder such that we have reliable communication. In our\napproach, we let (X n , Y n ) be i.i.d. and distributed according to p(x, y). We deﬁne two random mappings on X n as follows: to each x n , we assign two random bin indices m ∈ [1 : 2 nR ] and f ∈ [1 : 2 n ˜ R ], uniformly and independently. Then the induced random pmf is\nNow, Slepian-Wolf binning shows that if ˜ R > H(X|Y ), the decoder can reliably reconstruct X n using (F, Y n ) and therefore decode M which is a function of X n . Thus, for almost any F = f , the decoding is reliable. Hence, if the transmitter and receiver agree on an appropriate F = f , then we can choose P (x n |m, f ) as the encoder and the Slepian- Wolf decoder as a decoder. Note that P (y n |x n , F = f ) = p(y n |x n ), so conditioning on f simulates a DMC channel. However, notice that after conditioning on F = f , P (m|f ) is not necessarily uniform. But if R + ˜ R < H(X), Theorem 1 implies that M has uniform distribution and roughly speaking, is independent of F . Hence R < I(X; Y ) is achievable using this encoder and decoder. More precisely, applying Theorem 1 and Lemma 1 results in the existence of n , δ n → 0, such that\nwhere (4) follows from the ﬁrst part of Lemma 2 and the fact that (M, F ) is a function of X n and (5) follows from the third part of Lemma 2.\nNow the deﬁnition of ≈ and (5) guarantee the existence of a ﬁxed binning ¯ B with the corresponding pmf ¯ p such that (5) is satisﬁed with the ﬁxed pmf ¯ p instead of P . The second part of Lemma 2 implies that there exists F = f such that\nwhere β n = 2( n + δ n ) → 0. Finally, identifying ¯ p(x n |m, f ) as the encoder and ¯ p SW (ˆ x n |y n , f ) as the decoder results in a pair of encoder-decoder with the probability of error at most β n .\nConsider the problem of lossy compression of a source within a desired distortion. In this setting, there is an i.i.d. source X n distributed according to p(x), an (stochastic) en- coder mapping X n to M ∈ [1 : 2 nR ], a decoder that recon- structs a lossy version of X n (namely Y n ) and a distortion measure d : X × Y → [0, d max ]. A rate R is said to be achievable at the distortion D, if E(d(X n , Y n )) ≤ D + n , where n → 0 and d(X n , Y n ) is the average per letter distortion. Here we wish to prove the achievability of the rate R > I(X; Y ) when Ed(X, Y ) < D. Note that the joint\npmf between (X n , M, Y n ) factors as p(x n )p(m|x n )p(y n |m). The goal is to determine p(m|x n ) and p(y n |m) as the pair of encoder-decoder. As the case of channel coding, let (X n , Y n ) be i.i.d. and distributed according to p(x, y). We have E p(x n ,y n ) d(X n , Y n ) = Ed(X, Y ) < D. Again, to each y n , we assign two random bins m and f at the rates R and ˜ R, respectively. Since the decoder aims to produce Y n which has the desired distortion with X n , we assume that R+ ˜ R > H(Y ) resulting in reliable decoding of Y n from (M, F ). Hence, we expect that if the encoder and decoder agree on an appropriate value of F = f , the decoder can reliably recover Y n using the S-W decoder. However, conditioning on F = f changes the distribution p(x n ) to P (x n |f ) and we obtain an scheme that achieves distortion D w.r.t. P (x n |f ) rather than p(x n ). Despite this fact, roughly speaking, Theorem 1 says that F and X n are independent and P (x n |f ) is close to p(x n ) if ˜ R < H(Y |X). Hence, we can choose the encoder-decoder w.r.t. P (x n |f ) for p(x n ). Now, we go through the details of the proof. Let ˆ Y n be the output of the S-W decoder, P SW (ˆ y n |m, f ). Theorem 1 and Lemma 1 imply that there exist n , δ n → 0 such that\nNow, there exists a ﬁxed binning with the corresponding pmf ¯ p such that (7) is satisﬁed with ¯ p. Deﬁne\nWe have q n +δ n ≈ ˜ q and ˜ q(x n , ˆ y n ) = p X n Y n (x n , ˆ y n ), thus we get q X n ˆ Y n n +δ n ≈ p X n Y n . This can lead to a proof that the average distortion between X n and ˆ Y n is close to the average distortion between X n and Y n . However, if we want to prove the stronger claim that the latter distortion is in the vicinity of the former distortion with probability close to one , we can proceed as follows: using the weak law of large number (WLLN), we show that there exists f such that q(d(X n , ˆ Y n ) < D|f ) is close to one. Since d is per letter distortion and p is i.i.d., WLLN yields that\nThis implies that there exists f such that q X n ˆ Y n |f (d(X n , ˆ Y n ) < D) > 1 − ˜ n − n − δ n . This also shows that for sufﬁciently large n, E q [d(X n , ˆ Y n )|f ] < D. Note that\nFinally, specifying ¯ p(m|x n , f ) as the encoder (which is equivalent to generating a random sequence y n according to ¯ p(y n |x n , f ) and then transmitting the bin index m assigned to y n ) and ¯ p SW (ˆ y n |m, f ) as the decoder results in a pair of encoder-decoder obeying the desired distortion.\nConsider the problem of lossy transmission of an i.i.d. source S n distributed according to p(s), over the broadcast channel p(y 1 , y 2 |x). Here, the sender wishes to communicate the source to the two receivers within desired distortions (D 1 , D 2 ). Formally, there are\n\u2022 an encoder that assigns a random sequence x n to each s n according to p enc (x n |s n ),\n\u2022 two decoders, where decoder j = 1, 2 assigns an estimate ˆ s n j ∈ ˆ S j to each y n j according to p dec j (ˆ s n j |y n j ),\nA distortion pair (D 1 , D 2 ) is said to be achievable, if there exists a sequence of encoder-decoder such that Ed j (S n , ˆ S n j ) ≤ D j + n , j = 1, 2 and n → 0.\nTheorem 2: A distortion pair (D 1 , D 2 ) is achievable for the lossy transmission of the source S over the broadcast channel p(y 1 y 2 |x), if there exist a pmf p(u [0:2] ), an encoding function x(u [0:2] , s) and two decoding functions ˆ s 1 (u 0 , u 1 ) and ˆ s 2 (u 0 , u 2 ) such that Ed j (S, ˆ S j ) ≤ D j , j = 1, 2 and the following inequalities hold:\n+ I(U 1 ; Y 1 |U 0 ) + I(U 2 ; Y 2 |U 0 ), I(U 0 U 1 ; S) + I(U 0 U 2 ; S) < I(U 0 U 1 ; Y 1 ) + I(U 0 U 2 ; Y 2 )\nRemark 2: The above result generalizes the result of Han and Costa, [9] for the lossless transmission of correlated sources over broadcast channels if we assume that S is of the form (S 1 , S 2 ). See [10].\nSketch of the proof: Let (S n , U n [0:2] , Y n 1 , Y n 2 ) be i.i.d. and distributed according to p(s, u [0:2] , y 1 , y 2 ) such that Ed j (S, ˆ S j ) < D j , j = 1, 2. We have Ed j (S n , ˆ S n j ) < D j , j = 1, 2. Since ˆ S n j is a function of (U n 0 , U n j ), if the decoder j can decode (U n 0 , U n j ), then it can construct an estimate ˆ S n j\nwithin the desired distortion. We proceed in a similar way to the case of lossy coding. First, we use the following random binning for the decoding of (U n 0 , U n j ).\n\u2022 For j = 1, 2, to each pair (u n 0 , u n j ) assign a random bin index f j ∈ [1 : 2 nR j ].\nApplying the S-W decoder and setting X 1 = U 0 , X 2 = U 0 U j , Z = Y j in Lemma 1, yields that the decoding of U n 0 U n j is reliable if,\nHence if the encoder and decoders agree on an appropriate F [0:2] = f [0:2] , decoder j = 1, 2 can reliably decode U n 0 , U n j . However, like the case of channel coding and lossy coding, we\nmust take care of the change in the given pmf p(s n ), when conditioning on f [0:2] . Again substituting X 1 = U 0 , X 2 = U 0 U j , Z = S in Theorem 1 implies that S n is nearly independent of F [0:2] and that its pmf is close to p(s n ), if\nApplying Fourier-Motzkin elimination (FME) on (9) and (10) gives (8). More precisely, following similar steps as in the case of lossy coding, we can prove the existence of a ﬁxed binning with the corresponding pmf ¯ p such that ¯ p(f [0:2] , s n ) ≈ p U (f [0:2] )p(s n ) and ¯ p SW j (u n 0 , u n j , ˆ u n 0,j , ˆ u n j ) ≈ 1{ˆ u n 0,j = u n 0 , ˆ u n 1 = u n 1 } for j = 1, 2, where ( ˆ U 0,j , ˆ U j ) is the output of S-W decoder ¯ p SW j (ˆ u 0,j , ˆ u j |y n j , f {0,j} ). Deﬁne\nU n j Y n j . The proof contin- ues through the same lines as in the lossy source coding. First we apply WLLN to prove the existence of f [0:2] such that E q [d(S n , ˆ S n j |f [0:2] ] < D j . Then by expanding q we adopt (¯ p(u n [0:2] |f [0:2] , s n ), x n (u n [0:2] , s n )) as the encoder and (¯ p SW j (ˆ u 0,j , ˆ u j |y n j , f {0,j} ), ˆ s n j (ˆ u 0,j , ˆ u j , y n j )) as the decoder j.\nConsider the problem of secure transmission over a broad- cast channel with a wiretapper, p(y 1 , y 2 , z|x). Here, we wish to securely transmit a common message m 0 ∈ [1 : 2 nR 0 ] to the receivers Y 1 , Y 2 and two private messages m j ∈ [1 : 2 nR j ], j = 1, 2 to the receivers Y j , j = 1, 2, respectively, while concealing them from the wiretapper. We use the total variation distance as a measure for analyzing the secrecy. Formally speaking there are,\n\u2022 Three messages M 0 , M 1 , M 2 which are mutually inde- pendent and uniformly distributed,\n\u2022 Two decoders, where decoder j assigns an estimate ( ˆ m 0,j , ˆ m j ) of (m 0 , m j ) to each y n j .\nA rate-tuple (R 0 , R 1 , R 2 ) is said to be achievable if Pr{∪ j=1,2 ( ˆ M 0,j , ˆ M j ) = (M 0 , M j )} → 0 and M [0:2] is nearly independent of the wiretapper output, Z n , that is,\nwhere, here p(z n ) is the induced pmf on Z n and is not an i.i.d. pmf.\nTheorem 3: A rate-tuple (R 0 , R 1 , R 2 ) is achievable for the secure transmission over the wiretap broadcast channel, if\n+ I(U 2 ; Y 2 |U 0 ) − I(U 1 ; U 2 |U 0 ) − I(U [0:2] ; Z) 2R 0 + R 1 + R 2 < I(U 0 U 1 ; Y 1 ) − I(U 0 U 1 ; Z) + I(U 0 U 2 ; Y 2 )\nSketch of the proof: Let (U n [0:2] , X n , Y n 1 , Y n 2 , Z n ) be i.i.d. and distributed according to p(u [0:2] , x, y 1 , y 2 , z). Consider the following random binning:\n\u2022 To each u n 0 assign independently two random bins m 0 ∈ [1 : 2 nR 0 ] and f 0 ∈ [1 : 2 n ˜ R 0 ],\nW decoder for decoding of the messages (M 0 , M j ) through decoding of (U n 0 , U n j ), when the decoder accesses to Y n j\nand F 0 F j . Lemma 1 yields that the decoding of (U n 0 , U n j ) is reliable if\nThus, if the encoder and decoders agree on an appropriate f [0:2] , decoder j can reliably decode (U n 0 , U n j ) and therefore (M 0 , M j ). However, as in the case of channel coding, we must take care of uniformity of the messages when conditioned on f [0:2] , as well as the mutual independence of the messages and the wiretapper\u2019s output, Z n . Now, Theorem 1 yields that if\nthen F [0:2] , M [0:2] and Z n are nearly independent, and roughly speaking M [0:2] has uniform distribution and the pmf P (z n ) is close to i.i.d. pmf p(z n ). The proof continues in the similar steps as in the previous cases. Finally, applying FME on (12) and (13) gives (11).\nRemark 3: If one only wants to securely transmit a com- mon message M 0 as in [8] (i.e. R 1 = R 2 = 0), we have shown in [10] the following lower bound on R 0 . This lower bound subsumes the one given in [8] under weak secrecy criterion:\nMost of the achievability proofs in NIT are based on two primitive lemmas, namely packing lemma and covering lemma [1]. Thus it would be interesting to see how our probabilistic proofs relate to these lemmas. We show that Theorem 1 implies a certain form of multivariate covering (but not exactly the one mentioned in [1]). The discussion on packing lemma is similar and can be found at [10].\nMultivariate covering : We prove a version of multivariate covering that is similar to Marton coding [1]. Consider r.v.\u2019s X [1:T ] Z. Roughly speaking, we want to prove that under cer- tain conditions on R i \u2019s, there exists a partition of set of typical sequences of X n i into 2 nR i bins of size 2 nR i = 2 n(H(X i )−R i ) for i = 1 : T , such that if we choose any of the partitions of X n 1 , and any of the partitions of X n 2 , etc, we can ﬁnd sequences x n 1 , x n 2 ,..., x n T in these partitions such that they are jointly typical with each other and with Z n with high probability, for almost all choice of partitions. The conditions imposed on\nthe rate of the bins, R i are given in inequality (14). This is a generalization of the mutual information terms showing up in Marton coding and match the ones reported in [1].\nTo show this let T n [X [1:T ] Z] be the set of strongly typical sequences w.r.t. p X [1:T ] Z . Theorem 1 says that if\nthen P (b [1:T ] , z n ) ≈ p U (b [1:T ] )p(z n ). One can show that with high probability the number of the typical sequences as- signed to each bin b i ∈ [1 : 2 nR i ] is about 2 nR i , for i = 1 : T , provided that R i < H(X i ) . This fact alongside with Theorem 1 implies that there exists a ﬁxed binning with the correspond- ing pmf ¯ p such that ¯ p(z n , b [1:T ] ) ≈ p U (b [1:T ] )p(z n ) and the number of the typical sequences assigned to each bin b i ∈ [1 : 2 nR i ] is about 2 nR i , provided that (14) is satisﬁed. Let q(b [1:T ] , x n [1:T ] , z n ) = p U (b [1:T ] )p(z n )¯ p(x n [1:T ] |b, z n ). Since ¯ p(x n [1:T ] , z n ) = p(x n [1:T ] , z n ), we have ¯ p(T n [X [1:T ] Z] c ) <\nn }) → 0. Therefore for almost all the choices of b [1:T ] , the probability of the typical set conditioned on b [1:T ] is large, implying a non- zero intersection of the typical set and the product partition set (see [10] for details).\nWe are ready to state the steps of the probabilistic proof, precisely. The probabilistic proof is done in three steps:\n1) Specifying r.v.\u2019s via which the information is transmitted, e.g., X in the channel coding, U 0:2 in the last two examples.\n2) Solving the S-W problem corresponding to decoding the desired information at each receiver through random binning, e.g., decoding of U n [0:1] in the last two examples at decoder 1.\n3) Sharing an appropriate bin between terminals such that decoding is reliable, while ensuring that conditioning on a bin does not affect the problem formulation, e.g. the uniformity of the message in channel coding and independence of the messages and wiretapper\u2019s output in the secrecy problems, etc."},"refs":[{"authors":[{"name":"A. El Gamal"},{"name":"Y.-H. Kim"}],"title":{"text":"Lecture notes on network information theory"}},{"authors":[{"name":"D. Slepian"},{"name":"J. K. Wolf"}],"title":{"text":"Noiseless coding of correlated information sources"}},{"authors":[{"name":"I. Csiszar"},{"name":"J. Korner"}],"title":{"text":"Information theory: coding theorems for discrete memoryless systems"}},{"authors":[{"name":"J. M. Renes"},{"name":"R. Renner"}],"title":{"text":"Noisy channel coding via privacy ampliﬁca- tion and information reconciliation"}},{"authors":[{"name":"M. Bloch"}],"title":{"text":"Channel Intrinsic Randomness"}},{"authors":[{"name":"I. Csiszar"}],"title":{"text":"Almost Independence and Secrecy Capacity"}},{"authors":[{"name":"T. S. Han"}],"title":{"text":"Information-spectrum methods in information theory"}},{"authors":[{"name":"Y.-K. Chia"},{"name":"A. El Gamal"}],"title":{"text":"3-receiver broadcast channels with common and conﬁdential messages"}},{"authors":[{"name":"T. S. Han"},{"name":"M. M. H. Costa"}],"title":{"text":"Broadcast channels with arbitrarily correlated sources"}},{"authors":[{"name":"M. H. Yassaee"},{"name":"M. R. Aref"},{"name":"A. Gohari"}],"title":{"text":"Achievability proof via output statistics of random binning"}},{"authors":[{"name":"M. H. Yassaee"},{"name":"A. Gohari"},{"name":"M. R. Aref"}],"title":{"text":"Channel simulation via interactive communications"}},{"authors":[{"name":"F. Haddadpour"},{"name":"M. H. Yassaee"},{"name":"A. Gohari"},{"name":"M. R. Aref"}],"title":{"text":"Coordination via a relay"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565655.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S6.T7.4","endtime":"12:50","authors":"Mohammad Hossein Yassaee, Mohammad Reza Aref, Amin Aminzadeh Gohari","date":"1341318600000","papertitle":"Achievability Proof via Output Statistics of Random Binning","starttime":"12:30","session":"S6.T7: Tools for Bounding Capacity","room":"Stratton (407)","paperid":"1569565655"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
