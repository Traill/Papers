{"id":"1569566641","paper":{"title":{"text":"Estimation with a Helper Who Knows the Interference"},"authors":[{"name":"Yeow-Khiang Chia"},{"name":"Rajiv Soundararajan"},{"name":"Tsachy Weissman"}],"abstr":{"text":"Abstract\u2014We consider the problem of estimating a signal corrupted by independent interference in which the estimator (decoder) is aided by a helper (encoder) with a limited power budget that has knowledge of the interfering signal noncausally. In the Gaussian case, this problem is equivalent to the problem of Assisted Interfer- ence Suppression considered by Grover and Sahai and we obtain an improved lower bound for this problem using Verd ´u\u2019s relationship between mismatched estimation and relative entropy. We also extend our analysis to consider the case when the signal to be estimated, in addition to the interference, is known to the helper. We establish a lower bound that improves on those recently derived by Huang and Narayanan."},"body":{"text":"Consider a joint source channel coding problem as depicted in Figure 1. We have two memoryless sources S 1 (the desired signal) and S 2 (the interfering sig- nal). The decoder (or estimator)\u2019s aim is to recon- struct the source sequence S n 1 from Y n , with the goal of minimizing the average per symbol distor- tion E( n i =1 d( ˆ S 1i (Y n ), S 1i ))/n. The encoder (helper), which knows the interfering signal S 2 , aids the decoder in reconstructing the signal S 1 by modifying the inter- ference through X, subject to a cost constraint ρ(X), so as to minimize the distortion that an optimum decoder incurs.\nPotential applications for such a setting may arise in sensor networks or cognitive radio systems. As a motivating example, suppose Alice is talking to Bob in his ofﬁce. As a result of ongoing construction work near Bob\u2019s ofﬁce, there is high interference which makes it hard for Bob to listen to Alice. Fortunately, Bob recently purchased a noise cancellation device which has a microphone placed near the construction site. The microphone measures the interfering signal from the\nFig. 1: Estimation with a helper that knows the interfer- ence.\nconstruction site and transmit it to a noise cancellation speaker situated in Bob\u2019s ofﬁce. Since electromagnetic waves travel faster than sound, the noise cancellation speaker knows the interfering signal noncausally. Due to a power constraint on the speaker, it cannot cancel the interference fully. What then, is the minimum distortion that can be achieved by Bob in trying to reconstruct Alice\u2019s speech?\nOur setup is closely related to several strands of work involving communication over channels with state. [1] and [2] considered the problem of State Ampliﬁcation, where a message is to be sent to the decoder and the de- coder also wishes to reconstruct S n 2 subjected to a list or distortion constraint. This setting is similar to our setting, with the main difference being that the decoder wishes to reconstruct S 2 instead of S 1 . When our setting is spe- cialized to the Gaussian case with the distortion measure being the mean square error between the reconstruction and the signal, our setting becomes equivalent to the problem of Assisted Interference Suppression considered in [3]. As detailed in [3], this problem is closely related to Witsenhausen\u2019s counterexample in Optimum Control Theory [4]. In this paper, we focus on the following:\n1) Gaussian Estimation with a helper: In the Gaussian case, where S 1 and S 2 are independent Gaussian random variables with ﬁnite variance and the distortion measure is the mean square error, we note that our setting is equivalent to the problem of Assisted Interference\nSuppression problem. For this setting, we give a lower bound on the minimum achievable distortion that can be strictly larger than the lower bound given in [3], and its improved version given in [5]. Interestingly, the proof of our lower bound relies on an application of a recently established result between relative entropy and mismatched estimation in Gaussian noise [6]. Re- cently, there has been interest in applying relationships between Estimation Theory and Information Theory to problems in Information Theory (see for e.g. [7]). Our lower bound, which seems difﬁcult to obtain by tradi- tional techniques such as the Entropy Power Inequality [8, Chapter 2], provides another application of these information-estimation relations.\n2) Gaussian Estimation with a helper that knows both the interference and the source: We also extend our analysis to consider the related setting when the encoder has access to S 1 noncausally in addition to S 2 . This setting turns out to be a special case of a problem considered in [9]. We give a lower bound for this setting that contains the previous bounds in [9] and can be strictly better than the previous bounds. Furthermore, we also prove constant gap results between the achievable distortion and our lower bound.\nThe rest of the paper is as follow. We ﬁrst provide the formal deﬁnitions in the next section. Section III deals with the case of Gaussian estimation with a helper that knows only the interfering signal. Section IV deals with the setting of Gaussian estimation with a helper that knows both the interference and the source. Several proofs and other results omitted from this paper are given in the extended version [10] posted online.\nIn this section, we give formal deﬁnitions for our problem settings. We will follow the notation of [8], and assume throughout this paper that the channel in consideration is memoryless. That is, p(y n |x n , s n 1 , s n 2 ) =\np(y i |x i , s 1i , s 2i ). We also assume that S n 1 and S n 2 are independent and identically distributed (i.i.d.) sequences.\nA (n, C) code for the setting shown in Figure 1 when the interference is known noncausally consists of\n\u2022 An encoder that maps the interference S n 2 to X n , f : S n 2 → X n ;\n\u2022 A decoder that maps the output Y n to the recon- struction sequence ˆ S n 1 , g : Y n → ˆ S n 1 ;\nsuch that E n i =1 ρ(X i )/n ≤ C. The expected per symbol distortion, D, is given by D = E d(S n 1 , ˆ S n 1 ) = E n i =1 d(S 1i , ˆ S 1i )/n.\nA distortion D is said to be achievable if there exists a sequence of (n, C + n ) codes, where n → 0 as n → ∞, such that\nThe minimum achievable distortion, D(C) min , is then deﬁned as the inﬁnum of the set of achievable distortions under the cost constraint C.\nB. Estimation with source and interference known at the helper\nThis setting is shown in Figure 2 for the Gaussian case. For this setting, As the deﬁnitions are similar to the previous setting, we only mention the difference. That is, the encoder now maps both S n 1 and S n 2 to X n :\nFig. 2: Gaussian estimation with a helper that knows both the interference and the source.\nFor both settings, since we only consider the Gaussian case in this paper, we assume S 1 and S 2 are indepen- dent Gaussian random variables, S 1 ∼ N (0, P 1 ) and S 2 ∼ N (0, P 2 ), both known noncausally at the encoder. The distortion measure is the mean square error between S 1 and it\u2019s reconstruction, d(s 1 , ˆ s 1 ) = (s 1 − ˆs 1 ) 2 . The channel is speciﬁed by Y = X + S 1 + S 2 for the setting deﬁned in subsection II-A and Y = X + S 1 + S 2 + Z, where Z ∼ N (0, N) is independent of S 1 and S 2 for the setting deﬁned in subsection II-B. The cost constraint in both cases is the expected power constraint:\nAs shown in [3], it sufﬁces to consider only P 1 = 1 for this setting. We ﬁrst state a number of results without proof.\nTheorem 1: An achievable distortion for the problem of Gaussian estimation with a helper is given by\n2 \t , E U 2 = P + 2γβ P P 2 + γ 2 P 2 ,\n2 + γβ P P 2 + β P P 2 + γP 2 ,\nand the minimization is over −1 ≤ α ≤ 1, −1 ≤ β ≤ 1 and γ ∈ R satisfying the constraint\nThe achievable distortion-cost region presented here is similar to the scheme presented in [3], but we derive it via different means in [10], where we used a hybrid coding scheme proposed in [11]. We now state the following lower bound given in [3].\nTheorem 2: [3] A lower bound for the problem of Gaussian estimation with helper is given by\nWe now turn to our lower bound for this setting. For clarity, we ﬁrst present a proof of a special case of our lower bound before turning to the more general expression.\nProposition 1: A lower bound for the problem of Gaussian estimation with helper is given by\n√ P √ P\n√ P √ P\nIt should be noted that any γ ≥ 1 constitutes a lower bound for D(P ) min . Hence, Proposition 1 is computable and in fact, gives a family of lower bounds.\nProof Sketch (See [10] for details): The proof hinges on an application of a relationship between mismatched estimation and relative entropy given in [6, Equality 14]. The main idea behind the proof lies in considering a decoder that performs the estimation (and reconstruction) using a wrong (or mismatched) distribution for P S n 1 |Y n . In particular, we will consider a mismatched decoder that attempts to estimate S n 1\nassuming that X n ≡ 0. The estimation error incurred by the mismatched decoder, M SE Q , is clearly larger than that incurred by an optimum decoder that uses the correct (true) distribution, D(P ) min . We then rely on results in [6] to lower bound the difference between D(P ) min and M SE Q , thereby giving us a lower bound on D(P ) min .\nTo derive our bound, we ﬁrst consider a more general source S 1 ∼ N(0, 1/γ) and let S 2 ∼ N(0, P 2 ) as before. The value of γ that we are concerned about is γ = 1, which will appear later in the proof.\nand note that ˆ S 1 = αY is the Minimum Mean Square Error (MMSE) estimate of S 1 that the decoder would employ if it assumes that X n ≡ 0. We ﬁrst give a lower bound for M SE Q (γ). Using the fact that under the true distribution, E ||X n || 2 ≤ nP , we can show that\nNow, let ˜ S n = S n 2 + X n and let P ˜ S n denote the distribution of S n 2 + X n under the optimum encoding scheme. Let Q ˜ S n denote the corresponding distribution under the encoding scheme of X n ≡ 0. It can be shown that\nFurther, even when the decoder knows that ˜ S n is dis- tributed according to P ˜ S n , we still have\nM M SE(γ) := E ||S n 1 − E P (S n 1 |Y n ) || 2 = E || ˜ S n − E P ( ˜ S n |Y n ) || 2\nNote that nD(P ) min = M M SE(1). Next, using [6, Equality 14], we have\nHere, P Y n represents the distribution of Y n induced by P ˜ S n . Similarly, Q Y n represents the distribution of Y n induced by Q ˜ S n . It can be shown that\nfor γ 1 ≥ γ 0 . Using the fact that M M SE P, ˜ S n (γ) is a non-increasing function in γ and hence,\n(γ 1 − γ 0 )M M SE(γ 0 ), the bounds given in (1) and (5), setting γ 0 = 1 and the fact that divergence is non-negative then give us our lower bound.\nProposition 1 can be generalized by using a different mismatched distribution for X n , leading to our gen- eralized lower bound in Theorem 3. The mismatched distribution for X n is chosen as an i.i.d. sequence with X = cS 2 + Z, where c is any real number and Z ∼ N(0, rP ) with r ≥ 0 and independent of S 2 . Proof is given in [10].\nTheorem 3: A lower bound for the problem of Gaus- sian estimation with helper is given by\n(γ − 1)D(P ) min ≥ log( 1 + γP I 1 + P\nI ) − \t P 2 P\n+ ax ∗2 − bx ∗ , where a = \t 1 P\n \nComparison of bounds: Figure 3 shows the distortion versus P for P 2 = 10. As we see from the plots, Theorem 3 can be strictly better than Theorem 2 for a wide range of P . Comparisons with the improved version of Theorem 2, reported in [5], is given in the extended version [10].\nWe now turn our attention to the setting deﬁned in II-B. We ﬁrst state a number of results without proof.\nTheorem 4: (See also [9]) An acheivable distortion- cost region is given by\n \n \nThe next two results are lower bounds for this setting, one of which were established in [9]. For details, readers may see [9] or the proof of Theorem 5 below.\nProposition 2: [9]. A lower bound for the distortion- cost region is given by\nProposition 3: A lower bound for the distortion-cost region is given by\nWe now give our lower bound, which contains the bounds stated above.\nTheorem 5: A lower bound for the problem of esti- mation with a helper who knows both the interference and the signal noncausally is given by\n)N M SE(α)\nIt can be shown that setting α = 1 and α → ∞ recover the bounds in Propositions 2 and 3, respectively.\nWe note that while computation of the lower bound re- quires solving an optimization problem, the optimization problem is quadratic and can be efﬁciently solved [12].\nProof Sketch (See [10] for details): The idea behind the proof lies in giving S n 1 + αS n 2 to the decoder, and reﬁning the bound using linear estimation and convex optimization. It can be shown that, by giving S n 1 + αS n 2\nto the decoder, the following is a bound on the achievable distortion:\n2 log 2πeD(P ) min ≤ h(Y |S 1 + αS 2 ) − 1 2 log 2πeN.\nFocusing on the term h(Y |S 1 + αS 2 ), we can show that h(Y |S 1 + αS 2 )\nwhere k is deﬁned as the coefﬁcient of the linear minimum mean square estimator:\nE(X + (1 − α)S 2 + Z − k(S 1 + αS 2 )) 2 = P + (1 − α) 2 P 2 + 2(1 − α)ρ XS 2\n1 + α 2 P 2 := M SE(α, ρ XS 1 , ρ XS 2 ).\nNote now that for α ﬁxed, M SE(α, ρ XS 1 , ρ XS 2 ) is a concave (quadratic) function of ρ XS 1 and ρ XS 2 , and the constraints on ρ XS 1 and ρ XS 2 are linear constraints. Hence, we can ﬁnd the maximum value using convex optimization. Denoting the maximum by M SE(α), we arrive at the lower bound stated in the Theorem.\nComparison of bounds: We let P 1 = 1, N = 1 and P = 1, and vary P 2 and compare the bounds obtained with different values of P 2 . Figure 4 shows a plot of distortion v.s. P 2 for various bounds. As can be seen from Figure 4, the lower bound given by Theorem 5 can be strictly better than that given by previous lower bounds.\nFrom Figure 4, we see that the upper and lower bounds are close when P 2 is large. We now state a condition on P 2 for a constant multiplicative gap of between the upper and lower bounds.\nwith γ = (P +N ) 2P , 0 ≤ ≤ P P +N , then the multi- plicative gap between the upper bound in Theorem 4, D achievable , and the lower bound in Proposition 2, D lb , is at most 1/(1 − ). That is, D achievable /D lb ≤ 1/(1− ).\nWe thank Mr Gowtham Kumar and Professor Sriram Vishwanath for helpful discussions."},"refs":[{"authors":[{"name":"Y. H. Kim"},{"name":"A. Sutivong"},{"name":"T. Cover"}],"title":{"text":"State ampliﬁcation"}},{"authors":[{"name":"C. Choudhuri"},{"name":"Y.-H. Kim"},{"name":"U. Mitra"}],"title":{"text":"Causal state ampliﬁ- cation"}},{"authors":[{"name":"P. Grover"},{"name":"A. Sahai"}],"title":{"text":"Witsenhausen\u2019s counterexample as assisted interference suppression"}},{"authors":[{"name":"H. Witsenhausen"}],"title":{"text":"A counterexample in stochastic optimum control"}},{"authors":[{"name":"P. Grover"},{"name":"A. Wagner"},{"name":"A. Sahai"}],"title":{"text":"Information embedding and the triple role of control"}},{"authors":[{"name":"S. Verd´u"}],"title":{"text":"Mismatched estimation and relative entropy"}},{"authors":[{"name":"D. Guo"},{"name":"Y. Wu"},{"name":"S. Shamai"},{"name":"S. Verd´u"}],"title":{"text":"Estimation in Gaussian noise: Properties of the minimum mean-square error"}},{"authors":[{"name":"A. El Gama"},{"name":"Y. H. Ki"}],"title":{"text":"Network Information Theory, 1st ed"}},{"authors":[{"name":"Y.-C. Huang"},{"name":"K. Narayanan"}],"title":{"text":"Joint source-channel coding with correlated interference"}},{"authors":[{"name":"Y.-K. Chia"},{"name":"R. Soundararajan"},{"name":"T. Weissman"}],"title":{"text":"Estimation with a helper who knows the interference"}},{"authors":[{"name":"S. H. Lim"},{"name":"P. Minero"},{"name":"Y.-H. Kim"}],"title":{"text":"Lossy communication of correlated sources over multiple access channels"}},{"authors":[{"name":"S. Boy"},{"name":"L. Vandenbergh"}],"title":{"text":"Convex Optimization"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566641.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T8.2","endtime":"17:20","authors":"Yeow-Khiang Chia, Rajiv Soundararajan, Tsachy Weissman","date":"1341248400000","papertitle":"Estimation with a helper who knows the interference","starttime":"17:00","session":"S4.T8: Information and Estimation","room":"Stratton (491)","paperid":"1569566641"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
