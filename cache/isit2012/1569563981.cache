{"id":"1569563981","paper":{"title":{"text":"Gaussian Robust Sequential and Predictive Coding"},"authors":[{"name":"Lin Song"},{"name":"Jun Chen"},{"name":"Jia Wang"},{"name":"Tie Liu"}],"abstr":{"text":"Abstract\u2014We introduce two new source coding problems: robust sequential coding and robust predictive coding. For the Gauss-Markov source model, we characterize certain supporting hyperplanes of the rate region of these two coding problems. Our investigation also reveals a class of extremal inequalities and minimax theorems."},"body":{"text":"The sequential coding problem was ﬁrst introduced by Viswanathan and Berger in [1]. Due to its potential rele- vance to video coding applications, this problem has received renewed interests in recent years [2], [3]. In a sequential coding system, L sources X 1 , · · · , X L , each representing a video frame, are encoded and decoded in a causal manner, where Encoder i has access to X 1 , · · · , X i , i = 1, · · · , L, and Decoder i reconstructs X i based on the outputs from the ﬁrst i encoders, i = 1, · · · , L. If Encoder i is only allowed to have access to X i as well as the outputs from the ﬁrst i − 1 encoders (if i ≥ 2), then the resulting problem is known as predictive coding. It is shown in [4] that the rate regions of these two coding problems are identical if X 1 −X 2 −· · ·−X L form a Markov chain. Note that this Markov chain condition is trivially satisﬁed when L = 2.\nThe existing coding schemes for sequential coding and pre- dictive coding rely critically on the assumption that Decoder i has access to the ﬁrst i encoded frames (i.e., the outputs from the ﬁrst i encoders) when reconstructing the ith frame (i.e., X i ). As a consequence, these schemes are vulnerable to the loss of encoded frames at the decoder end. Motivated by this observation, we introduce a robust version of these two coding problems. Speciﬁcally, we require that Decoder i has to meet a certain ﬁdelity constraint even when it only has access to the ith encoded frame (i.e., the output from the ith encoder).\nThe remainder of this paper is organized as follows. We state our main results in Section II. Section III contains a class of extremal inequalities, which will play an important role in subsequent analysis. The proofs of some main results are given in Sections IV and V. We conclude the paper with a discussion of certain minimax theorems inspired by our analysis.\nThroughout this paper, for any random object W and 1 × n random vector X n we deﬁne σ 2 X n = 1 n E[X n (X n ) T ] and\nIn this work we focus on the special case where X 1 − X 2 − · · · − X L form a Gauss-Markov chain. With no essential loss of generality, we assume X i +1 = X i + ∆ i , i = 1, · · · , L −\n1, where X 1 , ∆ 1 , · · · , ∆ L −1 are mutually independent zero- mean Gaussian random variables with σ 2 X 1 > 0 and σ 2 ∆ i > 0, i = 1, · · · , L−1. Let {(X 1 (t), · · · , X L (t))} ∞ t =1 be i.i.d. copies of (X 1 , · · · , X L ).\nDeﬁnition 1: A rate vector R (R 1 , · · · , R L ) is said to be achievable with a sequential coding system subject to individ- ual distortion constraint d (d {1} , · · · , d {L} ) and hierarchical distortion constraint δ (δ {1,2} , · · · , δ {1,··· ,L} ) if there exist encoding functions f (n) i : R i ×n → C i , i = 1, · · · , L, such that\nwhere C i = f (n) i (X n 1 , · · · , X n i ), i = 1, · · · , L. The rate region R S (d, δ) is the closure of the set of all the rate vectors achievable with a sequential coding system subject to individual distortion constraint d and hierarchical distortion constraint δ.\nDeﬁnition 2: A rate vector R \t (R 1 , · · · , R L ) is said to be achievable with a predictive coding system subject to individual distortion constraint d \t (d {1} , · · · , d {L} ) and hierarchical distortion constraint δ (δ {1,2} , · · · , δ {1,··· ,L} ) if there exist encoding functions f (n) 1 : R n → C 1 and f (n) i : C 1 × · · · × C i −1 × R n → C i , i = 2, · · · , L, such that\nwhere C 1 = f (n) 1 (X n 1 ) and C i = f (n) i (C 1 , · · · , C i −1 , X n i ), i = 2, · · · , L. The rate region R P (d, δ) is the closure of the set of all the rate vectors achievable with a predictive system subject to individual distortion constraint d and hierarchical distortion constraint δ.\nWithout loss of generality, we assume 0 < d {i} ≤ σ 2 X i , i = 1, · · · , L, and 0 < δ {1,··· ,i} ≤ σ 2 X i , i = 2, · · · , L. Since both R S (d, δ) and R P (d, δ) are closed convex sets, it sufﬁces to characterize their supporting hyperplanes, i.e., to solve the following optimization problems\nwhere α = (α 1 , · · · , α L ) with α i ≥ 0, i = 1, · · · , L. In view of the fact that R P (d, δ) ⊆ R S (d, δ), we must have\nTo state the main results of this paper, we need to deﬁne the following function:\nwhere θ = (θ 1 , · · · , θ L −1 ). Furthermore, let κ l (α, d, δ) = \t sup\nψ (α, d, d {1,2} , · · · , d {1,··· ,L} , θ ), κ u (α, d, δ) = \t inf\nTheorem 1: For α with α 1 ≥ · · · ≥ α L ≥ 0, inf\nTheorem 2: For α with α i ≥ 0, i = 1, · · · , L, inf\nTheorem 3: For α with α 1 ≥ · · · ≥ α L ≥ 0, κ l (α, d, δ) = κ u (α, d, δ).\nThe proofs of Theorem 1 and Theorem 2 are given in Sec- tion IV and Section V, respectively. The proof of Theorem 3 is omitted. These theorems together provide a characterization of certain supporting hyperplanes of R S (d, δ) and R P (d, δ). In particular, setting α 1 = · · · = α L = 1 gives the minimum sum rate of these two rate regions. For the special case L = 2, one can verify that κ l (α, d, δ) and κ u (α, d, δ) have the following explicit expression:\n    \n   \nThe extremal inequality in the following theorem is a generalization of the one in [5, Lemma 1]. The proof is omitted.\nTheorem 4: Let N n i be a zero-mean Gaussian random vec- tor with i.i.d. entries of variance σ 2 N i , i = 1, 2, 3; it is assumed that σ 2 N 2 ≥ σ 2 N 3 . Let µ 1 , µ 2 , β 1 , β 2 , and δ be arbitrary real numbers satisfying µ 1 ≥ µ 2 ≥ 0 and δ > 0. Then for any random vector S n and random object W , jointly independent of (N n 1 , N n 2 , N n 3 ), such that σ 2 S n |W ≤ δ, we have\nβ 2 2 d + σ 2 N 3 . Remark:\n1) One can readily show by setting β 1 = 0 and β 2 = 1 that\nFurthermore, choosing µ 1 = 1 and µ 2 = 0 yields the following elementary result\n2) It can be shown by setting µ 1 = β 1 = 1 and µ 2 = 0 that\nd \t , \t (3) where the minimum is achieved at d = δ. This is a variant of the well-known worst additive noise lemma by Ihara [6] as well as Diggavi and Cover [7, Lemma II.2].\nThe next result can be viewed as a variant of [8, Theorem 5]. The proof is omitted.\nTheorem 5: Let N n i be a zero-mean Gaussian random vec- tor with i.i.d. entries of variance σ 2 N i , i = 1, 2, 3. Let δ be an arbitrary positive real number. Then for any random vector S n and random object W , jointly independent of (N n 1 , N n 2 , N n 3 ), such that σ 2 S n |W ≤ δ, we have\nh (S n + N n 1 |W ) − h(S n + N n 2 |W ) − h(S n + N n 3 |W ) ≤ max\nTheorem 4 will play an instrumental role in the proof of Theorem 1. Although Theorem 5 is not needed for proving our main results (i.e., Theorems 1, 2, and 3), it provides valuable insight into the choice of auxiliary random vectors in Section IV and will be used to establish the strengthened version of the minimax theorem in Section VI.\nOur proof relies on Theorem 4 as well as the techniques developed in [5], [9], [10].\nLet f (n) i : R i ×n → C i , i = 1, · · · , L, be L encoding functions such that\nn [I(C 1 , · · · , C i −1 ; C i ) + I(X n i ; C 1 , · · · , C i ) − I(X n i ; C 1 , · · · , C i −1 )].\nLet Z n i be a zero-mean Gaussian random vector with i.i.d. entries of variance σ 2 Z i , i = 1, · · · , L−1; moreover, we assume Z n i is independent of (X n i +1 , C 1 , · · · , C i +1 ), i = 1, · · · , L−1. Note that\n= I(X n i + Z n i −1 ; C 1 , · · · , C i −1 ) + I(X n i + Z n i −1 ; C i ) + I(C 1 , · · · , C i −1 ; C i |X n i + Z n i −1 ) − I(X n i + Z n i −1 ; C 1 , · · · , C i )\n≥ I(X n i + Z n i −1 ; C 1 , · · · , C i −1 ) + I(X n i + Z n i −1 ; C i ) − I(X n i + Z n i −1 ; C 1 , · · · , C i ), i = 2, · · · , L.\ni −1 ; C 1 , · · · , C i −1 ) + I(X n i + Z n i −1 ; C i ) − I(X n i + Z n i −1 ; C 1 , · · · , C i ) + I(X n i ; C 1 , · · · , C i ) − I(X n i ; C 1 , · · · , C i −1 ))\nd {1,··· ,i} + σ 2 ∆ i \t , i = 2, · · · , L − 1. (5)\n(6) h (X n L + Z n L −1 |C 1 , · · · , C L ) − h(X n L |C 1 , · · · , C L )\nwhere (6), (7), and (8) are due to (1), (3), and (2), respectively. Substituting (5), (6), (7), (8) into (4) yields 1\n1 n\nThe proof is complete in view of the fact that this lower bound holds for arbitrary θ i ∈ (0, σ 2 X i+1 ), i = 1, · · · , L − 1.\nIt sufﬁces to prove that for any d {1,··· ,i} ∈ (0, δ {1,··· ,i} ), i = 2, · · · , L,\nFor simplicity, we assume that d {i} < σ 2 X i , i = 1, · · · , L, and δ {1,··· ,i} < σ 2 X i , i = 2, · · · , L. The general case where some d {i} and/or δ {1,··· ,i} are equal to σ 2 X i can be handled via a continuity argument.\nThe following lemma is a variant of [5, Lemma 3] and its proof is omitted.\nLemma 1: There exist d (d {1} , · · · , d {L} ) and d {1,··· ,i} , i = 2, · · · , L, with\nAn inner bound of R P (d, δ) is stated in the following lemma, which can be proved via the standard random coding argument.\nLemma 2: For any (U {1} , · · · , U {L} ) jointly Gaussian with (X 1 , · · · , X L ) such that\n\u2022 (X j ) j =i − (X i , U {1} , · · · , U {i−1} ) − U {i} form a Markov chain, i = 2, · · · , L,\n≤ d {i} , i = 1, · · · , L, and σ 2 X i |U {1} , ··· ,U {i} ≤ δ {1,··· ,i} , i = 2, · · · , L,\nNow we proceed to construct (U {1} , · · · , U {L} ) with certain desired properties. We assume E[U {i} ] = 0, i = 1, · · · , L. For d {i} , i = 1, · · · , L, and d {1,··· ,i} , i = 2, · · · , L, speciﬁed in Lemma 1, deﬁne\nLet U {1} be jointly Gaussian with (X 1 , · · · , X L ) such that E[U 2 {1} ] = E[X 1 U {1} ] = σ 2 X 1 − d {1}\nand (X 2 , · · · , X L ) − X 1 − U {1} form a Markov chain. Now successively from i = 2 to L, let U {i} be jointly Gaussian with (X 1 , · · · , X L , U {1} , · · · , U {i−1} ) such that\nand ((X j ) j =i , U {1} , · · · , U {i−1} ) − (X i , U {1,··· ,i−1} ) − U {i} form a Markov chain, where we deﬁne U {1,··· ,k} = E[X k +1 |U {1,··· ,k−1} , U {k} ], k = 2, · · · , L−1. It is easy to see that the constructed (U {1} , · · · , U {L} ) satisﬁes the conditions in Lemma 2; as a consequence,\nψ (α, d , d {1,2} , · · · , d {1,··· ,L} , θ ) ≤ \t max\n(12) where (12) follows from Lemma 1. Combining (10), (11), and (12) completes the proof.\nWe have characterized certain supporting hyperplanes of the rate region of robust sequential coding and robust predictive coding for the Gauss-Markov source model. Note that our main results can be viewed as a manifestation of certain information-theoretic minimax theorems. Indeed, the proofs of Theorems 1, 2, and 3 can be leveraged to establish the following result. We assume that X n \t (X n 1 , · · · , X n L ), (d {1} , · · · , d {L} ), (δ {1,2} , · · · , δ {1,··· ,L} ) are deﬁned as in Section I and α 1 ≥ · · · ≥ α L ≥ 0.\nTheorem 6: Let P U denote the set of conditional distri- butions p U |X n with U \t (U 1 , · · · , U L ) such that X n i +1 − X n i − (U 1 , · · · , U i ) form a Markov chain, i = 1, · · · , L − 1,\ni = 2, · · · , L. Let Z n i be a zero-mean Gaussian random vector with i.i.d. entries of variance σ 2 Z i , i = 1, · · · , L − 1; more- over, we assume Z n i is independent of (X n i +1 , U 1 , · · · , U i +1 ), i = 1, · · · , L − 1. Deﬁne σ 2 Z = (σ 2 Z 1 , · · · , σ 2 Z L −1 ) and\n+ I(X n i + Z n i −1 ; U i ) − I(X n i + Z n i −1 ; U 1 , · · · , U i ) + I(X n i ; U 1 , · · · , U i ) − I(X n i ; U 1 , · · · , U i −1 )).\nIn fact, Theorem 5 implies that the following strengthened version is also true, which explains why our choice of Gaus-\nsian auxiliary random vectors Z n 1 , · · · , Z n {L−1} in the proof of Theorem 1 leads to a tight lower bound.\nTheorem 7: Let V \t (V 1 , · · · , V L ) be jointly distributed with (X n , U ) such that V i − X n i +1 − (U 1 , · · · , U i +1 ) form a Markov chain, i = 1, · · · , L − 1. Deﬁne\n) instead of f (p U |X n , p V |X n ,U ) because f depends on p V |X n ,U only through p V i |X n\nThe work of Lin Song and Jun Chen was supported in part by an Early Research Award from the Province of Ontario and in part by the Natural Science and Engineering Research Council (NSERC) of Canada under a Discovery Grant. The work of Jia Wang was supported in part by the NSFC under Grant 60802020 and in part by the 973 Program (2010CB731401, 2010CB731406). The work of Tie Liu was supported by the National Science Foundation under Grant CCF-09-16867."},"refs":[{"authors":[{"name":"H. Viswanathan"},{"name":"T. Berger"}],"title":{"text":"Sequential coding of correlated sources"}},{"authors":[{"name":"N. Ma"},{"name":"P. Ishwar"}],"title":{"text":"On delayed sequential coding of correlated sources"}},{"authors":[{"name":"E.-H. Yang"},{"name":"L. Zheng"},{"name":"D.-K. He"},{"name":"Z. Zhang"}],"title":{"text":"Rate distortion theory for causal video coding: characterization, computation algorithm, and comparison"}},{"authors":[{"name":"J. Wang"},{"name":"X. Wu"}],"title":{"text":"Information ﬂows in video coding"}},{"authors":[{"name":"J. Chen"}],"title":{"text":"Rate region of Gaussian multiple description coding with individual and central distortion constraints"}},{"authors":[{"name":"S. Ihara"}],"title":{"text":"On the capacity of channels with additive non-Gaussian noise"}},{"authors":[{"name":"S. N. Diggavi"},{"name":"T. M. Cover"}],"title":{"text":"The worst additive noise under a covariance constraint"}},{"authors":[{"name":"J. Chen"},{"name":"J. Wang"}],"title":{"text":"Vector Gaussian multiple description coding with individual and central distortion constraints"}},{"authors":[{"name":"L. Ozarow"}],"title":{"text":"On a source coding problem with two channels and three receivers"}},{"authors":[{"name":"H. Wang"},{"name":"P. Viswanath"}],"title":{"text":"Vector Gaussian multiple description with individual and central receivers"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569563981.pdf"},"links":[{"id":"1569566381","weight":4},{"id":"1569566485","weight":4},{"id":"1569566725","weight":14},{"id":"1569565377","weight":4},{"id":"1569565867","weight":4},{"id":"1569565067","weight":4},{"id":"1569559665","weight":4},{"id":"1569566875","weight":4},{"id":"1569566981","weight":9},{"id":"1569559259","weight":9},{"id":"1569566591","weight":4},{"id":"1569552245","weight":23},{"id":"1569566415","weight":4},{"id":"1569566469","weight":4},{"id":"1569566765","weight":9},{"id":"1569565547","weight":4},{"id":"1569565461","weight":4},{"id":"1569564227","weight":4},{"id":"1569565837","weight":4},{"id":"1569563411","weight":9},{"id":"1569566821","weight":4},{"id":"1569556713","weight":4},{"id":"1569565771","weight":4},{"id":"1569566157","weight":4},{"id":"1569560613","weight":4},{"id":"1569566903","weight":4},{"id":"1569566579","weight":4},{"id":"1569558483","weight":4},{"id":"1569566173","weight":4},{"id":"1569565347","weight":4},{"id":"1569565455","weight":4},{"id":"1569566795","weight":4},{"id":"1569566709","weight":4},{"id":"1569566523","weight":9},{"id":"1569564189","weight":4},{"id":"1569566985","weight":4},{"id":"1569564613","weight":4},{"id":"1569566167","weight":4},{"id":"1569561085","weight":4},{"id":"1569566759","weight":4},{"id":"1569559995","weight":4},{"id":"1569566511","weight":9},{"id":"1569561143","weight":14},{"id":"1569565833","weight":4},{"id":"1569564611","weight":4},{"id":"1569565535","weight":4},{"id":"1569566325","weight":9},{"id":"1569566811","weight":9},{"id":"1569566851","weight":4},{"id":"1569553909","weight":9},{"id":"1569566939","weight":4},{"id":"1569552251","weight":14},{"id":"1569566209","weight":9},{"id":"1569565559","weight":4},{"id":"1569566371","weight":9},{"id":"1569565151","weight":4},{"id":"1569558985","weight":14},{"id":"1569566473","weight":4},{"id":"1569566913","weight":4},{"id":"1569566809","weight":9},{"id":"1569566257","weight":4},{"id":"1569565033","weight":9},{"id":"1569566447","weight":4},{"id":"1569563897","weight":4},{"id":"1569565887","weight":4},{"id":"1569565929","weight":9},{"id":"1569566721","weight":4},{"id":"1569566037","weight":4},{"id":"1569565029","weight":4},{"id":"1569565357","weight":23},{"id":"1569561245","weight":9},{"id":"1569565363","weight":4},{"id":"1569566695","weight":4},{"id":"1569565909","weight":4},{"id":"1569555787","weight":4},{"id":"1569566673","weight":4},{"id":"1569565739","weight":4},{"id":"1569565311","weight":9},{"id":"1569566233","weight":9},{"id":"1569566297","weight":4},{"id":"1569564097","weight":4},{"id":"1569565741","weight":9},{"id":"1569566387","weight":4},{"id":"1569565439","weight":4},{"id":"1569563395","weight":4},{"id":"1569551347","weight":4},{"id":"1569565415","weight":14},{"id":"1569555367","weight":9},{"id":"1569566383","weight":4},{"id":"1569565571","weight":4},{"id":"1569565885","weight":4},{"id":"1569566929","weight":4},{"id":"1569565611","weight":4},{"id":"1569566983","weight":4},{"id":"1569566479","weight":4},{"id":"1569565397","weight":9},{"id":"1569566129","weight":14},{"id":"1569565215","weight":4},{"id":"1569565385","weight":4},{"id":"1569564131","weight":4},{"id":"1569561221","weight":4},{"id":"1569566691","weight":4},{"id":"1569566823","weight":4},{"id":"1569552025","weight":4},{"id":"1569566237","weight":4},{"id":"1569566283","weight":4},{"id":"1569565375","weight":9},{"id":"1569566713","weight":14},{"id":"1569565541","weight":4},{"id":"1569564787","weight":4},{"id":"1569556759","weight":4},{"id":"1569561185","weight":47},{"id":"1569566301","weight":4},{"id":"1569558779","weight":19},{"id":"1569565669","weight":4},{"id":"1569566001","weight":4},{"id":"1569560235","weight":4},{"id":"1569566817","weight":28},{"id":"1569566911","weight":4},{"id":"1569564923","weight":4},{"id":"1569566299","weight":23},{"id":"1569564769","weight":4},{"id":"1569565805","weight":14},{"id":"1569563919","weight":14},{"id":"1569566577","weight":4},{"id":"1569557851","weight":9},{"id":"1569566847","weight":4},{"id":"1569564961","weight":4},{"id":"1569559251","weight":4},{"id":"1569567013","weight":14},{"id":"1569566583","weight":4},{"id":"1569565337","weight":4},{"id":"1569564253","weight":4},{"id":"1569550425","weight":4},{"id":"1569563725","weight":4},{"id":"1569564505","weight":9},{"id":"1569565635","weight":14},{"id":"1569556327","weight":4},{"id":"1569564931","weight":4},{"id":"1569566987","weight":4},{"id":"1569551751","weight":4},{"id":"1569564419","weight":9},{"id":"1569566443","weight":4},{"id":"1569560581","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S8.T4.1","endtime":"17:00","authors":"Lin Song, Jun Chen, Jia Wang, Tie Liu","date":"1341333600000","papertitle":"Gaussian Robust Sequential and Predictive Coding","starttime":"16:40","session":"S8.T4: Multiple Description Coding","room":"Stratton 20 Chimneys (306)","paperid":"1569563981"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
