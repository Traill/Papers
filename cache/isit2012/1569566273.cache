{"id":"1569566273","paper":{"title":{"text":"Properties and encoding aspects of direct product convolutional codes"},"authors":[{"name":"Vladimir Sidorenko,"},{"name":"Francesca Vatta"}],"abstr":{"text":"Abstract\u2014In this paper we investigate the properties of the generator matrices of a new class of convolutional codes, called product convolutional codes, which were previously deﬁned and investigated by the authors. The new codes are constructed using the well-known method of the direct product for combining block codes. Convolutional codes are considered as block codes over the ﬁeld of rational functions F (D). The description of convolutional codes as block codes allows the successful application of the direct product method to convolutional codes, and, in addition, leads to a general method to construct new convolutional codes based on already known combining methods for block codes. Expressions for the generator matrices of the product convolutional codes are given and several of their properties, which were not addressed before, are determined. The relationship between the properties of the direct product encoder generator matrix and the properties of the vertical and horizontal constituent encoders generator matrices is derived. Rational generator matrices, as well as polynomial generator matrices, are addressed."},"body":{"text":"Powerful codes can be designed by combining already known codes. The new codes can have several advantageous features such as large distance, capability of burst and random error correction, large blocklength and low-density parity check matrices. In order to be useful, these new codes must have a structure suitable for decoding using techniques with low complexity such as iterative decoding.\nFor block codes many combining methods are suggested and analyzed [1]. One of the earliest methods, product or iterated codes, was proposed by Elias [2] in 1954. Product of block codes gives a block code which has not the best distance. However, in this way long codes with large distance can be constructed and efﬁciently decoded. The algebraic structure of these codes was investigated in the literature so that their performance can be analytically determined without simulations. This is important for optical transmission and other similar applications, where the block error rate might be so small that it is not possible to estimate it by simulations.\nThe same advantages still hold for the product of convolu- tional codes if the methods of combining block codes can be\napplied for convolutional codes combination as well, as done in [3] where a more general work on this subject was started. For convolutional codes only a few combining methods have been suggested. Besides the well-known parallel [4] and serial [5] concatenation of convolutional codes, perhaps one of the most versatile and interesting methods, woven convolutional codes, was considered in [6]. In our work, we have taken a different approach to construct a new class of convolutional codes, called product convolutional codes, which has been deﬁned and investigated in [7]. The new codes have been constructed using the well-known method of the direct product for combining block codes. Convolutional codes were consid- ered as block codes over the ﬁeld of rational functions F (D) ([8]) and, thus, the new concept of block weight and block distance of a convolutional code was deﬁned in [3]. There, it was shown that the free distance of the product code equals the product of component codes free distances if at least one of the component codes block distance equals its free distance.\nAfterwards, some algorithms for the termination and tailbit- ing of direct product convolutional codes were proposed and investigated in [9] for the rate 1/n case and in [10] for the rate- k/n case, assuming to encode these codes by feedback convolutional encoders realized in controller canonical form.\nIn [9] and [10], however, the relationship between the canonicality of the constituent encoders generator matrices and the canonicality of the direct product encoder generator matrix was not investigated. Moreover, in [7] were given some theorems, with no proof, stating the relationship between the properties of the constituent encoders and the direct product encoder polynomial generator matrices. Thus, purpose of this paper is to give proof of theorems in [7] and to investigate on the relationship between the canonicality of the constituent encoders generator matrices and the canonicality of the direct product encoder generator matrix.\nThe paper is organized as follows. In Section II, we give some deﬁnitions, showing how a direct product convolutional code can be obtained from its vertical and horizontal con- stituent encoders. In Section III, we recall some properties of the generator matrices. In Section IV, we investigate on the properties of the direct product encoder generator matrix\nand the relationship between the properties of the constituent encoders and those of the direct product encoders obtained from their concatenation. Finally, Section V summarizes and concludes the paper.\nUsing Forney\u2019s algebraic theory of convolutional codes [8] we consider convolutional codes as block codes over a ﬁeld of rational functions. Let F be a ﬁeld, and F (D) denote the ﬁeld of rational functions over F in the indeterminate D. Each non-zero element a(D) of F (D) can be represented as a ratio of polynomials over F , a(D) = p(D)/q(D), where p(D) and q(D) are relatively prime.\nSimilar to linear block codes, we consider the following deﬁnition of a convolutional code [8] and of its generator matrix.\nDeﬁnition 1 : An (n, k) convolutional code C over a ﬁeld F is a k-dimensional subspace of F (D) n , i.e., a subspace of the vector space of n-tuples over the rational ﬁeld F (D).\nThe rate R of the (n, k) convolutional code C is deﬁned to be R = k/n.\nDeﬁnition 2 : A k × n matrix G(D) over F (D) whose rows form a basis of an (n, k) convolutional code C is called a generator matrix of the code.\nIn our work, we consider only binary convolutional codes, i.e., F = GF (2).\nLet v (D) = {v 1 (D), · · · , v n (D)} be a codeword of an (n, k) convolutional code and let u(D) = {u 1 (D), · · · , u k (D)} be the information word. Then, the encoding rule, similar to block codes, is\nDeﬁnition 3 : Let C | and C − be convolutional (n | , k | ) and (n − , k − ) codes. Then, the direct product convolutional code C | ⊗ C − is deﬁned to be the code whose codewords consist of all n | × n − arrays in which the columns belong to C | and the rows to C − .\nDeﬁnition 4 : Let C ⊗ be a direct product convolutional code generated from the concatenation of two component convolutional codes C − of rate R − = k − /n − (horizontal code) and C | of rate R | = k | /n | (vertical code). The rate R ⊗ of this direct product code is then R ⊗ = k | k − n | n − = R | R − .\nThe generator matrix G | (D) of the vertical component encoder and the generator matrix G − (D) of the horizontal component encoder are described by:\n     \n     \n    \n    \n \n \nbe the matrix which describes the information sequences to be encoded.\nWe can perform a Row-Column encoding, ﬁrst using the horizontal encoder to encode the rows of U (D), obtaining V (D) = U (D) G − (D). Then the vertical encoder is used to encode the columns of V (D), obtaining the code sequence [7]\nWe can also apply Column-Row encoding and get the same matrix by matrix multiplication in another order:\nFrom (5) it follows that every column of C ⊗ (D) belongs to C | , and, from (6), that every row of C(D) belongs to C − . Hence, by Deﬁnition 3, we obtain a codeword C ⊗ (D) ∈ C | ⊗ C − . Denote by C ⊗ the set of matrices C ⊗ (D) obtained by encoding all matrices U (D) using (5). In the following, we show that C ⊗ is an (n | n − , k | k − ) convolutional code, and C ⊗ = C | ⊗ C − , by verifying that every C ⊗ (D) ∈ C ⊗ satisﬁes Deﬁnition 1, and all matrices C ⊗ (D) that satisfy Deﬁnition 3 are included in C ⊗ .\nFirst, let us introduce the operator row and consider the Kronecker or direct product of two matrices.\nDeﬁnition 5 : The matrix operation of ordering the rows of a matrix one next to the other to form a single row vector is called row and the notation is row(A), where A is a matrix.\nDeﬁnition 6 : Let A be an m × n matrix and let B be a p × q matrix. Then the Kronecker product of A and B is that (mp) × (nq) deﬁned by\n   \na 1,1 B a 1,2 B · · · a 1,n B a 2,1 B a 2,2 B · · · a 2,n B .. . \t .. . \t .. .\n   \nThe following property of the Kronecker product can be derived from [11]:\nLet us return to the product convolutional code C ⊗ = C | ⊗ C − and consider replacing every matrix C ⊗ (D) ∈ C ⊗ by a vector c ⊗ (D) = row(C ⊗ (D)) to obtain a code C ⊗ of vectors.\nTake the encoding procedure (5) for the product code and apply the operator row. Then, using the notation u (D) = row(U (D)), the encoding procedure (5) for the product code can be rewritten in traditional vector form\nwhere the generator matrix G ⊗ (D) is the generator matrix of the product convolutional code C ⊗ . G ⊗ (D) is given by the following theorem.\nTheorem 1: The generator matrix G ⊗ (D) of a product convolutional code C ⊗ = C | ⊗ C − is given by the Kronecker product of the generator matrices of the constituent codes\nG | (D) and G − (D) are the generator matrices of the constituent codes C | and C − , respectively.\nProof: Let C ⊗ (D) be a codeword of a product convolu- tional code C ⊗ = C | ⊗ C − . Using (5), C ⊗ (D) may be written as a vector c (D) using the operator row. Thus:\nBy Deﬁnition 2, the generator matrices G | (D) and G − (D) of the constituent codes have full rank. Since G | (D) and G − (D) have elements from F (D), i.e., they are rational matrices, from (10) it follows that G ⊗ (D) is also a rational k | k − × n | n − matrix of full rank. Therefore, the code C ⊗ generated by G ⊗ (D) is a k | k − -dimensional linear subspace of {F (D)} n | n − . This means that all matrices C ⊗ (D) that satisfy Deﬁnition 3 are included in C ⊗ .\nFinally, we conclude that the code C ⊗ generated by (5) or (9) is\n\u2022 the direct product of convolutional codes C | and C − ; \u2022 an (n | n − , k | k − ) convolutional code.\nWe call the code C ⊗ a product convolutional code. Following Theorem 1, G ⊗ (D) is given by:\n        \n        \n(13) or also by:\n     \n     \n     \n     \nThe properties of the generator matrix G ⊗ (D) of a product convolutional code (10) can be derived from the structural properties of the generator matrices of the constituent codes. In the following, we recall some deﬁnitions and properties from [12] of the generator matrix G(D) of an (n, k) convolutional code and determine the properties of G ⊗ (D).\n  \n  \nbe a generator matrix for an (n, k) convolutional code C where g i,j (D) and q i (D) are polynomials with gcd(g i, 1 (D), · · · , g i,n (D), q i (D) = 1. The following properties hold:\n1) G(D) is a rational generator matrix if at least one q i (D) has degree larger than 1.\n2) If q i (D) is delay free, i.e., q i (0) = 0, ∀i, then the generator matrix is called realizable, i.e., it is realizable in controller or observer canonical form.\n3) If all the entries of G(D) are polynomials, then G(D) is called a polynomial generator matrix for C.\n4) A generator matrix G(D) for an (n, k) convolutional code C is said to be systematic if the information sequence u(D) is an unchanged part of the encoded sequence v(D) = u(D)G(D).\n5) Two generator matrices G(D) and G (D) are equivalent if they encode the same code. Two generator matrices are equivalent iff G (D) = T (D)G(D), where T (D) is a rational and invertible k × k matrix.\n6) A generator matrix G(D) is called encoding matrix if G(0) has full rank. An encoding matrix is realizable and delay free. In addition, it has a realizable right inverse G −1 (D).\n7) A convolutional generator encoding matrix is called basic if it is polynomial and it has a polynomial right inverse. A convolutional encoder is called basic if its generator matrix is basic.\n8) A minimal basic encoding matrix is a basic encoding matrix whose overall constraint length ν is minimal over all equivalent basic encoding matrices.\n9) A canonical generator matrix is a rational encoding matrix whose overall constraint length ν is minimal over all equivalent rational generator matrices.\nSome well known parameters of a k × n generator matrix G(D), using the notation in [12], are:\n1) The constraint length ν i of the i-th row of G(D) is the maximum degree of all the numerator and denominator polynomials of this row:\nν i ≡ max{deg(g i, 1 (D)), · · · , deg(g i,n (D), deg(q i (D))}. (17)\n2) The overall constraint length ν is the sum over all constraint lengths\nUsing these deﬁnitions we can state the following properties of the generator matrices of product convolutional codes.\nTheorem 2 : Let G | (D) and G − (D) be polynomial generator matrices with memory m | and m − and overall constraint length ν | and ν − , respectively. The memory m ⊗ and overall constraint length ν ⊗ of the generator matrix G ⊗ (D) of the product convolutional code are\nProof: Since the resulting matrix G ⊗ (D) is the direct product of the component matrices (10), the constraint length of the i-th row of G ⊗ (D) is ν ⊗ i = ν | p +ν − q , for the correspond- ing rows p and q of G | (D) and G − (D), respectively. Thus, the memory m ⊗ of the product convolutional code generator matrix G ⊗ (D) is given by:\n{(ν | p + ν − q )} = max\nTheorem 3 : If G | (D) and G − (D) are rational delay free generator matrices, the generator matrix of the product con- volutional code G ⊗ (D) is delay free.\nProof: By deﬁnition of the direct product of two matrices (10), every entry q ⊗ i (D) of G ⊗ (D) (15) is the product of the corresponding entries of G | (D) and G − (D) (14), i.e., q ⊗ i (D) = q | p (D) q − r (D). If G | (D) and G − (D) are rational delay free generator matrices, then q | p (0) = 0, ∀p, and q − r (0) = 0, ∀r. Thus, ∀i q ⊗ i (0) = 0, i.e., G ⊗ (D) is delay free.\nTheorem 4 : If G | (D) and G − (D) are systematic generator matrices, the generator matrix of the product convolutional code G ⊗ (D) is systematic.\nProof: Let G | (D) = (I k | , R | (D) and G − (D) = (I k − , R − (D), where I k | and I k − are k | × k | and k − × k − identity matrices, and R | (D) and R − (D) are k | × (n | − k | ) and k − × (n − − k − ) matrices with rational elements.\nFollowing the encoding procedure given by (5), it is clear that for any information k | × k − matrix U (D) (4) over F (D), the resulting codeword n | × n − matrix C(D) (5) includes a k | × k − submatrix in the ﬁrst position which corresponds exactly to U (D). Then, it is obvious that the generator matrix G ⊗ (D) of the product code is a systematic generator matrix.\nTheorem 5 : Let G ⊗ 1 (D) = G | 1 (D) ⊗ G − 1 (D) and G ⊗ 2 (D) = G | 2 (D) ⊗ G − 2 (D) be rational generator matrices. If G | 1 (D) is equivalent to G | 2 (D) and G − 1 (D) is equivalent to G − 2 (D), then G ⊗ 1 (D) and G ⊗ 2 (D) are equivalent.\nT − (D) G − 2 (D) where T | (D) and T − (D) are nonsingular matrices of size k | × k | and k − × k − , respectively. Thus,\n= (T | (D) ⊗ T − (D))(G | 2 (D) ⊗ G − 2 (D)) = T ⊗ (D)G ⊗ 2 (D)\n(24) T | (D) and T − (D) are nonsingular. Then, T ⊗ (D) =\nT | (D) ⊗ T − (D) is nonsingular. In fact, |T ⊗ (D)| = |T | (D) | k | |T − (D) | k − = 0. Thus, (T ⊗ (D)) −1 exists and T ⊗ (D) is nonsingular.\nSince T ⊗ (D) is a k | k − ×k | k − nonsingular matrix, G ⊗ 1 (D) is equivalent to G ⊗ 2 (D).\nTheorem 6 : If G | (D) and G − (D) are basic encoding matrices, the generator matrix of the product convolutional code G ⊗ (D) is a basic encoding matrix.\nProof: If G | (D) and G − (D) are polynomial, then G ⊗ (D) is polynomial. If G | (D) and G − (D) are polynomial and have polynomial inverses (basic), then G ⊗ (D) has a polynomial inverse. In fact, being G ⊗ (D) = G | (D)⊗G − (D), it follows that G ⊗ (D) −1 = G | (D) −1 ⊗ G − (D) −1 .\nTheorem 7 : If G | (D) and G − (D) are minimal basic encod- ing matrices, the generator matrix of the product convolutional code G ⊗ (D) is a minimal basic encoding matrix.\nProof: Let G(D) be a basic encoding matrix, and [G(D)] h be a (0, 1)-matrix with 1 in the position (i, j) where deg(g i,j (D)) = ν i (see (17)) and 0 otherwise.\nIf G | (D) and G − (D) are minimal basic, [G | (D)] h and [G − (D)] h have full rank [12]. By the deﬁnition of the Kro- necker product [7], the maximum degree in every row of the resulting matrix G ⊗ (D) will appear in the elements that are the product of the elements of highest degree in each row corresponding to G | (D) and G − (D). Then,\n(26) Thus, [G ⊗ (D)] h has full rank. Therefore, G ⊗ (D) is mini-\nA minimal basic encoding matrix has the following impor- tant properties [12]:\n3) Every minimal basic encoding matrix is a minimal encoding matrix.\n4) The controller canonical form of a minimal basic encod- ing matrix is a minimal encoder.\nThus, if we construct a product convolutional code from two codes, encoded with minimal basic encoding matrices, the resulting generator matrix is a non-catastrophic encoding matrix, the controller canonical form of the encoding matrix is a minimal encoder and the corresponding Forney trellis of the code is a minimal trellis.\nSince the position of minimal basic encoding matrices within the class of polynomial generator matrices corresponds to that of canonical encoding matrices within the class of ratio- nal generator matrices [12], we state and prove the following theorem, too.\nTheorem 8 : If G | (D) and G − (D) are canonical generator matrices, the generator matrix of the product convolutional code G ⊗ (D) is a canonical generator matrix.\nProof: A canonical generator matrix G(D) is a rational generator matrix whose overall constraint length ν is minimal over all equivalent rational generator matrices.\nIf G | (D) and G − (D) are canonical generator matrices, their overall constraint lengths ν | and ν − are minimal over all\ntheir equivalent rational generator matrices. Thus, being the overall constraint length ν ⊗ of G ⊗ (D) = G | (D) ⊗ G − (D) given by (21) in Theorem 2, and given that, if the constituent encoders are equivalent, also the corresponding direct product encoders are equivalent (Theorem 5), it follows that if G | (D) and G − (D) are canonical generator matrices also G ⊗ (D) is canonical.\nA canonical generator matrix has the following important properties [12]:\n\u2022 A canonical generator matrix is a canonical encoding matrix.\nThe algebraic description of convolutional codes allows us to apply the various methods for combining block codes to convolutional codes. In this work, one of these combining methods, the direct product, has been considered. We have given a formal and simple deﬁnition of a product convolutional code. In addition, it was possible to determine expressions for the generator matrices in a simple form. The encoder prop- erties were derived based on the properties of the horizontal and vertical constituent codes."},"refs":[{"authors":[{"name":"F. J. MacWilliams"},{"name":"N. J. Sloane"}],"title":{"text":"The theory of Error Correcting Codes"}},{"authors":[{"name":"P. Elias"}],"title":{"text":"Error Free Coding"}},{"authors":[{"name":"V. Sidorenko"},{"name":"C. Medina"},{"name":"M. Bossert"}],"title":{"text":"From Block to Convolutional Codes using Block Distances"}},{"authors":[{"name":"C. Berrou"},{"name":"A. Glavieux"},{"name":"P. Thitimajshima"}],"title":{"text":"Near Shannon limit error correcting coding and decoding: Turbo codes"}},{"authors":[{"name":"S. Benedetto"},{"name":"D. Divsalar"},{"name":"G. Montorsi"},{"name":"F. Pollara"}],"title":{"text":"Serial concate- nation of interleaved codes: performance analysis, design and iterative decoding"}},{"authors":[{"name":"S. H ¨ost"},{"name":"R. Johannesson"},{"name":"V. Zyablov"}],"title":{"text":"Woven Convolutional Codes I: Encoder Properties"}},{"authors":[{"name":"M. Bossert"},{"name":"C. Medina"},{"name":"V. Sidorenko"}],"title":{"text":"Encoding and distance estimation of product convolutional codes"}},{"authors":[{"name":"G. D. Forney"}],"title":{"text":"Convolutional codes 1: Algebraic structure"}},{"authors":[{"name":"F. Vatta"},{"name":"A. Schiavi"},{"name":"V. Sidorenko"},{"name":"M. Bossert"}],"title":{"text":"Termination and tailbiting of direct product convolutional codes"}},{"authors":[{"name":"F. Vatta"},{"name":"V. Sidorenko"},{"name":"M. Bossert"}],"title":{"text":"Termination and tailbiting of rate-k/n direct product convolutional codes"}},{"authors":[{"name":"W.-H. Stee"}],"title":{"text":"Kronecker Product of Matrices and Applications, BI- Wissenschaftverlag, Mannheim, Wien, Zurich, 1991"}},{"authors":[{"name":"R. Johannesso"},{"name":"K. Sh"}],"title":{"text":"Zigangirov, Fundamentals of Convolutional Coding , IEEE Press, Piscataway, New Jersey, 1999"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566273.pdf"},"links":[{"id":"1569564605","weight":4},{"id":"1569559617","weight":4},{"id":"1569566605","weight":4},{"id":"1569566697","weight":4},{"id":"1569565551","weight":16},{"id":"1569564481","weight":4},{"id":"1569564805","weight":4},{"id":"1569566409","weight":4},{"id":"1569565355","weight":4},{"id":"1569560427","weight":4},{"id":"1569564849","weight":16},{"id":"1569566739","weight":8},{"id":"1569562685","weight":4},{"id":"1569566749","weight":4},{"id":"1569565257","weight":4},{"id":"1569566447","weight":4},{"id":"1569565847","weight":16},{"id":"1569556671","weight":4},{"id":"1569564973","weight":4},{"id":"1569565469","weight":8},{"id":"1569565393","weight":4},{"id":"1569566407","weight":4},{"id":"1569565571","weight":4},{"id":"1569565493","weight":4},{"id":"1569559199","weight":4},{"id":"1569565665","weight":4},{"id":"1569565241","weight":4},{"id":"1569566547","weight":4},{"id":"1569566823","weight":4},{"id":"1569565597","weight":4},{"id":"1569566397","weight":12},{"id":"1569565039","weight":4},{"id":"1569565769","weight":4},{"id":"1569566147","weight":4},{"id":"1569565737","weight":16},{"id":"1569566375","weight":4},{"id":"1569566113","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T6.1","endtime":"15:00","authors":"Vladimir Sidorenko, Martin Bossert, Francesca Vatta","date":"1341499200000","papertitle":"Properties and encoding aspects of direct product convolutional codes","starttime":"14:40","session":"S13.T6: Convolutional and Turbo Codes","room":"Kresge Rehearsal A (033)","paperid":"1569566273"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
