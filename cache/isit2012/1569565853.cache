{"id":"1569565853","paper":{"title":{"text":"On Network Coding Capacity under On-Oﬀ Scheduling"},"authors":[{"name":"Mayank Bakshi"},{"name":"Michelle Eﬀros"}],"abstr":{"text":"Abstract\u2014We investigate the tradeo ﬀs between rate (bits per channel use) and throughput (bits per time) in network coding over networks of independent noisy or noiseless channels under on-o ﬀ scheduling. While some networks exhibit an inherent trade- o ﬀ between rate and throughput at ﬁnite blocklengths, our main result shows that rate and throughput can be simultaneously maximized in the limit of large blocklengths."},"body":{"text":"Consider a communication scenario in which nodes transmit data to their respective destinations over a network of directed links. The capacities of such networks are studied under a variety of models. In acyclic network models, nodes are often operated sequentially (e.g., [1], [2]), so that all incoming information is available to each node before it begins to transmit. In other models (e.g., [3], [4]), all nodes are operated simultaneously in every time step.\nThis paper investigates the communications implications of operating codes under this range of scenarios. Speciﬁcally, since n uses of each channel require n|V| time steps in the ﬁrst scenario and n time steps in the second, we choose α ∈ [1, |V|] and investigate the \u201cα-capacity,\u201d which is deﬁned in Section II to be the set of asymptotically achievable rates using codes that employ n uses of each channel over at most αn time steps.\nLarge values of α make more information available to each node prior to transmission, thereby potentially increasing the rate (bits per channel use) that can be reliably delivered. This beneﬁt is gained, however, at the cost of longer run time, thereby potentially decreasing the same code\u2019s throughput (bits per unit time). Since the rate, R, and throughput, T, of a code employing n uses of each channel over αn time steps are related as T = R/α, the α-capacity captures the tradeoﬀ between rate and throughput. Understanding this tradeoﬀ is useful for considering scenarios where both power and time are constrained resources. Restricting power may prohibit the transmission of every node in every time step. Restricting time may prohibit operating very few nodes at each time. Investigating α-capacity sheds light on this tradeoﬀ as the number of uses of each channel grows without bound.\nThe main result appears as Theorem 1 in Section III. We begin by formally introducing the problem set-up.\nII. Preliminaries A. Network Model\ntransmission symbol and that distribution p e satisﬁes p e (y (e) |x (e) ) = 0 if (x (e) , y (e) = ) or (x (e) = , y (e) ) 1 if x (e) = , y (e) = .\nThe channels are independent, memoryless, and time-invariant by assumption, and thus N is a multi-terminal channel char- acterized as\n      \nFor each t ∈ N + and e ∈ E, let X (e) t ∈ X (e) and Y (e) t ∈ Y (e) , respectively, be the random variables denoting the transmitted and received values for edge e at time t. For simplicity, we shorten notation as C 1:t = (C 1 , . . . , C t ); for example,\nX t X (e) t : e ∈ E and Y t Y (e) t : e ∈ E ; the corresponding alphabets are\nA network solution, also called a network code, is a mech- anism for establishing connections across a network N. For each u ∈ V, let\ndenote the receiver sets for all possible unicast (|V| = 1) and multicast (|V| > 1) connections from node u; then\nis the set of transmitter and receiver-set pairs for all possible connections. For each (u, V) ∈ M, let W (u→V) ∈ W (u→V) denote the message to be transmitted from u to all nodes v ∈ V; further, let\nbe the messages outgoing from node u and incoming to node v, respectively; the corresponding alphabets are\nGiven any m ∈ N, we deﬁne a blocklength-m network solution S(N) to be a collection of encoders f = ( f (e) t : e ∈ E, t ∈ {1, . . . , m}) and decoders g = (g (v) : v ∈ V). For each (u, v) and t, encoder\nas a function of outgoing messages W (u→∗) and prior channel outputs Y (u) 1:t−1 . Decoder\nof incoming messages W (∗→v) as a function of outgoing messages W (v→∗) and channel outputs Y (v) 1:m . C. Fixed-Schedule Codes\nThe inclusion of the no-transmission symbol implies that any network solution operates according to a transmission schedule T = (T (e) : e ∈ E), where\nhowever, simpler to analyze and also to implement. This work focuses exclusively on ﬁxed-schedule codes.\nWe similarly denote the times at which node v ∈ V operates by\nLet n be the maximal number of uses of any single channel under a given blocklength-m network coding solution S(N); that is, n = max e∈E |T (e)|. Then the performance of solution S(N) is characterized by its error probability\nwhich is the probability that at least one node decodes at least one message incorrectly, its rate vector\nwhich is the vector describing the number of message bits per channel use for each message, and its expansion factor\nwhich is the solution\u2019s ratio of blocklength to channel uses. Expansion factor α(S(N)) characterizes the ratio of rate to throughput for a given solution. That is, any solution achieving rate vector R and expansion factor α achieves throughput vector T = R/α = (R (u→V) /α : (u, V) ∈ M).\nDeﬁnition 1: Rate vector R = (R (u→V) : (u, V) ∈ M) is α- achievable on network N if for any λ > 0 there exists a ﬁxed- schedule solution S(N) with error probability P e (S(N)) ≤ λ, rate R(S(N)) ≥ R, 1 and expansion ratio α(S(N)) ≤ α. The α -capacity region R(N) for network N is the closure of the set of α-achievable rates.\nProof: By the given deﬁnitions, R 1 (N) ⊆ R α (N) for each α ∈ [1, |V|]. Thus, we need only show that R α (N) ⊆ R 1 (N).\nFix α ∈ [1, |V|], λ > 0, and R ∈ int(R α (N)), where int(R α (N)) is the relative interior of set R α (N). Since R ∈ int(R α (N)), there exists a solution S(N) for N with rate R (S(N) ≥ R, expansion α(S(N)) ≤ α, and error probability P e (S(N)) ≤ λ. Lemma 2 shows that given solution S(N) and any µ > 0, there exists a solution ˜ S(N) with rate R ( ˜ S(N)) ≥ (1 − 2H(λ))R, expansion α( ˜ S(N)) ≤ (1 + µ) and error probability P e (S(N)) ≤ λ. Let m be the blocklength of\nsolution S(N); that is, solution S(N) is implemented over m time steps and n ≥ m/(1 + µ) channel uses. Using solution ˜\nS(N), we build a solution ˜ S (N) implemented over m time steps and m channel uses. Solution ˜ S (N) sets each channel input precisely equal to the corresponding input in ˜ S(N) in the active time steps and sends arbitrary symbols (other than the no-transmission symbol) in the remaining time steps. Using this approach, R( ˜ S (N)) ≥ (1 − 2H(λ))R/(1 + µ), P e ( ˜ S (N)) ≤ λ, and α( ˜ S (N)) = 1. Since λ and µ can be made arbitrarily small, R ∈ R 1 (N).\nTheorem 1 demonstrates that asymptotically there is no tradeoﬀ between rate and throughput. That is, since a rate- R , expansion-α code has throughput T = R/α, and the space of achievable rate vectors R is invariant in α, R and T can always be simultaneously maximized by setting α to 1. It is important to note, however, that the tradeoﬀ vanishes only as the number of channel uses, n, grows without bound. For any ﬁnite n and any error probability λ, increasing α may, in fact, increase achievable rate while certainly decreasing throughput \u2013 giving the anticipated tradeoﬀ between these two criteria. What Theorem 1 demonstrates is that that tradeoﬀ becomes negligible as n grows without bound.\nAnother interesting implication of Theorem 1 is that scalar and vector coding are asymptotically equivalent. This result does not contradict [5], [6], which together suggest a diﬀerence between the two in the domain of linear coding. Instead, it highlights a distinction between our system model and the ones used in [5], [6], which consider only codes with a single channel use over either a scalar or a vector alphabet.\nTheorem 1 is related to [7, Theorem 2], which shows that adding any ﬁnite collection of delays to a network has no impact on the network\u2019s capacity. Theorem 1 diﬀers from this earlier result, however, in two critical ways. First, the \u201cdelays\u201d are here controlled by the code designer, and second, the number of delays grows without bound as the number of channel uses goes to inﬁnity.\nBefore proceeding to Lemma 2, which is used in the proof of Theorem 1, we ﬁrst introduce Lemma 1, which is used in the proof of Lemma 2. Lemma 1 shows that every solution S(N) can be modiﬁed to obtain a solution in which at most one node transmits at any time instant. We call such solutions non-overlapping solutions, as deﬁned formally below.\nDeﬁnition 2: A solution S(N) is a non-overlapping solution if for each v 1 , v 2 ∈ V such that v 1 v 2 , T (v 1 ) ∩ T (v 2 ) = φ.\nLemma 1: Given any solution S(N), there exists a non- overlapping solution ˜ S(N) with expansion factor α( ˜ S(N)) = |V| that achieves the same error probability and rate (P e ( ˜ S(N)) = P e (S(N)) and R( ˜S(N)) = R(S(N))).\nProof: Consider an arbitrary solution S(N) for network N with vertices {v 1 , . . . , v |V| }. Let n, m, f , and g be the number of channel uses, number of time steps, encoders, and decoders, respectively, for solution S(N), as deﬁned in Section II. Without loss of generality, we assume that |T (e)| = n for all e ∈ E; if T (e) < n for some e, then we modify S(N)\nby increasing the number of channel uses for the edge e by picking n − |T (e)| time steps from the set {1, 2 . . . , m} \\ T (e) and transmitting arbitrary symbols at these time steps on the edge e.\nWe next design a non-overlapping solution ˜ S(N) with en- coders ˜f and decoders ˜g. Solution ˜ S(N) sequentially operates each time-1 encoder, followed by each time-2 encoder, and so on to obtain a code that employs n uses of each channel over |V|n time steps. For each i ∈ {1, . . . , |V|} and each t ∈ T (v i ), let F(t, v i ) denote the time at which encoder f (v i ) t operates under the above ordering; then\nThe transmission schedule ˜T for the resulting code is ˜T (v, w) = {F(t, v) : (v, w) ∈ E(t, v)}.\nFor each t ∈ {1, . . . , n|V|}, let ˜E(t) = {(v, w) ∈ E : t ∈ ˜T (v, w)} be the set of edges that operate at time t under schedule ˜T ; all edges in set ˜E(t) emanate from the same node by deﬁnition. Solution ˜ S(N) is deﬁned as follows.\n\u2022 For each t ∈ {1, . . . , n|V|} and (v, w) ∈ ˜E(t), ﬁnd τ such that F(τ, v) = t. Then\n\u2022 For each v ∈ V and (u, v) ∈ E, decoder ˜g (v) (·) discards channel outputs for all t ˜T (u, v) and then applies decoder g (v) (·) to its outgoing messages and remaining channel outputs.\nSince solution ˜ S(N) gives the same reconstructions as solution S(N), P e ( ˜ S(N)) = P e (S(N)) and R( ˜S(N)) = R(S(N)). Since solution ˜\nS(N) operates over |V|n time steps, α( ˜ S(N)) = |V|. Lemma 2 uses Lemma 1 and the notion of a stacked network\nintroduced in [4] to show that any solution with expansion factor α can be used to construct an expansion-1 solution with similar error probability and rate. Before proceeding to Lemma 2, we brieﬂy review the deﬁnition of stacked networks.\nAs deﬁned in [4], given any network N, the N-fold stacked network N comprises N copies of N, denoted by N(1), N(2), . . . , N(N), with inﬁnite-capacity bidirectional edges connecting all N copies of a given vertex v ∈ V to each other. Thus, for each v ∈ V, ∈ {1, . . . , N}, and t ∈ {1, . . . , m}, the symbol X (v) t ( ) transmitted by node v in layer at time t may be a function of messages W (v→∗) and received vectors Y (v) 1:t−1 , where\nThe capacity region R(N) for stacked network N is normalized by the number of layers N in the stack. In [4], it is shown that the capacity regions for N and N are equal.\nLemma 2: Given any µ > 0 and any solution S(N) for network N, there exists a solution ˜ S(N) with the same error probability (P e ( ˜ S(N)) = P e (S(N))) such that\nR ( ˜ S(N)) = (1 − 2H(P e (S(N))))R(S(N)) α ( ˜ S(N)) ≤ 1 + µ.\nProof: By Lemma 1, there is no loss of generality in assuming that solution S(N) is a non-overlapping solution with expansion α(S(N)) = |V|. Let S(N) be implemented over m = n|V| time steps and n channel uses. Let the encoder and decoder mappings for S(N) be f and g. Let P e (S(N)) = λ.\nGiven any N ∈ N + and δ > 0, let N be the N|V|-fold stacked network for N. The argument that follows ﬁrst constructs a solution S(N) with message alphabet (W) N(1−δ)|V| ; solution S(N) operates over m + N|V| − 1 time steps, achieving error probability P e (S(N)). The next step builds a solution ˜S(N) that \u201cunravels\u201d S(N) across time \u2013 operating the solution for the stacked network over N by appropriately sequencing its transmissions. The ﬁnal step bounds the error probability, rate, and expansion ratio of solution ˜ S(N).\nConstruction of S(N): For each i ∈ {1, . . . , N|V|}, let S [i] (N) be the solution for N that operates solution S(N) in time steps i, . . . , i + m − 1, as shown in Figure 1. Then\nbe a channel code that is designed by choosing the codeword for each ˜ W (u→V) ∈ ˜ W (u→V) independently and uniformly at\nbe the corresponding maximum likelihood decoder. Network solution S(N) ﬁrst channel codes each message ˜\nW (u→V) ∈ W (u→V) to obtain a coded message W (u→V) ∈ W (u→V) and then independently operates solution S [ ] (N) in each layer ∈ {1, . . . , N} of the stack, as shown in Figure 2. The formal deﬁnition follows.\n\u2022 For each (u, V) ∈ M, message ˜W (u→V) ∈ ˜ W (u→V) at node u is mapped to coded message W (u→V) ∈ W (u→V) using channel code F (u→V) as\n\u2022 For ∈ {1, . . . , N|V|}, solution S [ ] (N) is applied to messages (W (u→V) ( ) : (u, V) ∈ M) in layer of stacked network N, giving reconstructions\nof messages W (u→V) = (W (u→V) ( ) : ∈ {1, . . . , N|V|}) for each (u, V) ∈ M and each receiver v ∈ V.\n\u2022 For each (u, V) ∈ M and v ∈ V, applying channel de- coder G (u→V) to vector ˆ W (u→V,v) yields decoded message\nError analysis for S(N): For each ∈ {1, . . . , N|V|}, (u, V) ∈ M, and v ∈ V ,\nSince solution S(N) operates the N|V| layers of N indepen- dently and the use of a random channel code with codewords drawn uniformly from the given ﬁnite alphabet creates inde- pendent messages in each layer of the stack, the mapping of W (u,V) to ˆ W (u→V,v) for each (u, V) ∈ M and each v ∈ V is equivalent to N|V| independent uses of a discrete memoryless channel with crossover probability P e (S(N)) and capacity no less than 1−H(P e (S(N))). Thus, the randomly chosen channel encoder F (u→V) , which, by our choice of δ, has rate less than (1 − H(P e (S(N)))) for each (u, V) ∈ M, yields expected error\nprobability approaching zero as N grows without bound by the channel coding theorem [8]. Speciﬁcally,\nfor N suﬃciently large. Therefore, for N large enough, E Pr( ˜ W ˆ˜W) ≤\nwhere, the expectation is taken with respect to the random channel code design. Given this bound on the expected error probability of a randomly designed, there exists a solution S(N) that does at least as well.\nConstruction of ˜ S(N): Next, we operate the solution S(N) over the network N to obtain a solution ˜ S(N) with message alphabets ( ˜ W (u→V) : (u, V) ∈ M). Solution ˜S(N) is imple- mented over mn + nN|V| − n time steps.\nLet T be the schedule for solution S(N). For each v ∈ V, let T (e) = {τ(e, 1), . . . , τ(e, |T (e)|)} be the elements of T (e), where τ(e, j) is increasing in j. Recall that S(N) is implemented over n channel uses. Therefore, |T (e)| ≤ n for each e ∈ E. Applying the notation used for S(N), we denote the network messages by ( ˜ W (u→V) : (u, V) ∈ M) with ˜\nW (u→V) ∈ ˜ W (u→V) . We denote the encoders and decoders for ˜ S(N) by ˜f and ˜g. These mappings are related to the encoder and decoder mappings for S(N) as described below and illustrated in Figure 3.\n\u2022 For each e ∈ E, i ∈ {1, . . . , m + N|V| − 1}, and j ∈ {1, . . . , |T (e)|}, encoder ˜f (e) (i−1)n+ j in ˜ S(N) operates\nIf |T (e)| < n for some e ∈ V, encoder ˜f (e) (i−1)n+t transmits symbol for all t ∈ {|T (e)| + 1, . . . , n}.\n\u2022 At each v ∈ V, the decoder ﬁrst groups together the received vectors corresponding to each S [i] (N) for i = 1, 2, . . . , N|V|, decodes using the decoder functions g (v) [i] (·), and ﬁnally creates reconstructions ( ˆ˜W (u→V,v) : (u, V) ∈ M, v ∈ V) by passing through the decoders (G (u→V) (·) : (u, V) ∈ M).\nError and rate analysis for ˜ S(N): Since the encoder functions that determine the codewords transmitted on an edge e under the solutions ˜ S(N) and S(N) are the same, the error probabil- ities for ˜ S(N) and S(N) are also the same. Therefore, there exists N ∗ such that for all N > N ∗ , the error probability for ˜\nFor each N, the solution ˜ S(N) has throughput nN(1 − δ )|V| / nm + nN|V| − n T bits per second and rate (1 − δ)R. Since δ > H(P e (S(N))) is arbitrary, this shows that a through- put of nN(1−2H(P e (S(N))))|V| / nm+nN|V|−n T and rate (1 − 2H(P\ne (S(N))))R are simultaneously achievable with an error probability P e (S(N)) for all N > N ∗ .\nBy choosing n and m large enough, noting that m = n|V|, and letting N grow without bound, this shows that for every µ > 0, throughput (1 − 2H(P e (S(N))))T/(1 + µ)|V| and rate (1 − 2H(P e (S(N))))R are simultaneously achievable with an error probability P e (S(N)). Finally, note that the expansion ratio for such codes is 1 + µ to complete the proof."},"refs":[{"authors":[{"name":"R. Ahlswede"},{"name":"N. Cai"},{"name":"S. Y. R. Li"},{"name":"R. Yeung"}],"title":{"text":"Network information ﬂow"}},{"authors":[{"name":"R. Koetter"},{"name":"M. Medard"}],"title":{"text":"An algebraic approach to network coding"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory"}},{"authors":[{"name":"R. Koetter"},{"name":"M. Eﬀros"},{"name":"M. M´edard"}],"title":{"text":"A theory of network equivalence - part i: Point-to-point channels"}},{"authors":[{"name":"A. Rasala-Lehman"},{"name":"E. Lehman"}],"title":{"text":"Complexity classiﬁcation of network information ﬂow problems"}},{"authors":[{"name":"M. Medard"},{"name":"M. Eﬀros"},{"name":"T. Ho"},{"name":"D. Karger"}],"title":{"text":"On coding for non- multicast networks"}},{"authors":[{"name":"M. Eﬀros"}],"title":{"text":"On dependence and delay: Capacity bounds for wireless networks"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A mathematical theory of communication"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565853.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T1.3","endtime":"12:30","authors":"Mayank Bakshi, Michelle Effros","date":"1341403800000","papertitle":"On Network Coding Capacity under On-Off Scheduling","starttime":"12:10","session":"S10.T1: Network Coding: Capacity and Bounds","room":"Kresge Rehearsal B (030)","paperid":"1569565853"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
