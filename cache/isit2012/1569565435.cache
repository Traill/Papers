{"id":"1569565435","paper":{"title":{"text":"On Establishing the Shannon Ordering for Discrete Memoryless Channels"},"authors":[{"name":"Yuan Zhang"},{"name":"Cihan Tepedelenlioglu"}],"abstr":{"text":"School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ 85287, USA Email: {yzhang93, cihan}@asu.edu\nAbstract\u2014 This paper studies several problems concerning channel inclusion, which is a partial ordering between discrete memoryless channels (DMCs) proposed by Shannon. Speciﬁcally, checkable conditions are derived for channel inclusion between DMCs with certain special structure, and these conditions are related to the mathematical theory of majorization. The condi- tions for channel inclusion between binary erasure channel (BEC) and binary symmetric channel (BSC) are also derived, which are shown to be equivalent to those for channel output degradation. Furthermore, the determination of channel inclusion is consid- ered as a convex optimization problem, and the sparsity of the weights related to the representation of the worse DMC in terms of the better one is revealed when channel inclusion holds between two DMCs."},"body":{"text":"The comparison between different communication chan- nels has been a long-standing problem since the establishment of Shannon theory. Speciﬁcally, such comparison is usually established through partial ordering between two channels. Channel inclusion [1] is a partial ordering deﬁned for DMCs, when one DMC is obtained through randomization at both the input and the output of another, and the latter is said to include the former. Such an ordering between two DMCs implies that for any code over the worse (included) DMC, there exists a code of the same rate over the better one with a lower error rate. Channel inclusion can be viewed as a generalization of the comparisons of statistical experiments established in [2], [3], in the sense that the latter involves output randomization (degradation, with a condition given in [4, Proposition 4.1]) but not input randomization. There are also other kinds of channel ordering, e.g. more capable ordering and less noisy ordering [5], which enable the characterization of capacity regions of broadcast channels.\nGiven these kinds of ordering of DMCs, it is of interest to know how it can be determined if two DMCs have a speciﬁc ordering relation analytically and numerically. To the best of our knowledge, regarding the conditions for channel inclusion, the only results beyond Shannon\u2019s paper [1] are provided in [6], and there is not yet any discussion on the numerical characterization of channel inclusion in existing literature. In this paper, we derive checkable conditions for channel inclusion between DMCs with certain special structure, as well as between BEC and BSC, which complement the results in [6] in useful ways, and relate channel inclusion to the well- established majorization theory. In addition, we delineate the\ncomputational aspects of channel inclusion, by formulating a convex optimization problem for determining if one DMC includes another, and proving that if inclusion is established, sparsity can be exploited to dramatically reduce the complexity of ﬁnding the optimal code for the better DMC based on the code for the worse one.\nThe rest of this paper is organized as follows. Section II describes the notations and literature background involved in this paper. Section III derives checkable conditions for channel inclusion between DMCs with special structure, and also between BEC and BSC. Computational issues regarding channel inclusion are addressed in Section IV, and Section V concludes the paper.\nThroughout this paper, we use lower case letters such as α to denote scalars, upper case letters such as K to denote matrices, and bold letters such as a to denote row vectors, unless otherwise speciﬁed. The entry of K with index (i, j) and the entry of a with index i are denoted by [K] (i,j) and a (i) respectively. The i-th row and j-th column of K are denoted by [K] (i,·) and [K] (·,j) respectively. Also for convenience, we use the same letter to denote a DMC and its stochastic matrix, and apply the terminologies \u201csquare\u201d, \u201cdoubly stochastic\u201d and \u201ccirculant\u201d for matrices directly to DMCs. We next provide a sketch of the literature background related to this paper. We have the following deﬁnitions of channel inclusion drawn from [1], and also degradation based on [2], [3] which is a special case of channel inclusion.\nDeﬁnition 1: A DMC described by matrix K 1 is said to include another DMC K 2 , denoted by K 1 ⊇ K 2 or K 2 ⊆ K 1 , if there exists a probability vector g ∈ R β + and β pairs of stochastic matrices {R α , T α } β α=1 such that\nIntuitively, K 2 can be thought of as an input/output processed version of K 1 , with g (α) being the probability that K 1 is processed by R α (input) and T α (output). K 1 and K 2 are said to be equivalent if K 1 ⊇ K 2 and K 2 ⊇ K 1 .\nDeﬁnition 2: A DMC K 2 is said to be a degraded version of another DMC K 1 , if there exists a stochastic matrix T such that K 1 T = K 2 .\nThere are several analytical conditions for channel inclu- sion derived in [6]. Speciﬁcally, in [6] a DMC is treated\nas an equivalence class of stochastic matrices, with any two matrices in the class equivalent in the sense of Deﬁnition 1. With this notion, the author considers two kinds of DMCs: an equivalence class containing a 2×2 full-rank stochastic matrix P , and an equivalence class containing an n × n stochastic matrix with identical diagonal entries p and identical off- diagonal entries (1 − p)/(n − 1). Necessary and sufﬁcient conditions for K 1 ⊇ K 2 are derived for the cases in which K 1 and K 2 are of either of the two kinds, in terms of the relations between the parameters {[P ] (i,j) } (2,2) (i,j)=(1,1) and (n, p) pertaining to K 1 and K 2 [6, Theorems 5, 7, 8 ,9].\nChannel inclusion can be equivalently deﬁned with R α \u2019s and T α \u2019s in Deﬁnition 1 being stochastic matrices in which all the entries are 0 or 1, as stated in [1], where R α \u2019s and T α \u2019s of this kind are called pure channels. This is easily corroborated based on the fact that every stochastic matrix can be represented as a convex combination of such \u201cpure\u201d matrices [7, Theorem 1]. Given this equivalent deﬁnition, K 2 ⊆ K 1 has the implication that if there is a set of m code words {w l } m l=1 of length n, such that an error rate of P e is achieved with the code words being used with probabilities {p l } m l=1 under K 2 , then there exists a set of m code words of length n, such that an error rate of P e ≤ P e is achieved under K 1 with the code words being used with probabilities {p l } m l=1 . In [8, p.116], this implication is stated as one DMC being better in the Shannon sense than another (different from the ordering itself), and it is pointed out that K 1 ⊇ K 2 is a sufﬁcient but not necessary condition for K 1 to be better in the Shannon sense than K 2 , with the proof provided in [9].\nChannel inclusion, as deﬁned, is a partial ordering between two DMCs: it is possible to have two DMCs K 1 and K 2 such that K 1 K 2 and K 2 K 1 . Under this situation, it cannot be asserted that there exists a code for one DMC performing at least as well as the given code for the other. For the purpose of making it possible to compare an arbitrary pair of DMCs, particularly their error rate performance under the situation that the code for one DMC is constructed based on the code for the other, a metric based on the total variation distance, namely Shannon deﬁciency is introduced in [10]. In our notation, the Shannon deﬁciency of K 1 with respect to K 2 is deﬁned as\n(2) where g ∈ R β + is a probability vector, R α \u2019s and T α \u2019s are stochastic matrices, A ∞ max i [A] (i,·) 1 = A T 1 , and we impose matrix transpose since we treat channel matrices as row-stochastic instead of column-stochastic. Intuitively, the above Shannon deﬁciency quantiﬁes how far is K 1 apart from including K 2 . With the notion of Shannon deﬁciency, it is possible to quantify how close K 1 can perform with respect to K 2 when K 1 K 2 and the code for K 1 is constructed based on the one for K 2 . Speciﬁcally, it is shown in [10] than given K 2 achieving error rate with some code and δ S (K 1 , K 2 ) ≤\nfound. Furthermore, other useful deﬁciency-like quantities are established in [10] by substituting the total variation distance with other divergence obeying a data processing inequality between probability distributions.\nHere we describe some important mathematical concepts involved in this paper.\nDeﬁnition 3: For two vectors a, b ∈ R n , a is said to weakly majorize (or dominate) b, written a w b, if and only if k i=1 a ↓ (i) ≥ k i=1 b ↓ (i) for k = 1, . . . , n, where a ↓ (i) and b ↓ (i) are entries of a and b sorted in decreasing order. If a w b and n i=1 a (i) = n i=1 b (i) , a is said to majorize (or dominate) b, written a b.\nDeﬁnition 4: A circulant matrix is a square matrix in which the i-th row is generated from cyclic shift of the ﬁrst row by i − 1 positions to the right.\nDeﬁnition 5: An n × n matrix P is said to be dou- bly stochastic if the following conditions are satisﬁed: (i) [P ] (i,j) ≥ 0 for i, j = 1, . . . , n; (ii) i [P ] (i,j) = 1 for j = 1, . . . , n; (iii) j [P ] (i,j) = 1 for i = 1, . . . , n.\nDeﬁnition 6: A DMC is called symmetric if its rows are permutations of each other, and its columns are permutations of each other. A DMC is called weakly symmetric if its rows are permutations of each other, and its every columns have identical sums (drawn from [11, p.190]).\nIt is easy to verify that if a symmetric DMC has a square matrix, then it must be doubly stochastic. In this paper, we will focus mostly on square DMCs (i.e. DMCs with equal size of input and output alphabets), and we assume this condition unless otherwise speciﬁed.\nIn general, given two DMCs K 1 and K 2 , there is no straightforward method to check if one includes the other based on their entries. Nevertheless, it is possible to char- acterize the conditions for channel inclusion between them in terms of some checkable relations between their entries, for the cases in which both K 1 and K 2 have special structure. There are several different special properties pertaining DMCs, and we reveal the relations between them as follows. As we have described in Section II, the n × n channel considered in [6] has identical off-diagonal entries, making it a special case of circulant DMC. It is also clear that circulant DMC is a special case of symmetric DMC as deﬁned in [11, p.190], and symmetric DMC is a special case of weakly symmetric DMC deﬁned therein. Furthermore, weakly symmetric DMC is a special case of doubly stochastic DMC if they are square. In this section, we aim to derive checkable conditions for the cases of doubly stochastic and circulant DMCs, while the ap- plicability of these conditions can be extended considering the relations between DMCs with different special properties as described above. Also, we derive the conditions for inclusion between BEC and BSC, since they are commonly used in information theory.\nConsidering that doubly stochastic matrices have signiﬁ- cant theoretical importance, and doubly stochastic DMCs can\nbe thought of as a generalization of square symmetric DMCs, we ﬁrst introduce the following theorem\nTheorem 1: Let K 1 and K 2 be n × n doubly stochastic DMCs, with w 1 and w 2 being the n 2 × 1 vectors containing all the entries of K 1 and K 2 respectively. Then w 2 w 1 is a necessary condition for K 2 ⊆ K 1 .\nConsider the case of both K 1 and K 2 being n×n circulant. This kind of channels are known to be involved in discrete degraded interference channels [12]. We have the following result\nTheorem 2: Let K 1 and K 2 be n×n circulant DMCs, with vectors v 1 and v 2 being the ﬁrst rows of them respectively. Then for K 2 ⊆ K 1 , a necessary condition is v 2 \t v 1 . A sufﬁcient condition is that v 2 can be represented as the circular convolution of v 1 and another probability vector in R n + , i.e. the existence of a vector x ∈ R n + such that n i=1 x (i) = 1 and v 1 ⊗ x = v 2 .\nIt is clear that a 2 × 2 doubly stochastic DMC is circulant and characterized solely by the cross-over probability, thus the condition for the inclusion between two 2 × 2 such DMCs boils down to the comparison between their cross- over probabilities. Furthermore, for n = 3, 4, it is easy to verify that if an n × n symmetric DMC is not circulant, it can be transformed into circulant through row and column permutations (i.e. by multiplying permutation matrices on the left and right), therefore we can conclude that\nCorollary 1: For n = 3, 4, let K 1 and K 2 be n × n symmetric DMCs, which can be transformed into circulant DMCs K 1 and K 2 respectively through row and column permutations. Let v 1 and v 2 be the ﬁrst rows of K 1 and K 2 respectively. Then for K 2 ⊆ K 1 , a necessary condition is that v 2 v 1 , while a sufﬁcient condition is that v 2 can be represented as the circular convolution of v 1 and another probability vector in R n + .\nSo far we have derived several conditions for channel inclusion between DMCs, considering two of the same size. Now we consider what the conditions are like between BEC and BSC, which are commonly considered channels in in- formation theory and are of different sizes. It is well-known that a BEC is characterized by its erasure probability and a BSC is characterized by its cross-over probability p, with the stochastic matrices given as K 1 ( ) = [1 − , 0, ; 0, 1 − , ] and K 2 (p) = [1 − p, p; p, 1 − p] respectively. Therefore, the conditions for the inclusion between the two DMCs will boil down to how and p are related. Here we assume 0 ≤ p ≤ 1/2 for convenience, since it is easy to see that two BSCs with cross-over probabilities p and 1 − p are equivalent in the sense of Deﬁnition 1. We have the following theorem characterizing the inclusion between K 1 ( ) and K 2 (p) through and p.\nTheorem 3: For a BEC K 1 ( ) with erasure probability ∈ [0, 1] and a BSC K 2 (p) with cross-over probability\nwhile K 1 ( ) ⊆ K 2 (p) if and only if p = 0. In addition, each of these conditions coincides with the condition for the worse DMC to be a degraded version of the better one.\nIn Section III, we have established some analytical condi- tions for determining if a DMC K 1 includes another DMC K 2 or not. It is also of interest to know how this can be determined numerically. Furthermore, once it has been determined that K 2 ⊆ K 1 , as long as low complexity is sought for the purpose of obtaining the optimal code for K 1 based on the one for K 2 , it is desirable for g (α) \u2019s in (1) to contain as many zeros as possible. In this section, we provide a sketch of how these can be implemented, from both theoretical and empirical aspects. We begin by formulating the optimization problem for determining if channel inclusion holds between two DMCs. For the case in which channel inclusion holds, we prove the sparsity of g and give discussions on obtaining sparse solution of g through compressed sensing.\nWe ﬁrst take a look at the solvability of determining if K 2 ⊆ K 1 through convex optimization. For K 1 of size n 1 × m 1 and K 2 of size n 2 × m 2 , with g (α) \u2019s as variables, the problem can be formulated as\nwhere R α \u2019s of size n 2 × n 1 and T α \u2019s of size m 1 × m 2 are stochastic matrices, and K 2 ⊆ K 1 is determined if the optimal value is zero. As mentioned in Section II, R α \u2019s and T α \u2019s can be equivalently treated as pure channels, so there are at most n n 2 1 m m 1 2 different {R α , T α } pairs, and consequently there are ﬁnitely many g (α) \u2019s involved in the problem (3) (i.e. β < ∞). It is easy to see that (3) is a convex optimization problem, and it can be re-formulated as the linear programming problem\nWe also notice that (4) provides a way to evaluate the Shannon deﬁciency of K 1 with respect to K 2 deﬁned by (2), thereby implying the upper bound of the error rate performance of K 1 with respect to the error rate performance of K 2 .\nIn the above analysis, it is worth noticing that the maxi- mum possible number of {R α , T α } pairs, given by n n 2 1 m m 1 2\n(or (n!) 2 if both K 1 and K 2 are n×n doubly stochastic), grows very rapidly with the sizes of K 1 and K 2 . With K 2 ⊆ K 1\nalready determined, it is natural to ask if (1) can hold with some reduced number of {R α , T α } pairs. In other words, we seek to have a sparse solution of g. We have the following theorem regarding the sparsity of g given K 2 ⊆ K 1 .\nTheorem 4: For two DMCs K 1 of size n 1 × m 1 and K 2 of size n 2 × m 2 , if K 2 ⊆ K 1 , there exist a probability vector g ∈ R β 1 + and β 1 pairs of stochastic matrices {R α , T α } β 1 α=1 such that (1) holds with β = β 1 ≤ n 2 (m 2 − 1) + 1. If both K 1 and K 2 are n × n doubly stochastic, the number of necessary {R α , T α } pairs in (1) can be improved as β 1 ≤ (n − 1) 2 + 1.\nFor a numerical example, consider a DMC K 1 = [0.7, 0.3; 0.2, 0.8], and construct another DMC\n(5) which is clearly included in K 1 and boils down to K 2 = [0.325, 0.675; 0.625, 0.375]. On the other hand, it can be veriﬁed that\n(6) from which we can see that it is sufﬁcient to represent K 2 in terms of K 1 together with 2 · (2 − 1) + 1 = 3 {R α , T α } pairs, thereby corroborating Theorem 4.\nIt is well-known that a typical approach to recover a sparse signal vector from its linear measurements is compressed sensing with 1 norm minimization. For applying this approach to our problem, we can formulate it as\nwith variables g ∈ R β . It is easy to prove that the optimal g always comes out non-negative given K 2 ⊆ K 1 . However, (7) normally does not give sparse solution of g, since the number of independent measurements (n 2 (m 2 −1) or (n−1) 2 ) in (7) is usually less than 2β 1 (up to 2n 2 (m 2 − 1) + 2 or 2(n−1) 2 +2), and the condition for sparse probability vector to be solvable through 1 norm minimization [13] is not satisﬁed. Empirically, we ﬁnd that sparse solution of g is obtainable with non-optimal sparsity, by replacing the objective function in (7) with α∈A |g (α) | where A ⊂ {1, . . . , β}, although there is no established theory regarding this approach. There are also other sparsity-inducing numerical methods such as matching pursuit, which will be considered in our future work.\nIn this paper, we investigate the characterization of channel inclusion between DMCs through analytical and numerical approaches. We have established several checkable conditions\nfor inclusion between DMCs with certain special structure (doubly stochastic, circulant etc), and also derived necessary and sufﬁcient conditions for inclusion between BEC and BSC. For the issues of determining channel inclusion through numerical computation, we formulate a linear programming problem leading to the quantitative result on how far is one DMC apart from including another, which has an implication on the comparison of their error rate performance. In addition, for the case in which one DMC includes another, we derive the upper bound for the necessary number of pairs of pure channels involved in the representation of the worse DMC in terms of the better, which is signiﬁcantly less than the maximum possible number of such pairs. This kind of sparsity implies reduced complexity of ﬁnding the optimal code for the better DMC based on the code for the worse one.\nWe start from (1) with R α \u2019s and T α \u2019s being pure channels, as equivalent to Deﬁnition 1. It is clear that every entries of K 2 are linear combinations of the entries of K 1 . Considering the fact that there is a one-to-one mapping between the entries of K 1 and the entries of w 1 , as well as the same situation for K 2 and w 2 , it follows that there is a matrix P such that w 2 = P w 1 , and the conditions for K 2 ⊆ K 1 are related to certain properties the combining coefﬁcients [P ] (i,j) \u2019s have. Based on Birkhoff\u2019s Theorem [14, p.30], both K 1 and K 2 are inside the convex hull of n×n permutation matrices, therefore it is sufﬁcient for R α \u2019s and T α \u2019s to contain only permutation matrices; otherwise β α=1 g (α) R α K 1 T α will fall out of the convex hull of n × n permutation matrices, which contradicts with the doubly stochastic condition. Consequently, for each α, R α K 1 T α gives a matrix having exactly the same set of entries as K 1 , generated by permuting the columns and rows of K 1 . In other words, R α \u2019s and T α \u2019s do not replace any row of K 1 with the duplicate of another row, or merge any column into another column and then replace it with zeros.\nWe now consider the implications of the structure of R α K 1 T α on the properties of [P ] (i,j) \u2019s. We have j [P ] (i,j) =\ng (α) = 1 for i = 1, . . . , n 2 since each entry of K 1 is contained in R α K 1 T α exactly once, for α = 1, . . . , β. On the other hand, since each entry [K 2 ] (i,j) of K 2 is the convex combination of the entries with the same index (i, j) of R α K 1 T α \u2019s, while each entry of R α K 1 T α is exactly an entry of K 1 , it follows that i [P ] (i,j) = β α=1 g (α) = 1 for j = 1, . . . , n 2 . Also, it is straightforward to see that [P ] (i,j) ≥ 0 for i, j = 1, . . . , n 2 due to the non-negativeness of g (α) \u2019s. Consequently, P is doubly stochastic, and w 2 = P w 1 implies that w 2 w 1 [14, p.155], completing the proof.\nLet w 1 and w 2 be the n 2 × 1 vectors containing all the entries of K 1 and K 2 respectively. It is easy to see that w 1 and w 2 contain the entries of v 1 and v 2 each duplicated n times respectively. Given K 2 ⊆ K 1 , based on Theorem 1 we know that w 2 \t w 1 , thus k i=1 n · v ↓ 1(i) ≥ k i=1 n · v ↓ 2(i) for k = 1, . . . , n, and it follows that k i=1 v ↓ 1(i) ≥ k i=1 v ↓ 2(i)\nfor k = 1, . . . , n. In addition, n i=1 v 1(i) = n i=1 v 2(i) = 1 as required for stochastic matrices, therefore v 2 v 1 is necessary for K 2 ⊆ K 1 .\nWe next prove that the existence of a probability vector x ∈ R n + such that v 1 ⊗ x = v 2 is sufﬁcient for K 2 ⊆ K 1 . Let P be the n × n permutation matrix such that xP is cyclic shifted to the right by 1 with respect to x, and let X be the n × n matrix with the i-th column being P i−1 x T . It is easy to see that both P i−1 and X are circulant. Given v 1 ⊗x = v 2 , it follows that v 1 X = v 2 due to the deﬁnition of circular convolution. Also, notice that P i−1 X = XP i−1 since the multiplication of two circulant matrices are exchangeable. Consequently, the i-th row of K 1 , given by v 1 P i−1 , and the i-th row of K 2 , given by v 2 P i−1 , are related through (v 1 P i−1 )X = v 1 XP i−1 = (v 2 P i−1 ). It then follows that K 1 X = K 2 with a stochastic matrix X, i.e. Deﬁnition 1 is satisﬁed, and the proof is complete.\nConsider for (1) in Deﬁnition 1, with R α \u2019s and T α \u2019s being pure channels. Clearly R α \u2019s should be 2 × 2 and T α \u2019s should be 3 × 2, and there are ﬁnitely many different cases of them. Given these conditions, it is clear that K 1 ( )T α can only be the following cases: [1, 0; 1, 0], [1 − , ; 0, 1], or two other 2 × 2 matrices obtained by column permutations on these two. Consequently, R α K 1 ( )T α can only be the following cases: [1, 0; 1, 0], [1− , ; 0, 1], [1− , ; 1− , ], [0, 1; 1− , ], or four other matrices obtained by column permutations on these four. Similar to the geometrical interpretation of channel inclusion given in [1], we can see that the point (1−p, p) ∈ R 2 should be in the convex hull of the points (0, 0), (1, 1), (1 − , 0), (1, ), (0, 1 − ) and ( , 1). With the considerations of geometrical symmetry and 0 ≤ p ≤ 1/2, it follows that (1 − p, p) should have no smaller vertical coordinate than (1− /2, /2) which is the mid-point of (1− , 0) and (1, ). Thus for K 2 (p) ⊆ K 1 ( ), it is required that p ≥ /2, i.e. 0 ≤ ≤ 2p.\nTo prove the sufﬁciency of ≤ 2p for K 2 (p) ⊆ K 1 ( ), we can see that\nx \t 1 − x 1 − x \t x\nholds with x = (1 − p − /2)/(1 − ). Given 0 ≤ p ≤ 1/2 and ≤ 2p, it follows that 0 ≤ x ≤ 1, thus Deﬁnition 1 is satisﬁed\nand K 2 (p) ⊆ K 1 ( ). Also, it can be seen that Deﬁnition 2 is satisﬁed, and K 2 (p) is a degraded version of K 1 ( ).\nWe now consider what is necessary and sufﬁcient for K 1 ( ) ⊆ K 2 (p). Consider the Z-channel\n1 0 0 1 0 1\nIt is clear that K 3 ( ) ⊆ K 1 ( ), and based on the transitivity of channel inclusion [1], we have K 3 ( ) ⊆ K 2 (p). Similar to the geometrical interpretation of channel inclusion given in [1], we can see that the point (1 − , 0) should be inside the\nconvex hull of the points (0, 0), (1, 1), (1−p, p) and (p, 1−p). Clearly, this condition is satisﬁed only when p = 0, therefore p = 0 is necessary for K 1 ( ) ⊆ K 2 (p). The sufﬁciency of p = 0 for K 1 ( ) ⊆ K 2 (p) can be easily seen from\n0 \t 1 − \t (10) which implies that both Deﬁnition 1 and Deﬁnition 2 are satisﬁed, and the proof is complete.\nSince an n 2 × m 2 stochastic matrix is determined by its n 2 rows and ﬁrst m 2 − 1 columns, the class of all n 2 × m 2 stochastic matrices can be viewed as a convex polytope in n 2 (m 2 −1) dimensions. We apply Carath´eodory\u2019s theorem [15, p.155], which asserts that if a subset S of R m is k-dimensional, then every vector in the convex hull of S can be expressed as a convex combination of at most k + 1 vectors in S, on (1) with K 1 of size n 1 × m 1 and K 2 of size n 2 × m 2 . It is clear that R α K 1 T α and K 2 are at most n 2 (m 2 − 1)-dimensional. Therefore if K 2 is in the convex hull of {R α K 1 T α } β α=1 , it can be expressed as a convex combination of at most n 2 (m 2 −1)+ 1 matrices in {R α K 1 T α } β α=1 , i.e. the number of necessary {R α , T α } pairs can be bounded as β = β 1 ≤ n 2 (m 2 − 1) + 1 if (1) holds. Similar proof can follow for the case of both K 1 and K 2 being n × n doubly stochastic, in which they are at most (n − 1) 2 -dimensional."},"refs":[{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A note on a partial ordering for communication channels"}},{"authors":[{"name":"D. Blackwell"}],"title":{"text":"Comparison of experiments"}},{"authors":[],"title":{"text":"Equivalent comparisons of experiments"}},{"authors":[{"name":"J. E. Cohe"},{"name":"J. H. B. Kemperman"},{"name":"G. Zbagan"}],"title":{"text":"Comparisons of Stochastic Matrices with Applications in Information Theory, Statistics, Economics and Population , 1st ed"}},{"authors":[{"name":"J. Korner"},{"name":"K. Marton"}],"title":{"text":"Comparison of two noisy channels"}},{"authors":[{"name":"H. J. Helgert"}],"title":{"text":"A partial ordering of discrete, memoryless channels"}},{"authors":[{"name":"W. Schaefer"}],"title":{"text":"A representation theorem for stochastic kernels and its application in comparison of experiments"}},{"authors":[{"name":"I. Csisza"},{"name":"J. Korne"}],"title":{"text":"Information theory: coding theorems for discrete memoryless systems , 1st ed"}},{"authors":[{"name":"M. A. Karmazin"}],"title":{"text":"Solution of a problem of Shannon (in Russian)"}},{"authors":[{"name":"M. Raginsky"}],"title":{"text":"Shannon meets Blackwell and Le Cam: Channels, codes, and statistical experiments "}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, 2nd ed"}},{"authors":[{"name":"N. Liu"},{"name":"S. Ulukus"}],"title":{"text":"The Capacity Region of a Class of Discrete Degraded Interference Channels"}},{"authors":[{"name":"A. Cohen"},{"name":"A. Yeredor"}],"title":{"text":"On the use of sparsity for recovering discrete probability distributions from their moments"}},{"authors":[{"name":"A. W. Marshal"},{"name":"I. Olki"},{"name":"B. C. Arnol"}],"title":{"text":"Inequalities: Theory of Majorization and Its Applications , 2nd ed"}},{"authors":[{"name":"R. T. Rockafella"}],"title":{"text":"Convex Analysis"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565435.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T7.3","endtime":"10:50","authors":"Yuan Zhang, Cihan Tepedelenlioglu","date":"1341311400000","papertitle":"On Establishing the Shannon Ordering for Discrete Memoryless Channels","starttime":"10:30","session":"S5.T7: Continuous Time Channels and Shannon Ordering","room":"Stratton (407)","paperid":"1569565435"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
