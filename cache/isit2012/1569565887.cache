{"id":"1569565887","paper":{"title":{"text":"Joint Source-Channel Coding for Cribbing Models"},"authors":[{"name":"Eliron Amir"},{"name":"Yossef Steinberg"}],"abstr":{"text":"Abstract\u2014 In this work we study problems of joint source-channel coding for the multiple access channel with cribbing encoders. These problems are motivated by modern communication scenarios such as an uplink channel for cellular users. Provided that the users are close enough to each other, they can causally crib to the output signals of their neighbours, thus obtaining some measure of cooperation. Three scenarios are considered in this work: (i) the symmetric model, where both encoders crib strictly causally at each other\u2019s output, (ii) the model where encoder 1 cribs strictly causally at the output of encoder 2, and encoder 2 cribs causally at the output of encoder 1, and (iii) the model where only one encoder cribs, in a causal or non-causal manner. For the symmetric case, model (i), sufﬁcient conditions are derived for lossless transmission of a correlated pair source (U, V ) via the multiple access channel (MAC). For the non- symmetric scenarios (ii) and (iii), necessary and sufﬁcient conditions are derived, for transmissibilty of the pair source via the MAC. The main focus of this work is on lossless transmission, however, for case (iii) we allow distortion in one of the source components."},"body":{"text":"A discrete memoryless multiple access channel (MAC) is a quadruple X 1 , X 2 , P Y |X 1 ,X 2 , Y where X 1 , X 2 are the input alphabets, Y the output alphabet, and P Y |X 1 ,X 2 a transition prob- ability matrix from X 1 × X 2 to Y. For shorthand notation, we will refer to the MAC by P Y |X 1 ,X 2 . A source pair is a triplet {U , V, P U,V } where P U,V is a distribution on U × V. We use the following notation: random variables are denoted by uppercase letters. Sequence of n letters from alphabet X is denoted by x n , and the substring x i , x i+1 , . . . , x j as x j i . When the dimension n is understood from the context, n-vectors are denoted by boldface letters: x n = x. Similar notation holds for random variables and random vectors, e.g., V , U, U n , etc. The channel and source are assumed memoryless, thus probabilities of n-sequences are given by\nWe investigate a joint source-channel coding schemes for the discrete memoryless MAC with cribbing encoders. The sources, U and V , deliver their output to two separate encoders 1 and 2. Several cribbing models will be discussed, each of them deﬁned by a set of encoding functions:\n\u2022 Strictly causal cribbing by encoder 2: f 1 : U n → X m 1\n\u2022 Non causal cribbing by encoder 2: f 1 : U n → X m 1\nf 1,i : U n × X i−1 2 → X 1,i i = 1, . . . , m f 2,i : V n × X i−1 1 → X 2,i i = 1, . . . , m\n\u2022 Strictly causal cribbing by encoder 1 and Causal cribbing by encoder 2:\nf 1,i : U n × X i−1 2 → X 1,i i = 1, . . . , m f 2,i : V n × X i 1 → X 2,i i = 1, . . . , m\nAn (n, m, ) joint source-channel code for lossless transmission of the source P U,V via the channel P Y |X 1 ,X 2 consists of a pair of encoding functions, according to one of the models (1), (2), (3), (4), or (5), and a decoding function\nThe rate of the code is deﬁned as ρ = n/m. A source pair (U, V ) is said to be transmissible via the MAC P Y |X 1 ,X 2 with strictly causal cribbing at rate ρ if for every > 0, δ > 0, and sufﬁciently large n there exists an (n, n/(ρ − δ), ) code for the source pair and the channel.\nWe are interested also in the case where one of the sources is transmitted with distortion. Thus let ˆ V be the reproduction alphabet of the source V , and let d : V × ˆ V → [0, ∞) be a single letter distortion function. Distortion between sequences is deﬁnes as the normalized sum of single letter distortions:\nd(v n , ˆ v n ) = 1 n\nFor simplicity of notation it is understood in the sequel that the source U is transmitted without distortion. An (n, m, D, ) joint\nsource channel code with non-causal cribbing by encoder 2 consists of a pair of encoders (3) and a pair of decoders\nd m u : Y m → U n d m v : Y m → ˆ V n\nsuch that the probability of error in decoding U does not exceed :\nand the average distortion in decoding V is at most D: Ed(V n , d m v (Y m )) ≤ D\nTransmissibility conditions for the communication models de- ﬁned by encoders (1) and (2) were presented in [1]. We are interested in transmissibility conditions for models (3), (4) and (5). We start by stating previous relevant results, on capacity regions of MAC with cribbing ([5]), and on distributed source coding ([4]). We cite only the results that are directly relevant to us; the reference to these works is by no means comprehensive.\nTheorem 1. Willems & van der Meulen [5]: The capacity region for causal and non-causal cribbing by encoder 2 is given by the collection of all rate pairs (R 1 , R 2 ) such that\nR 1 ≤ H (X 1 ) \t (8a) R 2 ≤ I (X 2 ; Y | X 1 ) \t (8b)\nThe capacity region for strictly causal cribbing by both encoders is given by the collection of all rate pairs (R 1 , R 2 ) such that\nR 1 ≤ H(X 1 | W ) \t (9a) R 2 ≤ H(X 2 | W ) \t (9b)\nP W,X 1 ,X 2 ,Y (w, x 1 , x 2 , y) = P W (w) P X 1 |W (x 1 | w) ·\nwhere W is an external random variable whose size can be bounded as |W| ≤ min {|X 1 | · |X 2 | + 1, |Y| + 2}.\nThe capacity region for strictly causal cribbing by encoder 1 and causal cribbing by encoder 2 is given by the collection of all rate pairs (R 1 , R 2 ) such that\nR 1 ≤ H(X 1 ) \t (10a) R 2 ≤ H(X 2 | X 1 ) \t (10b)\nTheorem 2. Slepian & Wolf [4]: For the distributed lossless source coding problem with source (U, V ) drawn i.i.d ∼ P U,V , the achievable rate region is given by:\nH (U | V ) ≤ R 1 \t (11) H (V | U ) ≤ R 2 \t (12)\nOur ﬁrst result states sufﬁcient conditions for transmissibility when the two encoders crib causally, that is, when the encoding functions are given by (4).\nTheorem 3. A source (U, V ) ∼ P U,V is transmissible via the MAC P Y |X 1 ,X 2 with strictly causal cribbing by both encoders at rate ρ = 1 if\nH (U | V ) ≤ H (X 1 | W, V ) \t (14) H (V | U ) ≤ H (X 2 | W, U ) \t (15)\nP U,V (u, v) P W (w) P X 1 |W,U (x 1 | w, u) · P X 2 |W,V (x 2 | w, v) P Y |X 1 ,X 2 (y | x 1 , x 2 )\nwhere W is an external random variable whose size can be bounded as |W| ≤ min {|X 1 | · |X 2 | + 1, |Y| + 2}.\nThe proof of Theorem 3 is omitted due to space considerations. We proceed now to state necessary and sufﬁcient conditions for transmission according to the model (5).\nTheorem 4. A source (U, V ) ∼ P U,V is transmissible via the MAC P Y |X 1 ,X 2 with strictly causal cribbing by the ﬁrst encoder and causal cribbing by the second encoder at rate ρ if and only if there exists a joint input distribution P X 1 ,X 2 such that:\nρH (U | V ) ≤ H (X 1 ) \t (17) ρH (V | U ) ≤ H (X 2 | X 1 ) \t (18)\nThe proof of the achievability part of Theorem (4) is based on separation and resembles the proof for causal cribbing only by encoder 2, presented in [1]. It is omitted due to space considera- tions. The proof of the converse part is given in section V. We turn next to the case of non-causal cribbing by encoder 2, that is, the transmission model described by (3). Here U is decoded losslessly, and we allow distortion for the source V .\nTheorem 5. A source (U, V ) ∼ i p (u i , v i ) can be sent with arbitrarily small probability of error P e for U and distortion D for V over a multiple access channel, with causal or non-causal cribbing by the second encoder at rate ρ = 1 if and only if\nI (V ; W | U ) ≤ I (Y ; X 2 | X 1 ) \t (21) H (U ) + I (V ; W | U ) ≤ I (Y ; X 1 , X 2 ) \t (22)\nwhere W is a random variable deﬁned on the reconstruction alphabet ˆ V.\nAn outline of the proof of Theorem 5 for non-causal cribbing is given in sections VI and VII.\nTheorem 3 provides an achievability scheme in which the channel code is based on the source messages. This scheme does not use the separation principle. Theorem 4 implies that a source P U,V is transmissible via the MAC P Y |X 1 ,X 2 with cribbing model deﬁned by (5) at rate ρ if and only if the Slepian-Wolf region scaled by ρ intersects the capacity region of the MAC with the same cribbing model. This implies that for model (5), a separation strategy, where we use a Slepian-Wolf code to compress the source and a channel code to transmit the source codewords via the MAC, is optimal.\nTheorem 5 reﬂects a weaker form of separation. The encoders construct the channel codebooks independent of the sources. How- ever, encoder 2 must decode encoder\u2019s 1 message in order to choose the appropriate compressed word w. Thus, separation is preformed only in the code design stage but not operatively.\nProof: Suppose that for an i.i.d source P U,V (u, v) and a MAC P Y |X 1 ,X 2 with a cribbing model deﬁned in (5), there exists a (n, n/ρ, )-code. We then have the following inequalities:\n≤ I (U n ; Y m | V n ) + mδ (P e ) \t (24) ≤ I (U n , X m 1 ; Y m | V n ) + mδ (P e ) \t (25) = I (X m 1 ; Y m | V n ) + mδ (P e ) \t (26) ≤ H (X m 1 | V n ) + mδ (P e )\nand (26) follows from the Markovity of (U n , V n ) → (X n 1 , X n 2 ) → Y n .\n≤ I (V n ; Y m | U n ) + mδ (P e ) \t (29) ≤ I (V n , X m 2 ; Y m | U n ) + mδ (P e )\n= I (X m 2 ; Y m | U n ) + mδ (P e ) \t (30) ≤ H (X m 2 | U n ) + mδ (P e )\nHere (29) follows from Fano inequality, (30) follows from the Markovity of (U n , V n ) → (X m 1 , X m 2 ) → Y m , and (31) follows from the encoding equations (5).\nHere (33) follows from Fano inequality, and (34) is due to the Markovity of (U n , V n ) → (X 1i , X 2i ) → Y i\nObserve that the right hand side of (27),(32),(34) (that is, the \"channel coding part\") is independent of the source and the recon- struction, (U i , V i , W i ). Therefore it is possible to choose X 1 , X 2 independent of U i , V i , W i (35), without affecting the constraints. Deﬁning X 1 = (X 1Q , Q) , X 2 = (X 2Q , Q) , Y = (Y Q , Q),\nenables us to use the standard time-sharing argument, resulting in the ﬁnal conditions of Theorem (4). Thus, the converse is proved.\nFix P W |U,V (w | u, v) , P X 1 (x 1 ), P X 2 |X 1 (x 2 | x 1 ) satisfying the conditions of Theorem 5. Let R V |U = I (V ; W | U ) + δ for some δ > 0.\n1) Generate 2 2nR V |U codewords w of length n, sampled iid from a marginal distributions P W |U (w | u) and assign them to a bin denoted by B (u).\n2) For every pair (u, v) ﬁnd a sequence w ∈ B (u) such that (u, v, w) ∈ A (U, V, W ). If no sequence is found, choose randomly. Label it w (v | u) .\n3) For every sequence u generate a sequence x 1 with probabil- ity P r {x 1 } = n i=1 p (x 1i ) . Label it x 1 (u).\n4) For every pair (w (v | u) , x 1 ) generate a sequence x 2 with probability P r {x 2 } = n i=1 p (x 2i | x 1i ) . Label it x 2 (w, x 1 ).\nGiven a source sequences u, encoder 1 transmits the codeword x 1 (u). Encoder 2 estimates ˆ u by cribbing into x 1 , and then transmits x 2 (w (v | ˆ u) , x 1 ).\nGiven a channel output sequence y the decoder looks for a source sequence ˆ u and a \u201ccompressed\u201d sequence ˆ w such that\ndeclares an error and incurs a distortion equal to D max . D. Error Probability:\nSuppose that (u, v) was the source output pair, then an error is made if\n(u , w , x 1 (u ), x 2 (w , x 1 ), y) ∈ A (U, W, X 1 , X 2 , Y ) w ∈ B(u )\nE \t (u, w, x 1 (u), x 2 (w, x 1 ), y) / ∈ A (U, W, X 1 , X 2 , Y )\nE v \t No sequence w ∈ B (u) was found such that (u, v, w) ∈ A (U, V, W )\nE u \t An error was made by encoder 2 in decoding u.\nE uw \t An error was made by the decoder in decoding w.\nE u w \t An error was made by the decoder in decoding u.\nE u w An error was made by the decoder in decoding both w and u.\nThe event that an error was made at the decoder E, is the union of the above events:\nE = E ∪ E v ∪ E u ∪ E uw ∪ E u w ∪ E u w We denote the total probability of error with P e .\n= P r {∃u = u : x 1 (u ) = x 1 (u)} =\n= P r {∃w = w : (u, w , x 1 (u) , x 2 (w , x 1 ) , y) ∈ A (U, W, X 1 , X 2 , Y ) , w ∈ B(u)}\nP r {(x 1 (u) , x 2 (w , x 1 ) , y) ∈ A (X 1 , X 2 , Y )}\n5) P r E u w b \t = P r E u w b \t : Where the equality above is due to binning. That is, if the decoder falsely decodes u then it must also falsely decode w since w ∈ B (u) .\n= P r {∃u = u, w = w : (u , w , x 1 (u ) , x 2 (w , x 1 ) , y) ∈ A (U, W, X 1 , X 2 , Y ) , w ∈ B(u )}\nIt follows that for n large enough, P e < if the source P U,V satisﬁes the conditions of Theorem 5.\nThe distortion of the proposed scheme is bounded by ˆ D ≤ (1 − P e ) D + P e D max .\nProof: Suppose that for an i.i.d source P U,V (u, v) and a MAC P Y |X 1 ,X 2 with a cribbing model deﬁned in (3), there exists a (n, n/ρ, )-code. Deﬁne W i \t φ i (Y m ), where φ i is a deterministic function of Y m . We have the following inequalities:\nwhere (36) follows from the deﬁnition of W i \t φ i (Y m ), (37) from the encoding equations (3), and (38) from the Markovity of (U n , V n ) → (X 1i , X 2i ) → Y i .\n= H (U n ) + H (V n | U n ) − H (V n | U n , W n ) = H (U n ) + I (V n ; W n | U n ) ≤ H (U n ) + I (V n ; Y m | U n )\nwhere (39) is due to Fano inequality, and (40) due to the Markovity of (U n , V n ) → (X 1i , X 2i ) → Y i . Therefore we obtained:\nAs for the joint distribution, the Markov conditions are satisﬁed. We omit the details due to space considerations. Thus, it is possible to choose an arbitrary input joint distribution P X 1 ,X 2 . By choosing ρ = 1 and using the time-sharing argument we get the conditions of Theorem 5."},"refs":[{"authors":[{"name":"E. Amir"},{"name":"Y. Steinberg"}],"title":{"text":"The Multiple Access Channel with Correlated Sources and Cribbing Encoders\""}},{"authors":[{"name":"T. M. Cover"},{"name":"A. A. E. Gamal"},{"name":"M. Salehi"}],"title":{"text":"Multiple access channels with arbitrarily correlated sources"}},{"authors":[{"name":"I. Csiszá"},{"name":"J. Körne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems , Academic Press, London, 1981"}},{"authors":[{"name":"D. Slepian"},{"name":"J. K. Wolf"}],"title":{"text":"A coding theorem for multiple access channels with correlated sources"}},{"authors":[{"name":"J. Willems"},{"name":"C. van der Meulen"}],"title":{"text":"The discrete memoryless multiple access channel with cribbing encoders"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565887.pdf"},"links":[{"id":"1569566381","weight":7},{"id":"1569566725","weight":7},{"id":"1569564635","weight":7},{"id":"1569564481","weight":15},{"id":"1569566765","weight":7},{"id":"1569563411","weight":7},{"id":"1569565291","weight":7},{"id":"1569566751","weight":7},{"id":"1569566843","weight":7},{"id":"1569566579","weight":7},{"id":"1569565455","weight":23},{"id":"1569566709","weight":15},{"id":"1569564189","weight":7},{"id":"1569565907","weight":15},{"id":"1569563981","weight":7},{"id":"1569566753","weight":7},{"id":"1569558859","weight":7},{"id":"1569565213","weight":15},{"id":"1569561143","weight":15},{"id":"1569566423","weight":7},{"id":"1569558901","weight":7},{"id":"1569553909","weight":7},{"id":"1569559111","weight":7},{"id":"1569566939","weight":7},{"id":"1569552251","weight":15},{"id":"1569554881","weight":7},{"id":"1569565655","weight":15},{"id":"1569566909","weight":15},{"id":"1569558985","weight":15},{"id":"1569566257","weight":7},{"id":"1569565033","weight":15},{"id":"1569566447","weight":7},{"id":"1569563897","weight":7},{"id":"1569566721","weight":7},{"id":"1569565633","weight":7},{"id":"1569565219","weight":7},{"id":"1569566037","weight":7},{"id":"1569565029","weight":7},{"id":"1569561245","weight":7},{"id":"1569565311","weight":7},{"id":"1569566667","weight":7},{"id":"1569566317","weight":7},{"id":"1569564097","weight":7},{"id":"1569566229","weight":15},{"id":"1569562551","weight":7},{"id":"1569563395","weight":15},{"id":"1569555367","weight":7},{"id":"1569566383","weight":7},{"id":"1569565665","weight":7},{"id":"1569565397","weight":7},{"id":"1569565263","weight":7},{"id":"1569566129","weight":7},{"id":"1569565385","weight":7},{"id":"1569565919","weight":15},{"id":"1569565661","weight":15},{"id":"1569565511","weight":7},{"id":"1569561221","weight":7},{"id":"1569564595","weight":7},{"id":"1569565829","weight":7},{"id":"1569566237","weight":7},{"id":"1569566283","weight":7},{"id":"1569565375","weight":7},{"id":"1569565597","weight":7},{"id":"1569564247","weight":7},{"id":"1569556759","weight":15},{"id":"1569561185","weight":15},{"id":"1569566075","weight":7},{"id":"1569566435","weight":7},{"id":"1569567483","weight":7},{"id":"1569566299","weight":15},{"id":"1569564769","weight":7},{"id":"1569565805","weight":7},{"id":"1569561713","weight":7},{"id":"1569557851","weight":30},{"id":"1569565389","weight":7},{"id":"1569559251","weight":15},{"id":"1569565635","weight":7},{"id":"1569565113","weight":15},{"id":"1569564257","weight":15},{"id":"1569564931","weight":7},{"id":"1569564141","weight":7},{"id":"1569551751","weight":7},{"id":"1569565139","weight":7},{"id":"1569564419","weight":7},{"id":"1569564807","weight":7}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T4.4","endtime":"11:10","authors":"Eliron Amir, Yossef Steinberg","date":"1341485400000","papertitle":"Joint Source-Channel Coding for Cribbing Models","starttime":"10:50","session":"S11.T4: Joint Source-Channel Coding in Networks","room":"Stratton 20 Chimneys (306)","paperid":"1569565887"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
