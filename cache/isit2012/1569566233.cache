{"id":"1569566233","paper":{"title":{"text":"Directed Information on Abstract Spaces: Properties and Extremum Problems"},"authors":[{"name":"Charalambos D. Charalambous"},{"name":"Photios A. Stavrou"}],"abstr":{"text":"Abstract\u2014This paper describes a framework in which directed information is deﬁned on abstract spaces. The framework is employed to derive properties of directed information such as convexity, concavity, lower semicontinuity, by using the topology of weak convergence of probability measures on Polish spaces. Two extremum problems of directed information related to capac- ity of channels with memory and feedback, and non-anticipative and sequential rate distortion are analyzed showing existence of maximizing and minimizing distributions, respectively."},"body":{"text":"Directed information from a sequence of Random Variables (RV\u2019s) X n = {X 0 , X 1 , . . . , X n } ∈ X 0,n = × n i=0 X i , to another sequence Y n = {Y 0 , Y 1 , . . . , Y n } ∈ Y 0,n = × n i=0 Y i is often deﬁned via [1], [2] 1\n(2) ≡ I X n →Y n (P X i |X i−1 ,Y i−1 , P Y i |Y i−1 ,X i : i = 0, . . . , n) (3)\nP Y j |Y j−1 ,X j (dy j |y j−1 , x j ), the notation I X n →Y n (·, ·) denotes the functional dependence on two collections of non-anticipative or causal conditional distributions {P X i |X i−1 ,Y i−1 (·|·, ·), P Y i |Y i−1 ,X i (·|·, ·) : i = 0, 1, . . . , n}.\nIn information theory, directed information (1)-(3) or its variants are used to characterize capacity of channels with memory and feedback [3], [4], [5], lossy data compression with feedforward information at the decoder [6], lossy data compression of sequential codes [4], lossy data compression of non-anticipative codes [7], and capacity of networks such as the two-way channel, multiple access channel [8], [9], etc. The previous references derive coding theorems based on a) stationary ergodic processes {(X i , Y i )} ∞ i=0 , b) Dobrushin\u2019s stability of the information density\n, and c) via information spectrum methods [10].\noperational deﬁnition of channels with memory and feedback is given by\nSequential and Non-Anticipative Rate Distortion Function. Based on a) or b) the operational deﬁnition of sequential and non-anticipative rate distortion function is given by expression\nThe complete investigation of existence, characterization, and properties of the above extremum problems requires ex- tensive analysis of the functional I X n →Y n (·, ·) as deﬁned in (3). This is analogous to capacity of channels without feedback which involves maximization of mutual information I(X n ; Y n ) over the power constraint set, and to classical rate distortion function which involves minimization of mutual information I(X n ; Y n ) over the ﬁdelity constraint. However, mutual information I(X n ; Y n ) ≡ I X n ;Y n (P X n , P Y n |X n ), in- herits from its information divergence deﬁnition I(X n ; Y n ) = D(P X n ,Y n ||P X n × P Y n ), several important functional proper- ties such as convexity, concavity, lower semicontinuity, etc. These properties are vital both for ﬁnite alphabet spaces, as well as abstract alphabet spaces [11], [12]. The difﬁ- culty associated with directed information I(X n → Y n ), rises from the fact that this information measure (1)-(3) is a functional I X n →Y n (·, ·) of the collection of conditional distributions {P X i |X i−1 ,Y i−1 (·|·, ·) : i = 0, 1, . . . , n} and {P Y i |Y i−1 ,X i (·|·, ·) : i = 0, 1, . . . , n}.\nThe objective of this paper is to address the following ques- tions, when X 0,n and Y 0,n are complete separable metric spaces (Polish spaces).\n1. Is there an equivalent directed information deﬁnition ex- pressed via information divergence D(·||·) as a functional of two appropriate conditional distributions P(·|y) on\nX N = × ∞ i=0 X i for y = (y 0 , y 1 , . . .) ∈ Y N = × ∞ i=0 Y i and Q(·|x) on Y N for x ∈ X N which uniquely deﬁne {P X i |X i−1 ,Y i−1 : i = 0, 1, . . .} and {P Y i |Y i−1 ,X i : i = 0, 1, . . .}, respectively, and vice-versa?\n2. Is directed information convex and concave functional with respect to the conditional distributions P(·|y) and Q(·|x)?\n3. Is directed information a lower semicontinuous functional of the conditional distributions P(·|y) and Q(·|x)?\n4. What are appropriate conditions for existence of the max- imizing encoder admissible distributions and minimizing distortion admissible distributions?\nThis paper answers the above questions by invoking the topology of weak convergence of probability measures on Polish spaces and Prohorov\u2019s theorems.\nThe paper is organized as follows. Section II provides the construction of two equivalent deﬁnitions of causal channels on abstract spaces while in Section III the main properties of directed information are given. Finally, in Section IV the extremum problems (4), (5) are discussed. The derivations of theorems are outlined since they are quite lengthy.\nIn this section, the aim is to establish two equivalent deﬁ- nitions of conditional distributions or basic processes, which deﬁne any probabilistic channel with causal feedback, that relates causally the input-output behavior of any channel. This formulation is necessary to investigate questions 1.\u20134.\nLet N = {0, 1, 2, . . .}, and N n = {0, 1, 2, . . . , n}. Intro- duce two sequence of spaces {(X n , B(X n )) : n ∈ N} and {(Y n , B(Y n )) : n ∈ N}, where X n , Y n , n ∈ N are topological spaces, and B(X n ) and B(Y n ) are Borel σ−algebras of subsets of X n and Y n , respectively. Points in X N = × n∈N X n , Y N = × n∈N Y n are denoted by x = {x 0 , x 1 , . . .} ∈ X N , y = {y 0 , y 1 , . . .} ∈ Y N , respectively, while their restrictions to ﬁnite coordinates by x n = {x 0 , x 1 , . . . , x n } ∈ X 0,n , y n = {y 0 , y 1 , . . . , y n } ∈ Y 0,n , for n ∈ N.\nLet B(X N ) = i∈N B(X i ) denote the σ−algebra on X N generated by cylinder sets {x = (x 0 , x 1 , . . .) ∈ X N : x 0 ∈ A 0 , x 1 ∈ A 1 , . . . , x n ∈ A n }, A i ∈ B(X i ), 0 ≤ i ≤ n, n ≥ 1, and similarly for B(Y N ) = i∈N B(Y i ). Hence, B(X 0,n ) and B(Y 0,n ) denote the σ−algebras of cylinder sets in X N and Y N , respectively, with bases over A i ∈ B(X i ), and B i ∈ B(Y i ), 0 ≤ i ≤ n, respectively.\nBackward or Feedback Channel. Suppose for each n ∈ N, the distributions {p n (dx n |x n−1 , y n−1 ) : n ∈ N} with p 0 (dx 0 |x −1 , y −1 ) = p 0 (x 0 ) satisfy the following conditions.\ni) For n ∈ N, p n (·|x n−1 , y n−1 ) is a probability measure on B(X n );\nGiven the collection {p n (dx n |x n−1 , y n−1 ) : n ∈ N} satisfying conditions i), ii), one can construct a family of distributions\nLet C ∈ B(X 0,n ) be a cylinder set of the form C = x ∈ X N : x 0 ∈ C 0 , x 1 ∈ C 1 , . . . , x n ∈ C n , C i ∈ B(X i ), 0 ≤ i ≤ n. Deﬁne a family of measures P(·|y) on B(X N ) by\nn−1 ) is used to denote the restriction of the measure P(·|y) on cylinder sets C ∈ B(X 0,n ), for n ∈ N. Thus, if conditions i) and ii) hold then for each y ∈ Y N , the right hand side of (6) deﬁnes a consistent family of ﬁnite-dimensional distribution on (X N , B(X N )), and hence there exists a unique measure on (X N , B(X N )), from which p n (dx n |x n−1 , y n−1 ) is obtained. This leads to the ﬁrst, usual deﬁnition of a feedback channel, as a family of functions p n (dx n |x n−1 , y n−1 ) satisfying conditions i) and ii).\nAn alternative, equivalent deﬁnition of a feedback channel is established as follows. Introduce the assumption\niii) {X n : n ∈ N} are complete separable metric spaces (Polish Spaces) and {B(X n ) : n ∈ N} are the σ−algebras of Borel sets.\nConsider a family of measures P(·|y) on (X N , B(X N )) satis- fying the following consistency condition.\nC1: If E ∈ B(X 0,n ), then P(E|y) is B(Y 0,n−1 )−measurable function of y ∈ Y N .\nThen, by assumption iii), for any family of measures P(·|y) satisfying C1 one can construct a collection of versions of conditional distributions {p n (dx n |x n−1 , y n−1 ) : n ∈ N} satisfying conditions i) and ii) which are connected with P(·|y) via relation (6).\nTherefore, for Polish Spaces {X n : n ∈ N} the second equivalent deﬁnition is given by a family of measures P(·|y) on (X N , B(X N )) depending parametrically on y ∈ Y N and satisfying the consistency condition C1.\nThe point to be made here is that the second equivalent deﬁ- nition of a feedback channel, together with similar deﬁnition for the forward channel is convenient to deﬁne directed infor- mation via relative entropy, similar to the mutual information deﬁnition, and extend well-known functional properties of mutual information to directed information.\nForward Channel. The previous methodology is repeated for the collection of functions {q n (dy n |y n−1 , x n ) : n ∈ N} which satisfy the following conditions.\niv) For n ∈ N, q n (·|y n−1 , x n ) is a probability measure on B(Y n );\nSimilarly as before, there exists a unique measure on (Y N , B(Y N )) for which the family of distributions {q n (dy n |y n−1 , x n ) : n ∈ N} is obtained. Introduced the assumption\nvi) {Y n : n ∈ N} are Polish Spaces and {B(Y n ) : n ∈ N} are the σ−algebras of Borel sets.\nConsider a family of measures Q(D|x) satisfying the following consistency condition.\nC2: If F ∈ B(Y 0,n ), then Q(F |x) is B(X 0,n )−measurable function of x ∈ X N .\nThen, by assumption vi), for any family of measures Q(·|x) on (Y N , B(Y N )) satisfying consistency condition C2 one can construct a collection of functions {q n (dy n |y n−1 , x n ) : n ∈ N} satisfying conditions iv) and v) which are connected with Q(·|x) via relation (8). Note that Kolmogorov\u2019s extension theorem guarantees the construction of countable additive probability measures for both P(·|y) and Q(·|x).\nGiven the basic measures P(·|y) on X N and Q(·|x) on Y N satisfying consistency condition C1 and C2, respectively, construct the collections of conditional distributions as follows.\nLet A (n) = {x : x n ∈A}, A ∈ B(X n ) and B (n) = {y : y n ∈B}, B ∈ B(Y n ). In addition, let P(A (n) |y|B(X 0,n−1 )) denote the conditional probability of A (n) with respect to B(X 0,n−1 ) calculated on the probability space X N , B(X N ), P(·|y) , and similarly for Q(B (n) |x|B(Y 0,n−1 )). Then\nNote that p n (·; ·, ·) ∈ Q(X n ; X 0,n−1 ×Y 0,n−1 ) and q n (·; ·, ·) ∈ Q(Y n ; Y 0,n−1 × X 0,n ) are stochastic kernels [13], determined from P(·|·) and Q(·|·), respectively, (e.g., related via (6), (8)). The distribution of RV\u2019s {(X i , Y i ) : i ∈ N} is deﬁned by\nHence, for any P(·|·) and Q(·|·) satisfying consistency con- ditions there exist a probability space and a sequence of RV\u2019s {(X i , Y i ) : i ∈ N} deﬁned on it, whose joint probability distribution is deﬁned uniquely via P(·|·) and Q(·|·).\nIn this section, directed information I(X n → Y n ) will be deﬁned via relative entropy, using the basic measures P(·|y)\nGiven conditional distributions P(·|·) ∈ Q C1 (X N ; Y N ) and Q(·|·) ∈ Q C2 (Y N ; X N ) deﬁne the following measures.\nP3: The marginal distributions on Y N deﬁned uniquely for B i ∈ B(Y i ), 1 ≤ i ≤ n by\nP4: The measure − → Π 0,n : B(X 0,n ) B(Y 0,n ) → [0, 1] deﬁned uniquely for A i ∈ B(X i ), B i ∈ B(Y i ), 1 ≤ i ≤ n by\nP5: The measure ← − Π 0,n : B(Y 0,n ) B(X 0,n ) → [0, 1] deﬁned uniquely for A i ∈ B(X i ), B i ∈ B(Y i ), 1 ≤ i ≤ n by\nBy invoking the deﬁnition of directed information (1) or (2), it can be shown that\nThe right hand side of (11) follows from repeated application of chain rule of relative entropy [13], while (12) follows from the fact that\n⊗ − → Q 0,n << ← − P 0,n ⊗ ν 0,n if and only if\n0,n ⊗ ν 0,n then the Radon- Nikodym derivative (\nLet Q C1 (X 0,n ; Y 0,n−1 ), Q C2 (Y 0,n ; X 0,n ) be the restrictions of Q C1 (X N ; Y N ) and Q C2 (Y N ; X N ), respectively, to cylin- der sets with bases over A i ∈ B(X i ), and B i ∈ B(Y i ), i = 0, 1, . . . , n. These are regular conditional distributions.\nTheorem 1. Let {(X n , B(X n )) : n ∈ N}, {(Y n , B(Y n )) : n ∈ N} be Polish spaces. Then\n2) I X n →Y n ( ← − P 0,n , − → Q 0,n ) is a convex functional of − → Q 0,n ∈ Q C2 (Y 0,n ; X 0,n ) for a ﬁxed ← − P 0,n ∈ Q C1 (X 0,n ; Y 0,n−1 ).\n3) I X n →Y n ( ← − P 0,n , − → Q 0,n ) is a concave functional of ← − P 0,n ∈ Q C1 (X 0,n ; Y 0,n−1 ) for a ﬁxed − → Q 0,n ∈ Q C2 (Y 0,n ; X 0,n ).\nProof: 1) Utilize the convexity of regular conditional distributions, and then the consistency condition C1, C2. 2), 3), follow from log-sum formulae.\nThis part discusses the lower-semicontinuity and continuity of directed information as a functional of\nn−1 ) ∈ Q C1 (X 0,n ; Y 0,n−1 ) and − → Q 0,n (·|x n ) ∈ Q C2 (Y 0,n ; X 0,n ). Be- fore establishing the main results, sufﬁcient conditions for weak compactness of the set of measures Q C1 (X 0,n ; Y 0,n−1 ), Q C2 (Y 0,n ; X 0,n ), and joint and marginal measures are given.\nTheorem 2. Part A. Let Y 0,n be a compact Polish space and X 0,n a Polish space. Assume\nn−1 ) ∈ Q C1 (X 0,n ; Y 0,n−1 ) satisfy the following condition.\nCA: For all g(·)∈BC(X 0,n ), where BC(X 0,n ) denotes the set of bounded continuous real-valued functions on X 0,n ,\nis jointly continuous in (x n−1 , y n−1 ) ∈ X 0,n−1 × Y 0,n−1 . Then the following weak convergence results hold. A1) Let\nQ α 0,n (·|x n ) α≥1 ∈ Q C2 (Y 0,n ; X 0,n ). Then the joint measure (\n)(dx n , dy n ) \t w =⇒ (\nQ α 0,n (·|x n ) α≥1 ∈ Q C2 (Y 0,n ; X 0,n ) and deﬁne the family of joint measures (\n)(dx n , dy n ) α≥1 having marginals {ν α 0,n } α≥1 on Y 0,n and {µ α 0,n } α≥1 on X 0,n . Then ν α 0,n (dy n ) \t w =⇒ ν 0 0,n (dy n ) and µ α 0,n (dx n ) w =⇒ µ 0 0,n (dx n ) where ν 0 0,n ∈ M 1 (Y 0,n ) and µ 0 0,n ∈ M 1 (X 0,n ) are the marginals of (\nA3) The sets of measures Q C1 (X 0,n ; Y 0,n−1 ), and Q C2 (Y 0,n ; X 0,n ) are weakly compact.\n. Then − → Π α\n(dy n ) w =⇒ ← − P\n(dx n , dy n ), where ν 0 0,n ∈ M 1 (Y 0,n ) is the weak limit of ν α 0,n ∈ M 1 (Y 0,n ).\nPart B. Let X 0,n be a compact Polish space and Y 0,n a Polish space. Assume\n(·|x n ) ∈ Q C2 (Y 0,n ; X 0,n ) satisfy the following condition.\nis jointly continuous in (x n , y n−1 ) ∈ X 0,n × Y 0,n−1 . The statements of Part A hold by interchanging\nProof: The proof is quite lengthy and it is based on Prohorov\u2019s theorem relating tightness and weak compactness of a family of probability measures [13].\nThe results of Theorem 2 are sufﬁcient to establish lower semicontinuity of directed information I(X n → Y n ) ≡\nTheorem 3. 1) Suppose the conditions in Theorem 2, Part A hold. Then I X n →Y n ( ← − P 0,n , − → Q 0,n ) is lower semi- continuous on\nProof: Utilizes (11), Theorem 2, and lower semiconti- nuity of relative entropy.\nFor capacity problems, it is desirable to identify conditions so that I X n →Y n ( ← − P 0,n , − → Q 0,n ) as a function of ← − P 0,n for ﬁxed − →\nn ) ∈ Q C2 (Y 0,n ; X 0,n ), and a closed family of feedback channels Q c,C1 (X 0,n ; Y 0,n−1 ) ⊆ Q C1 (X 0,n ; Y 0,n−1 ). Suppose there exists a family of measures ¯ ν 0,n (dy n ) on (Y 0,n , B(Y 0,n )) such that\n1) \t the \t family \t of \t Radon-Nikodym \t derivatives ξ ¯ ν 0,n (x n , y n ) is continuous on X 0,n × Y 0,n , and ξ ¯ ν 0,n (x n , y n ) log ξ ¯ ν 0,n (x n , y n ) is uniformly integrable over {¯ ν 0,n ⊗\n2) for a ﬁxed y n ∈ Y 0,n , the Radon-Nikodym derivative ξ ¯ ν 0,n (x n , y n ) is uniformly integrable over\nThen, the directed information I X n →Y n ( ← − P 0,n , − → Q 0,n ) as a functional of {\nConsider a communication channel with memory and feed- back\n(·|x n ) ∈ Q C2 (Y 0,n ; X 0,n ) and power constraint ← − P\nwhere for any n ∈ N, g 0,n : X 0,n ×Y 0,n−1 −→ [0, ∞] is Borel measurable, and\n0,n (P ) non-empty. In the absence of any power constraints the set of input conditional distributions is\nP 0,n (P ) or Q C1 (X 0,n ; Y 0,n−1 ) (e.g., with or without power constraints) is deﬁned by\nTheorem 5. Suppose the assumptions of Theorem 2, Part A are satisﬁed.\n2) Suppose g 0,n : X 0,n × Y 0,n−1 −→ [0, ∞] is measurable and continuous in (x n , y n−1 ) ∈ X 0,n × Y 0,n−1 . Then the set\n3) If in addition the assumptions of Theorem 4 are sat- isﬁed (here the assumption on Q C1 (X 0,n ; Y 0,n−1 ) is satisﬁed by 1) and 2) ) then C f 0,n has a maximum in Q C1 (X 0,n ; Y 0,n−1 ) (without constraints) or in ← − P 0,n (P ) (with power constraints).\nProof: 1) Utilize the fact that probability measures on compact Polish spaces are compact. 2) Utilize the fact that closed subset of weakly compact set is compact. 3) Follows from Weierstrass theorem.\nB. Existence of Non-Anticipative Rate Distortion Achieving Distribution\n(·|x n ) ∈ Q C2 (Y 0,n ; X 0,n ), a ﬁxed source µ 0,n (dx n ) ∈ M 1 (X 0,n ), and deﬁne the ﬁdelity constraint by\nwhere D ≥ 0, and for each n ∈ N, d 0,n : X 0,n × Y 0,n −→ [0, ∞] is Borel measurable, and − → Q 0,n (D) is non-empty. The ﬁnite horizon minimization of directed information over − →\nTheorem 6. Let X 0,n be a Polish space and Y 0,n a compact Polish space. Assume ∀ h(·)∈BC(Y 0,n ), the function\nis continuous jointly in (x n , y n−1 ) ∈ X 0,n × Y 0,n−1 . Then\n2) Assume d 0,n : X 0,n × Y 0,n −→ [0, ∞] is measurable and continuous on y n ∈ Y 0,n . Then\n0,n (D) is a closed subset of Q C2 (Y 0,n ; X 0,n ).\nProof: Utilize Theorem 2, Part A, and generalize the derivation in [12] to (n + 1)−fold convolution measures.\nIn this paper we have provided a general framework through which the properties of mutual information are extended to directed information on Polish spaces. The existence of extremums to capacity problems with memory and feedback, and to lossy non-anticipative data compression problems are discussed."},"refs":[{"authors":[{"name":"H. Marko"}],"title":{"text":"The bidirectional communication theory\u2013A generalization of information theory"}},{"authors":[{"name":"J. L. Massey"}],"title":{"text":"Causality, feedback and directed information"}},{"authors":[{"name":"S. Tatikonda"},{"name":"S. Mitter"}],"title":{"text":"The capacity of channels with feedback"}},{"authors":[{"name":"S. C. Tatikonda"}],"title":{"text":"Control over communication constraints"}},{"authors":[{"name":"J. Chen"},{"name":"T. Berger"}],"title":{"text":"The capacity of ﬁnite-state Markov channels with feedback"}},{"authors":[{"name":"R. Venkataramanan"}],"title":{"text":"Information-theoretic results on communication problems with feed-forward and feedback"}},{"authors":[{"name":"P. A. Stavrou"},{"name":"C. D. Charalambous"},{"name":"C. K. Kourtellaris"}],"title":{"text":"Causal rate distortion function on abstract alphabets: Optimal reconstruction and properties"}},{"authors":[{"name":"G. Kramer"}],"title":{"text":"Directed information for channels with feedback"}},{"authors":[],"title":{"text":"Capacity results for the discrete memoryless network"}},{"authors":[{"name":"T. S. Han"},{"name":"S. Verdu"}],"title":{"text":"Approximation theory of output statistics"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"Arbitrarily varying channels with general alphabets and states"}},{"authors":[],"title":{"text":"On an extremum problem of information theory"}},{"authors":[{"name":"P. Dupui"},{"name":"R. S. Elli"}],"title":{"text":"A Weak Convergence Approach to the Theory of Large Deviations "}},{"authors":[{"name":"M. Fozunbal"},{"name":"S. McLaughlin"},{"name":"R. Schafer"}],"title":{"text":"Capacity analysis for continuous-alphabet channels with side information, part I: A general framework"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566233.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T8.1","endtime":"15:00","authors":"Charalambos D Charalambous, Photios A. Stavrou","date":"1341240000000","papertitle":"Directed Information on Abstract Spaces: Properties and Extremum Problems","starttime":"14:40","session":"S3.T8: Directed Information, Common Information, and Divergence","room":"Stratton (491)","paperid":"1569566233"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
