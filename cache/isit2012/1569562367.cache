{"id":"1569562367","paper":{"title":{"text":"The minimax risk of truncated series estimators for symmetric convex polytopes"},"authors":[{"name":"Adel Javanmard ∗"},{"name":"Li Zhang"}],"abstr":{"text":"Abstract\u2014We study the optimality of the minimax risk of truncated series estimators over symmetric convex polytopes. We show that the optimal truncated series estimator is within O(log m) factor of the optimal if the polytope is deﬁned by m hyperplanes. This represents the ﬁrst such bounds towards general convex bodies. In proving our result, we ﬁrst deﬁne a geometric quantity, called the approximation radius, for lower bounding the minimax risk. We then derive our bounds by es- tablishing a connection between the approximation radius and the Kolmogorov width, the quantity that provides upper bounds for the truncated series estimator. Besides, our proof contains several ingredients which might be of independent interest: 1. The notion of approximation radius depends on the volume of the body. It is an intuitive notion and is ﬂexible to yield strong minimax lower bounds; 2. The connection between the approximation radius and the Kolmogorov width is a consequence of a novel duality relationship on the Kolmogorov width, developed by utilizing some classical results from convex geometry [1], [18], [6]."},"body":{"text":"In this paper, we study the minimax risk of estimators over symmetric convex polytopes. We show that for a symmetric convex polytope deﬁned by m hyperplanes, the truncated series estimator, a well-known special type of linear estimator, is within O(log m) factor of the optimal.\nIn non-parametric statistics, the minimax risk of an estima- tor measures the worst case expected loss of the estimator for input coming from some subset X ⊆ R n (see Section II-B for a formal deﬁnition). Tremendous work has been done on understanding the optimal minimax risk for various families of X. But it is usually very difﬁcult to design the optimal estimator. Truncated series estimators are a family of linear estimators that simply project an observation to a properly chosen subspace. Despite its simplicity, the truncated series estimator is surprisingly powerful and is shown to be nearly optimal for wide families of convex bodies. [16] shows that such estimator is nearly optimal for ellipsoids. In [4], it is shown that it is nearly optimal for the wider family of orthosymmetric and quadratically convex objects, including p balls for p ≥ 2. Another family of sets for which minimax risk has been explored are the full approximation sets. Lorentz [10] proposed metric entropy bounds on this family. Using these bounds, Yang and Barron [21] showed that truncated series estimators indeed achieve near optimal minimax risk over these sets.\nIn this paper, we show that the power of truncated series estimator extends to the rich class of symmetric polytopes.\nSpeciﬁcally, we show that for a symmetric convex polytope de- ﬁned by m hyperplanes, the truncated series estimator is within O(log m) factor of the optimal. Previously, such results have only been obtained for particular family of convex polytopes such as those corresponding to the Lipschitz condition [13] or satisfying certain isometric conditions [17]. As a motivating example, we discuss one application of our result in estimating values of a Lipschitz function.\nExample. One important estimation problem in the literature is the estimation of functions satisfying certain continuity or Lipschitz conditions from noisy measurements. Consider a univariate Lipschitz function f : [0, 1] → R. Suppose that x i = f (t i ) for i = 1, . . . , n, and we have measurements y i according to the model y i = x i + w i for some gaussian noise w i . Then Lipschitz condition, with constant L, translates to the linear constraints:\nWhen the sampling is uniform, i.e. t i = (i − 1)/(n − 1), then X has a more special form of X = {x : |x i+1 − x i | ≤ L/(n − 1)}. In this case, previous work [13], [19] has shown that the best truncated series estimator is nearly optimal. As a consequence of our work, the truncated series estimator is nearly optimal (within O(log n) factor) for estimating Lips- chitz function at arbitrary sample set {t 1 , . . . , t n }. In addition, this extends to higher order Lipschitz or continuity conditions as those conditions can be represented as n O(1) symmetric linear constraints.\nAt the high level, the proof of our results follows a very simple strategy. We choose a family of \u201cobstruction objects\u201d for which we can obtain lower bounds of the minimax risk. Then we show a \u201cduality\u201d result that if X does not have a good truncated series estimator, then it will have to contain a \u201clarge\u201d obstruction, and therefore no estimator can do well on X. Of course, the difﬁculty is in choosing the obstruction so that we can prove the corresponding duality result. Some natural obstructions include hyper-rectangles and Euclidean balls, for which we know very tight minimax lower bound. But they turn out to be too restrictive to allow a strong enough duality result. To overcome this difﬁculty, we consider a broader family consisting of objects which contain a \u201cnon-negligible\u201d fraction\nof a \u201clarge\u201d Euclidean ball; whence we are able to establish a desired duality relationship.\nMore speciﬁcally, we ﬁrst deﬁne a geometric measure for any set, called approximation radius, and then develop a lower bound technique which bounds the minimax risk of any body by its approximation radius. Intuitively, the approximation radius of an object X is the maximum radius of a ball with \u201cnon-negligible\u201d volume fraction inside X. By reﬁning the technique in [21], [17], we can show that the minimax risk of X is asymptotically as large as that of the ball with X\u2019s approximation radius (see Theorem III.2). On the other hand, the minimax risk of truncated series estimator is determined by the Kolmogorov width of the object. Our bound is then derived by showing that the approximation radius is asymptotically close to the Kolmogorov widths for the symmetric polytopes (see Theorem III.5). For the connection, we ﬁrst derive a duality relationship between the Kolmogorov widths of X and its polar dual X ◦ (see Theorem III.4), by utilizing some results from convex geometry started in [1]. The Kolmogorov width of X ◦ is then shown, by probabilistic arguments, to be intimately related to the approximation radius of X.\nThere is a vast body of work on the minimax estimators and it is beyond the scope of this paper to survey them. We refer to [13], [19], [9] for comprehensive survey and will describe some work most relevant to this paper. Since we focus on the mean squared error (MSE), all the subsequent discussion is in the context of MSE.\nThe minimax bounds have been developed for various families of convex bodies through intensive research in the past decades. Asymptotically tight bounds have been proposed for convex bodies that correspond to various continuity or energy conditions; the classes of H¨older balls, Sobolev balls, and Besov balls. We refer to Chapter 2.8 in [19] for a com- prehensive recount of the references. Despite these remarkable results, it is still largely unknown how to compute the minimax risk for an arbitrary convex body. Some previous work does attempt to deal with less speciﬁc objects (see [17] and the references therein), but all the optimality results are under (fairly strong) isometric assumption about the objects.\nOn the other hand, the truncated series estimator has a nice geometric interpretation and is related to the classical Kolmogorov width of the underlying object. In addition to its simplicity, [4] shows that it is asymptotically optimal for the class of orthosymmetric and quadratically convex objects. This includes the class of diagonally stretched p balls for p ≥ 2. Present paper shows that the power of truncated series estimators also extend to the family of symmetric convex polytopes, as long as the polytope is deﬁned by not so many hyperplanes.\nTo achieve our result, we develop a lower bound technique based on a geometric quantity which we dub approximation radius. Using Fano\u2019s inequality and the reﬁnement developed in [21], [17], we show that the minimax risk of a convex body is lower bounded by that of the ball with radius equal to the\napproximation radius of that body. Compared to the existing lower bound techniques, such as the Bernstein bound and the bound followed from considering the worst (typically discrete) distributions (see [13], [19] and [3], [5]), the approximation radius relies on a volume estimation and is both convenient to operate and ﬂexible to provide strong lower bounds.\nOne center piece in this paper is the connection established between the approximation radius and the Kolmogorov width. Towards this step, we use some results developed in Banach space geometry which was initialized in [1] for investigating the invertibility of matrices with large \u201crobust\u201d rank and subsequently developed by [18], [6]. In particular, we show a duality relationship between the Kolmogorov widths of a convex body and its polar dual body. Our result has a similar ﬂavor to the classical duality in [12] but is tighter when the dimension gap is small.\nDue to space limitations, most of the details in the proofs are omitted from this abstract and can be found in [8]. But we will present all the deﬁnitions (Section II) and results (Section III) as well as crucial steps in proving our results (Section IV). We will also discuss some applications and open questions in Section V.\nFor a vector x = (x 1 , · · · , x n ) and a real number p ≥ 1, denote by x p the p -norm of x, and x ∞ = max i |x i |. When p is absent, it means 2 norm. Let B n p (x, r) denote the n dimensional p ball with radius r and center x. Whenever the center is at the origin, it is denoted by B n p (r). Also, we drop the superscript n, whenever the dimension is clear from the context, and suppress the argument r for r = 1.\nA set X ⊂ R n is called centrally symmetric (or simply symmetric) if for any x ∈ X, we have −x ∈ X. For a set X, the 2 -radius of X is deﬁned as\nIn particular, when p = ∞, F m,n ∞ consists of symmetric convex polytopes deﬁned by m hyperplanes. Throughout we consider bounded convex bodies. Our results easily extend to unbounded convex bodies, but the presentation would be cumbersome including separate case analysis which does not add any new insight.\nSuppose we are given measurements of an unknown n- dimensional vector x, according to the model\ny = x + w, \t (3) where w ∈ R n follows the normal distribution, w ∼ N(0, σ 2 I), and x lies in X, a compact convex set in R n . The goal of the\nminimax estimation problem is to estimate vector x, with small error loss, and to evaluate the estimator under the minimax principle.\nFor any estimator M : R n → R n , the maximum mean squared error of M on (X, σ) is deﬁned as\nEstimators generally can be nonlinear function. We denote by R L (X, σ) the minimax risk when M is linear. A special and commonly used linear estimator is the truncated series estimator [4]. Truncated series estimator is deﬁned by M (y) = P y for some orthogonal projection P from R n to some lower dimensional space. The minimax risk for truncated estimators is deﬁned as\nwhere the minimum is taken over all the orthogonal projec- tions. Since truncated series estimators are linear, we clearly have R(X, σ) ≤ R L (X, σ) ≤ R T (X, σ).\nIt turns out that the minimax risk for truncated series estimators is completely characterized by the Kolmogorov k- width d k of X, deﬁned as [15]\nwhere the minimum is taken over all k-dimensional projec- tions. Then, we have\nFor the mean squared error considered in this paper, there is a more direct equivalent deﬁnition of the Kolmogorov k-width under 2 metric.\nwhere P k denotes all the k-codimensional projections. Recall rad(X) denotes the 2 -radius of X, i.e. max x∈X x 2 . C. Approximation radius\nWe deﬁne the notion of approximation radius, a geometric measure of any convex body, which as we shall show, provides a lower bound for the minimax risk of the body.\nWe use vol(X) to denote the volume of X and H k n to denote all the k dimensional subspaces in R n . Assume X ⊆ R n is a convex body that contains the origin. For any r > 0, the volume ratio vr(X, r) of X is deﬁned as\n(r)) vol(B n 2 (r))\nand the k-volume ratio vr k (X, r) of X is deﬁned as the maximum volume ratio over all the k dimensional central cut of X, i.e.\nClearly, 0 ≤ vr(X, r) ≤ 1. The following is a simple but very useful fact about volume ratio of convex bodies\nFact II.1. If X is convex and contains the origin, then vr(X, r), and hence vr k (X, r) for any k, is non-increasing in r.\nCentral to lower bounding the minimax risk is the notion of approximation radius.\nDeﬁnition II.2. For 0 ≤ c ≤ 1, and integer 1 ≤ k ≤ n, the (c, k)-approximation radius of X, denoted by z c,k (X), is deﬁned as the maximum r such that vr k (X, r) ≥ c, i.e.\nNote that if X contains the origin in its interior, then z c,k (X) is always deﬁned for 0 ≤ c ≤ 1.\nIn this paper, we are interested in the minimax risk of the truncated series estimator for symmetric convex bodies. Deﬁne β(X) = max σ>0 R T (X, σ)/R(X, σ), and β m,n p \t = max X∈F m,n p β(X). Our main result is stated below.\nTheorem III.1. If n = Ω(log m), then β m,n ∞ ≤ c·log m, where c < 2 · 10 8 . Furthermore, β m,n ∞ = Ω( log m/ log log m).\nThe lower bound follows immediately from previous works. As shown in [3] (Theorem 3), for the unit 1 ball X = B n 1 , R T (X, 1/\nn) = O( log n/n). Since B n 1 ∈ F m,n ∞ where m = 2 n , we have β m,n ∞ = Ω( log m/ log log m) for n = Ω(log m). In this paper, our main result is to provide a nearly matching upper bound of O(log m). The upper bound is the consequence of the following theorems: Theorem III.2 lower bounds the minimax risk by the approximation radius; Theorems III.4, III.5 together establish a lower bound on the approximation radius by the Kolmogorov width, which in turn upper bounds the minimax risk of the truncated series estimator.\nTheorem III.2. There exists a universal constant C > 0 such that for any convex set X, and any 0 < c ∗ ≤ 1,\nThe connection between the Kolmogorov width and the approximation radius is established via the polar dual of the body.\nDeﬁnition III.3. For any K ⊂ R n , denote by K ◦ the (polar) dual set of K,K ◦ = {y | x · y ≤ 1 for all x ∈ K}. If K lies on a lower dimensional subspace, K ◦ is understood as the dual set on the lowest dimensional subspace that contains K. Theorem III.4. For any convex centrally symmetric X ⊂ R n and any 0 ≤ k ≤ n and 0 < < 1,\nTheorem III.5. For any X ∈ F m,n ∞ , 0 < c ∗ ≤ 0.2, and 0 < k ≤ n,\nThe paper is mainly devoted to proving Theorems III.2, III.4, and III.5, which together imply Theorem III.1. In the following section, we will sketch the proof of these results. The details can be found in [8].\nThe proof is based on the information-theoretic bound established in [21], [17]. In this technique, the minimax risk is lower bounded by restricting to a maximal ﬁnite set of points {x 1 , · · · , x r } in X, separated from each other by at least an amount in the loss metric. The following proposition is from [17], [21].\nProposition IV.1. For any set X, let N (X) be any -net for X and M δ (X) be a δ-packing in X. Then,\nδ 2\nFor any k and c ∗ consider the k-dimensional central cross section Y of X that attains the approximation radius z c ∗ ,k . Let r k = min{z c ∗ ,k (X),\nkσ}, and Y k = Y ∩ B 2 (r k ). We can then provide an upper bound of N (Y k ) of Y k by N (B 2 (r k )) and a lower bound of M δ (Y k ) by the volume bound. Theorem III.2 then follows from Proposition IV.1.\nWe take a detour to establish the connection between the Kolmogorov width and the approximation radius. The connection is via a novel duality relationship between the Kolmogorov widths of X and those of its polar dual, as stated in Theorem III.4. The proof is an application of some celebrated works in convex geometry [1], [18], [6].\nDeﬁnition IV.2. A set of vectors V = {v 1 , · · · , v s } is called δ-wide if for any 1 ≤ i ≤ s, dist(v i , span[V /{v i }]) ≥ δ.\nThe following proposition concerns an interesting property of δ-wide sets, and can be gleaned from [1], [18], [6].\nProposition IV.3. For any δ-wide set V = {v 1 , · · · , v s }, there exists σ ⊆ {1, . . . , s} with |σ| ≥ (1 − )s such that for any α = (α j ) j∈σ ,\nWrite δ = d k (X). Consider the k + 1 points V = {v 1 , . . . , v k+1 } inside X which forms the largest k+1 simplex. We can show that V is δ-wide. By Proposition IV.3, we show\nthat the convex hull spanned by {±v 1 , . . . , ±v k+1 } contains an Ω((1 − )k) dimensional ball with radius Ω( /kδ). The theorem is proved using the following property of the polar dual of a body.\nFact IV.4. Let H be a subspace of R n . Denote by P H the projection on H. Then P H (K ◦ ) = (H ∩ K) ◦ .\nBy setting = 1/2 in Theorem III.4, we have that X con- tains an Ω(k)-dimension ball with radius Ω( 1/k d k (X)). In the proof of Theorem III.5, we show that we can expand this ball by factor k/ log m so that it still has non-negligible intersection with X. The intuition is that in k-dimensional space, in order to bound a sphere tightly, one needs exp(Ω(k)) hyperplanes. We use a probabilistic argument to make this intuition rigorous.\nThe main theorem follows easily from all these prepara- tions. Let k ∗ = min{k ≥ 1|d k (X) 2 ≤ kσ 2 }. By Eq. (4), R T (X, σ) ≤ d k ∗ (X) 2 + k ∗ σ 2 ≤ 3 min{d k ∗ −1 (X) 2 , k ∗ σ 2 }. On the other hand, by Theorem III.5 and III.4,\nApplying Theorem III.2, we have R(X, σ) = Ω((log m) −1 · min d k ∗ −1 (X) 2 , k ∗ σ 2 ). Combining both bounds, we have that\nThe problem of estimating values of a Lipschitz function, at a set of sampled points, from noisy measurements is discussed in the introduction. Since the Lipschitz condition can be repre- sented as linear conditions, Theorem III.1 is widely applicable to such problems. For example, the function can be deﬁned on any metric space, the sampling points can be arbitrary set of points, and the Lipschitz or continuity condition can be of higher order and even include constraints with different orders. As long as the number of corresponding linear constraints is bounded by n O(1) for n samples, the approximation factor is within a small factor of O(log n) of the optimal.\nIn the above, we showed that β m,n ∞ = O(log m). The celebrated Pinsker bound [16] states that β m,n 2 \t = O(1). What about β m,n p for other p\u2019s? By plugging σ = 1/\nn in Theorem 3 in [3], we have that for 1 ≤ p < 2, β n,n p = Ω((n/ log n) 1−p/2 ). So we will not be able to obtain a similar bound to Theorem III.1 when p < 2. On the other hand, we conjecture that similar upperbound holds when p ≥ 2.\nConjecture V.1. For any p ≥ 2, there exists a constant C = C(p), such that for any m, n ≥ 2, β m,n p ≤ C log m.\nDeﬁne the distance d(X, Y ) between two centrally sym- metric convex body X, Y as the smallest c such that there exists a uniformly scaled orthogonal transformation F such that F Y ⊆ X ⊆ cF Y . We note that d(·, ·) is similar to but different from the classical Banach-Mazur distance in which F is any linear transformation. Also log d(·, ·) is a pseudometric (non-negative, symmetric, and with triangular inequality). By straightforward arguments, β(X) ≤ d(X, Y ) 2 β(Y ). Since d(B n p , B n 2 ) = n 1/2−1/p and d(B n p , B n ∞ ) = n 1/p for p ≥ 2, we have the following nontrivial bound.\nCorollary V.2. For p ≥ 2, β m,n p = O(min(n 1−2/p , m 2/p log m)). In particular, for p ≥ 2, β n,n p = O(\nWe have used the approximation radius to lower bound the minimax risk of a convex body X. How tight is this bound? By comparing this lower bound with the one obtained from the hardest rectangular subproblem (Bernstein width), we can show that it is asymptotically optimal for B n p when p ≥ 2 [8]. Here, we consider B n p for 1 ≤ p < 2 and show that the lower bound of using approximation radius is very close to the minimax upper bound but does leave a small gap of factor of Θ((log n) 1−p/2 ).\nWe start by computing (the order of) z c,k (X). Using a result in [11] regarding the volume of the sections of p balls, we have that\nWhen σ ≤ 1, we choose k ≈ σ −p in Theorem III.2 and obtain a lower bound of R(B n p , σ) = Ω(σ 2−p ). By [3], the optimal upper bound for B n p is R = Θ(σ 2−p (2 log nσ p ) 1−p/2 ) for (1/n) 1/p \t σ \t 1/ log n. Hence the approximation radius bound leaves a gap of Θ((log n) 1−p/2 ). Actually, the largest gap we know of is\nlog n by setting p = 1 in the above bound.\nWe have shown that the truncated series estimator is close to optimal for symmetric convex polytopes. For the family of ellipsoids F m,n 2 , the optimal truncated series estimator can be computed by singular value decomposition. However, com- puting the best truncated series estimator, or the Kolmogorov width, for symmetric convex polytopes, is NP hard. When k = 0, d 0 (X) is the radius of X, and it is exactly the 2 -norm maximization problem considered in [2]. As shown in [2], for 2 ≤ p < ∞ the p -norm-maxima over (even symmetric) polytopes cannot be computed in polynomial time within a factor of 1.090, unless P=NP.\nOn the other hand, using semi-deﬁnite programming (SDP) relaxation, one can compute O(\nlog m) approximation of the radius [7], [14], i.e. d 0 (X). However, it is not known how to approximate d k (X) for k > 1. [20] showed that if the number of vertices of X is v, then SDP gives an O(\nlog v) approx- imation for d k . Nevertheless, in our problem, the number of vertices of a symmetric polytope can be exponential in n. So the technique in [20] does not directly apply to our problem.\nIn this paper, we showed that the truncated series estimator can achieve nearly optimal minimax risk for symmetric poly- topes deﬁned by few hyperplanes. There are some outstanding open questions raised by this work.\n1) What is the best bound for β m,n ∞ ? Our work leaves a gap of Ω( log m/ log log m) and O(log m).\n2) What is the best bound for β m,n p for p ≥ 2? We conjecture it is O(log m).\n3) How tight is the approximation radius bound for lower bounding the minimax risk for convex bodies? For 1 ball, it has a gap of Θ(\nlog n). This is the largest gap we know of. 4) How to efﬁciently approximate the optimal truncated\nAcknowledgment. The authors thank the reviewers for their insightful comments."},"refs":[{"authors":[{"name":"J. Bourgai"},{"name":"L. Tzafriri"}],"title":{"text":"Invertibility of \u2018large\u2019 submatrices with applications to the geometry of banach spaces and harmonic analysis"}},{"authors":[{"name":"A. Brieden"},{"name":"X. Discret"}],"title":{"text":"Geometric optimization problems likely not contained in AP Computational Geometry, 28:201\u2013209, 2002"}},{"authors":[{"name":"D. Donoh"},{"name":"I. M. Johnstone"}],"title":{"text":"Minimax risk over p -balls for q -error"}},{"authors":[{"name":"D. Donoh"},{"name":"R. Li"},{"name":"B. MacGibbon"}],"title":{"text":"Minimax risk over hyperrect- angles, and implications"}},{"authors":[{"name":"D. L. Donoh"},{"name":"I. Johnston"},{"name":"A. Malek"},{"name":"A. Montanari"}],"title":{"text":"Com- pressed sensing over p -balls: Minimax mean square error"}},{"authors":[{"name":"A. Giannopoulous"}],"title":{"text":"A note on the Banach-Mazur distance to the cube"}},{"authors":[{"name":"M. X. Goeman"},{"name":"D. P. Williamson"},{"name":"J. AC"}],"title":{"text":"Improved approximation algo- rithms for maximum cut and satisﬁability problems using semideﬁnite programming"}},{"authors":[{"name":"A. Javanmar"},{"name":"L. Zhang"}],"title":{"text":"The minimax risk of truncated series estimators for symmetric convex polytopes"}},{"authors":[{"name":"I. M. Johnstone"}],"title":{"text":"Gaussian estimation: Sequence and wavelet models"}},{"authors":[{"name":"G. G. Lorentz"}],"title":{"text":"Metric entropy and approximation"}},{"authors":[{"name":"M. Meye"},{"name":"A. Pajor"}],"title":{"text":"Sections of the unit ball of l n p "}},{"authors":[{"name":"V. Milman"}],"title":{"text":"Spectrum of a position of a convex body and linear duality relations"}},{"authors":[{"name":"A. Nemirovski"}],"title":{"text":"Topics in Non-parametric Statistics"}},{"authors":[{"name":"A. Nemirovsk"},{"name":"C. Roo"},{"name":"T. Terlaky"}],"title":{"text":"On maximization of quadratic form over intersection of ellipsoids with common center"}},{"authors":[{"name":"A. Pinkus"}],"title":{"text":"Widths in Approximation Theory"}},{"authors":[{"name":"M. S. Pinsker"}],"title":{"text":"Optimal ﬁltering of square integrable signals in Gaussian white noise"}},{"authors":[{"name":"G. Raskutt"},{"name":"M. J. Wainwrigh"},{"name":"B. Yu"}],"title":{"text":"Minimax rates of estimation for high-dimensional linear regression over q -balls"}},{"authors":[{"name":"S. J. Szare"},{"name":"M. Talagrand"}],"title":{"text":"An isomorphic version of the Sauer- Shelah lemma and the Banach-Mazur distance to the cube"}},{"authors":[{"name":"A. B. Tsybakov"}],"title":{"text":"Introduction to Nonparametric Estimation"}},{"authors":[{"name":"K. R. Varadaraja"},{"name":"S. Venkates"},{"name":"Y. Y"},{"name":"J. Zhang"},{"name":"J. Comput"}],"title":{"text":"Approximating the radii of point sets"}},{"authors":[{"name":"Y. Yan"},{"name":"A. Barron"}],"title":{"text":"Information-theoretic determination of minimax rates of convergence"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569562367.pdf"},"links":[{"id":"1569565883","weight":2},{"id":"1569565867","weight":2},{"id":"1569566683","weight":2},{"id":"1569566855","weight":2},{"id":"1569565551","weight":2},{"id":"1569566943","weight":2},{"id":"1569552245","weight":2},{"id":"1569565227","weight":4},{"id":"1569566373","weight":2},{"id":"1569564897","weight":2},{"id":"1569565547","weight":2},{"id":"1569565461","weight":2},{"id":"1569558325","weight":2},{"id":"1569565291","weight":2},{"id":"1569566999","weight":2},{"id":"1569564903","weight":2},{"id":"1569564387","weight":2},{"id":"1569561679","weight":2},{"id":"1569567009","weight":6},{"id":"1569555999","weight":2},{"id":"1569566657","weight":2},{"id":"1569566643","weight":2},{"id":"1569566939","weight":2},{"id":"1569564441","weight":2},{"id":"1569566513","weight":2},{"id":"1569566425","weight":2},{"id":"1569554971","weight":2},{"id":"1569566209","weight":2},{"id":"1569565559","weight":2},{"id":"1569565033","weight":2},{"id":"1569565595","weight":2},{"id":"1569566223","weight":2},{"id":"1569565357","weight":2},{"id":"1569566481","weight":2},{"id":"1569566387","weight":2},{"id":"1569566245","weight":6},{"id":"1569560503","weight":2},{"id":"1569566831","weight":2},{"id":"1569566983","weight":2},{"id":"1569566873","weight":2},{"id":"1569565093","weight":2},{"id":"1569566711","weight":2},{"id":"1569565319","weight":2},{"id":"1569566737","weight":2},{"id":"1569565353","weight":2},{"id":"1569566595","weight":2},{"id":"1569552025","weight":2},{"id":"1569565375","weight":2},{"id":"1569566755","weight":4},{"id":"1569566819","weight":2},{"id":"1569565541","weight":2},{"id":"1569565425","weight":2},{"id":"1569564437","weight":2},{"id":"1569564787","weight":2},{"id":"1569565529","weight":11},{"id":"1569556759","weight":2},{"id":"1569566397","weight":2},{"id":"1569567483","weight":2},{"id":"1569567691","weight":2},{"id":"1569565861","weight":2},{"id":"1569555891","weight":4},{"id":"1569565997","weight":2},{"id":"1569563725","weight":2},{"id":"1569561397","weight":2},{"id":"1569565113","weight":2},{"id":"1569564509","weight":2},{"id":"1569566825","weight":2},{"id":"1569566443","weight":2},{"id":"1569566727","weight":2},{"id":"1569565315","weight":2}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T8.4","endtime":"11:10","authors":"Adel Javanmard, Li Zhang","date":"1341399000000","papertitle":"The minimax risk of truncated series estimators for symmetric convex polytopes","starttime":"10:50","session":"S9.T8: Portfolios and Estimation","room":"Stratton (491)","paperid":"1569562367"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
