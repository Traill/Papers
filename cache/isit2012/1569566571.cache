{"id":"1569566571","paper":{"title":{"text":"Achievable Rates for Intermittent Communication"},"authors":[{"name":"Mostafa Khoshnevisan"},{"name":"J. Nicholas Laneman"}],"abstr":{"text":"Abstract\u2014We formulate a model for intermittent communi- cations that can capture bursty transmissions or a sporadically available channel, where in either case the receiver does not know a priori when the transmissions occur. Focusing on the point-to-point case, we develop two decoding schemes and their achievable rates for such communication scenarios. One scheme determines the transmitted codeword, and another scheme ﬁrst locates the information symbols and then uses them to decode. The two-stage scheme leads to a higher achievable rate because it uses a generalization of the method of types in the ﬁrst stage, which leads to a notion of partial divergence. We illustrate the results in the case of an intermittent binary symmetric channel."},"body":{"text":"Communication systems are traditionally analyzed assum- ing continuous transmission of encoded symbols through the channel. However, in many practical applications such an assumption is not appropriate, and transmitting a codeword can be intermittent due to lack of synchronization, shortage of transmission energy, or burstiness of the system. The challenge is that the receiver does not explicitly know whether a given output symbol of the channel is a result of sending a symbol of the codeword or is simply a noise symbol containing no information about the message. In this paper, we model such intermittent communication and determine some achievable rates.\nAsynchronous communication is modeled in [1] and [2] by a single block transmission that starts at a random time, unknown to the receiver, in an exponentially large window, known to the receiver. In this model, the transmission is contiguous; once it starts, the whole codeword is transmitted, and the receiver observes only noise before and after transmis- sion. However, in intermittent communication, a codeword is not transmitted contiguously. Instead, some number of noise symbols occur between the symbols of a codeword before going through the channel. We assume that the receive window and the codewords length, and therefore the number of inserted noise symbols, is ﬁxed. In contrast to [1], where the interesting scenario arises if the receive window scales exponentially with the codeword length, the interesting scenario in intermittent communication arises if the receiver window scales linearly with the codeword length. Our system model can be also interpreted as an insertion channel, in which a random number of noise symbols is inserted between consecutive symbols of the codeword. Although different from the insertion channels in the literature [3] and [4], our results may provide some insights about them.\nAfter formulating the problem and speciﬁcally mentioning the applications, we describe the ﬁrst decoding scheme and analyze its achievable rate. Then, we consider a two-stage decoding procedure and characterize its achievable rate using a notion of partial divergence that results from a generalization of the method of types. Finally, we illustrate the achievable rates for an intermittent binary symmetric channel (BSC).\nA transmitter wants to send a message m ∈ {1, 2, ..., e kR = M } to a receiver through a discrete memoryless channel. Let X and Y denote the input and output alphabets of the channel, and let W denote its probability transition matrix. Assume that {c k (m), m = 1, 2, ..., M } are the codewords of integer length k available to the transmitter, and let X n and Y n denote the integer length n input and output vectors of the channel, respectively, where n ≥ k, and α := n/k, α ≥ 1. Assume that X n consists of the transmitted codeword at k arbitrary time slots and is equal to a noise symbol denoted by ∈ X at the other n − k time slots, where the codeword c k appears in the sequence X n in the same order, i.e., c i cannot appear after c j in the sequence if i ≤ j. It is equivalent to say that n − k noise symbols are arbitrarily inserted between the symbols of a codeword. Figure 1 shows a block diagram for the system model, which we call intermittent communication and denote by ( X, Y, W, , α). The communication rate is deﬁned as log M/k. Assuming that the decoded message is denoted by ˆ m, we say that rate R is achievable if there exists a sequence of length k codes of size e kR with 1 M M m=1 P( ˆ m = m) → 0 as k → ∞. The capacity is the supremum of all the achievable rates. A natural question is, what is the capacity of intermittent communication? In this paper, we ﬁnd some achievable rates.\nThe intermittent communication model can also represent bursty communication in which either the transmitter or the channel is bursty. We assume that the receiver does not know the positions of the codeword symbols in the channel input\nsequence. As we will see, this natural assumption seems to make the decoder\u2019s task more difﬁcult. We also assume that the transmitter cannot decide on these positions, so it cannot encode any timing information. Note that in a bursty com- munication scenario, the process of burstiness is usually out of the transmitter\u2019s control, and the receiver usually does not know the realization of the bursts, which are consistent with these assumptions. The level of intermittency (or burstiness) is controlled by how large n is compared to k. The larger the value of α, the more intermittent the system is; if α = 1, the system is not intermittent and corresponds to contiguous communication. Not surprisingly, the achievable rates obtained in this paper are a function of α: increasing α generally reduces the achievable rates.\nThe described system model applies to several practical communication scenarios. If the intermittent process compo- nent in Figure 1 is considered as a part of the channel behavior, then we say that the channel is intermittent in the sense that it takes a symbol of the codeword as an input at some time slots, and takes a noise symbol as an input at other time slots. As an example, consider an insertion channel in which, after the i th symbol from the codeword, N i noise symbols are inserted, where N i \u2019s are i.i.d. random variables with mean α −1. At the decoder, there are N symbols, where N is a random variable, but we have\nIt turns out that the achievability results in the sequel are valid for such insertion channels, as we discuss in more detail in Section IV. Note that this is a speciﬁc class of insertion channels, and we refer the interested reader to See [3], [4] for more general classes of insertion channels and associated results.\nOn the other hand, if the intermittent process component in Figure 1 is considered as a part of the transmitter, then we say that the transmitter is intermittent. Practical examples are energy harvesting systems, where the transmitter harvests energy usually from a natural source and uses it for transmis- sion. Assuming that the noise symbol can be transmitted with zero energy, the transmitter sends the symbols of the codeword if there is enough energy for transmission, and sends noise symbols otherwise. If the transmitter can store energy in an energy buffer with enough capacity, then it can decide when to send the symbols of the codeword and when to transmit a noise symbol to save some energy based on the amount of the available energy. In that case, the transmitter can also encode some timing information, which is beyond the scope of this paper.\nNotation: We use o(·) to denote quantities that grow strictly slower than their arguments. Most of the notation in this paper follows that in [1] and [5]. By X ∼ P (x), we mean X is distributed as P . The empirical distribution (or type) of a sequence x n ∈ X n is denoted by ˆ P x n . Joint empirical distributions are denoted similarly. We say a sequence x n has type P if ˆ P x n = P and denote it by x n ∈ T n P , where T n P or\nmore simply T P is the set of all sequences that have type P . We use P X to denote the set of distributions over the ﬁnite alphabet X. For simplicity, we deﬁne W (·) := W (·|x = ). In this paper, we use the convention that n k = 0 if k < 0 or n < k, and the entropy H(P ) = −∞ if P is not a probability mass function, i.e., one of its elements is negative or the sum of its elements is larger than one. h(·) is the binary entropy function. Finally, if 0 ≤ ρ ≤ 1, then ¯ ρ := 1 − ρ.\nWe ﬁrst introduce a simple decoding scheme, obtaining an achievable rate (Proposition 1) in addition to providing some ingredients for obtaining an achievable rate for a two stage decoding scheme (Theorem 1).\nFor a ﬁxed input distribution P , the codebook is ran- domly and independently generated, i.e., all C i (m), i ∈ {1, 2, ..., k}, m ∈ {1, 2, ..., M } are i.i.d. according to P . For a ﬁxed typicality parameter µ > 0, the decoder observes the sequence y n , chooses k symbols out of those n symbols, denoted by ˜ y k , and performs joint typicality decoding, i.e., checks if\nfor all (x, y) ∈ X × Y and a unique index m, where P m is the joint probability mass function induced by the type of codeword c k (m) and the channel W , deﬁned by [1]\nFor convenience, we write ˜ y k ∈ T [W ] µ (c k (m)), if (1) is satisﬁed for m. If the decoder ﬁnds a unique m satisfying (1), it declares m as the transmitted message. Otherwise, it makes another choice for the k symbols from the sequence y n and again attempts typicality decoding. If at the end of all n k\nchoices the typicality decoding procedure did not declare any message, then the decoder declares an error. In order to analyze the probability of error, we state the following fact, which is proved in [1, Equations (24) and (25)] based on the method of types [5, Chapter 1.2].\nFact 1. Assume that C k (m) and ˜ Y k are independent, C k (m) is generated i.i.d. according to P , and (X, Y ) ∼ P (x)W (y|x), then the probability that C k (m) together with ˜ Y k satisfy (1) for this speciﬁc m, is upper bounded as\nP( ˜ Y k ∈ T [W ] µ (C k (m))) ≤ poly(k)e −k(I(X;Y )− ) (2) for all k sufﬁciently large, where can be made arbitrarily small by choosing a small enough typicality parameter µ.\nProposition 1. For intermittent communication with ( X, Y, W, , α), rates not exceeding C − αh(1/α) are achievable, where C is the capacity of the DMC with stochastic matrix W .\nProof: Let P be the capacity achieving input distribution for the DMC with stochastic matrix W , and consider the encoding and decoding strategies described above. For any\n> 0, we prove that if R = C − αh(1/α) − 2 , then the average probability vanishes as k → ∞.\nwhere p 1 e is the probability of error conditioned on the sending of message w = 1, and where (3) results from the union bound in which the second term is the probability that the typicality decoding fails for the right codeword given that the k chosen output symbols are the correct ones, which vanishes as k → ∞ according to [5, Lemma 2.12]. Using the union bound for the ﬁrst term in (3), we have\nbecause there are n k choices for the k output symbols, and for each choice, we use the union bound for all M − 1 = e kR − 1 messages other than m = 1. Using the Stirling\u2019s approximation, we have\nn k\nas k → ∞. Note that, conditioned on message m = 1 being sent, C k (2) and ˜ Y k are independent for any choice of output symbols. Therefore, using Fact 1 with the capacity achieving input distribution, we have\nP( ˜ Y k ∈ T [W ] µ (C k (2))|m = 1) ≤ poly(k)e −k(C− ) \t (6) Combining (3), (4), (5), and (6), we obtain\nNote that the maximum probability of error vanishes as well using standard expurgation arguments. The form of the achievable rate is reminiscent of communications overhead as the cost of constraints [6], where the constraint is the system\u2019s burstiness, and the overhead cost is αh(1/α).\nIn this section, we introduce a two-stage decoding procedure that strictly improves upon the achievable rate in Proposition 1. As before, the decoder chooses k of the n symbols from the output vector y n . Let ˜ y k denote the chosen output symbols, and ˆ y n−k denote the other output symbols. The ﬁrst stage consists of checking if ˜ y k is the transmitted sequence (if ˜ y k ∈ T P W ), and if ˆ y n−k is generated by noise (if ˆ y n−k ∈ T W ). If both of these conditions are satisﬁed, then we perform the typicality decoding as described in Section III-A, which is called the second stage here. Otherwise, we make another choice for the k symbols and repeat the two-stage decoding procedure. At any step that we run the second stage, if the typicality decoding declares a message as being sent, then\ndecoding ends. If the decoder does not declare any message as being sent by the end of all n k choices, then the declares an error.\nIn order to analyze the probability of error, we need to generalize the method of types in [5] to be able to bound the probability of certain events in the ﬁrst stage of the two-stage decoding procedure. To that end, we establish Lemma 1, which is a generalization of [5, Lemma 2.6].\nLemma 1. Consider an alphabet with t symbols, i.e., X = {0, 1, ..., t − 1}. Consider three distributions P, Q, Q ∈ P X , where P := (p 0 , p 1 , ..., p t−1 ), Q := (q 0 , q 1 , ..., q t−1 ), and Q := (q 0 , q 1 , ..., q t−1 ). We assume that all of the elements of these three PMF\u2019s are nonzero. A random sequence X k is generated as follows: k 1 symbols are i.i.d. according to Q and k 2 symbols are i.i.d. according to Q , where k 1 + k 2 = k and ρ := k 1 /k. The probability that X k has type P is upper bounded as\nBecause of space limitations, we prove the lemma for the case of binary alphabets, i.e, t = 2, in Appendix A. The generalization to t > 2 is straightforward, but lengthy. Specializing Lemma 1 for Q = Q results in [5, Lemma 2.6], and we have d(P, Q, Q, ρ) = D(P ||Q). Lemma 1 enables us to evaluate the probability that a sequence consisting of symbols from two different distributions has a speciﬁc type P , which will be useful in proving the result in Theorem 1. Basically, we are interested in a special case of Lemma 1 for which Q = P . In other words, we need to upper bound the probability that a sequence has type P when partially generated according to Q and partially according to P , where the ratio of the mismatched symbols (generated from Q) to all the symbols is ρ = k 1 /k. For this special case, we deﬁne d ρ (P ||Q) := d(P, Q, P, ρ). The function d ρ (P ||Q), which we call partial divergence between P and Q, has some interesting properties (such as 0 ≤ d ρ (P ||Q) ≤ ρD(P ||Q)), on which we will elaborate in another paper.\nTheorem 1. For intermittent communication with ( X, Y, W, , α), rates not exceeding max P {I(X; Y ) − f (P, W, α)} are achievable, where\nProof: Fix the input distribution P , and consider the encoding and decoding strategies described in Sections III-A and III-B, respectively. For any > 0, we prove that if R = I(X; Y ) − f (P, W, α) − 2 , then the average probability vanishes as k → ∞.\np avg e = p 1 e ≤ P( ˆ m ∈ {2, 3, ..., M }|m = 1)+P( ˆ m = e|m = 1), (11)\nwhere the second term is the probability that the decoder declares an error (does not ﬁnd any message) at the end of all n k choices, which implies that even if we pick the correct output symbols, the decoder either does not pass the ﬁrst stage or does not declare m = 1 in the second stage. Therefore,\nwhere Y k is the output of the channel if the input is C k (1), and Y is the output of the channel if the input is the noise symbol, and where we use union bound to establish (12). The limit (13) is because all the three terms in (12) vanish as k → ∞ according to [5, Lemma 2.12].\nThe ﬁrst term in (11) is more challenging. It is the proba- bility that for at least one choices of the output symbols, the decoder passes the ﬁrst stage and then the typicality decoder declares an incorrect message. Let the index k i denote the condition on the i th choice of the output symbols out of all n k\nchoices. We characterize the choices based on the number of incorrectly chosen output symbols, i.e., the number of symbols in ˜ y k that are in fact outputs corresponding to a noise input, which is equal to the number of symbols in ˆ y n−k that are in fact outputs corresponding to an input symbol of the codeword. Let the index k 1 denote the condition that the number of wrongly chosen output symbols is equal to k 1 . For a speciﬁc k 1 there are k k\nchoices (According to Vandermonde\u2019s identity, we have n−k k1=0 ( k k1 )( n−k k1 ) = ( n k )). Using the union bound for all the choices and all the messages ˆ w = 1, we have\nNote that message ˆ m = 2 is declared at the decoder only if it passes the ﬁrst stage, and then it is the unique message that\nwhere (15) follows from the independence of the events { ˜ Y k ∈ T [W ] µ } and { ˆ Y n−k ∈ T W } conditioned on k 1 mismatched symbols, and (16) follows from using Lemma 1 for the ﬁrst two terms in (15) and using Fact 1 for the last term in (15), because conditioned on message m = 1 being sent, C k (2) and ˜ Y k are independent regardless of the other conditions in the last term. Substituting (16) into the summation in (14), we have\nwhere (18) is obtained by ﬁnding the exponent of the sum in (17), which is equal to the largest exponent of each term in the summation, since the number of terms is polynomial in k. To that end, let β := k 1 /(n − k) (0 ≤ β ≤ 1), using Stirling\u2019s approximation as in (5) for each of the combinatorial terms in (17), we ﬁnd that the largest exponent is f (P, W, α), which is deﬁned in (10); and where (19) is obtained by substituting R = I(X; Y ) − f (P, W, α) − 2 . Now, combining (11), (13), and (19), we have p avg e → 0 as k → ∞, which proves the Theorem.\nConsider a BSC with crossover probability 0 ≤ p ≤ 0.5. Figure 2 shows the two achievable rates obtained in Proposi- tion 1 and Theorem 1 versus p for different values of α ≥ 1.\nNot surprisingly, the achievable rate in Theorem 1 (indicated by \u201cThm 1\u201d) is always larger than the one in Proposition 1 (indicated by \u201cProp 1\u201d) since the two-stage decoding proce- dure takes advantage of the fact that the choice of the k output symbols might not be a good one. Speciﬁcally, the exponent obtained in Lemma 7 in terms of the partial divergence helps the decoder detect the right symbols, and therefore, achieve a larger rate. The arrows in Figure 2 show this difference and suggests that the beneﬁt of using the two-stage decoding is larger for increasing α. Note that the larger α is, the smaller the achievable rate would be for a ﬁxed p. Also, note that as p → 0 the achievable rates approach a limit, which is equal\nto 1 − αh( 1 α ) (bits/s) for the ﬁrst achievable rate. However, it is more involved to obtain it for the second achievable rate, which is denoted by R Insertion and can be proven to be equal to max 0≤p 0 ≤1 {2h(p 0 )−max 0≤β≤1 {(α−1)h(β)+h((α−1)β)+ (1−(α−1)β)h( p 0 −(α−1)β 1−(α−1)β )}}. Note that this is the achievable rate if the channel is noiseless (p = 0), which models the insertion channel we discussed in Section II.\nFigure 3, shows the value of R Insertion and the value of achievable rates for different p, versus α. Not surprisingly, as α → 1, the capacity of the BSC is approach for both of the achievable rates. Again, the difference between the achievable\nrates is more obvious for larger α. In this example, we cannot achieve a positive rate if α ≥ 2, even for the case of a noiseless channel (p = 0). However, this is not true in general, because even the ﬁrst achievable rate can be positive for a large α, if the capacity of the channel is sufﬁciently large. Figure 3 can be interpreted as the achievable region for (R, α). The results suggest that, as communication becomes more intermittent and α becomes larger, the achievable rate is decreased due to the additional uncertainty in the positions of the information symbols at the decoder.\nWe have t = 2, X = {0, 1}, P = (p 0 , p 1 ), Q = (q 0 , q 1 ), and Q = (q 0 , q 1 ). Let N (0) denote the number of 0\u2019s in X k 1 1 , and N (0) denote the number of 0\u2019s in X k k 1 +1 . We have\nwhere (20) and (21) follow from the fact that N (0) and N (0) are independent and have binomial distributions, (22) follows from the deﬁnition of the entropy and divergence functions, (23) follows by the same procedure used to ob- tain (18) from (17), i.e., ﬁnding the largest exponent of the sum by deﬁning θ 0 := l/(kp 0 ), where with simple algebraic manipulation the exponent becomes e(P, Q, Q , ρ) deﬁned in (9); and where (24) follows from deﬁnition (8)."},"refs":[{"authors":[{"name":"A. Tchamkerten"},{"name":"V. Chandar"},{"name":"G. W. Wornell"}],"title":{"text":"Asynchronous communica- tion: Capacity bounds and suboptimality of training"}},{"authors":[{"name":"D. Wang"},{"name":"V. Chandar"},{"name":"S.-Y. Chung"},{"name":"G. Wornell"}],"title":{"text":"Error exponents in asyn- chronous communication"}},{"authors":[{"name":"R. Venkataramanan"},{"name":"S. Tatikonda"},{"name":"K. Ramchandran"}],"title":{"text":"Achievable Rates for Channels with Deletions and Insertions"}},{"authors":[{"name":"M. Mitzenmacher"}],"title":{"text":"Capacity bounds for sticky channels"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems; 2nd edition, Cambridge University Press, 2011"}},{"authors":[{"name":"J. N. Laneman"},{"name":"B. P. Dunn"}],"title":{"text":"Communications Overhead as the Cost of Constraints"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566571.pdf"},"links":[{"id":"1569566381","weight":6},{"id":"1569566725","weight":12},{"id":"1569559665","weight":6},{"id":"1569566875","weight":18},{"id":"1569566683","weight":6},{"id":"1569565091","weight":6},{"id":"1569566591","weight":6},{"id":"1569552245","weight":6},{"id":"1569566415","weight":12},{"id":"1569565613","weight":6},{"id":"1569565931","weight":12},{"id":"1569565837","weight":6},{"id":"1569566319","weight":6},{"id":"1569566941","weight":6},{"id":"1569565291","weight":6},{"id":"1569564203","weight":6},{"id":"1569556713","weight":6},{"id":"1569566467","weight":6},{"id":"1569565859","weight":6},{"id":"1569565809","weight":6},{"id":"1569565455","weight":12},{"id":"1569565953","weight":6},{"id":"1569566895","weight":6},{"id":"1569566985","weight":6},{"id":"1569566095","weight":6},{"id":"1569564311","weight":6},{"id":"1569566239","weight":6},{"id":"1569566679","weight":12},{"id":"1569566617","weight":6},{"id":"1569566905","weight":6},{"id":"1569566753","weight":6},{"id":"1569558681","weight":6},{"id":"1569555999","weight":6},{"id":"1569566511","weight":6},{"id":"1569566531","weight":18},{"id":"1569567665","weight":18},{"id":"1569561143","weight":12},{"id":"1569567015","weight":6},{"id":"1569566437","weight":6},{"id":"1569558901","weight":6},{"id":"1569553537","weight":6},{"id":"1569552251","weight":18},{"id":"1569564209","weight":6},{"id":"1569566425","weight":6},{"id":"1569566209","weight":18},{"id":"1569565151","weight":12},{"id":"1569565087","weight":6},{"id":"1569564857","weight":6},{"id":"1569566629","weight":6},{"id":"1569565033","weight":6},{"id":"1569565055","weight":6},{"id":"1569565633","weight":6},{"id":"1569565279","weight":6},{"id":"1569565219","weight":6},{"id":"1569566003","weight":6},{"id":"1569565469","weight":6},{"id":"1569565357","weight":6},{"id":"1569566603","weight":12},{"id":"1569555787","weight":6},{"id":"1569565467","weight":12},{"id":"1569566233","weight":12},{"id":"1569560997","weight":12},{"id":"1569566501","weight":6},{"id":"1569560503","weight":6},{"id":"1569565439","weight":12},{"id":"1569551347","weight":6},{"id":"1569565415","weight":6},{"id":"1569565571","weight":6},{"id":"1569566779","weight":12},{"id":"1569566479","weight":6},{"id":"1569565765","weight":6},{"id":"1569566261","weight":6},{"id":"1569565093","weight":6},{"id":"1569565919","weight":12},{"id":"1569566927","weight":6},{"id":"1569565661","weight":12},{"id":"1569566887","weight":6},{"id":"1569566267","weight":6},{"id":"1569564919","weight":6},{"id":"1569565353","weight":6},{"id":"1569564291","weight":6},{"id":"1569566823","weight":6},{"id":"1569566595","weight":6},{"id":"1569565375","weight":18},{"id":"1569566771","weight":6},{"id":"1569564437","weight":25},{"id":"1569564861","weight":6},{"id":"1569566487","weight":6},{"id":"1569556759","weight":12},{"id":"1569561185","weight":6},{"id":"1569566397","weight":6},{"id":"1569566817","weight":12},{"id":"1569566299","weight":6},{"id":"1569564769","weight":6},{"id":"1569566171","weight":6},{"id":"1569566933","weight":6},{"id":"1569566577","weight":31},{"id":"1569565389","weight":6},{"id":"1569565561","weight":12},{"id":"1569566583","weight":6},{"id":"1569565889","weight":12},{"id":"1569565113","weight":6},{"id":"1569566375","weight":12},{"id":"1569564257","weight":6},{"id":"1569566555","weight":12},{"id":"1569564141","weight":12},{"id":"1569565031","weight":6},{"id":"1569564509","weight":6},{"id":"1569566113","weight":12}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S8.T3.2","endtime":"17:20","authors":"Mostafa Khoshnevisan, J. Nicholas Laneman","date":"1341334800000","papertitle":"Achievable Rates for Intermittent Communication","starttime":"17:00","session":"S8.T3: Energy Issues in Communication Systems","room":"Stratton S. de P. Rico (202)","paperid":"1569566571"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
