{"id":"1569566113","paper":{"title":{"text":"Error Exponents of Optimum Erasure/List and Ordinary Decoding for Channels with Side Information"},"authors":[{"name":"Erez Sabbag"},{"name":"Neri Merhav"}],"abstr":{"text":"Abstract\u2014In this work we analyze achievable random coding error exponents pertaining to erasure/list decoding for channels with side information present at the transmitter. The analysis is carried out using the optimal decoding rule proposed by Forney. A key ingredient in the analysis is the evaluation of moments of a certain distance enumerator. This approach leads to a new exponentially tight bounds. These results are obtained by exploring a random binning code with conditionally constant composition codewords previously proposed by Moulin and Wang. Later, these results are used to obtain an achievable random coding error exponent for ordinary decoding."},"body":{"text":"Decoding with an erasure option and variable list size decoding are generalizations of the ordinary decoding rule in which the decoder gives one estimate for the transmitted message. A decoder with an erasure option is a decoder which has the option of not deciding, i.e., to declare an \u201cerasure\u201d. On the other hand, a variable size list decoder is a decoder which produces a list of estimates for the correct message rather than a single estimate, where a list error occurs when the correct message is not on the list. In [1], Forney analyzed random coding error exponents of erasure/list decoding for discrete memoryless channels (DMC\u2019s). These bounds were obtained by exploring the optimal decoding rule [1, eq. (11)]\nwhere Pr(y, x m ) is the joint probability of the channel output y and the codeword x m , R m is the decision region of message m, and T is an arbitrary parameter. The bounds were obtained using Gallager\u2019s bounding techniques. It was shown that the erasure option and the list option are \u201ctwo sides of the same coin\u201d, namely, by changing the value of T one can switch from list decoding (T < 0) to decoding with an erasure option (T > 0).\nLater, Csisz´ar and K¨orner [2, p.175, Th. 5.11] derived for DMCs universally achievable error exponents for a decoder\nwith an erasure option. These bounds were obtained by exploring a generalized maximum mutual information (MMI) decoder [2, p. 164] which has an erasure option.\nRecently, upper bounds on the error probabilities under generalized decoding rules were provided for linear block codes over memoryless symmetric channels in [3].\nIn a recent paper [4], a decoder with an erasure option and a variable size list decoder for channels with non-casual side information at the transmitter were considered. A universally achievable region of error exponents was presented for de- coding with an erasure option using a parameterized decoder in the spirit of Csisz´ar and K¨orner\u2019s decoder. Moreover, the proposed decoding rule was generalized by extending the range of its parameters to allow variable size list decoding. This extension gives a uniﬁed treatment for erasure/list de- coding. An achievable region of exponential bounds on the probability of list error and the average number of incorrect messages on the list were given. The results were obtained using a random binning code with conditionally constant composition codewords proposed by Moulin and Wang [5] with the proposed parameterized decoder.\nWhile these results extended the work of Csisz´ar and K¨orner to channels with side information present at the transmitter non-causally and generalized it to the case of variable list size decoding, the results in [4] were based on heuristic decoding rule with no proven relation to the optimal decoding rule. Moreover, while in [1] both error exponents were achieved by the same codebook distribution, in [4] the two error exponents might be achieved by different codebook distributions. This fact may indicate that the results of [4] are not tight. These two weak spots of [4] were the main motivation for the current work.\nWhen reviewing Forney\u2019s proof [1, p.218] it is evident that the proof can be applied to the case of channels with side-information present at the transmitter if we could handle the analysis of two expressions: (i) E C P λ (y|m), namely, the moments of the probability of the channel output given that the correct message was sent, and (ii) E C m =m P (y|m ) λ the moments of the sum of the probabilities of all competing\nmessages, where the expectation is taken with respect to the code ensemble. Forney tackled these expressions using the H¨older\u2019s and Jensen\u2019s inequalities and the fact that the code- words were chosen in a symbol-by-symbol fashion. However, when the binning technique is introduced to the encoding rule x = f (m, s), which maps a channel state sequence s and a message index m into a channel input sequence x using an auxiliary sequence u, Forney\u2019s analysis does not seem to be applicable.\nTo overcome these difﬁculties a different approach was taken. In this work we apply the distance enumerator approach (see [6] for detailed tutorial on the subject). This approach was applied in recent years to other problems in Information Theory with considerable success (see [7], [8], [9], [10]). In [11], the analysis of random coding error exponents for er- ausre/list decoding for DMCs was revisited, where the distance enumerator approach was used instead of the use of H¨older\u2019s and Jensen\u2019s inequalities as done by Forney. The resulting bounds, which are at least as tight as Forney\u2019s bounds, are simpler in the sense that it involves an optimization over one parameter only as opposed to Forney\u2019s bounds which involves two parameters. Moreover, when applying this technique to a certain universal decoder with erasures the tightness of these bounds is exempliﬁed.\nIn this work we use the distance enumerator approach to analyze the optimal decoding rule proposed by Forney while adopting a random binning code with conditionally constant composition codewords (CCC) proposed by Moulin and Wang [5]. Later, these results are applied on Gallager\u2019s well-known upper bound presnted in [12] to get an achievable error exponent for the above channel model and code construction.\nThe outline of this work is the following: In Section II we present some notation which will be used throughout the paper, present the channel model, describe the codebook construction and the decoding rule which will be used during this paper. In Section III we present the main result of the paper. Section IV describes the case of ordinary decoding rule and presents the corresponding achievable error exponent. In Section V we discuss the results and present some directions for future research.\nWe begin with some notation and deﬁnitions. Throughout this work, capital letters represent scalar random variables (RVs), and speciﬁc realizations of them are denoted by the corresponding lowercase letters. Random vectors of dimension N will be denoted by bold-face letters. The notation 1{A}, where A is an event, will designate the indicator function of A (i.e., 1{A} = 1 if A occurs and 1{A} = 0 otherwise). Let the vector ˆ Px = ˆ Px(a), a ∈ X denote the empirical distribution induced by a vector x ∈ X n , where ˆ Px(a) =\n1{x i = a}. The type class Tx is the set of vectors ˜ x ∈ X n such that ˆ P ˜ x = ˆ Px. A type class induced by the em- pirical distribution ˆ Px will be denoted by T ( ˆ Px). Similarly, the joint empirical distribution induced by (x, y) ∈ X n × Y n\nˆ Pxy(a, b) = 1 n\ni.e., ˆ Pxy(a, b) is the relative frequency of the pair (a, b) along the pair sequence (x, y). Likewise, the type class Txy is the set of all pairs (˜ x, ˜ y) ∈ X n × Y n such that ˆ P ˜ x ˜ y = ˆ Pxy. The conditional type class Ty | x, for given vectors x ∈ X n , and y ∈ Y n is the set of all vectors ˜ y ∈ Y n such that Tx ˜ y = Txy. The Kullback-Leibler divergence between two distributions P and Q on B, where |B| < ∞ is deﬁned as\nwith the conventions that 0 ln 0 = 0, and p ln p 0 = ∞ if p > 0. We denote the empirical entropy of a vector x ∈ X n by ˆ H(x), where ˆ H(x) = − a∈X ˆ Px(a) ln ˆ Px(a). Other information theoretic quantities governed by empirical distributions (e.g., conditional empirical entropy, empirical mutual information) will be denoted similarly.\nIn this section, we describe the channel model and the code construction. Note that the channel model is the same channel model described in [4], and the code construction is almost identical. However, the decoding rule used in this work is based on the optimal decoding rule, while in [4] a sub-optimal decoding rule was used.\nWe consider a discrete memoryless state-dependent chan- nel with a ﬁnite input set X , a ﬁnite state alphabet S, a ﬁnite output alphabet Y, and a transition probability distri- bution W (y|x, s). Given an input sequence x and a state sequence s, emitted from a discrete memoryless source P S (s) = N i=1 P S (s i ), the channel output sequence y is gen- erated according to the conditional distribution W (y|x, s) =\nW (y i |x i , s i ). A message m ∈ {1, . . . , M }, where M = exp{N R} and R is the code rate, is to be transmitted to the receiver. We assume that the state sequence s is available at the transmitter non-causally, but not at the receiver. We also assume that all messages are a-priori equiprobable. Given s and m, the transmitter produces a sequence x = f N (s, m) which is used to convey message m to the decoder. We note that the channel is ﬁxed and known to all parties, as presented in the work of Gel\u2019fand and Pinsker [13].\nIn [5, p. 1337], Moulin and Wang used in their derivation a binning codebook with conditionally constant composition (CCC) codewords. A similar codebook will be used in our proofs. For the sake of completeness, we brieﬂy describe the codebook construction and the encoding process. The decoding part will be described in detail later. The codebook construction requires the use of an auxiliary random variable U ∈ U which takes on values in a ﬁnite set of size |X ||S| as will be shown later.\nFor a given empirical conditional distribution ˆ P ∗ u | s, a sub- codebook C( ˆ Ps) is constructed for each state sequence type\nclass Ts = T ( ˆ Ps). Given a state type class T ( ˆ Ps), compute the marginal distribution\nwhere ˆ Ps is the empirical distribution induced by Ts. Note that ˆ P ∗ u(u) is a function of ˆ Ps and it might be different for other state type classes. Draw exp{N [R + ρ( ˆ Ps)]} random vectors independently from the type class T ∗ U ( ˆ Ps) induced by ˆ P ∗ u, according to uniform distribution where ρ( ˆ Ps) = I ∗ U S ( ˆ Ps) + , and\nThis choice ensures that the probability of encoding error vanishes at a double-exponentially rate [5, p. 1338]. Arrange the vectors in an array with M = exp{N R} columns and\nexp{N ρ( ˆ Ps)} rows. The codebook C is the union of all sub-codebooks, i.e., C = ˆ P s C( ˆ Ps). Note that the number of these sub-codebooks is polynomial in N (at most (N + 1) |S| ).\nTo encode message m given a state sequence s, the fol- lowing two steps are done: (i) Find an index l such that u l,m ∈ C( ˆ Ps) is a member of the conditional type class T ∗ u\nˆ Ps}. If more than one such l exists, pick one at random under the uniform distribution. If no such l can be found, pick u at random from T ∗ u | s under the uniform distribution. (ii) The channel input sequence is given by x = x(s 1 , u 1 ), x(s 2 , u 2 ), . . . , x(s N , u N ) where x : S × U → X is some ﬁxed function. Since there is a deterministic mapping from S × U we can deﬁne a direct channel ˜ W : S × U → Y as\nGiven a codebook C, a decoder with an erasure option is a partition of Y N into (M + 1) regions R 0 , R 1 , . . . , R M . The decoder decides in favor of message m if y ∈ R m , m = 1, . . . , M , or it declares \u201cerasure\u201d if y ∈ R 0 . Following Forney [1], let us deﬁne two error events. The event E 1 is the event in which y does not fall in the decision region of the correct message, namely, the event in which the decoder decides wrongly. The event E 2 is the event of undetected error, namely, the event in which y falls in R m , m = 0, while m = m was transmitted. The average probabilities of these error events are given by\nwhere P (y|m) = \t s ∈S N P S (s)W y|x m (s), s , and the average probability of erasure is given by:\nIn [1], Forney showed that the optimal tradeoff between the two error events is attained by the following rule [1, p.208]:\notherwise, declare \u201cerasure\u201d ( i.e., y ∈ R ∗ 0 ), where T ≥ 0 is a parameter which controls the trade-off between E 1 and E 2 . When taking a closer look on the decoding rule derived by Forney [1, p.208], it is clear that these results can be applied to the channels with side-information available at the encoder non-causally as describe above where the only difference is that the probability of received channel output y given a message index m, denoted by P (y|m), is more involved.\nFor a given channel output sequence y and a message index m deﬁne\nwhere U (s, m) is the encoding rule. Following the ﬁrst steps of the derivation of [1, p.218], for a given codebook we get:\nwhere 0 ≤ λ ≤ 1 is an arbitrary parameter, and ¯ λ 1 − λ. Similarly, for E 2 we get that\nNotice that the difference between these two expressions is given by a constant factor e −N T , therefore, we will concen- trate on achieving an upper bound on Pr{E 1 }. It is important to note that starting this point on, our derivation is exponentially tight. Now, taking the expectation with respect to the code ensemble and using the fact that the encoding rule U (s, m) (i.e., the process of choosing u given m and s) is independent of all other encoding rules U (s, m ), we get that\nwhere E C {·} designates the expectation operator with respect to the code ensemble.\nFor a given joint distribution Q SU Y on S × U × Y, a conditional distribution W Y |SU : S × U → Y and a non- negative constant γ deﬁne\nwhere E Q is the expectation operator associated with Q. For a given constant K ≥ 0, a conditional distribution P U |S on S × U and a distribution P Y on Y, deﬁne the following set of distributions\nand deﬁne the complement of G(P Y , K), denoted by G c (P Y , K), as\nThe complete proof of the Theorem can be found in [14], [15]. We brieﬂy describe the main items of the proof.\nThe technique used in the proof of Theorem 1 can also be used to derive an achievable error exponent for ordinary decoding, i.e., a decoder which gives a single estimate for the transmitted message without the ability to declare \u201cerase\u201d, when the same channel model and code-construction presented in Section II-A are used.\nAgain, by re-examining the derivation of Gallager\u2019s upper bound on the error probability presented in [16, Sec.2.4], it can be seen that the same results apply to channels with side- information available at the encoder non-causally where the only difference is that the probability of a received channel output y given a channel input x m (denoted by P (y|x m )) should be replaced by the probability of a received channel output y given a message index m (denoted by P (y|m)). Hence, we get that\nfor ρ > 0. Taking the expectation with respect to the code ensemble, and using the fact that U (s, m) is independent of all other encoding rules U (s, m ), we get\nThe following theorem provides an upper bound for the average decoding error.\nThe results of the above theorem were achieved by analyzing two moments E C P 1 1+ρ (y|m) and E C \t m =m P 1 1+ρ (y|m ) ρ using the same machinery used to derive Theorem 1 presented in the previous section.\nTheorem 1 gives achievable error exponents of decoding with erasure/list decoding using the optimal decoding rule, originally proposed by Forney for DMCs, for the Gel\u2019fand- Pinsker channel. The main approach used in the analysis is the distance enumerator method which was applied in [11] to obtain new error exponents of erasure/list decoding for DMCs. Theorem 2 is achieved using the same machinery. Both results originated from an optimal decoding rule (the use of Forney\u2019s approach in the ﬁrst theorem, and Gallager\u2019s approach in the second). Additional generalization can be made to the way the channel input sequence x is chosen. While in [4], x was chosen randomly from the conditional type class T ∗ x | su induced by a conditional distribution P ∗ (X|U, S) which can be optimized, in this work the encoder computes the channel input sequence x using a deterministic symbol-by- symbol function x i = f (s i , m) (see Section II-A). While in [13] such a deterministic mapping was sufﬁcient to achieve the capacity of the channel it is not clear whether a deterministic mapping is optimal also in the error exponent regime.\nThe main challenge of the proof is to analyze two main expressions: E C P ¯ λ (y|m) and E C m =m P (y|m ) λ . We brieﬂy describe how we tackle the ﬁrst expression. Analyzing the second expression is more involved, but, uses the same guidelines. Starting with E C P ¯ λ m , where ¯ λ 1 − λ:\n \n  \n \n \nwhere Ly(Q SU |Y ) is the number of codewords corresponding to message m in the sub codebook C( ˆ P (Ts)) whose joint conditional empirical distribution with y is Q SU |Y . Note that by introducing the term Ly(Q SU |Y ) the two summations are taken over subexponential number of terms which in turn enables the simpliﬁcation of the expressions (a similar expression is introduced in the second part of the proof which simpliﬁes the additional summtion over all messgaes m = m. See [14]). Later, it can be shown that Ly(Q SU |Y ) is upper\nwhere Z m,l (Q SU |Y , y) is a random variable which takes on two values:\nHence, we can rewrite Ly(Q SU |Y ) as an exponential sum of independent and identically distributed binary RV:\nThe rest of the proof is mainly technical. It follows the distance enumerator method with similar guidelines shown in [11]."},"refs":[{"authors":[{"name":"G. D. Forney"}],"title":{"text":"Exponential error bounds for erasure, list, and decision feedback schemes"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems "}},{"authors":[{"name":"E. Hof"},{"name":"I. Sason"},{"name":"S. Shamai"}],"title":{"text":"Performance bounds for erasure, list and feedback schemes with linear block codes"}},{"authors":[{"name":"E. Sabbag"},{"name":"N. Merhav"}],"title":{"text":"Achievable error exponents for channels with side information \u2013 erasure and list decoding"}},{"authors":[{"name":"P. Moulin"},{"name":"Y. Wang"}],"title":{"text":"Capacity and random-coding error exponents for channel coding with side information"}},{"authors":[{"name":"N. Merhav"}],"title":{"text":"Statistical physics and information theory"}},{"authors":[],"title":{"text":"Relations between random coding exponents and the statistical physics of random codes"}},{"authors":[{"name":"Y. Kaspi"},{"name":"N. Merhav"}],"title":{"text":"Error exponents for broadcast channels with degraded message sets"}},{"authors":[{"name":"R. Etkin"},{"name":"N. Merhav"},{"name":"E. Ordentlich"}],"title":{"text":"Error exponents of optimum decoding for the interference channel"}},{"authors":[{"name":"A. S. Baruch"},{"name":"N. Merhav"}],"title":{"text":"Exact random coding exponents for erasure decoding"}},{"authors":[{"name":"N. Merhav"}],"title":{"text":"Error exponents of erasure/list decoding revisited via moments of distance enumerators"}},{"authors":[{"name":"R. Gallager"}],"title":{"text":"A simple derivation of the coding theorem and some applications"}},{"authors":[{"name":"S. Gel\u2019fand"},{"name":"M. Pinsker"}],"title":{"text":"Coding for channels with random param- eter"}},{"authors":[{"name":"E. Sabbag"},{"name":"N. Merhav"}],"title":{"text":"Achievable error exponents for channels with side information - erasure and list decoding"}},{"authors":[{"name":"E. Sabbag"}],"title":{"text":"Topics in channel reliability and error exponent analysis"}},{"authors":[{"name":"A. J. Viterb"},{"name":"J. K. Omur"}],"title":{"text":"Principles of Digital Communication and Coding "}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566113.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T8.2","endtime":"12:10","authors":"Erez Sabbag, Neri Merhav","date":"1341575400000","papertitle":"Error Exponents of Optimum Erasure/List and Ordinary Decoding for Channels with Side Information","starttime":"11:50","session":"S16.T8: Error Exponents","room":"Stratton (491)","paperid":"1569566113"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
