{"id":"1569555891","paper":{"title":{"text":"Results on the Fundamental Gain of Memory-Assisted Universal Source Coding"},"authors":[{"name":"Ahmad Beirami"},{"name":"Mohsen Sardari"},{"name":"Faramarz Fekri"}],"abstr":{"text":"Abstract\u2014Many applications require data processing to be performed on individual pieces of data which are of ﬁnite sizes, e.g., ﬁles in cloud storage units and packets in data networks. However, traditional universal compression solutions would not perform well over the ﬁnite-length sequences. Recently, we proposed a framework called memory-assisted universal compression that holds a signiﬁcant promise for reducing the amount of redundant data from the ﬁnite-length sequences. The proposed compression scheme is based on the observation that it is possible to learn source statistics (by memorizing previous sequences from the source) at some intermediate entities and then leverage the memorized context to reduce redundancy of the universal compression of ﬁnite-length sequences. We ﬁrst present the fundamental gain of the proposed memory-assisted universal source coding over conventional universal compression (without memorization) for a single parametric source. Then, we extend and investigate the beneﬁts of the memory-assisted universal source coding when the data sequences are generated by a compound source which is a mixture of parametric sources. We further develop a clustering technique within the memory- assisted compression framework to better utilize the memory by classifying the observed data sequences from a mixture of parametric sources. Finally, we demonstrate through computer simulations that the proposed joint memorization and cluster- ing technique can achieve up to 6-fold improvement over the traditional universal compression technique when a mixture of non-binary Markov sources is considered."},"body":{"text":"Since Shannon\u2019s seminal work on the analysis of commu- nication systems, many researchers have contributed toward the development of compression schemes with the average code length as close as possible to the entropy. In practice, we usually cannot assume a priori knowledge on the statistics of the source although we still wish to compress the unknown stationary ergodic source to its entropy rate. This is known as the universal compression problem [1]\u2013[3]. However, unfortu- nately, universality imposes an inevitable redundancy depend- ing on the richness of the class of the sources with respect to which the code is universal [4]\u2013[6]. While an entire library of concatenated sequences from the same context (i.e., source model) can usually be encoded to less than a tenth of the original size using universal compression [5], [6], it is usually not an option to concatenate and compress the entire library at once. On the other hand, when an individual sequence is universally compressed regardless of other sequences, the performance is fundamentally limited [4], [5].\nIn [7], the authors observed that forming a statistical model from a training data would improve the performance of universal compression on ﬁnite-length sequences. In [8], we introduced memory-assisted universal source coding, where\nwe proposed memorization of the previously seen sequences as a solution that can fundamentally improve the performance of universal compression. As an application of memory- assisted compression, we introduced the notion of network compression in [9], [10]. It was shown that by deploying memory in the network (i.e., enabling some nodes to memorize source sequences), we may remove the redundancy in the network trafﬁc. In [9], [10], we assumed that memorization of the previous sequences from the same source provides a fun- damental gain g over and above the conventional compression performance of the universal compression of a new sequence from the same source. Given g, we derived the network-wide memorization gain G on both a random network graph [9] and a power-law network graph [10] when a small fraction of the nodes in the network are capable of memorization. However, [9], [10] did not explain as to how g is computed.\nAlthough the memory-assisted universal source coding natu- rally arises in a various set of problems, we deﬁne the problem setup in the most basic network scenario depicted in Fig 1. We assume that the network consists of the server S, the intermediate (relay) node R, and the client C, where S wishes to send the sequence x n to C. We assume that C does not have any prior communication with the server, and hence, is not capable of memorization of the source context. However, as an intermediate node, R has observed several previous sequences from S when forwarding them from S to clients other than C (not shown in Fig. 1). Therefore, R has formed a memory of the previous communications shared with S. Note that if the intermediate node R was absent, the source could possibly apply universal compression to x n and transmit to C whereas the presence of the memorized sequences at R can potentially reduce the communication overhead in the S-R link.\nThe objective of the present paper is to characterize the fundamental gain g of memorization of the context from a server\u2019s previous sequences in the universal compression of a new individual sequence from the same server. Clearly, a single stationary ergodic source does not fully model a real content generator server (for example the CNN news website in the Internet). Instead, a better model is to view every content generator server as a compound (mixture) of several information sources whose true statistical models are not readily available. In this work, we try to address this issue and propose a memorization and clustering technique for compression that is suitable for a compound source. Namely,\nwe would like to answer the following questions in the above setup: 1) Would the deployment of memory in the encoder (S) and the decoder (R) provide any fundamental beneﬁt in the universal compression? 2) If so, how does this gain g vary as the sequence length n and the memorized context length m change? 3) How much performance improvement should we expect from the joint memorization and clustering versus the memorization without clustering? 4) How should we realize the clustering scheme to achieve good performance from compression with the joint memorization and clustering?\nIn this section, we motivate the context memorization prob- lem by demonstrating the signiﬁcance of redundancy in the universal compression of small to moderate length sequences. Let A be a ﬁnite alphabet. Let the parametric source be deﬁned using a d-dimensional parameter vector θ = (θ 1 , ..., θ d ), where d denotes the number of the source parameters. Denote μ θ as the probability measure deﬁned by the parameter vector θ on sequences of length n. We also use the notation μ θ to refer to the parametric source itself. We assume that the d parameters are unknown. Denote P d as the family of sources with d-dimensional unknown parameter vector θ. We use the notation x n = (x 1 , ..., x n ) ∈ A n to present a sequence of length n from the alphabet A.\nLet H n (θ) be the source entropy given θ, i.e., H n (θ) = E log\nIn this paper log( ·) always denotes the logarithm in base 2. Let c n : A n → {0, 1} ∗ be an injective mapping from the set A n of the sequences of length n over A to the set {0, 1} ∗ of binary sequences. Further, let l n (x n ) denote a universal length func- tion for the codeword associated with the sequence x n . Denote R n (l n , θ ) as the expected redundancy of the code with length function l n (·), deﬁned as R n (l n , θ ) = El n (X n )−H n (θ). Note that the expected redundancy is always non-negative.\nLet I n (θ) be the Fisher information matrix, i.e., I n (θ)= {I ij n (θ)}= 1 n log e E ∂ 2 ∂θ\n(X n ) . (2) Fisher information matrix quantiﬁes the amount of informa- tion, on the average, that each symbol in a sample sequence x n from the source conveys about the source parameters. Let Jeffreys\u2019 prior on the parameter vector θ be denoted by\n|I(λ)| 1 2 dλ . Jeffreys\u2019 prior is optimal in the sense that the average minimax redundancy is achieved when the parameter vector θ is assumed to follow Jeffreys\u2019 prior [11]. Further, let ¯ R n be the average minimax redundancy given by [11], [12]\nn . (3) In [5], we obtained a lower bound on the average re-\ndundancy of the universal compression for the family of conditional two\u2013stage codes, where the unknown parameter is ﬁrst estimated and the sequence is encoded using the estimated parameter, as the following [5]:\nTheorem 1 Assume that the parameter vector θ follows Jef- freys\u2019 prior in the universal compression of the family of parametric sources P d . Let δ be a real number. Then,\nTheorem 1 can be viewed as a tighter variant of Theorem 1 of Merhav and Feder in [4] for parametric sources.\nTo demonstrate the signiﬁcance of the above theorem, we consider an example using a ﬁrst-order Markov source with alphabet size k = 256. This source may be represented using d = 256 × 255 = 62580 parameters. Fig. 2 shows the average number of bits per symbol required to compress the class of the ﬁrst-order Markov sources normalized to the entropy of the sequence for different values of entropy rates in bits per source symbol (per byte). In this ﬁgure, the curves demonstrate the lower bound on the compression rate achievable for at least 95% of sources , i.e., the probability measure of the sources from this class that may be compressed with a redundancy smaller than the curve is at most = 0.05. As can be seen, if the source entropy rate is 1 bit per byte (H n (θ)/n = 1), the compression overhead is 38%, 16%, 5.5%, 1.7%, and 0.5% for sequences of lengths 256kB, 1MB, 4MB, 16MB, and 64MB, respectively. Hence, we conclude that redundancy is signiﬁcant in the compression of ﬁnite-length low-entropy sequences, such as the Internet trafﬁc. It is this redundancy that we hope to remove using the memorization technique.\nIn this section, we present the problem setup and deﬁne the context memorization gain. We assume that the compound source comprises of a mixture of K information sources. Denote [ K] as the set {1, ..., K}. As the ﬁrst step, in this paper, we assume that K is ﬁnite and ﬁxed. We consider parametric sources with θ (i) as the parameter vector for the source i (i ∈ [K]). As in [5], we assume that θ (i) = (θ (i) 1 , θ (i) 2 , . . . , θ (i) d ) follows Jeffreys\u2019 prior for all i ∈ [K]. We consider the following scenario. We assume that, in Fig. 1, both the encoder (at S) and the decoder (at R) have access to a memory of the previous T sequences from the compound source. Let m = (n 0 , . . . , n T −1 ) denote the lengths of the previous T sequences generated by S. Further, denote y = {y n j (j)} T −1 j =0\nas the previous T sequences from S visited by the memory unit R. Note that each of these sequences might be from a different source model. We denote p = (p 1 , ..., p K ), where K\nwhich the information sources in the compound source are se- lected for sequence generation, i.e., the source i is picked with probability p i . Let the random variable Z j denote the index of the source that has generated the sequence y n j (j), and hence, Z j follows the distribution p over [K]. Therefore, at time step j , sequence y n j (j) is generated using the parameter vector θ (Z j ) . Further, denote Z as the vector Z = (Z 0 , ..., Z T −1 ). We wish to compress the sequence x n with source index Z T , when both the encoder and the decoder have access to a realization y of the random vector Y. This setup, although very generic, can incur in many applications. As the most basic example, consider the communication scenario in Fig. 1. The presence of memory y at R can be used by S to compress (via memory- assisted source coding) the sequence x n which is requested by client C from S. The compression can reduce the transmission cost on the S −R link while being transparent to the client, i.e., R decodes the memory-assisted source code and then applies conventional universal compression to x n and transmits to C.\nIn order to investigate the fundamental gain of the context memorization in the memory-assisted universal compression of the sequence x n over conventional universal source coding, we compare the following three schemes.\n\u2022 Ucomp (Universal compression), in which a sole univer- sal compression is applied on the sequence x n without regard to the memorized sequence y.\n\u2022 UcompM (Universal compression with context memo- rization), in which the encoder S and the decoder R both have access to the memorized sequence y from the compound source, and they use y to learn the statistics of the source for the compression of the sequence x n .\n\u2022 UcompCM (Universal compression with source-deﬁned clustering of the memory), which assumes that the mem- ory y is shared between the encoder S and the decoder R (i.e., the memory unit). Further, the source deﬁned clustering of memory implies that both S and R exactly know the index Z of the memorized sequences.\nThe performance of Ucomp is characterized using the expected redundancy R n (l n , θ ), which is discussed in Sec. II. Let Q (l n , ˆ l n , θ ) be deﬁned as the ratio of the expected codeword length with length function l n to that of ˆ l n , i.e.,\nH n (θ) + R n (l n , θ ) H n (θ) + R n (ˆl n , θ )\nFurther, let be such that 0 < < 1. We deﬁne g(l n , ˆ l n , ) as the gain of the length function ˆ l n as compared to l n . That is\nz : P Q(l n , ˆ l n , θ ) ≥ z ≥ 1 − . (5) In the case of UcompM, let l n |m be the length function\nwith context memorization, where the encoder S and the decoder R have access to sequences y with lengths m. Let m |m| = T −1 j =0 n j denote the total length of memory. 2 Further, let φ θ (Z T ) . Denote R n (l n |m , φ ) as the expected redundancy of encoding a sequence of length n form the para- metric source μ φ using the length function l n |m . We denote g M (n, m, φ, , p) \t E Z g (l n , l n |m , φ, ) as the fundamental gain of the context memorization on the family of parametric\nsources P d on a sequence of length n using context memory lengths m for a fraction (1− ) of the sources. In other words, context memorization provides a gain at least g M (n, m, φ, , p) for a fraction (1 − ) of the sources in the family.\nSimilarly in the case of UcompCM, let l n |m,Z denote the length function for the universal compression of a sequence of length n with memorized sequences y, where the vector Z of the source indices is known. We denote g CM (n, m, φ, , p)\nE Z g (l n , l n |m,Z , φ, ) as the fundamental gain of the context memorization in UcompCM. The following is a trivial lower bound on the context memorization gain in UcompCM.\nFact 2 The fundamental gain of context memorization is: g CM (n, m, φ, , p) ≥ 1.\nFact 2 simply states that the context memorization with source deﬁned clustering does not worsen the performance of the universal compression. We stress again that the saving of memory-assisted compression in terms of ﬂow reduction is only obtained in the S-R link. For example, for the given memorization gain g CM (n, m, φ, , p) = g 0 , the expected number of bits needed to transfer x n to R is reduced from El n (X n ) in Ucomp to 1 g 0 El n (X n ) in UcompCM.\nIn this section, we present our main results on the mem- orization gain with and without clustering. The proofs are omitted due to the lack of space. We give further consideration to the case K = 1 since it represents the memorization gain when all of the memorized sequences are from a single ﬁxed source model.\nIn this case, since p = 1 and Z = 1 is known, there is no distinction between UcompM and UcompCM, and hence, we drop the subscript of g. The next theorem characterizes the fundamental gain of memory-assisted source coding:\nTheorem 3 Assume that the parameter vector θ follows Jef- freys\u2019 prior in the universal compression of the family of parametric sources P d . Then,\nFurther, let g(n, ∞, φ, , p) be deﬁned as the achievable gain of memorization where there is no constraint on the length of the memory, i.e, g(n, ∞, φ, , p) lim m →∞ g (n, m, φ, , p). The following Corollary quantiﬁes the memorization gain for unbounded memory size.\nCorollary 4 Assume that the parameter vector θ follows Jeffreys\u2019 prior in the universal compression of the family of parametric sources P d . Then,\nNext, we consider the case where the sequence length n grows to inﬁnity. Intuitively, we would expect that the memorization gain become negligible for the compression of long sequences. Let g( ∞, m, φ, , p) lim n →∞ g (n, m, φ, , p). In the fol- lowing, we claim that memorization does not provide any beneﬁt when n → ∞:\nProposition 5 g (n, m, φ, , 1) approaches 1 as the length of the sequence x n grows, i.e., g (∞, m, φ, , 1) = 1.\nAs stated in the problem setup, the sequences in the memory may be from various sources. This raises the question that whether a naive memorization of the previous sequences using UcompM without regard to which source parameter has indeed generated the sequence would sufﬁce to achieve the memorization gain. Let D be an upper bound on the size of the context tree used in compression. Denote ¯ μ D θ as the probability measure that is deﬁned on the tree of depth D from the mixture of the sources. Further, let D n (μ φ ||μ ¯ θ ) =\n(x n ) . The following proposition char- acterizes the performance of UcompM when applied to a compound source for K ≥ 2.\nProposition 6 Let θ (i) (i ∈ [K]) follow Jeffreys\u2019 prior. Then, the memorization gain in UcompM as m → ∞ is upper bounded by\nNote that since K ≥ 2, then D n (μ φ ||¯μ D θ ) = Θ(n), 3 (unless θ (i) = θ (j) for all i, j ∈ [K], which occurs with zero probabil- ity). Therefore, the redundancy of UcompM is R n (l n |m , φ ) = Θ(n) with probability one. Proposition 6 states that when the context is built using the mixture of the sources, with probability one, the redundancy of UcompM is worse than the redundancy of Ucomp for a sufﬁciently large sequence, i.e., the memorization gain becomes less than unity for sufﬁciently large n. Therefore, the crude memorization of the context by node R in Fig. 1 from the previous communications not only does not improve the compression performance but also asymptotically makes it worse. We shall see some discussion on validation of this claim based on simulations in Sec. VI.\nThus far, we demonstrated in Proposition 6 that the crude memorization in the memory unit is not beneﬁcial when a compound source is present. This necessitates to ﬁrst appro- priately cluster the sequences in the memory. Then, based on the criteria as to which cluster the new sequence x n belongs to, we utilize the corresponding memorized context for the compression. In the following, we analyze the problem for the source-deﬁned clustering (deﬁned in Sec. III). In this clustering, we assume both S and R (in Fig. 1) can exactly know the index i ∈ [K] and hence all the sequences that belong to the same source θ (i) in the compound source are assigned to the same cluster (for all i ∈ [K]). Further, we assume that S can exactly classify the new sequence x n to the cluster with parameter θ (Z T ) . In Sec. V, however, we will relax these assumptions and study the impact of clustering in practice.\nLet H( p) = − K i =1 p i log(p i ) be the entropy of the source model. The following proposition quantiﬁes the achievable redundancy and the memorization gain of UcompCM.\nTheorem 7 Let θ (i) (i ∈ [K]) follow Jeffreys\u2019 prior. Then, the memorization gain of UcompCM is lower bounded by g CM (n, m, φ, , p)≥1+\nIn this section, we try to answer the main question in the memory-assisted compression setup we introduced: \u201cHow do we utilize the available memory to better compress a sequence generated by a compound source?\u201d It is obvious that the performance of conventional universal compression schemes (those without memory) cannot be improved by clustering of the compound source as x n is encoded without regard to y. However, because of a compound source, clustering is necessary to effectively utilize the memory in the proposed memory-assisted compression. Within this framework, we identify two interrelated problems: 1) How do we perform clustering of the memorized data to improve the performance of memory-assisted compression? 2) Given a set of clustered memory, how do we classify an incoming new sequence into one of the clusters in the memory using which the performance of memory-assisted compression is maximized? This relaxes the assumption of knowing Z by the encoder and the decoder in the analysis of Sec. IV.\nAs one approach, it is natural to adapt a clustering algo- rithm, among the many, that has the codelength minimization as its principle criterion. Thus, the goal of the clustering is to group the sequences in the memory such that the total length of all the encoded sequences is minimized (i.e., the sequences are grouped such that they are compressed well together). We employ a Minimum Description Length (MDL) [13] approach suggested by [14]. The MDL model selection approach is based on ﬁnding shortest description length of a given se- quence relative to a model class. We do not have a proof that the MDL clustering is necessarily optimal for our goal (for all sequence lengths and memory sizes). However, as we will see in Sec. VI, for the cases of interest where the length of memory is larger than the length of the new sequence, the MDL clustering demonstrates a very good performance close to that of assuming to know Z (in source-deﬁned clustering).\nNow, we would like to ﬁnd a proper class for a new sequence x n generated by one of the K sources. Given a set of T sequences taken from K different sources, we assume those T sequences have already been clustered into K clusters C 1 , . . . , C K . Then, the classiﬁcation algorithm for x n is as follows. We include the sequence x n in each cluster one at a time and ﬁnd the total description length of all sequences in the K clusters. Then, we label x n with the cluster whose resulting total description length is the minimum.\nNext, we describe as to how we cluster the T sequences in memory. A good clustering is such that it allows efﬁcient compression of the whole data set. Equivalently, the sequences that are clustered together should also compress well together. We start by an initial clustering of the data set in the memory. Through experiments, we observed that this initial clustering has a considerable impact on the convergence of the clustering algorithm which is in accordance with the observation in [14]. We found that an initial clustering based on the estimated entropy of the sequences greatly reduces the number of iterations till convergence. The clustering is done iteratively by surﬁng the data set and moving a sequence from cluster i to cluster j if this swapping results in a shorter total description length of the whole data set.\nIn this section, we characterize the performance of the proposed memorization scheme through computer simulations. In order to illustrate the importance of clustering for efﬁ- cient use of memory, we have evaluated the memory-assisted compression gain (over the performance of the conventional universal compression) for three cases: g M , g CM , and g MDL . Note that g MDL is deﬁned as the gain of memorization with MDL clustering in Sec. V. We demonstrate the signiﬁcance of the memorization through an example, where we again consider K ﬁrst-order Markov sources with alphabet size k = 256, source entropy H n (φ) n = 1 bit per byte, and = 0.05.\nFig. 3 considers the single source case (i.e., K = 1). The lower bound on the memorization gain is demonstrated as a function of the sequence length n for different values of the memory size m. As can be seen, signiﬁcant improvement in the compression may be achieved using memorization. As demonstrated in Fig. 3, the memorization gain for a memory of length m = 8MB is very close to g(n, ∞, φ, 0.05, 1), and hence, increasing the memory size beyond 8MB does not result in the substantial increase of the memorization gain. We observe that more than 50% improvement is achieved in the compression performance of a sequence of length n = 128kB with a memory of m = 8MB. On the other hand, as n → ∞, the memorization gain becomes negligible as expected.\nFor the rest of the experiments, we ﬁxed the length of the sequences generated by the source, i.e., n j = n for all j. We used K = 10 with uniform distribution, i.e., p i = 1 10 for i ∈ [K]. Further, we performed compression using Context Tree Weighting (CTW) [3] and averaged the simulation results over multiple runs of the experiment. Fig. 4 depicts g CM . As can be seen, joint memorization and clustering achieves up to 6-fold improvement over the traditional universal compression. Fig. 5 depicts the experimental g CM using CTW, the theoretical lower bound on g CM derived in Sec. IV, and the experimental g M for memory m = 10MB. As we expected, with no clustering, the memory-assisted compression may result in a worse compression rate than compression with no memory\nvalidating our theoretical result in Sec. IV-B. Finally, the experimental results of memory-assisted compression gain g MDL under MDL clustering, summarized in Table I, show that g MDL is close to g CM , demonstrating the effectiveness of MDL clustering for compression.\nIn conclusion, this paper demonstrated that memorization (i.e., learning the source statistics) can lead to a fundamental performance improvement over the traditional universal com- pression. This was presented for both single and compound sources. We derived theoretical results on the achievable gains of memory-assisted source coding for a compound (mixture) source and argued that clustering is necessary to obtain mem- orization gain for compound sources. We also presented a fast MDL clustering algorithm tailored for the compression prob- lem at hand and demonstrated its effectiveness for memory- assisted compression of ﬁnite-length sequences."},"refs":[{"authors":[{"name":"L. Davisson"}],"title":{"text":"Universal noiseless coding"}},{"authors":[{"name":"M. Weinberger"},{"name":"J. Rissanen"},{"name":"M. Feder"}],"title":{"text":"A universal ﬁnite memory source"}},{"authors":[{"name":"F. Willems"},{"name":"Y. Shtarkov"},{"name":"T. Tjalkens"}],"title":{"text":"The context-tree weighting method: basic properties"}},{"authors":[{"name":"N. Merhav"},{"name":"M. Feder"}],"title":{"text":"A strong version of the redundancy-capacity theorem of universal coding"}},{"authors":[{"name":"A. Beirami"},{"name":"F. Fekri"}],"title":{"text":"Results on the redundancy of universal compression for ﬁnite-length sequences"}},{"authors":[{"name":"J. Rissanen"}],"title":{"text":"Universal coding, information, prediction, and estimation"}},{"authors":[{"name":"G. Korodi"},{"name":"J. Rissanen"},{"name":"I. Tabus"}],"title":{"text":"Lossless data compression using optimal tree machines"}},{"authors":[{"name":"A. Beirami"},{"name":"F. Fekri"}],"title":{"text":"Memory-assisted universal source coding"}},{"authors":[{"name":"M. Sardari"},{"name":"A. Beirami"},{"name":"F. Fekri"}],"title":{"text":"On the network-wide gain of memory-assisted source coding"}},{"authors":[],"title":{"text":"Memory-assisted universal compression of network ﬂows"}},{"authors":[{"name":"B. Clarke"},{"name":"A. Barron"}],"title":{"text":"Information-theoretic asymptotics of Bayes methods"}},{"authors":[{"name":"K. Atteson"}],"title":{"text":"The asymptotic redundancy of Bayes rules for Markov chains"}},{"authors":[{"name":"A. Barron"},{"name":"J. Rissanen"},{"name":"B. Yu"}],"title":{"text":"The minimum description length principle in coding and modeling"}},{"authors":[{"name":"P. Kontkanen"},{"name":"P. Myllymaki"},{"name":"W. Buntine"},{"name":"J. Rissanen"},{"name":"H. Tirri"}],"title":{"text":"An MDL framework for data clustering"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569555891.pdf"},"links":[{"id":"1569566527","weight":8},{"id":"1569565489","weight":8},{"id":"1569566943","weight":4},{"id":"1569565091","weight":4},{"id":"1569552245","weight":4},{"id":"1569565227","weight":4},{"id":"1569551535","weight":12},{"id":"1569565461","weight":4},{"id":"1569564233","weight":4},{"id":"1569565291","weight":4},{"id":"1569566821","weight":4},{"id":"1569562685","weight":4},{"id":"1569566579","weight":8},{"id":"1569564989","weight":4},{"id":"1569564311","weight":4},{"id":"1569566905","weight":4},{"id":"1569558681","weight":8},{"id":"1569565365","weight":20},{"id":"1569567665","weight":8},{"id":"1569566687","weight":4},{"id":"1569566939","weight":8},{"id":"1569553537","weight":4},{"id":"1569553519","weight":4},{"id":"1569554881","weight":4},{"id":"1569565559","weight":4},{"id":"1569566721","weight":4},{"id":"1569565633","weight":4},{"id":"1569565219","weight":4},{"id":"1569565469","weight":4},{"id":"1569566853","weight":8},{"id":"1569562551","weight":4},{"id":"1569566901","weight":4},{"id":"1569565415","weight":4},{"id":"1569557275","weight":4},{"id":"1569565181","weight":4},{"id":"1569565421","weight":8},{"id":"1569565013","weight":8},{"id":"1569566771","weight":4},{"id":"1569551905","weight":8},{"id":"1569565271","weight":4},{"id":"1569565669","weight":4},{"id":"1569564281","weight":4},{"id":"1569562367","weight":8},{"id":"1569565337","weight":4},{"id":"1569551539","weight":4},{"id":"1569565113","weight":58},{"id":"1569564257","weight":8},{"id":"1569564931","weight":4},{"id":"1569551541","weight":4},{"id":"1569564807","weight":4},{"id":"1569566113","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T1.1","endtime":"15:00","authors":"Ahmad Beirami, Mohsen Sardari, Faramarz Fekri","date":"1341326400000","papertitle":"Results on the Fundamental Gain of Memory-Assisted Universal Source Coding","starttime":"14:40","session":"S7.T1: Lossless and Universal Source Coding","room":"Kresge Rehearsal B (030)","paperid":"1569555891"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
