{"id":"1569565545","paper":{"title":{"text":"Subsampling at Information Theoretically Optimal Rates"},"authors":[{"name":"Adel Javanmard"},{"name":"Andrea Montanari"}],"abstr":{"text":"Abstract\u2014We study the problem of sampling a random signal with sparse support in frequency domain. Shannon famously considered a scheme that instantaneously samples the signal at equispaced times. He proved that the signal can be reconstructed as long as the sampling rate exceeds twice the bandwidth (Nyquist rate). Cand`es, Romberg, Tao introduced a scheme that acquires instantaneous samples of the signal at random times. They proved that the signal can be uniquely and efﬁciently reconstructed, provided the sampling rate exceeds the frequency support of the signal, times logarithmic factors.\nIn this paper we consider a probabilistic model for the signal, and a sampling scheme inspired by the idea of spatial coupling in coding theory. Namely, we propose to acquire non-instantaneous samples at random times. Mathematically, this is implemented by acquiring a small random subset of Gabor coefﬁcients. We show empirically that this scheme achieves correct reconstruction as soon as the sampling rate exceeds the frequency support of the signal, thus reaching the information theoretic limit."},"body":{"text":"For the sake of simplicity, we consider a discrete-time model (analogous to the one of [4]) and denote signals in time domain as x 2 C n , x = (x(t)) 1 tn = (x(1), . . . , x(n)) T . Their discrete Fourier transform is denoted by bx 2 C n , b x = ( b x(!)) ! 2⌦ n , where ⌦ n = {! = 2⇡k/n : k 2 {0, 1, . . . ,\nHere h · , · i denotes the standard scalar product on C n . Also, for a complex variable z, z is the complex conjugate of z. Notice that (b ! ) ! 2⌦ n is an orthonormal basis of C n . This implies Parseval\u2019s identity hbx 1 , b x 2 i = hx 1 , x 2 i. In addition, the inverse transform is given by\nWe will denote by T n = {1, . . . , n} the time domain, and will consider signals that are sparse in the Fourier domain.\nA sampling mechanism is deﬁned by a measurement matrix A 2 R m ⇥n . Measurement vector y = (y(1), . . . , y(m)) T 2 R m is given by\ny = Ax + w ⌘ y 0 + w , \t (3) where w is a noise vector with variance 2 , and y 0 is the vector of ideal (noiseless) measurements. In other words, y(i) = ha i , x i where we let a ⇤ 1 , . . . a ⇤ m be the rows of A.\nInstantaneous sampling corresponds to vectors a i that are canonical base vectors.\nMeasurements can also be given in terms of the Fourier transform of the signal:\nThe rows of A F are denoted by ba ⇤ 1 , . . . , ba ⇤ m , and obviously ba i = Fa i . Here and below, for a matrix M, M ⇤ is the hermitian adjoint of M, i.e. M ⇤\n= M ji . B. Information theory model\nIn [4], Cand`es, Romberg, Tao studied a randomized scheme that samples the signal instantaneously at uniformly random times. Mathematically, this corresponds to choosing the mea- surement vectors a i to be a random subset of the canonical basis in C n . They proved that, with high probability, these measurements allow to reconstruct x uniquely and efﬁciently, provided m C |S| log n, where S = {! 2 ⌦ : b x(!) 6= 0} is the frequency support of the signal.\nIn this paper, we consider a probabilistic model for the signal bx, namely we assume that the components bx(!), ! 2 ⌦ are i.i.d. with P{bx(!) 6= 0}  \" and E{|bx(!)| 2 }  C < 1. The distribution of bx(!) is assumed to be known. Indeed, information theoretic thinking has led to impressive progress in digital communication, as demonstrated by the development of modern iterative codes [14]. More broadly, probabilistic mod- els can lead to better understanding of limits and assumptions in relevant applications to digital communication and sampling theory.\nFollowing [4] that considers a discrete-time model, the author in [3] studied the sampling problem for multi band, spectrum-sparse continuous-time signals and showed that blind reconstruction near Landau rate is possible with high proba- bility.\nThe sampling scheme developed here is inspired by the idea of spatial coupling, that recently proved successful in coding theory [7], [16], [10], [11] and was introduced to compressed sensing by Kudekar and Pﬁster [9]. The basic idea, in this context, is to use suitable band diagonal sensing matrices. Krzakala et al. [8] showed that, using the appropriate mes- sage passing reconstruction algorithm, and \u2018spatially-coupled\u2019 sensing matrices, a random k-sparse signal bx 2 R n can be recovered from k+o(n) measurements. This is a surprising re- sult, given that standard compressed sensing methods achieve successful recovery from ⇥(k log(n/k)) measurements.\nThe results of [8] were based on statistical mechanics meth- ods and numerical simulations. A rigorous proof was provided in [5] using approximate message passing (AMP) algorithms [6] and the analysis tools provided by state evolution [6], [2]. Indeed, [5] proved a more general result. Consider a non-random sequence of signals bx (n) 2 R n indexed by the problem dimensions n, and such that the empirical law of the entries of bx (n) , p (n) b X (t) = n 1 P n i=1 b x (n) i , converges weakly to a limit p b X with bounded second moment. Then, spatially- coupled sensing matrices under AMP reconstruction achieve (with high probability) robust recovery of bx (n) , as long as the number of measurements is m d(p b X ) + o(n). Here d(p b X ) is the (upper) Renyi information dimension of the probability distribution p b X . This quantity ﬁrst appeared in connection with compressed sensing in the work of Wu and Verd´u [17]. Taking an information-theoretic viewpoint, Wu and Verd´u proved that the Renyi information dimension is the fundamental limit for analog compression.\nUsing spatial coupling and (approximate) message pass- ing, the approaches of [8], [5] allow successful compressed sensing recovery from a number of measurements achieving the information-theoretic limit. While these can be formally interpreted as sampling schemes for the discrete-time sam- pling problem introduced in Section I-A, they present in fact several unrealistic features. In particular, the entries of A are independent Gaussian entries with zero mean and suitably chosen variances. It is obviously difﬁcult to implement such a measurement matrix through a physical sampling mechanism.\nThe present paper aims at showing that the spatial coupling phenomenon is \u2013in the present context\u2013 signiﬁcantly more robust and general than suggested by the constructions of [8], [5]. Unfortunately, a rigorous analysis of message passing algorithms is beyond reach for sensing matrices with depen- dent or deterministic entries. We thus introduce an ensemble of sensing matrices, and show numerically that, under AMP reconstruction, they allow recovery at undersampling rates close to the information dimension. Similar simulations were already presented by Krzakala et al. [8] in the case of matrices with independent entries.\nOur matrix ensemble can be thought of as a modiﬁcation of the one in [4] for implementing spatial coupling. As mentioned above, [4] suggests to sample the signal pointwise (instanta- neously) in time. In the Fourier domain (in which the signal is sparse) this corresponds to taking measurements that probe all frequencies with the same weight. In other words, A F is not band-diagonal as required in spatial coupling. Our solution is to \u2018smear out\u2019 the samples: instead of measuring x(t ⇤ ), we modulate the signal with a wave of frequency ! ⇤ , and integrate it over a window of size W 1 around t ⇤ . In Fourier space, this corresponds to integrating over frequencies within a window W around ! ⇤ . Each measurement corresponds to a different time-frequency pair (t ⇤ , ! ⇤ ). While there are many possible implementations of this idea, the Gabor transform offers an\nanalytically tractable avenue. Our method can be thought of as a subsampling of a discretized Gabor transform of the signal.\nIn [13], Gabor frames have also been used to exploit the sparsity of signals in time and enable sampling multipulse signals at sub-Nyquist rates.\nThe sensing matrix A is drawn from a random ensemble denoted by M(n, m 1 , L, `, ⇠, ) . Here n, m 1 , L, ` are integers and ⇠, 2 (0, 1). The rows of A are partitioned as follows:\n(5) where |R k | = L, and |R 0 | = bn c. Hence, m = m 1 L + bn c. Notice that m/n = (m 1 L + bn c)/n. Since we will take n much larger than m 1 L , the undersampling ratio m/n will be arbitrary close to . Indeed, with an abuse of language, we will refer to as the undersampling ratio.\na r (t) = a(t; t r , ! r ) , \t (6) where {t r } r 2R 0 are independent and uniformly random in T n , and {! r } r 2R 0 are equispaced in ⌦ n . Finally, for t ⇤ 2 T n , and ! ⇤ 2 ⌦ n , we deﬁne\nHere P ⇠,` (t ⇤ , t) is the probability that a random walk on the circle with n sites {1, . . . , n} starting at time 0 at site t ⇤ is found at time ` at site t. The random walk is lazy, i.e. it stays on the same position with probability 1 ⇠ 2 (0, 1) and moves with probability ⇠ choosing either of the adjacent sites with equal probability.\nwhere sums on T n are understood to be performed modulo n. We can think of P ⇠,` as a discretization of a Gaussian kernel. Indeed, for 1 ⌧ ` ⌧ n 2 we have, by the local central limit theorem,\nn (t t ⇤ ) 2 2⇠`\n. \t (8) and hence C ` ⇡ (4⇡⇠`) 1/4 .\nThe above completely deﬁne the sensing process. For the signal reconstruction we will use AMP in the Fourier domain, i.e. we will try to reconstruct bx from y = A F b x + w . It is therefore convenient to give explicit expressions for the measurement matrix in this domain.\n1) For each k 2 {1, · · · , m 1 }, and each r 2 R k , we have ˆa r = e k , where e k 2 R n refers to the k th standard basis\nelement, e.g., e 1 = (1, 0, 0, · · · , 0). These rows are used to sense the extreme of the spectrum frequencies.\np n e i(! ! ⇤ )t ⇤ 1 ⇠ + ⇠ cos(! ! ⇤ ) ` . Again, to get some insight, we consider the asymptotic behav- ior for 1 ⌧ ` ⌧ n 2 . It is easy to check that ba is signiﬁcantly different from 0 only if ! ! ⇤ = O(` 1/2 ) and\n. Hence the measurement y i depends on the signal Fourier transform only within a window of size W = O(` 1/2 ), with 1/n ⌧ W ⌧ 1. As claimed in the introduction, we recognize that the rows of A are indeed (discretized) Gabor ﬁlters. Also it is easy to check that A F is roughly band-diagonal with width W .\nWe use a generalization of the AMP algorithm for spatially- coupled sensing matrices [5] to the complex setting. Assume that the empirical law of the entries of bx (n) converges weakly to a limit p b X , with bounded second moment. The algorithm proceeds by the following iteration (initialized with bx 1 i = E{ b X } for all i 2 [n]). For b x t 2 C n , r t 2 C m ,\nHere ⌘ t (v) = (⌘ t,1 (v 1 ), . . . , ⌘ t,n (v n )), where ⌘ t,i : C ! C is a scalar denoiser. In this paper we assume that the prior p b X is known and use the posterior expectation denoiser\nwhere b X ⇠ p b X and Z ⇠ N C (0, 1) is a standard com- plex normal random variable, independent of b X . Also, r t is the complex conjugate of r t and indicates Hadamard (entrywise) product. The matrix Q t 2 R m ⇥n , and the vector b t 2 R m are given by\nA F ) ⇤ r t ) i ), @⌘ t,i ⌘ @⌘ t,i (bx t i + ((Q t A F ) ⇤ r t ) i ). Throughout, ⌘ t,i (v) is viewed as a function of v, v, and v, v are taken as independent variables in the sense that @v/@v = 0. Then, @⌘ t,i and @⌘ t,i respectively denote the partial derivative of ⌘ t,i with respect to v and v. Also, derivative is understood here on the complex domain. (These are the principles of Wirtinger\u2019s calculus for the complex functions [15]). Finally,\nthe sequence { (t)} t 0 is determined by the following state evolution recursion.\nHere mmse( · ) is deﬁned as follows. If b X ⇠ p b X and Y = b X + s 1/2 Z for Z ⇠ N C (0, 1) independent of b X , then\n1 + s 1 i v i , (15) where 2 (z) = 1/(⇡ 2 ) exp{ zz/ 2 } is the density func- tion of the complex normal distribution with mean zero and variance 2 .\nOur ﬁrst set of experiments aims at illustrating the spatial coupling phenomenon and checking the predictions of state evolution. In these experiments we use \" = 0.1, = 0.001,\n= 0.15, n = 5000, ` = 800, m 1 = 20, L = 3, and ⇠ = 0.5. State evolution yields an iteration-by-iteration prediction\nof the AMP performance in the limit of a large number of dimensions. State evolution can be proved rigorously for sensing matrices with independent entries [2], [1]. We also refer to [5] for a heuristic derivation which provides the right intuition in the case of spatially-coupled matrices. We expect however the prediction to be robust and will check it through numerical simulations for the current sensing matrix A F . In particular, state evolution predicts that\n. (16) Figure 1 shows the evolution of proﬁle (t) 2 R m , given by\nthe state evolution recursion (13). This clearly demonstrates the spatial coupling phenomenon. In our sampling scheme, additional measurements are associated to the ﬁrst few coordi- nates of bx, namely, bx 1 , · · · , b x m 1 . This has negligible effect on the undersampling rate ratio because m 1 L/n ! 0. However, the Fourier components bx 1 , · · · , b x m 1 are oversampled. This leads to a correct reconstruction of these entries (up to a mean square error of order 2 ). This is reﬂected by the fact that\nbecomes of order 2 on the ﬁrst few entries after a few iterations (see t = 5 in the ﬁgure). As the iteration proceeds, the contribution of these components is correctly subtracted from all the measurements, and essentially they are removed from the problem. Now, in the resulting problem the ﬁrst few variables are effectively oversampled and the algorithm\nreconstructs their values up to a mean square error of 2 . Correspondingly, the proﬁle falls to a value of order 2 in the next few coordinates. As the process is iterated, all the variables are progressively reconstructed and the proﬁle\nfollows a traveling wave with constant velocity. After a sufﬁcient number of iterations (t = 400 in the ﬁgure), is uniformly of order 2 .\nIn order to check the prediction of state evolution, we compare the empirical and the predicted mean square errors\n1 n\n. (18) The values of MSE AMP and MSE SE versus iteration are depicted in Fig. 2. (Values of MSE AMP and the bar errors correspond to M = 30 Monte Carlo instances). This veriﬁes that the state evolution provides an iteration-by iteration pre- diction of AMP performance. We observe that MSE AMP (and MSE SE ) decreases linearly versus iteration.\nIn this section, we consider the noiseless compressed sens- ing setting, and reconstruction through different algorithms and sensing matrix ensembles.\nLet A be a sensing matrix\u2013reconstruction algorithm scheme. The curve \" 7! A (\") describes the sparsity-undersampling\ntradeoff of A if the following happens in the large-system limit n, m ! 1, with m/n = . The scheme A does (with high probability) correctly recover the original signal provided > A (\"), while for < A (\") the algorithm fails with high probability. We will consider three schemes. For each of them, we consider a set of sparsity parameters \" 2 {0.1, 0.2, 0.3, 0.4, 0.5}, and for each value of \", evaluate the empirical phase transition through a logit ﬁt (we omit details, but follow the methodology described in [6]).\n1) Scheme I: We construct the sensing matrix as described in Section II-A and for reconstruction, we use the algorithm described in Section II-B. An illustration of the phase transi- tion phenomenon is provided in Fig. 4. This corresponds to \" = 0.2 and an estimated phase transition location = 0.23.\nAs it is shown in Fig. 3, our results are consistent with the hypothesis that this scheme achieves successful reconstruction at rates close to the information theoretic lower bound > d(p b X ) = \". (We indeed expect the gap to decrease further by taking larger values of `, n.)\n2) Scheme II: The sensing matrix A F is obtained by choos- ing m rows of the Fourier matrix F at random. In time domain, this corresponds to sampling at m random time instants as in [4]. Reconstruction is done via AMP algorithm with posterior expectation as the denoiser ⌘. More speciﬁcally, through the following iterative procedure.\nb x t+1 = ⌘ t (bx t + A ⇤ r t ) , r t = y A b x t +\n(0, 1). Also @⌘ t,i ⌘ @⌘ t (bx t i + (A ⇤ r t ) i ), @⌘ t,i ⌘ @⌘ t (bx t i + (A ⇤ r t ) i ) and for a vector u 2 R n , hui = n 1 P n i=1 u i .\nWhen A has independent entries A ij ⇠ N(0, 1/m), state evolution (20) predicts the performance of the algorithm (19) [2]. Therefore, the algorithm successfully recovers the original signal with high probability, provided\ns · mmse(s) . \t (21) As shown in Fig. 3, the empirical phase transition for scheme II is very close to the prediction ˜(\"). Note that schemes I, II both use posterior expectation denoising. However, as observed in [8], spatially-coupled matrices in scheme I signif- icantly improve the performances.\n3) Scheme III: We use the spatially-coupled sensing matrix described in Section II-A, and an AMP algorithm with soft- thresholding denoiser\nz . \t (22) The algorithm is deﬁned as in Eq. (9), except that the soft- thresholding denoiser is used in lieu of the posterior expecta-\nW ai a (t) 1 , (23) and the sequence of proﬁles { (t)} t 0 is given by the follow- ing recursion.\nW ai E{|⌘ t,i ( b X + s 1/2 i Z; ↵ ⇤ s 1/2 i ) b X | 2 }. Finally ↵ ⇤ = ↵ ⇤ (\") is tuned to optimize the phase transition boundary. This is in fact a generalization of the complex AMP (CAMP) algorithm that was developed in [12] for unstructured matrices. CAMP strives to solve the standard convex relaxation\nFor a given \", we denote by ` 1 (\") the phase transition location for ` 1 minimization, when sensing matrices with i.i.d. entries are used. This coincides with the one of CAMP with optimally tuned ↵ = ↵ ⇤ (\") [18], [12].\nThe empirical phase transition of Scheme III is shown in Fig. 3. The results are consistent with the hypothesis that the phase boundary coincides with ` 1 . In other words, spatially- coupled sensing matrix does not improve the performances under ` 1 reconstruction (or under AMP with soft-thresholding denoiser). This agrees with earlier ﬁndings by Krzakala et al. for Gaussian matrices ([8], and private communications). This can be inferred from the the state evolution map. For AMP with posterior expectation denoiser, and for \" < < ˜(\"), the state evolution map has two stable ﬁxed points; one of order\n2 , and one much larger. Spatial coupling makes the algorithm converge to the \u2018right\u2019 ﬁxed point. However, the state evolution map corresponding to the soft-thresholding denoiser is concave and has only one stable ﬁxed point, much larger than 2 . Therefore, spatial coupling is not helpful in this setting.\nA.J. is supported by a Caroline and Fabian Pease Stanford Graduate Fellowship. Partially supported by NSF CAREER award CCF- 0743978 and AFOSR grant FA9550-10-1-0360. The authors thank the reviewers for their insightful comments."},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565545.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T9.2","endtime":"15:20","authors":"Adel Javanmard, Andrea Montanari","date":"1341500400000","papertitle":"Subsampling at Information Theoretically Optimal Rates","starttime":"15:00","session":"S13.T9: Fourier Subsampling","room":"Stratton West Lounge (201)","paperid":"1569565545"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
