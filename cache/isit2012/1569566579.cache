{"id":"1569566579","paper":{"title":{"text":"Piecewise Constant Prediction"},"authors":[{"name":"Erik Ordentlich"},{"name":"Marcelo J. Weinberger"},{"name":"Yihong Wu ∗"}],"abstr":{"text":"Abstract\u2014Minimax prediction of binary sequences is inves- tigated for cases in which the predictor is forced to issue a piecewise constant prediction. The minimax strategy is char- acterized for Hamming loss whereas, for logarithmic loss, an asymptotically minimax strategy which achieves the leading term of the asymptotic minimax redundancy, is proposed. The average redundancy case is also analyzed for i.i.d. distributions. The piecewise constant prediction paradigm may be of relevance to resource constrained settings."},"body":{"text":"Consider a game in which, as a predictor observes a binary sequence x n = x 1 x 2 · · · x n , it makes causal predictions on each bit x t+1 , t = 0, 1, · · · , n−1, based on the observed preﬁx x t . These predictions take the form of probability assignments p t+1 (a|x t ), a ∈ {0, 1}. Once x t+1 is revealed, the predictor incurs a loss, e.g., − log p t+1 (x t+1 |x t ) in the data compression problem, or 1 − p t+1 (x t+1 |x t ) for (expected) Hamming loss, which accumulates over time. The goal of the predictor is to approach the cumulative loss of the best constant predictor for x n (determined in hindsight, with full knowledge of x n , termed Bayes envelope), which is the empirical entropy of x n in the data compression problem, or min(n 0 (x n ), n 1 (x n )) for Hamming loss, where n a (x n ) denotes the number of occurrences of a ∈ {0, 1} in x n . In one version of the game, the goodness of the predictor is assessed by its excess loss over the Bayes envelope (termed regret, or redundancy in the data compression case) for the worst case sequence (maximum regret), the best strategy is thus termed minimax, and the corresponding maximum regret is the minimax regret. Notice that the minimax strategy (and the minimax regret) may depend on the horizon n of the game.\nNow, imagine a situation in which the predictor is forced to \u201cfreeze\u201d its prediction for a number of prediction rounds. In the simplest such scenario, for a given block length T , the probability assignments p iT +1 , p iT +2 , · · · , p (i+1)T , i = 0, 1, · · · , m − 1, must all coincide, where we assume that n = mT for some positive integer m. Thus, p iT +j , j = 1, 2, · · · , T , can only depend on x iT and, in particular, must be independent of x iT +1 , x iT +2 , · · · , x iT +T −1 . The question arises: How badly can the minimax regret be affected by the constraint of piecewise constant prediction? This question is of practical importance in scenarios in which the beneﬁts\nof a small minimax regret are severely offset by the cost of computing the minimax strategy for each round of the game. For example, as argued in [1], in an energy-constrained environment in which the role of data compression is to save storage or transmission power, the assessment of the beneﬁt of data compression should take into account the implementation cost of the data compression algorithm. Sav- ings in this cost obtained by \u201cfreezing\u201d the adaptation of the algorithm within a block may thus be beneﬁcial despite the corresponding compression loss. Piecewise constant predictors are also used in prediction games where the losses depend on previous moves [3] and in limited-delay lossy universal data compression [4]. In both cases, switching predictions entails a cost, which is amortized by freezing the predictor over a block. Piecewise constant prediction algorithms which are not minimax are proposed in [5].\nThe binary prediction problem was ﬁrst studied in the framework of the sequential decision problem [6]. The min- imax strategy for Hamming loss was devised by Cover [7], whereas for data compression it is given by the Normal- ized Maximum-Likelihood (NML) code, due to Shtarkov [8]. Cover\u2019s minimax scheme yields the same regret over all sequences, its leading asymptotic term being n/(2π). For data compression, the leading asymptotic term of the redun- dancy of the NML code is (1/2) log n in the binary case. A horizon-independent, simpler to implement approximation of this minimax strategy, which achieves the leading asymptotic term of the minimax redundancy, is given by the Krichevskii- Troﬁmov (KT) probability assignment [9].\nA variant of this problem which, as we shall see, is quite related to the piecewise constant setting, was proposed in [2]. In this variant, the predictor has access to a delayed version of the sequence, or is forced to make inferences on the observations a number of instants in advance. The delay d, which is assumed known, affects the prediction strategy in that p t+1 is now based on x 1 x 2 · · · x t−d only. It is shown in [2] that, in the delayed prediction setting, the minimax strategy consists of sub-sampling the data at a 1/(d + 1) rate, and applying the (non-delayed) minimax strategy to each of the d + 1 sub-sampled sub-sequences. This strategy results in a minimax regret of d + 1 times the non-delayed minimax regret for sequences of length n/(d + 1).\nOne can consider more general prediction games, in which the sequence of observations belongs to some ﬁnite alphabet\nA, and the player causally assigns probability distributions to a corresponding sequence of actions b 1 b 2 · · · b n , taken from an action space B. The observation and action incur an expected loss. Notice that piecewise constant binary prediction with block length T and Hamming loss can be cast as an unconstrained game over a sequence of length n/T , in which the observation alphabet is A = {0, 1} T , the action space is B = {0 T , 1 T }, and the loss function is Hamming. Thus, a general minimax solution for the unconstrained game would solve our problem. Unfortunately, minimax strategies for the general game cannot be characterized as easily as Cover\u2019s scheme for the binary case with Hamming loss [10].\nIn this paper, we start by characterizing, in Section II, the piecewise constant minimax strategy for Hamming loss, under the assumption that n = mT for a given block length T . The corresponding regret turns out to be T times the minimax regret for horizon m. The fact that this value is a lower bound follows easily from considering piecewise constant sequences x n . To show that it is an upper bound requires recursively deriving the minimax strategy. This is in contrast to the delayed prediction setting for which the optimal strategy is easily analyzed while the converse proof is more involved. Notice that, since the minimax regret grows as the square root of the horizon, the asymptotic penalization due to the piecewise constant constraint is a multiplicative factor\nIn Section III we study the piecewise constant binary data compression problem. While the argument leading to a lower bound extends to this case, yielding a multiplicative factor T as asymptotic penalization, we are unable to give a complete characterization of the minimax strategy. However, we prove that a simple variant of the KT probability assignment [9], in which the estimate is obtained by adding T /2 (instead of 1/2 as in the usual KT estimate) to the counts of 0\u2019s and 1\u2019s, is asymptotically piecewise constant minimax. Thus, the main asymptotic term of the piecewise constant minimax redundancy takes the form (T /2) log n. Again, the analysis of the upper bound is signiﬁcantly more involved than in the delayed data compression setting.\nIn Section IV we study the piecewise constant data com- pression problem in a probabilistic setting, in which the observations are assumed to be drawn from an (unknown) i.i.d. distribution, where again, for simplicity, we assume binary sequences. The goal here is to minimize the average (rather than the maximum) redundancy, for which lower and upper bounds with main term (1/2) log n are well known for the unconstrained case (i.e., respectively, Rissanen\u2019s lower bound [11], which holds for all distributions except for a set of measure zero, and the KT probability assignment, which is asymptotically optimal for every distribution). The question then is: Does the piecewise constant constraint affect the achievable average redundancy? Notice that the answer to the counterpart question in the delayed data compression setting is straightforwardly negative. Indeed, for i.i.d. distributions, the expected loss incurred at time t for a delayed strategy (with delay d) is the same as the expected loss that the predictor would incur, without delay, at time t − d. Therefore, ignoring\nthe delay and assigning at time t the same probability that a non-delayed compression strategy would have assigned at time t − d, we incur for the sequence x n d+1 the same loss as, without delay, for the sequence x n−d .\nThe conclusion in the piecewise constant scenario turns out to be similar, but the analysis is far less straightforward. By a nonstandard application of Rissanen\u2019s lower bound, we show that applying any asymptotically optimal (unconstrained) strategy at times (i−1)T +1 and \u201cfreezing\u201d the assigned probability for the entire i-th block, the average redundancy achieves Rissanen\u2019s lower bound for all distributions, except possibly for a set of vanishing volume. In fact, for such a general result, an exception set is unavoidable: indeed, we construct an asymptotically optimal (unconstrained) strategy whose piecewise constant version fails to yield optimal average redundancy for some distributions. However, we further show that if we specialize this approach to the (asymptotically optimal) KT probability assignment, then the frozen scheme is asymptotically optimal for all distributions, with no excep- tions.\nThroughout the sequel, for ﬁxed positive integers T (block size) and m (number of blocks), we shall formally deﬁne a piecewise constant probability assignment ˆ p with block size T and horizon n = mT as a probability distribution on binary sequences x n whose conditional probabilities satisfy ˆ p t+1 (·|x t ) = ˆ p T t/T +1 (·|x T t/T ) for all t. Thus, the con- ditional probabilities are constant over blocks of size T , and, therefore, such an assignment is completely determined by the conditional probabilities ˆ p iT +1 (·|x iT ), for i = 0, . . . , m − 1. The set of all piecewise constant probabilities with block size T and horizon n(= mT ) shall be denoted by P T ,n .\nIn this section, we study piecewise constant minimax predic- tion of binary sequences under Hamming loss. The (expected) Hamming loss of a piecewise constant predictor corresponding to a ˆ p ∈ P T ,m T on a sequence x m T ∈ {0, 1} m T is given by L(ˆ p, x m T ) = m T t=1 (1 − ˆ p t (x t |x t−1 )). Fixing throughout the horizon n = mT , let B( ) \t min( , n − ) denote the Bayes envelope for a sequence of length n containing\nones. For a given sequence x kT , 0 ≤ k ≤ m and subset Y ⊆ {0, 1} (m−k)T , deﬁne\nwhere x kT y denotes the concatenation of the sequences x kT and y. We are interested in the cases R k,T (x kT ) \t R k,T (x kT , {0, 1} (m−k)T ) and R k,T (x kT )\nR k,T (x kT , {0 T , 1 T } m−k ). Notice that the difference between R k,T and R k,T is in the inner maximization: in the latter, y is constrained to be piecewise constant, while in the former it is unconstrained. The domains of R k,T and R k,T are both unconstrained binary sequences of length kT . Notice also that R k,T and R k,T depend on x kT only through its appearance in the Bayes envelope of the concatenation of\nx kT and y (thereby affecting the maximizing sequence y and the minimizing strategy ˆ p). In particular, for k > 0, the cumulative loss and the Bayes envelope in the deﬁnition of R k,T correspond to sequences of different lengths.\nTheorem 1. For all k, 0 ≤ k ≤ m, and x kT ∈ {0, 1} kT , R k,T (x kT ) = R k,T (x kT ).\nBy deﬁnition, R 0,T is the minimax regret for piecewise constant strategies (the object of the study in this section), denoted R T (mT ), whereas R 0,T is T times Cover\u2019s minimax regret R 1 (m) for unconstrained strategies with horizon m. The main result of this section, Corollary 1, follows.\nCorollary 1. The minimax regret R T (mT ) for the piecewise constant prediction problem with Hamming loss, block length T , and horizon mT , satisﬁes R T (mT ) = T R 1 (m).\nThe fact that R T (mT ) ≥ T R 1 (m) is straightforward for more general prediction settings, including data compression. Indeed, the loss of a piecewise constant strategy over a piecewise constant sequence equals T times the loss of a cor- responding (unconstrained) strategy on sequences of length m, obtained by sub-sampling the original sequences. By deﬁnition of minimax regret, there exists a sequence x m for which the latter loss is at least R 1 (m) plus the Bayes envelope of x m . The desired inequality then follows by noticing that a T -fold replication of x m increases its Bayes envelope by a factor of T . As claimed in Corollary 1, in the case of binary prediction with Hamming loss, the above lower bound is actually tight.\nProof of Theorem 1: The proof will proceed by backward induction, with base case k = m. Each induction step will also include a proof of the following auxiliary claims: R k,T (x kT ) depends on x kT only through N = n 1 (x kT ), is a convex function of (integer) N , and for all nonnegative integers N , with a slight abuse of notation, |R k,T (N +1)−R k,T (N )| ≤ 1.\nFor the base case, from the above deﬁnitions, it is immediate that R m,T (x mT ) = R m,T (x mT ) = −B(n 1 (x mT )), and from the deﬁnition of B, it is immediate that the claimed (auxiliary) properties of R m,T hold. For the induction step, we have\nNotice that a causal strategy ˆ p∈P T ,(m−k+1)T can be viewed as a ﬁxed predictor ˆ p 1 = p for the ﬁrst block z, which incurs a loss p(T −n 1 (z))+pn 1 (z) (where p=1−p), followed by a causal strategy ˆ p∈P T ,(m−k)T , the choice of which can depend on z, for the remaining blocks y, incurring a loss L(ˆ p, y). Clearly, this dependency can be expressed by switching the maximum on z with the minimum on ˆ p∈P T ,(m−k)T , yielding\nwhere the last equality follows by the induction hypothesis that R k,T = R k,T . By the induction hypothesis that R k,T depends on x (k−1)T z through n 1 (x (k−1)T )+n 1 (z), we further have\nwhere (3) follows by the convexity (in n 1 (z)) of the expression in the maximization in (2) (implying that the maximum occurs at one of the extremes), which, in turn, follows from the auxiliary induction hypothesis that R k,T (N ) is convex in N . Now, starting from R k−1,T (x (k−1)T ), the same chain of equalities leading to (1) can be followed, with the maximiza- tion constrained to piecewise constant sequences. Clearly, the result would be the expression in the right-hand side of (3). Therefore, by (3), R k−1,T (x (k−1)T )=R k−1,T (x (k−1)T ), es- tablishing the induction step for the main claim, as well as the fact that R k−1,T depends on x (k−1)T through n 1 (x (k−1)T ).\nTo prove the remaining auxiliary induction steps, we note that (3) can be solved (by equating the two terms in the maximum) to yield that the minimizing p takes the form\nwhich satisﬁes p ∗ ∈[0, 1] by the Lipschitz condition induction hypothesis that |R k,T (N +1)−R k,T (N )| ≤ 1. Substituting (4) into (3) yields\n2 \t (5) from which the convexity and Lipschitz condition induction steps both readily follow from the respective induction hy- potheses.\nBy deﬁnition, the minimax piecewise constant probability assignment ˆ p kT +1 (·|x kT ) for Hamming loss is the ﬁrst move in the prediction strategy that achieves R k,T (x kT ), given by\np ∗ in (4). The recursion (5) can be explicitly solved and, substituting into (4), we obtain\nwhere Y={0 T , 1 T } n−k−1 . For T = 1, this expression is the probability that x kT y contains a majority of 1\u2019s when y is drawn uniformly from Y, with ties broken randomly (see [7]). For general T , however, the same interpretation holds only when the number of 1\u2019s in x kT is divisible by T .\nIn this section, we consider the piecewise constant minimax binary data compression problem. For simplicity, we maintain the assumption that n=mT , although the results hold in more generality. As observed in Section II, the minimax piecewise constant redundancy, deﬁned as\nwhere ˆ H(x n ) denotes the (normalized) empirical entropy of x n and all logarithms are in base 2, satisﬁes\nAs discussed in Section I, ˜ R 1 (n), attained by the NML code [8] for each n, satisﬁes the asymptotics\n˜ R 1 (n) = 1 2\nwhere the leading term in (8) is achieved by the KT probability assignment [9]. The main result of this section is given by the following theorem.\nTheorem 2. The minimax piecewise constant redundancy satisﬁes ˜ R T (n) = T 2 log n + O(1), 1 the right-hand side of which is attained by the piecewise constant probability assignment speciﬁed by\nProof: In view of the lower bound (7) and the deﬁnition of minimax redundancy (6), it sufﬁces to show that, for every sequence x mT , we have\nTo this end, we show that, for any x mT , with n 1 (x mT ) = a and n 0 (x mT ) = b = mT − a,\nWe proceed by induction on m, in the spirit of [12, Appendix II]. For m = 1, ˆ q(x T ) = 2 −T ≥ 1 2 T − T 2 for all T ≥ 1. Next,\nassume that (11) holds for m. Let n 1 (x (m+1)T ) = a + c and n 0 (x (m+1)T ) = b + d, with c + d = T, 0 ≤ c, d ≤ T . By the probability assignment in (9),\n(12) where the equality follows by a simple reordering of the terms. Now, let G(m) \t 1 + 1 m m+ 1 2 . It can be shown [12] that G is strictly decreasing on (0, ∞), so that G(m) ≥ G(∞) = e. Also, let F (a, c) \t a+ T 2 a+c c a a+c a , which can be shown to be a decreasing function of a on (0, ∞) (we omit the proof due to space limitations). Therefore, F (a, c) ≥ F (∞, c) = e −c and, similarly, F (b, d) ≥ e −d . It then follows from (12) that\nIn this section, we consider the piecewise constant univer- sal compression problem from an average redundancy point of view. Recall that, given a compressor expressed as a probability assignment ˆ p(x n ) on sequences of length n, the average case redundancy with respect to an i.i.d. source with marginal p is deﬁned as ˜ R avg (ˆ p, p) = D(p ⊗n ||ˆ p), where p ⊗n denotes the i.i.d. distribution on {0, 1} n with marginal p. As discussed in Section I, it was observed in [2] that the difference between the average code length of any compres- sion scheme and its delayed counterpart is O(1). Thus, the average redundancy of the delayed version of any compression scheme with average redundancy meeting Rissanen\u2019s lower bound of (1/2) log n+o(log n) (for binary sources), also meets Rissanen\u2019s lower bound. Theorem 3 below is a counterpart of this result for the piecewise constant setting.\nGiven any probability assignment ˆ p with ˜ R avg (ˆ p, p) = (1/2) log n + o(log n) for all p, and block length T , let ˆ p (T )\ndenote the T -piecewise constant version of ˆ p, deﬁned via the conditional probability assignments\nThus, ˆ p (T ) t+1 (·|x t ) is ﬁxed throughout each block of indices iT, iT +1, . . . , iT +T −1, with i being a nonnegative integer, to be the conditional probability that ˆ p would have induced at the start of the block.\nTheorem 3. Given any probability assignment ˆ p with max p ˜ R avg (ˆ p, p) = (1/2) log n + o(log n), the T -piecewise constant version ˆ p (T ) satisﬁes ˜ R avg (ˆ p (T ) , p) = (1/2) log n+ o(log n) for Lebesgue almost all p.\nProof: In analogy to ˆ p (T ) , for δ = 0, 1, . . . , T − 1, we can deﬁne the (T, δ)-piecewise constant versions of ˆ p as\nwith ˆ p t (·|·) = 1/2 for t ≤ δ. Thus, ˆ p (T ,δ) t+1 (·|x t ) is ﬁxed over blocks as above, but the blocks are offset by δ with respect to those deﬁning ˆ p (T ) . Note that ˆ p (T ,0) coincides with ˆ p (T ) .\nThe i.i.d. nature of the source and regrouping conditional probabilities yields 1 T T −1 δ=0 ˜ R avg (ˆ p (T ,δ) , p) = ˜ R avg (ˆ p, p) + O(1), which, in turn, implies that, if ˜ R avg (ˆ p, p) = (1/2) log n+o(log n) and, for some >0 and inﬁnitely many values of n, ˜ R avg (ˆ p (T ) , p)>(1/2) log n+ log n, then\n˜ R avg (ˆ p (T ,δ) , p) < (1/2) log n − ( /(2T )) log n, for some δ and inﬁnitely many values of n. However, by Rissanen\u2019s lower bound [11], this can only happen for p in a set of measure zero. The theorem then follows since δ is ﬁnite-valued.\nThe following example (for T = 2) shows that the measure zero exception set in Theorem 3 can arise, with the set being {0, 1} in this case. Deﬁne ˆ p as follows. For odd t, let\nt+1 \t (13) whereas, for even t, let\nwhere 1(x t−1 = 1) (resp. 1(x t−1 = 0)) is 1 (resp. 0) if x t−1 is all 1\u2019s (resp. 0\u2019s) and 0 otherwise. Notice that (13) corresponds to Laplace\u2019s estimator while (14) corresponds to the conditional probability distribution induced by an equal mixture of the Laplace probability assignment (corresponding to the Dirichlet-1 mixture of product distributions) and the respective deterministic distributions of all 1\u2019s and all 0\u2019s.\nFor p = 0, 1, the above ˆ p coincides with the Laplace estimator after the ﬁrst occurrence of a bit differing from x 1 . The probability of this event taking i symbols to occur falls off exponentially in i, and the code length difference between both estimators is O(log i) in the worst case. Therefore, the\nexcess redundancy over the Laplace estimator is O(1). Since the latter is well known to attain an average redundancy of (1/2) log n+o(log n), it follows that ˆ p does so as well (for p = 0, 1). For p = 0, 1, on the other hand, a simple calcu- lation shows that the redundancy arising from odd samples is (1/2) log n+o(log n), while the redundancy arising from even samples is o(log n). Thus, ˆ p meets the conditions of Theorem 3. As for the corresponding ˆ p (2) , it corresponds to using the Laplace estimator conditional probabilities (13) over each block of size 2 and again a simple calculation shows that the average redundancy is log n+o(log n), for p = 0, 1. This fact is inherently due to the known analogous redundancy for the underlying Laplace estimator for these deterministic cases.\nOur ﬁnal result for piecewise universal compression under average redundancy concerns the T -piecewise constant version of the KT probability assignment, denoted by ˆ p KT .\nThus, in the context of the previous theorem, the T - piecewise constant version of the KT probability assignment turns out to attain Rissanen\u2019s lower bound for all p. Proposi- tion 1 can be proved by bounding the difference between cer- tain conditional expectations of the logarithms of the piecewise constant and original KT probability assignments."},"refs":[{"authors":[{"name":"Y. Wu"},{"name":"E. Ordentlich"},{"name":"M. J. Weinberger"}],"title":{"text":"Energy-Optimized Loss- less Compression: Rate-Variability Tradeoff"}},{"authors":[{"name":"M. J. Weinberger"},{"name":"E. Ordentlich"}],"title":{"text":"On Delayed Prediction of Individ- ual Sequences"}},{"authors":[{"name":"N. Merhav"},{"name":"E. Ordentlich"},{"name":"G. Seroussi"},{"name":"M. J. Weinberger"}],"title":{"text":"On Sequential Strategies for Loss Functions with Memory"}},{"authors":[{"name":"N. Merhav"},{"name":"T. Weissman"}],"title":{"text":"On Limited-Delay Lossy Coding and Filtering of Individual Sequences"}},{"authors":[{"name":"S. Geulen"},{"name":"B. Voecking"},{"name":"M. Winkler"}],"title":{"text":"Regret Minimization for Online Buffering Problems Using the Weighted Majority Algorithm"}},{"authors":[{"name":"J. F. Hannan"}],"title":{"text":"Approximation to Bayes risk in repeated plays"}},{"authors":[{"name":"T. M. Cover"}],"title":{"text":"Behavior of sequential predictors of binary sequences"}},{"authors":[{"name":"Y. M. Shtarkov"}],"title":{"text":"Universal sequential coding of single messages"}},{"authors":[{"name":"R. E. Krichevskii"},{"name":"V. K. Troﬁmov"}],"title":{"text":"The performance of universal encoding"}},{"authors":[{"name":"T. H. Chun"}],"title":{"text":"Minimax Learning in Iterated Games via Distributional Majorization "}},{"authors":[{"name":"J. Rissanen"}],"title":{"text":"Universal coding, information, prediction, and estimation"}},{"authors":[{"name":"F. M. J. Willems"},{"name":"Y. M. Shtarkov"},{"name":"T. J. Tjalkens"}],"title":{"text":"The context- tree weighting method: Basic properties"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566579.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T8.2","endtime":"10:30","authors":"Erik  Ordentlich, Marcelo Weinberger, Yihong Wu","date":"1341310200000","papertitle":"Piecewise Constant Prediction","starttime":"10:10","session":"S5.T8: Prediction and Estimation","room":"Stratton (491)","paperid":"1569566579"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
