{"id":"1569565219","paper":{"title":{"text":"On Real\u2013Time and Causal Secure Source Coding"},"authors":[{"name":"Yonatan Kaspi"},{"name":"Neri Merhav \u2020"}],"abstr":{"text":"Abstract\u2014We investigate two source coding problems with secrecy constraints. In the ﬁrst problem we consider real\u2013time fully secure transmission of a memoryless source. We show that although classical variable\u2013rate coding is not an option since the lengths of the codewords leak information on the source, the key rate can be as low as the average Huffman codeword length of the source. In the second problem we consider causal source coding with a ﬁdelity criterion and side information at the decoder and the eavesdropper. We show that when the eavesdropper has degraded side information, it is optimal to ﬁrst use a causal rate distortion code and then encrypt its output with a key."},"body":{"text":"We consider two source coding scenarios in which an encoder, referred to as Alice, transmits outcomes of a memory- less source to a decoder, referred to as Bob. The comunnication between Alice and Bob is intercepted by an eavesdropper, referred to as Eve.\nIn the ﬁrst scenario, we consider real\u2013time communication between Alice and Bob and require full secrecy, meaning that the intercepted transmission does not leak any information about the source. In the second scenario, we consider lossy causal source coding when both Bob and Eve have access to side information (SI). We require that Eve\u2019s uncertainty about the source given the intercepted signal and SI will be higher than a certain threshold.\nReal\u2013time codes are a subclass of causal codes, as deﬁned by Neuhoff and Gilbert [1]. In [1], entropy coding is used on the whole sequence of reproduction symbols, introducing arbitrarily long delays. In the real\u2013time case, entropy coding has to be instantaneous, symbol\u2013by\u2013symbol (possibly taking into account past transmitted symbols). It was shown in [1], that for a discrete memoryless source (DMS), the optimal causal encoder consists of time\u2013sharing between no more than two memoryless encoders. Weissman and Merhav [2] extended [1] by including SI at the decoder, encoder or both.\nShannon [3] introduced the information-theoretic notion of secrecy, where security is measured through the remain- ing uncertainty about the message at the eavesdropper. This information-theoretic approach of secrecy allows to consider security issues at the physical layer, and ensures uncondition- ally (regardless of the eavesdroppers computing power and time) secure schemes, since it only relies on the statistical properties of the system. Wyner introduced the wiretap channel\nin [4] and showed that it is possible to send information at a positive rate with perfect secrecy as long as Eve\u2019s channel is a degraded version of the channel to Bob. When the channels are clean, two approaches can be found in the literature of secure communication. The ﬁrst assumes that both Alice and Bob agree on a secret key prior to the transmission of the source (through a separate secure channel for example). The second approach assumes that Bob and Eve (and possibly Alice) have different versions of side information and secrecy is achieved through this difference.\nFor the case of shared secret key, Shannon showed that in order for the transmission of a DMS to be fully secure, the rate of the key must be at least as large as the entropy of the source. Yamamoto ([5] and references therein) studied various secure source coding scenarios that include extension of Shannon\u2019s result to combine secrecy with rate\u2013distortion theory. In both [3],[5], when no SI is available, it was shown that separation is optimal. Namely, using a source code followed by encryption with the shared key is optimal. The other approach was treated more recently by Prabhakaran and Ramchandran [6] who considered lossless source coding with SI at both Bob and Eve when there is no rate constraint between Alice and Bob. It was shown that the Slepian-Wolf [7] scheme is not necessarily optimal when the SI structure is not degraded. Coded SI at Bob and SI at Alice where considered in [8]. These works were extended by Villard and Piantanida [9] to the case where distortion is allowed and coded SI is available to Bob. Merhav combined the two approaches with the wire\u2013 tap channel [10]. Note that we mentioned only a small sample of the vast literature on this subject.\nIn the works mentioned above, there were no constraints on the delay and/or causality of the system. As a result, the coding theorems of the above works introduced arbitrary long delay and exponential complexity.\nThe practical need for fast and efﬁcient encryption algo- rithms for military and commercial applications along with theoretical advances of the cryptology community, led to the development of efﬁcient encryption algorithms and standards which rely on relatively short keys. However, the security of these algorithms depend on computational complexity and the intractability assumption of some hard problems. To the best of our knowledge, there was no attempt so far to analyze the performance of a real\u2013time or causal secrecy system from an information theoretic point of view.\nreal\u2013time case is straightforward and is done by replacing the block entropy coding by instantaneous Huffman coding. The resulting bitstream between the encoder and decoder is composed of the Huffman codewords. However, this cannot be done when secrecy is involved, even if only lossless compression is considered. To see why, consider the case where Eve intercepts a Huffman codeword and further assume the bits of the codeword are encrypted with a one\u2013time pad. While the intercepted bits give no information on the encoded symbol (since they are independent of it after the encryption), the number of intercepted bits leaks information on the source symbol. For example, if the codeword is short, Eve knows that the encrypted symbol is one with a high probability (remember that Eve knows the source statistics). This suggests that in order to achieve full security, the lengths of the codewords emitted by the encoder should be independent of the source.\nIn the last example, we assumed that Eve is informed on how to parse the bitstream into separate codewords. This will be the case, for example, when each codeword is transmitted as a packet over a network and the packets are intercepted by Eve. Even if the bits are meaningless to Eve, she still knows the number of bits in each packet. We show in the sequel that, albeit the above example, the key rate can be as low as the average Huffman codeword length (referred hereafter as the Huffman length) of the source. Full secrecy, in this case, will be achieved by randomization at the encoder, which can be removed by Bob. In contrast to the works mentioned above, our results here are not asymptotic.\nWe also investigate the scenario where Eve doesn\u2019t have parsing information and cannot parse the bitstream into the separate codewords. This will be the case, for example if Eve acquires only the whole bitstream, not necessarily in real\u2013time, without the log of the network trafﬁc. Alternatively, it acquires an encrypted ﬁle after it was saved to the disk. In this case, when we assume that the length of transmission is inﬁnite, we show that that the best achievable rates of both the key and the transmission are given by the Huffman length of the source. In contrast to the results described in the previous paragraph, the results in this scenario are asymptotic in the sense that the probability that the system is not secure is zero when the transmission length is inﬁnite. Note that the length of the transmission was not an issue in the mentioned previous works since block coding was used. Therefore, the block length was known a-priori to Eve and leaked no information.\nIn the following two sections we deal with the real\u2013time and causal setting, receptively. Each section begins with a formal deﬁnition of the relevant problem.\nWe begin with notation conventions. Capital letters repre- sent scalar random variables (RV\u2019s), speciﬁc realizations of them are denoted by the corresponding lower case letters and their alphabets \u2013 by calligraphic letters. For i < j (i, j - positive integers), x j i will denote the vector (x i , . . . , x j ) , where for i = 1 the subscript will be omitted. For two random variables X, Y , with alphabets X , Y, respectively and joint\nprobability distribution {p(x, y)}, the average instantaneous codeword length of X conditioned on Y = y will be given by\n. \t (1) where A X is the set of all possible length functions l : X ! Z + that satisfy Kraft\u2019s inequality for alphabet of size |X |. L(X|Y = y) is obtained by designing a Huffman code for the probability distribution P (x|y). With the same abuse of notation common for entropy, we let L(X|Y ) denote the expectation of L(X|Y = y) with respect to the randomness of Y . The Huffman length of X is given by L(X).\nIn this section, the following real-time source coding prob- lem is considered: Alice, wishes to losslessly transmit the output of a DMS X with probability mass function P X (x) to Bob. The communication between Alice and Bob is inter- cepted by Eve. Alice and Bob operate without delay. When Alice observes X t she encodes it by an instantaneous code and transmits the codeword to Bob through a clean digital channel. Bob decodes the codeword and reproduces X t . A communication stage is deﬁned to start when the source emits X t and ends when Bob reproduces X t , i.e., Bob cannot use future transmissions to calculate X t . We will assume that both Alice and Bob have access to a completely random binary sequence, u = (u 1 , u 2 , . . .) , which is independent of the data and will be referred to as the key. Let m 1 , m 2 , . . . , m n , m i 2 be a non decreasing sequence of positive integers. At stage t, Alice uses l K t 4 = m t m t 1 bits that were not used so far from the key sequence. Let K t 4 = (u m t 1 +1 , . . . , u m t ) denote the stage t key. The parsing of the key sequence up to stage t should be the same at Alice and Bob. This can be done \u201con the ﬂy\u201d through the data already known to both Alice and Bob from the previous stages. We deﬁne the key rate to be R K = lim sup n !1 1 n P n t=1 El K t . We will also assume that Alice has access, at each stage, to a private source of randomness {V t }, which is i.i.d and independent of the source and the key. Neither Bob nor Eve have access to {V t }. Let Z be the set of all ﬁnite length binary strings. Denote Alice\u2019s output at stage t by Z t 2 Z and let B t denote the unparsed sequence, containing l B t bits, that were transmitted so far up to the end of stage t. The rate of the encoder is deﬁned by R 4 = lim sup n !1 1 n El B n .\nGiven the keys up to stage t, K t , Bob can parse B k into Z 1 , . . . , Z t for any k t . The legitimate decoder is thus a sequence functions X t = g t (K t , Z t ) .\nAs discussed in the Introduction, we will treat two security models. In the ﬁrst model we will assume that Eve can detect when each stage starts, i.e., it can parse B t into Z 1 , . . . , Z t . In the second model, we will assume that Eve intercepts the whole bitstream B n (assuming a total of n stages) but has no information on actual parsing of B n into Z 1 , . . . , Z n . These models are treated in the following two subsections.\nIn this subsection we assume that Eve can parse B n into Z 1 , Z 2 , . . . , Z n . In order for the system to be fully\nsecure, following [3], we will require that for any k, m, n, P (X k |Z n m ) = P (X k ) , i.e., acquiring any portion of the transmission leaks no information on the source, which was not known to Eve in advance.\nThe most general real\u2013time encoder is a sequence of func- tions Z t = f t (K t , V t , X t ) . In this paper, we will treat only a subclass of encoders that satisfy the Markov chain\nX t $ Z t $ K t 1 . \t (2) Namely, given the past and current encoder outputs, the current source symbol, X t , does not reduce the uncertainty regarding the past keys. We claim that this constraint, in the framework of complete security is relatively benign and, in fact, any encoder that calculates a codeword (possibly using the whole history of the source and keys, i.e., with the most general encoder structure), say ˆ Z t , and then outputs Z t = ˆ Z t K t will satisfy this constraint. Such a structure seems natural for one\u2013time pad encryption. Another example of encoders that will satisfy such a constraint are encoders with the structure Z t = f t (K t , V t , X t , Z t 1 ) (we omit the proof this structure will induce the Markov chain due to space limitations). The main result of this subsection is the following theorem:\nTheorem I. There exists a pair of fully secure real\u2013time encoder and decoder if and only if R K L(X) .\nThis theorem is in the spirit of the result of [3], where the entropy is replaced by the Huffman length due to the real- time constraint. As discussed in the introduction, variable\u2013rate coding is not an option when we want the communication to be fully secure. This means that the encoder should either output constant length (short) blocks or have the transmission length independent of the source symbol in some other way. Clearly, with constant length blocks, the rate of a lossless encoder cannot be as low as L(X) for all possible memoryless sources. The rate of the key, however, can be as low as L(X). In the proof of the direct part of Theorem I, we show that a constant rate encoder with block length corresponding to the longest Huffman codeword achieves this key rate. The padding is done by random bits from the encoder\u2019s private source of randomness. Note, however, that if both the key rate and encoder rate are log |X|, lossless fully secure communication is trivially possible. Although Theorem I does not give a lower bound on the rate of the encoder, the above discussion suggests that there is a trade-off between the key rate and the possible encoder rate that will allow secure lossless communication. Namely, there is a set of optimal rate pairs, (R, R K ) , which are possible. We prove Theorem I in the following subsections.\n1) Converse: For every lossless encoder\u2013decoder pair that satisﬁes the security constraint and (2), we lower bound the key rate as follows:\nThe ﬁrst equality is true since the key bits are incompressible and therefore the Huffman length is the same as the number of key bits. (3) is true since conditioning reduces the Huffman length (the simple proof of this is omitted). (4) follows since X t is a function of (K t , Z t ) (the decoder\u2019s function) and therefore, given (K t 1 , Z t ) , the code for K t also reveals X t . (5) is true since with the same conditioning on (K t 1 , Z t ) , the instantaneous code of (K t , X t ) cannot be shorter then the instantaneous code of X t . (6) is due to (2) and ﬁnally, (7) is true by the security model. We therefore showed that R K L(X) .\n2) Direct: We construct an encoder\u2013decoder pair that are fully secure with R K = L(X) . Let l max denote the longest Huffman codeword of X. We know that l max  |X| 1. The encoder output will always be l max bits long and will be built from two ﬁelds. The ﬁrst ﬁeld will be the Huffman codeword for the observed source symbol X t . Denote its length by l(X t ) . This codeword is then XORed with l(X t ) key bits. The second ﬁeld will be composed of l max l(X t ) random bits (taken from the private source of randomness) that will pad the encrypted Huffman codeword to be of length l max . Regardless of the speciﬁc source output, Eve sees constant length codewords composed of random uniform bits. Therefore no information about the source is leaked by the encoder outputs. When Bob receives such a block, it starts XORing it with key bits until it detects a valid Huffman codeword. The rest of the bits are ignored. Obviously, the key rate which is needed is L(X).\nIn this subsection, we relax our security assumptions and assume that Eve observes the whole transmission from Alice to Bob, but has no information on how to parse the bitstream B n into Z 1 , . . . , Z n . Although it is not customary to limit the eavesdropper in any way in information\u2013theoretic security, this limitation has a practical motivation, as discussed in the Introduction.\nWe will require that the following holds for every t and every x 2 X :\nP X (X t = x) a.s. \t (9) This means that when the bitstream is long enough, the eavesdropper does not learn from it anything about the source symbols. Note that the encoder from Section (II-A2) trivially satisﬁes this constraint since it was a constant block length\nencoder and the bits within the block where encrypted by a one\u2013time pad. We will see that with the relaxed secrecy requirement we can reduce the rate of the encoder to be the same as the rate of the key. In this section we deal with encoders that satisfy X t $ B n $ K t 1 . The discussion that followed the constraint (2) is valid here as well. We have the following theorem:\nTheorem II. There exists a lossless encoder\u2013decoder pair that satisﬁes the secrecy constraints (9) if and only if R L(X), R K L(X) .\nThe fact that R L(X) is trivial since we deal with a real time lossless encoder. However, unlike the case of Theorem I, here it can be achieved along with R K L(X) . The proof of the bound on R K follows the proof of the previous section up to (6) by replacing Z t by B n . We have: R K\nL(X t |B n ) . Now, since P (X t |B n ) ! P (X t ) a.s. we have that L(X t |B n ) ! L(X t ) a.s. . The direct part of the proof is achieved by separation. We ﬁrst encode X t using a Huffman code and then XOR the resulting bits with a one time pad. Therefore, both the encoder and key rate of this scheme are equal to L(X). We need to show that (9) holds. We outline the idea here. The bits of B n are independent of X t since we encrypted them with a one-time pad. Let l B n represent the number of bits in B n . Since B n is encrypted we have X t ! l B n ! B n . Therefore, we have that P (X t |B n ) = P (X t |B n , l B n ) = P (X t |l B n ) . From the law of large numbers, l B n ! nL(X) a.s. But if l B n = nL(X) then l B n leaks no information about X t (since this nL(X) is known a-priori to Eve). The full proof resembles the martingale proof of the strong law of large numbers and can be found in [11]. Discussion: Unlike Theorem I, Theorem II addresses the rate of the encoder as well as the rate of the key. The result here is asymptotic since only when the bitsream is long enough we have the independence of X t from B n . It can be shown that the probability that B n reveals information on X t vanishes exponentially fast with n. Note that if instead of deﬁning the security constraint as in (9), we would have required that for every n, t, P (X t |B n ) = P (X t ) then a counterpart of Theorem 1 will hold here. However, the encoder will, as in the direct part of Theorem I proof, work in constant rate.\nIn this section, we extend the work of [1],[2] to include secrecy constraints. We consider the following source model: Alice, Bob, and Eve observe sequences of random vari- ables X n , Y n , and W n respectively which take values over discrete alphabets X , Y, W, respectively. (X n , Y n , W n ) are distributed according to a joint distribution p(x n , y n , w n ) = Q\nP (x t )P (y t |x t )P (w t |y t ) , i.e., the triplets (X t , Y t , W t ) are created by a DMS with the structure X $ Y $ W . (Y n , W n ) are the SI sequences seen by Bob and Eve respec- tively. Unlike [6], [9], we will treat in this paper only the case of degraded SI. This model covers the scenarios where no SI is available or is available only to Bob as special cases. Both\nAlice and Bob have access to a shared secret key denoted by K , K 2 {0, 1, 2 . . . , M k } which is independent of the source.\nLet ˆ X be Bob\u2019s reproduction alphabet and let d : X ⇥ ˆ X ! [0, 1), d min = min x,ˆ x d(x, ˆ x) . Finally, let d(x n , ˆ x n ) =\nd(x t , ˆ x t ) . Alice encodes X n , using the key, K, and creates a bit sequence Z = Z 1 , Z 2 . . . which is transmitted through a clean channel to Bob. Bob uses (K, Y n , Z) to create an estimate sequence, ˆ X n , such that Ed(X n , ˆ X n )  D. We allow the decoder to fail and declare an error with a vanishing probability of error. Namely, for every > 0 there exists n large enough such that the probability of error is less than .\nWe assume that Eve intercepts the transmitted bits, Z. The security of the system is measured by the uncertainty of Eve with regard to the source sequence, measured by\nH(X n |W n , Z) . As in [1], we call the cascade of encoder and decoder a reproduction coder. We say that a reproduction function is causal relative to the source if\nˆ X t = f t (X 1 1 , K) = f t ( ˜ X 1 1 , K) if X t 1 = ˜ X t 1 (10) Note that we did not restrict the use of the key to be causal in any sense. Moreover, this deﬁnition does not rule out arbitrary delays and real\u2013time is not considered here. We will only treat the SI model covered in [2] where Y n in not used in the reproduction of ˆ X n but can be used for the compression of ˆ X n . More complicated models will be treated in [11]. A causal reproduction coder is characterized by a family of reproduction functions {f k } 1 k=1 , such that the reproduction ˆ X k of the kth source output X k is given by ˆ X k = f k (K, X k ) . If the decoder declares an error, we will have ˆ X k 6= f k (K, X k ) . The probability of this event is the probability of decoder error. The average distortion of an encoder-decoder pair with an induced reproduction coder {f k } is deﬁned by\n. \t (11) The encoder\u2019s rate is deﬁned by R = 1 n lim sup n !1 H(Z).\nLet R denote the set of positive quadruples (R, R K , D, h) such that for every ✏ > 0, > 0 and sufﬁciently large n, there exists an encoder and a decoder whose probability of error is less than , inducing a causal reproduction coder satisfying:\n(12) 1\n1 n\nn |W n , Z) \t h ✏ \t (15) Let r x |y (D) be the optimum performance theoretically attain- able function (OPTA) from [2] for the case where the SI is available only at the decoder. Namely,\nR k h H(X |W ) + r x |y (D). \t (17) If h H(X |W ) + r x |y (D)  0, no encryption is needed.\nIt is seen from the theorem, that separation holds in this case. The direct part of this proof is therefore straightforward: First, quantize the source within distortion D by the scheme given in [2]. As was shown in [2], this step requires time- sharing no more than two memoryless quantizers. Now use Slepian\u2013Wolf encoding to encode the resulting quantized symbols given the SI at Bob. Finally, use a one\u2013time pad of n(h H(X |W ) + r x |y (D)) bits on the block describing the bin number.\nWe now proceed to prove the converse part, starting with lower bounding the encoding rate. For any k, let ˜ X k = f k (K, X k ) . ˜ X k are equal to ˆ X k when there is no decoding error. Since the probability of decoder failure vanishes, we have from Fano\u2019s inequality ([12]) that for every ✏ > 0, there exists n large enough such that H( ˜ X n |K, Y n , Z)  n✏.\nFor n large enough and every encoder and decoder pair that induce a causal reproduction coder and satisfy (12)-(15) the following chain of inequalities hold:\nH(Z |K, Y n ) H(Z |K, ˜ X n , Y n ) = I( ˜ X n ; Z |K, Y n )\nwhere (18) follows from Fano\u2019s inequality. From here, using the independent of the key and the source and following the steps used in [2, Appendix, eq. A.11] we can show that R\nr x |y (D) . The key rate can be lower bounded as follows: nR K = H(K)\n= H(X n |W n , Z) H(X n |K, W n , Z) + H(K |X n , W n , Z) nh H(X n |K, W n , Z) + H(K |X n , W n , Z)\n(21) = H(Y n |W n ) H(Y n |X n , W n ) + H(X n |K, Y n , Z)\n(22) = I(X n ; Y n |W n ) + H(X n |K, Y n , ˜ X n , Z)\n nI(X; Y |W ) + H(X n |K, Y n , ˜ X n , Z) + n✏ \t (23)  n(H(X|W ) H(X |Y )) + H(X n |K, Y n , ˜ X n ) \t (24)\nwhere in (21) we used the degraded structure of the source. (22) is true since Z is a function of (K, X n ) and K is independent of the souce. (23) is true by Fano\u2019s inequality and the fact that ˜ X n is a function of (K, X n ) . Focusing on the last term of (24) we have\nH(X n |K, Y n , ˜ X n ) = H(X n |K, Y n ) I(X n ; ˜ X n |K, Y n ) = nH(X |Y ) I(X n ; ˜ X n |K, Y n )\n nH(X|Y ) r x |y (D) + ✏ \t (26) where (25) is true since ˆ X n is a function of K, X n through the reproduction coders and Fano\u2019s inequality. Finally the last line follows from (18). Combining (26) with (24) into (20) we showed that R K h H(X |W ) + r x |y (D) ."},"refs":[{"authors":[{"name":"D. Neuhoff"},{"name":"R. K. Gilbert"}],"title":{"text":"Causal source codes"}},{"authors":[{"name":"T. Weissman"},{"name":"N. Merhav"}],"title":{"text":"On causal source codes with side information"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"Communication theory of secrecy systems"}},{"authors":[{"name":"A. D. Wyner"}],"title":{"text":"The wire\u2013tap channel"}},{"authors":[{"name":"H. Yamamoto"}],"title":{"text":"Rate-distortion theory for the shannon cipher system"}},{"authors":[{"name":"V. Prabhakaran"},{"name":"K. Ramchandran"}],"title":{"text":"On secure distributed source coding"}},{"authors":[{"name":"D. Slepian"},{"name":"J. Wolf"}],"title":{"text":"Noiseless coding for correlated information sources"}},{"authors":[{"name":"D. Gunduz"},{"name":"E. Erkip"},{"name":"H. Poor"}],"title":{"text":"Secure lossless compression with side information"}},{"authors":[{"name":"J. Villard"},{"name":"P. Piantanida"}],"title":{"text":"Secure multiterminal source coding with side information at the eavesdropper"}},{"authors":[{"name":"N. Merhav"}],"title":{"text":"Shannon\u2019s secrecy system with informed receivers and its application to systematic coding for wiretapped channels"}},{"authors":[{"name":"Y. Kaspi"},{"name":"N. Merhav"}],"title":{"text":"On secure real\u2013time and causal source coding"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, 2nd ed"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565219.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T1.1","endtime":"15:00","authors":"Yonatan Kaspi, Neri Merhav","date":"1341240000000","papertitle":"On Real-Time and Causal Secure Source Coding","starttime":"14:40","session":"S3.T1: Lossy Source Coding","room":"Kresge Rehearsal B (030)","paperid":"1569565219"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
