{"id":"1569564337","paper":{"title":{"text":"WarmL1: A Warm-start Homotopy-based Reconstruction Algorithm for Sparse Signals"},"authors":[{"name":"Tien-Ju Yang"},{"name":"Yi-Min Tsai"},{"name":"Chung-Te Li"},{"name":"Liang-Gee Chen"}],"abstr":{"text":"Abstract\u2014A sparse signal can be reconstructed from a small amount of random and linear measurements by solving a system of underdetermined equations. In this paper, we study the reconstruction problem while the system undergoes dy- namic modiﬁcations. Resolving this problem from scratch re- quires high computational efforts. Therefore, we propose an efﬁcient homotopy-based reconstruction algorithm with warm- start, named WarmL1. WarmL1 quickly updates the previous solution to the desired one. Based on the concept of homotopy, WarmL1 breaks the reconstruction procedure into simple steps, and solves the problem iteratively. Four possible applications are presented and discussed to demonstrate the usage of WarmL1 for different warm-start situations. Experiments on these appli- cations are performed. The results show that WarmL1 achieves 3.2 × to 37.5× speeding up or up to 1/5100 l2-error at the same computational cost compared to related works."},"body":{"text":"The sparsity is the nature of natural signals. In recent years, compressed sensing (CS) [1], [2] makes progress in the signal processing ﬁeld. CS has been extended to many applications, such as image denoising, MRI imaging and sensor network. It claims that a signal can be reconstructed from only few measurements even below Nyquist-Shannon sampling rate if the signal is sparse. The measurements are generated by the equation\nwhere y ∈ R M ×1 is the measurement vector, ˘ x ∈ R N ×1 is a signal. A ∈ R M ×N is the sensing matrix, which can be an incoherent random matrix. Because N is larger than M, the signal reconstruction process includes solving a system of underdetermined equations. The CS theory states that if the signal is K-sparse and M ≥ cKlog(N/K) for a small constant c, it can be reconstructed by using the l1-norm minimization:\n1 2\nwhere x ∈ R N ×1 is a dummy variable, and λ > 0 is a regularization parameter. (2) is also known as LASSO or BPDN equation. In addition to CS, (2) is also the kernel of sparse representation. Sparse representation models a signal as a sparse linear combination of entries in a dictionary and has been proved to achieve state-of-the-art performance in the computer vision ﬁeld [3], [4].\nIn the literature on sparse signal reconstruction, there have been many methods proposed to solve (2). However, most\nof these algorithms assume there is no relationship between signals or systems, and reconstruct each signal independently. This induces high computational efforts for signal reconstruc- tion in real applications. In many situations, signals or systems do have relationship, such as frames in a video, nearby blocks in an image or reconstruction with sequential measurements. Using the previous reconstructed signal as a starting point can increase the performance signiﬁcantly.\nAssuming we have reconstructed a signal by (2), we would like to reconstruct the next signal through solving the new problem\n1 2\nwhere ˜ y ∈ R ˜ M ×1 and ˜ A ∈ R ˜ M ×N are the new measurement vector and the corresponding sensing matrix, respectively. The dimension of ˜ y and ˜ A may be different from that of y and A. If the solutions of these two problems ((2) and (3)) are similar, the concept of warm-start can be applied to speed up the reconstruction without sacriﬁcing accuracy. Several warm- start methods are presented in [5]\u2013[8], and the concept of warm-start has been proved to be effective and efﬁcient.\nIn this paper, we propose a warm-start algorithm for quickly reconstructing a sparse signal, and call it WarmL1. We present a novel formulation and adopt the homotopy continuation prin- ciple for warm-start sparse signal reconstruction. Moreover, we show that WarmL1 is the generalized and more efﬁcient algorithm against previous methods. The key contributions of this work are as follows. We propose a single algorithm, WarmL1, to unify many existent homotopy-based warm-start methods [5]\u2013[7]. By adopting some special ˜ A and ˜ y, WarmL1 is simpliﬁed to these algorithms (see Sec. III-A and Sec. III-C). Moreover, WarmL1 also handles other update problems that can not be solved by these algorithms, such as warm-start reconstruction with sensing matrix modiﬁcation (see Sec. III-B and Sec. III-D).\nThe remainder of this paper is organized as follows: Sec. II presents the formulation to this warm-start problem and the proposed algorithm. We discuss possible applications and the usage of WarmL1 in Sec. III. Experimental results and discussions are shown in Sec. IV. Sec. V concludes this paper.\nIn this section, we propose and derive the warm-start signal reconstruction algorithm, WarmL1, which is based on the\nhomotopy continuation principle. The homotopy continuation principle provides a general framework for transforming one formulation into another. Homotopy methods trace a solution path, characterized by homotopy parameters, along with the formulation transformation. The solution path is followed with optimality constraints satisﬁed. In the transforming process, these homotopy parameters evolve from initial values to given values. After given homotopy parameters are achieved, the formulation transformation terminates and the desired solution is solved.\nIn the following, we propose a formulation for connecting (2) and our objective (3), and ﬁnd out its optimality con- straints. The optimality constraints show that the solution path is piecewise linear. The equations of the direction and the step size to the nearest breakpoint are then derived. Finally, we show the scheme for updating the homotopy parameter, the solution and the support (a set formed from indices of nonzero elements) of the solution.\nTo link the two equations (2) and (3), we introduce a ho- motopy parameter µ ∈ R and give the following formulation:\nwhere x 0 is the solution of (2). When µ equals to 0, the for- mulation is just the equation (2). While µ is 1, this formulation becomes the equation (3). Therefore, with the same λ as (2), we vary µ from 0 to 1, and the formulation transforms from (2) to (3). It is worth to note that the objective function of the formulation (4) is convex for any µ ∈ R.\nProof: Because the l1-norm is convex and λ is always positive, we only need to prove the convexity of the l2-norm part. Let\nSince ˜ A T ˜ A is positive semideﬁnite, h(x) is convex. Hence, the objective function of the formulation (4) is convex for every µ ∈ R.\nTherefore, from the idea of the classical convex opti- mization, ∃i, ∂f(x)/∂x i = 0 is the necessary condition for getting the extreme value of a generic convex function f (x). Hence, the constraints of x and µ can be derived through differentiating the objective function of (4) by x and setting it to zero, and is\n(1 − µ)A T S (Ax 0 − y) = −λ sgn(x S ), \t (9) ∥ ˜ A T S C ( ˜ Ax − (1 − µ) ˜ Ax 0 − µ˜y) +\nwhere S is the support of x, S c is the complement of S, ˜ A S ∈ R M ×|S| is formed with columns of A corresponding\nto S, x S ∈ R |S|×1 is a vector composed of the elements in x corresponding to S, and |·| indicates the size of a set. Therefore, the solution path is piecewise linear when µ changes, and this is the key to the success of the following steps.\nIn this step, we calculate the direction and the step size of x to the nearest breakpoint. Denote the temporary solution x and the homotopy parameter µ in the ℓ th iteration as x ℓ and µ ℓ , respectively. Before reaching a breakpoint, x ℓ moves to any solution x \u2032 through x \u2032 = x ℓ + γ ∗ ∂x ℓ /∂µ, where ∂x ℓ /∂µ is the direction and γ ∗ is a step size.The direction can be solved through differentiating (9) by µ, and is\nThe breakpoint is caused by two conditions. The ﬁrst scenario leading to a breakpoint is when (10) is activated, i.e. an element needed to be added to the support. Let\n(12) Assuming that (10) is activated when µ ℓ moves to µ \u2032 ℓ or µ \u2032\u2032 ℓ , we have equations (13) and (14).\n(1 − µ \u2032 ℓ )A T S C (Ax 0 − y)} elem , \t (13) −λ = min{ ˜ A T S C ( ˜ Ax \u2032\u2032 ℓ − (1 − µ \u2032\u2032 ℓ ) ˜ Ax 0 − µ \u2032\u2032 ℓ ˜ y) +\nwhere x \u2032 ℓ and x \u2032\u2032 ℓ are the solutions when the homo- topy parameter equals to µ \u2032 ℓ or µ \u2032\u2032 ℓ respectively, and max {·} elem /min {·} elem indicates a maximum/minimum oper- ator to select the maximal/minimal element. The two possible step sizes are derived with (12), (13) and (14), and are\nwhere min {·} + indicates a minimum operator which is only taken over the positive elements, m(j) is deﬁned as the j th element in a generic vector m, and\nThe second scenario causing a breakpoint is an element of x ℓ becoming 0, and the step size for this condition is\nFinally, after calculating both the direction ∂x ℓ /∂µ and the maximum step size γ ℓ , we can update the homotopy parameter µ ℓ and the solution x ℓ by\nTo enable the next iteration of WarmL1, the support should be updated, too. How to update the support depends on which step size is chosen. If γ − ℓ is chosen, remove the element changing to 0 from the support. If γ + ℓ is used, add the element causing (10) activated to the support. Moreover, WarmL1 terminates when the homotopy parameter µ reaches 1.\nThe algorithm ﬂow of WarmL1 is presented in Algorithm 1. One of the most computationally intensive parts is computing ( ˜ A T S ˜ A S ) −1 (for ∂x ℓ /∂µ). In practice, it is more efﬁcient to compute ∂x/∂µ by solving the following linear system with Cholesky decomposition:\nBecause we only change one element of the support at a time, the Cholesky decomposition needs not be recalculated in each iteration, and can be updated by rank-1 methods [9].\nIn this section, we present four applications suitable for adopting WarmL1 and demonstrate the usage of WarmL1 under various dynamic system modiﬁcations. Fig. 1 illustrates these applications and the notations used.\nThis application is regarding the scenario that the measure- ments are input sequentially. A signal is ﬁrst reconstructed from already available measurements, and then updated by taking the newly received measurements into account. Let A pre ∈ R M ×N and y pre ∈ R M ×1 be the sensing matrix and the measurement vector of the previous reconstructed signal. y seq ∈ R α ×1 and A seq ∈ R α ×N are the newly received measurement vector and its corresponding sensing matrix. We can apply WarmL1 in this application with the following steps: \u2022 Substitute A and y with A pre and y pre .\nSometimes it is required to reconstruct a signal from partial measurements when a reconstructed result from all measure- ments is known. For instance, a few nodes in the sensor network may receive bad measurements due to malfunctioning nodes, packet corruption, or attack. The reconstructed signal from all measurements is incorrect. In this situation, we adopt some detection methods (ex. [10]) to identify the bad measurements. After identifying them, we remove these bad measurements and reconstruct the correct signal. Firstly, we separate the measurements y pre and the sensing matrix A pre each into two sets. They are y rem ∈ R α ×1 and y lef ∈ R (M −α)×1 , denoting the measurements to be removed and the left measurements. A rem ∈ R α ×N and A lef ∈ R (M −α)×N are the corresponding sensing matrices. Secondly, WarmL1 is applied by:\n\u2022 Substitute A and y with A pre and y pre . \u2022 Substitute ˜ A and ˜ y with A lef and y lef .\nSeveral signals vary with time and change slowly, such as a video sequence. If such signal is sampled by a constant\nsensing matrix, we can start from the reconstructed signal in the previous time t to compute the next signal at time t + 1. Let A pre and y pre be the sensing matrix and the measurement vector of the reconstructed signal at time t. y nex ∈ R M ×1 is the measurement vector at time t + 1. We can adopt WarmL1 to perform efﬁcient reconstruction with the following steps:\n\u2022 Substitute A and y with A pre and y pre . \u2022 Substitute ˜ A and ˜ y with A pre and y nex .\nIn applications, such as dictionary learning [11] or de- convolutional networks [12], they repeatedly solve (2) with different sensing matrices and the same measurement vector. Asif et al. [7] has presented a method to handle this case. However, it only works at pretty restricted condition. Let A pre and y pre be the sensing matrix and the measurement vector of the reconstructed signal at the previous iteration. y nex and A nex ∈ R M ×N are the measurement vector and its corresponding sensing matrix at the next iteration, and y nex is the same as y pre . With WarmL1, the warm-start reconstruction is achieved by:\nMoreover, it is worth to note that WarmL1 can also handle the more challenging case both the sensing matrix and the mea- surement vector changed at the same time. In this situation, we simply use a y nex different from y pre .\nIn this section, we compare WarmL1 to two related algo- rithms in the four applications mentioned in Sec. III. These algorithms are the homopopy-based algorithm without warm- start, StaLasso [13], and the gradient-based algorithm with warm-start, GPSR BB [8]. All the algorithms are implemented in Matlab and run on a PC equipped with an Intel Xeon X5680 CPU. The experiments are taken in a manner with\nall parameters ﬁxed (controlled variables) except for one parameter (independent variable). The initial setting of the system is as follows. N is 16384, M is 1024 and λ is 1e-5. The original signal (x pre ∈ R N ×1 ) is generated by a Gaussian random generator with 1% nonzeros (1% sparsity) and then normalized to unity l2-norm. The sensing matrix (A pre ) is also generated by a Gaussian random generator with all columns having unity l2-norm. Moreover, the measurement vector y pre is given by y pre = A pre x pre . Fig. 2 shows an example of WarmL1 reconstruction.\nWe model all computations in all algorithms as four basic operations, which are addition, multiplication, inversion and division, to give a computational cost estimation in terms of ﬂop. In the ﬁrst two experiments (Sec. IV-A, IV-B), we compare their computational costs under the same l2-error. In the other two experiments (Sec. IV-C, IV-D), it takes considerable time for GPSR BB to achieve the same error as StaLasso and WarmL1. Therefore, we terminate the calculation of GPSR BB when the computational cost of GPSR BB is almost the same as that of WarmL1, and compare their reconstruction errors.\nIn this experiment, different amounts of new measurements are given (1, 5, 9, 13, 17 and 20 at a time). Fig. 3 shows that the computational cost of WarmL1 increases when the number of new measurements increases. This lies in that with more measurements added, the available information about the signal increases, which causes a sparser reconstructed signal. Because WarmL1 adds/removes one element to/from the support at each iteration, more support changes between the two systems requires more iterations and so computation. WarmL1 is up to 18.6 × faster than StaLasso and 3.2× faster than GPSR BB in this experiment.\nFig. 4 shows the experimental results of the three algorithms with respect to different amounts (1, 5, 9, 13, 17 and 20 at a time) of measurements being removed. With less measure- ments available, the information about the signal is reduced, which leads to a denser reconstructed signal and more support changes between the previous and next system. Therefore, just as the discussions in the Sec. IV-A, the computational cost of\nWarmL1 increases. In this test, WarmL1 achieves 12.3 × and 5.2 × speeding up with respect to StaLasso and GPSR BB.\nIn this experiment, we model the next signal (x nex ) by the following steps. We ﬁrstly multiply each nonzero elements of the previous signal (x pre ) by a Gaussian random number with mean equal to 1 and variance equal to 0.1. Secondly, we change different amounts (0, 2, 4, 6, 8 and 10 at a time) of zero elements in this signal to nonzero values. Therefore, the two signals act as a time-varying signal at time t and t + 1. The experimental results are shown in Fig. 5. When the amount of changed zero elements increases, the support difference between the systems is larger, which causes the performance degradation of WarmL1. WarmL1 achieves 37.5 × speeding up with respect to StaLasso, and 1/5100 l2-error comparing to GPSR BB with the same computational cost.\nIn this experiment, we consider the challenging scenario that the sensing matrix and the underlying signal are both changed. That means A pre and A nex , x pre and x nex , y pre and y nex are all different. We multiply each nonzero elements of x pre by a Gaussian random number with mean equal to 1 and variance equal to 0.1 to model x nex . A pre is multiplied by a Gaussian random number with mean equal to 1 and different variances (0.01, 0.068, 0.126, 0.184, 0.242 and 0.3 at a time) to model A nex . y nex is then generated by A nex x nex . Fig. 6 shows the experimental results. In this scenario, we observe that the performance of WarmL1 decreases when the variance of the Gaussian random number multiplied with sensing matrix increases. This lies in that the difference of the previous and\nthe next problem is larger. However, the WarmL1 is still 18.7 × faster than StaLasso and the l2-error of WarmL1 is 1/4800 smaller than that of GPSR BB with the same computational cost.\nIn this work, a homotopy-based reconstruction algorithm for sparse signals with warm-start (named WarmL1) is proposed. WarmL1 provides a general method for quickly reconstructing a sparse signal under several dynamic system modiﬁcations. While the system is changed, WarmL1 solves the new problem by adopting the previous result as the starting point rather than recalculates the new solution from scratch. We present a novel formulation to connect the previous and the new problems, and adopt homotopy continuation principle to de- velop the algorithm. Four possible applications for WarmL1 are proposed and discussed. Through the experiments, we demonstrate that WarmL1 outperforms related works in these applications. WarmL1 achieves up to 37.5 × speeding up or 1/5100 l2-error with the same computational cost. Moreover, WarmL1 can be applied to research ﬁelds such as compressed sensing and sparse representation."},"refs":[{"authors":[{"name":"E. Candes"},{"name":"J. Romberg"},{"name":"T. Tao"}],"title":{"text":"Stable signal recovery from incomplete and inaccurate measurements"}},{"authors":[{"name":"D. Donoho"}],"title":{"text":"Compressed sensing"}},{"authors":[{"name":"J. Yang"},{"name":"K. Yu"},{"name":"Y. Gong"},{"name":"T. Huang"}],"title":{"text":"Linear spatial pyramid matching using sparse coding for image classiﬁcation"}},{"authors":[{"name":"X. Mei"},{"name":"H. Ling"}],"title":{"text":"Robust visual tracking using l1 minimization"}},{"authors":[{"name":"M. Asif"},{"name":"J. Romberg"}],"title":{"text":"Dynamic updating for sparse time varying signals"}},{"authors":[{"name":"P. Garrigues"},{"name":"L. Ghaoui"}],"title":{"text":"An homotopy algorithm for the lasso with online observations"}},{"authors":[{"name":"M. Asif"},{"name":"J. Romberg"}],"title":{"text":"Sparse signal recovery and dynamic update of the underdetermined system"}},{"authors":[{"name":"M. Figueiredo"},{"name":"R. Nowak"},{"name":"S. Wright"}],"title":{"text":"Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems"}},{"authors":[{"name":"T. A. Davis"},{"name":"W. W. Hager"}],"title":{"text":"Row modiﬁcations of a sparse cholesky factorization"}},{"authors":[{"name":"H. Kung"},{"name":"T. Lin"},{"name":"D. Vlah"}],"title":{"text":"Identifying bad measurements in compressive sensing"}},{"authors":[{"name":"M. Aharon"},{"name":"M. Elad"},{"name":"A. Bruckstein"}],"title":{"text":"K-svd: An algorithm for designing overcomplete dictionaries for sparse representation"}},{"authors":[{"name":"M. Zeiler"},{"name":"D. Krishnan"},{"name":"G. Taylor"},{"name":"R. Fergus"}],"title":{"text":"Deconvolutional net- works"}},{"authors":[{"name":"D. Donoho"},{"name":"Y. Tsaig"}],"title":{"text":"Fast solution of l1-norm minimization problems when the solution may be sparse"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564337.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T9.4","endtime":"12:50","authors":"Tien-Ju Yang, Yi-Min Tsai, Chung-Te Li, Liang-Gee Chen","date":"1341491400000","papertitle":"WarmL1: A Warm-start Homotopy-based Reconstruction Algorithm for Sparse Signals","starttime":"12:30","session":"S12.T9: L1-Regularized Least Squares and Sparsity","room":"Stratton West Lounge (201)","paperid":"1569564337"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
