{"id":"1569565805","paper":{"title":{"text":"Rate Distortion Codes for the Collective Estimation from Independent Noisy Observations"},"authors":[{"name":"Tatsuto Murayama"},{"name":"Peter Davis"}],"abstr":{"text":"Abstract\u2014We present a collective behavior in the optimal aggregation of noisy observations of a source. The quality of estimation of the source state involves a difﬁcult tradeoff between sensing quality which increases by increasing the number of sensors, and aggregation quality which decreases if the number of sensors is too large. We analytically study the optimal strategy for large scale aggregation, and obtain an explicit and exact result by introducing a basic model. We show that larger scale aggregation always outperforms smaller scale aggregation at higher noise levels, while below a critical value of noise, there exist moderate scale aggregation levels at which optimal estimation is realized. We also examine the practical tradeoff between the above two aggregation strategies by applying an iterative encoding to linear codes."},"body":{"text":"Device and sensor networks are shaping many activities in our society. These networks are being deployed in a growing number of applications as diverse as agricultural management, industrial controls, crime watch, and military applications. Indeed, sensor networks can be considered as a promising technology with a wide range of potential future markets. Still, for all the promise, it is often difﬁcult to integrate the individual components of a sensor network in a smart way. Although we see many breakthroughs in component devices, advanced software, and power managements, system- level understanding of the emerging technology is still weak. It requires a shift in our notion of \u2018what to look for\u2019 [1]. New properties can be revealed by the study of collective behavior and resulting trade-offs. This is the issue that we address in this paper.\nIn this paper, we consider the problem of reducing sensing error by collecting observations from many sensors. However, collecting observations from many sensors usually involves some cost in terms of network resources, or system communi- cation capacity [2]. If the amount of information transmitted for each observation is ﬁxed for a given capacity, there would be a limit on the number of sensors, and on the collective sensing quality. The number of sensors, and thus the sensing quality, could increase when it is possible to reduce the amount of information transmitted for each observation. This could be done by applying a compression scheme. If we restrict ourselves to lossless compression of a given set of observations, the number of sensors is strictly upper bounded by the system capacity, or the sum rate [3]. On the other hand,\nlossy coding of each individual observations enables us to use as many sensors as possible, since an observation could be coded and compressed beyond its entropy rate [4].\nThe latter case involves a new type of tradeoff, i.e., the tradeoff between the effect of reducing collective sensing error with increasing number of sensors, and the effect of increasing aggregation error with increasing number of sensors due to the need to reduce the amount of information below the entropy rate. This tradeoff and its implications for optimal size of systems is the problem that we address in this work. Due to the ongoing growth of practical network sensing systems it is expected to be a problem of rapidly increasing relevance and signiﬁcance [5].\nIn this paper, we consider a fundamental formulation of the problem with only one information source and suppose that all sensors are symmetrical, i.e., exchangeable with respect to their contributions to the ﬁnal result of aggregation. This allows us to treat our problem in terms of the theory of large deviations. Such a class of problems is often termed as the CEO problem [6]. Here we present a rigorous result by reﬁning the heuristic argument in our previous work [7]. The paper is divided into 5 sections. Section II provides mathematical preliminaries for the following sections. Section III presents our system model. The relation between the CEO problem and the problem treated in this work is also mentioned here. Section IV brieﬂy summarizes our main result and a brief proof for the proposition. Numerical ﬁndings are also presented here, including the ﬁgures of optimal data rates and the corresponding information gain which could be obtained by using the lossy rate distortion codes. Conclusions are given in Section V.\nWe brieﬂy review the following two notions before detailing the system model in Section III. The rate distortion function deﬁnes optimal data rates for independent noisy observations, while the large deviation property provides a theoretical basis for the optimality measure.\nLet X µ be a discrete random variable with alphabet Γ = {1, −1}. Assume that we have a source that produces a\nsequence X 1 , X 2 , . . . , X M , where each symbol is randomly drawn from the distribution, say\nfor p in the interval [0, 1]. A sequence of observations X µ is called the Bernoulli(p) trials. Suppose that we have a pair of an encoder and a decoder. The encoder describes the M -bit state X, X µ for µ = 1, . . . , M , by an N -bit codeword Z ν for ν = 1, . . . , N . The number M represents the length of a source sequence, while N represents the length of a codeword. The decoder represents X µ by an estimate ˆ X µ given a collection of Z ν . The rate is deﬁned by R = N/M in the interval (0, 1].\nA distortion function is a mapping d : Γ × Γ → R + from the set of source alphabet-reproduction alphabet pairs into the set of non-negative real numbers. Intuitively, the distortion d(X µ , ˆ X µ ) is a measure of the cost of representing the symbol X µ by the symbol ˆ X µ . This deﬁnition is quite general. Hereafter, the Hamming distortion measure is adopted as the ﬁdelity criterion; d(X µ , ˆ X µ ) = 1 − δ(X µ , ˆ X µ ) where δ is the Kronecker\u2019s delta function. The distortion measure is so far deﬁned on a symbol-by-symbol basis. We extend the deﬁnition to sequences, which is deﬁned by d(X, ˆ X) = (1/M )\nd(X µ , ˆ X µ ). Therefore, the distortion for a se- quence is the average distortion per symbol of the elements of the sequence. Finally, the distortion associated with the code is deﬁned as D = ⟨d(X, ˆ X) ⟩ where the braket denotes averaging over random variables X µ .\nA rate distortion pair (R, D) should be achievable if a sequence of rate distortion codes exist with ⟨d(X, ˆ X) ⟩ ≤ D in the limit M → ∞. Moreover, the closure of the set of achievable rate distortion pairs is called the rate distortion region for a source. Now we can deﬁne a function to describe the boundary.\nDeﬁnition 1 (Rate Distortion Function): The rate distor- tion function R(D) is the inﬁmum of rates R, so that (R, D) is in the rate distortion region of the source for a given distortion D.\nWe now ﬁnd the description rate R(D) required to describe the Bernoulli(p) source with an expected proportion of errors less than or equal to D. In this simpliﬁed case, according to Shannon, the boundary can be written as follows. The rate distortion function for a Bernoulli(p) source with Hamming distortion is given by\nwhere we deﬁne the binary entropy function as H 2 (p) = −p ln p − (1 − p) ln(1 − p). Its inverse function, the distortion- rate function D(R), could be also deﬁned since R(D) is a monotonically decreasing function. We may use either the distortion-rate function or the rate-distortion function to describe the optimal boundary, since the two descriptions are equivalent in the large M limit [8]. The D(R) is known as the theoretical lower bound of average distortion for a given rate R.\nSuppose that a collection of X µ is a Bernoulli(p) se- quence for µ = 1, . . . , N . Then the macroscopic behav- ior of the coin could be captured by a single number; M N = (1/N )\nX µ . This is the average value of the N Bernoulli(p) trials. Deﬁne P N to be the distribution of the sample mean M N and assume that a number m is the mean value of X µ . Suppose that A(x, ϵ) denotes the interval (x −ϵ, x+ϵ). Then the law of large numbers says that for any ϵ > 0,\nas N → ∞. In other words, if N is large enough, almost all the realizations of microscopic M N are very close to the macroscopic mean value m. In contrast, if |x − m| > ϵ then we observe P N {A(x, ϵ)} → 0 in the limit N → ∞. We reﬁne the latter statement to claim that the probability decays to zero exponentially fast. The key concept is given below.\nDeﬁnition 2 (Large Deviation Property): Suppose that a set A does not contain m. The sequence {M N } satisﬁes a large deviation property if there exists I(x) such that\nIt is possible to calculate I(x) using the distribution ρ(x) if the above statement holds. Below we give a brief prescription for the algebra without a proof. Suppose that t is a real number. Deﬁne for any t,\nThe function c(t) is called the cumulant generating function of ρ(x). Here it is an easy matter to check that c(t) is convex. Now assume that c(t) < ∞ for all t. Then it is possible to give its Legendre transform and write\nThe function I(x) is a convex function which provides the exponential rate of decay in the formula (1). Assume that {X µ } is a Bernoulli(p) sequence. By setting Γ = {1, −1}, we have c(t) = ln {pe t + (1 − p)e −t }. Here c(t) is convex and thus tx −c(t) is concave. Hence tx−c(t) attains its supremum at t if and only if\n− p p\n1 + x 1 − x\nln(1 − p) (3)\nfor −1 ≤ x ≤ 1 and ∞ otherwise. Notice that the form of I p (x) depends on the choice of Γ. By setting Γ = {0, 1}\nand ρ(x) = pδ(x, 1) + (1 − p)δ(x, 0), we have yet another representation. That is,\nIn short, if the large deviation property holds, the rate function I p (x) could be identiﬁed with the rate of decay. However, its actual form depends on the choice of alphabets for the Boolean variable X µ . In contrast, the form of the rate distortion function R(D) for the Bernoulli(p) sequence does not depend on the individual choice of the state space.\nOn the other hand, as far as {X µ } is a sequence of i.i.d. random variables, the next proposition tells us that the property is actually satisﬁed. It is known as Cram´er\u2019s Theorem in probability theory [9].\nProposition 3 (Cram´er\u2019s Theorem): Suppose that c(t) is ﬁ- nite for all real number t. Then the sequence {M N } satisﬁes the large deviation property (1) with the rate function deﬁned to be (2).\nSince the above statement holds for the Bernoulli(p) trials, the rate function I p (x) deﬁned to be (3) provides the decay rate for the Γ = {−1, 1}. Similarly, we have (4) for the choice of Γ = {0, 1}. In section IV, we use the latter formula in the analysis.\nThis section provides our system model for the sensing and data aggregation tasks described in Section I. In contrast with the original CEO problem, the optimal joint decoding task is replaced by the simple majority vote procedure for a collection of independently decoded observations. We also deﬁne the optimality criterion for the system which will be used in the following analysis.\nSuppose that we have L independent sensors which each in- dependently observe an M -bit state X, X µ for µ = 1, . . . , M , of a common, uniform binary source, and obtain an M - bit observation Y (a) (a = 1, . . . , L) where each bit Y µ (a) has common probability p of error, i.e. differing from the corresponding source bit X µ . The value of p speciﬁes the level of observation noise. Now the sensors independently compress their M -bit observation into shorter N -bit codewords, Z(a), and send them to the aggregator. The condition \u2018independent\u2019 excludes the possibility of mutual communications between sensors. We assume the rate R = N/M is common to all the sensors. In addition, we suppose that the sum total of the rate, the system capacity C, is ﬁxed, with\nThe aggregator then decodes every N bit codeword inde- pendently to obtain L separate M -bit reproductions ˆ Y (a) (a = 1, . . . , L). Finally, the ˆ Y (a) are used to obtain a single\ncollective estimator ˆ X. We analyze the behavior of the bit error probability, denoted p e (p, R; C), in the collective estimate.\nFirst, we assume that the average error due to lossy com- pression is independent of µ and a, and denoted by D, that is\nHere we used Kronecker\u2019s delta δ and the braket denotes averaging over random variables. This includes the standard exchangeable sensor ansatz for our model [6], [10], which means that all sensors have the same distortion D for a given rate R. The possible value of the distortion D depends on R, so we explicitly denote D as D(R). In particular, the smallest average distortion D(R) is obtained in the limit of M → ∞, and is called the distortion-rate function for the source Y (a) (See deﬁnitions in Section II). Now assume hereafter that D(R) represents the distortion-rate function for Y (a).\nThe combined error probability for ˆ Y µ (a), independent of µ and a, is obtained as\nThe combined error probability ρ is a function of both p and R. In particular, the eq. (6) implies that ρ is a decreasing function of R, since D(R) should be a decreasing function of R. It is obvious that p e (p, R; C) is a decreasing function of L if ρ is ﬁxed. However, due to the constraint (5), and the decrease in distortion D(R) with increase of R, ρ actually increases with an increase of L, resulting in contrary effects on p e (p, R; C). Therefore the challenge here is to incorporate consideration of the distortion D in a way which clariﬁes the interplay between the contrary effects induced by the constraint (5).\nIn this paper we use symmetric notations, i.e. X µ , Y µ (a) = ±1 and similarly for their estimates. Assume that the original X µ are i.i.d. random variables. That is, the underlying M -bit state X is a Bernoulli sequence. Let ˆ Y µ be an L-bit collection of µth symbols ˆ Y µ (a) of M -bit reproductions ˆ Y (a) for a = 1, . . . , L. Then the aggregator observes the ˆ Y µ and obtain an estimate X µ for a µ given. Write the posterior probability of X µ given the L-bit collection ˆ Y µ as P (X µ | ˆ Y µ ). Then the average error probability for the estimate ˆ X µ is\nP (+1 | ˆ Y µ ) −P (−1| ˆ Y µ ) for any µ. Since ˆ X µ = ±1, the optimal estimate ˆ X µ should be ˆ X µ = sign(m µ ). In other words, if M -bit X is a collection of i.i.d. random variables, the optimal estimate ˆ X µ could be given by calculating m µ for each µ independently.\nAssume for simplicity that the X is a collection of purely random variables X µ . Then the Bayes formula says that for every µ the optimal estimate ˆ X µ from the set of aggregated\nvalues ˆ Y µ could be obtained by the majority vote operation. That is,\nIn the rest of this paper, we only consider the case that X µ obey the Bernoulli(1/2) distribution.\nAssume that a network capacity C is given. Consider that the common rate R is ﬁrst allocated to all the sensors. The number of sensors L is thus determined as the maximum value of L satisfying the sum rate constraint RL ≤ C. Since C (and also L) should be a sufﬁciently large number, it is natural to think that (5) holds in the limit. In our system model, it is obvious to say that P {X µ ̸= ˆ X µ } → 0 as C → ∞. As is shown in Section IV, it is not hard to reﬁne the above statement of convergence and to prove that P {X µ ̸= ˆ X µ } decays to 0 exponentially fast as C → ∞. By analogy with large deviation theory [9], we deﬁne the exponential rate of decay by\nThe decay rate I p (R) describes the limiting behavior of the system from a macroscopic level, on which the rate R could be used as a control parameter. The case of R = 1 reduces to a naive aggregation scheme in which the sensors just send their noisy observations to the observer. For this smallest aggregation, we aggregate data from only L = C sensors. Hereafter, we call this scheme the level-1 aggregation. For a given R > 0, the level-R aggregation is deﬁned in which every sensor encodes its observations at the rate of R independently. As an extension of the deﬁnition of I p (R) for R > 0, we could naturally deﬁne the level-0 decay rate as I p (0) = lim R →0 I p (R).\nIn this section, we demonstrate a collective behavior in the large scale aggregation of independent noisy observations by using a simple majority vote model introduced in Section III. We ﬁrst prove the main statement by using the concepts described in Section II, and then resort to numerical analysis of the given formula.\nAssume that D(R) denotes the distortion rate function, which is the inverse function of R(D). Write the error indicator function δ(X µ , − ˆ Y µ (a)) as Z µ (a). For a given µ, this is a Bernoulli random variable that takes the value 1 with probability\nfor a = 1, . . . , L. Since the variables p, D are in the interval [0, 1/2), it follows that ρ p (R) is also in [0, 1/2). Consider the sample average deﬁned to be\nSince the expectation ⟨Z µ (a) ⟩ = ρ p (R) is ﬁnite, we know that M µ,L is approaching ρ p (R) by the law of large numbers. However the value of interest is the error probability P {X µ ̸=\nˆ X µ } for the majority vote procedure, which is identical to P {M µ,L ≥ 1/2} for the estimate (7). For 0 ≤ ρ < 1/2 and thus |ρ − 1/2| > 0, the vanishing P {M µ,L ≥ 1/2} is called the large deviation probability.\nConsider the rate function of Z µ (a). Since Z µ (1), Z µ (2), . . . , Z µ (L) are \t the \t L independent Bernoulli(ρ p (R)) random variables, the Legendre transform gives the rate function I ρ p (R) (z) for the sample average M µ,L as\nfor 0 ≤ z ≤ 1 and ∞ otherwise (See equation (4) in Section II). Since the number of sensors L is given by C/R, changing the variable C = LR yields\nSince the set A µ = {M µ,L ≥ 1/2} is closed and does not contain ρ p (R), the large deviation property (1) tells that\nThen it is an easy matter to check that I (1) p,R (z) = I (1) p,R (1/2). Write I p (R) = I (1) p,R (1/2) for the convenience. For a given R we conclude that\nThis completes the proof for the following proposition, which is the main result of this paper.\n− ρ p (R)) 2\nThe maximum of I p (R) is of great interest from an engineer- ing point of view. That is, we prefer larger values of I p (R). Below, we numerically examine the optimal levels deﬁned by R ∗ = argmax 0 ≤R≤1 I p (R). The optimal aggregation, for a given p, is called the level-R ∗ aggregation.\nWe now examine the behavior of formula (8) which gives the optimal levels R ∗ for the noise p. As is seen in Figure 1, the optimal aggregation scale diverges, i.e., the optimal data rate R ∗ per sensor diverges for noise levels larger than the critical point p 0 = 0.211. In this noisy region, we want the system to be as large as possible. The larger the system we have, the smaller the error probability. By deﬁnition, the optimal aggregation is said to be level-0. In contrast, we can always ﬁnd the non-zero optimal levels below p 0 . In particular, if the noise level is below p 1 = 0.024, our investigations indicate that the level-1 aggregation is optimal. Moderate aggregation levels could be optimal in the intermediate noise levels be- tween the two critical points. It is also worth noticing that the behavior of R ∗ of p is reminiscent of that of order parameters at a continuous phase transition in statistical mechanics [11]. The analytical results presented here are also consistent with numerical simulations for the system size C = 50.\nSince the optimal levels R ∗ are unique values for each noise p, we can evaluate the optimal decay rate I p (R ∗ ) as is given in Figure 2. The optimal rate I p (R ∗ ) describes the limiting be- havior of the smallest error probability P {X µ ̸= ˆ X µ } in terms of macroscopic variables. Clearly, it is a strongly decreasing function of the noise p. The discrepancy between the optimal I p (R ∗ ) and I p (1) implies the possible information gain by our system-level control. Figure 2 also shows typical results from a numerical experiment for the average values of per-bit error probability, p e (p, R; C), obtained using a linear code with an iterative encoder [12]. The linear codes are deﬁned by a class of sparse matrices having k ones (\u20191\u2019) per row and c ones per column, respectively, where R = k/c = N/M . However, the gain is less than that obtained for R(D), telling that there is\nThe results show that the optimal aggregation for a system of independent sensors with constrained communication ca- pacity exhibits a kind of threshold behavior with respect to the observation noise level, both theoretically and in practice."},"refs":[{"authors":[{"name":"P. W. Anderson"}],"title":{"text":"More Is Different"}},{"authors":[{"name":"I. Akyildiz"},{"name":"W. Su"},{"name":"Y. Sankarasubramaniam"},{"name":"E. Cayirci"}],"title":{"text":"A survey on sensor networks"}},{"authors":[{"name":"D. Slepian"},{"name":"J. Wolf"}],"title":{"text":"Noiseless coding of correlated information sources"}},{"authors":[{"name":"C. Shannon"}],"title":{"text":"Coding theorems for a discrete source with a ﬁdelity criterion"}},{"authors":[{"name":"D. Culler"},{"name":"H. Mulder"}],"title":{"text":"Smart sensors to network the world."}},{"authors":[{"name":"T. Berger"},{"name":"Z. Zhang"},{"name":"H. Viswanathan"}],"title":{"text":"The CEO problem"}},{"authors":[{"name":"T. Murayama"},{"name":"P. Davis"}],"title":{"text":"Universal behavior in large-scale aggre- gation of independent noisy observations"}},{"authors":[{"name":"T. Cove"},{"name":"J. Thoma"}],"title":{"text":"Elements of information theory"}},{"authors":[{"name":"R. Elli"}],"title":{"text":"Entropy, Large Deviations and Statistical Mechanics"}},{"authors":[{"name":"Y. Oohama"}],"title":{"text":"The rate-distortion function for the quadratic Gaussian CEO problem"}},{"authors":[{"name":"R. Monasson"},{"name":"R. Zecchina"},{"name":"S. Kirkpatrick"},{"name":"B. Selman"},{"name":"L. Troyan- sky"}],"title":{"text":"Determining computational complexity from characteristic\u2019phase transitions\u2019"}},{"authors":[{"name":"T. Murayama"}],"title":{"text":"Thouless-Anderson-Palmer approach for lossy compres- sion"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565805.pdf"},"links":[{"id":"1569565883","weight":3},{"id":"1569559617","weight":7},{"id":"1569566981","weight":7},{"id":"1569560629","weight":3},{"id":"1569566597","weight":3},{"id":"1569552245","weight":3},{"id":"1569565607","weight":3},{"id":"1569564481","weight":3},{"id":"1569566415","weight":7},{"id":"1569566081","weight":7},{"id":"1569564245","weight":3},{"id":"1569564227","weight":3},{"id":"1569565837","weight":3},{"id":"1569566119","weight":7},{"id":"1569566459","weight":15},{"id":"1569559541","weight":15},{"id":"1569562685","weight":7},{"id":"1569558483","weight":3},{"id":"1569556091","weight":3},{"id":"1569565347","weight":7},{"id":"1569566795","weight":3},{"id":"1569566523","weight":3},{"id":"1569565953","weight":3},{"id":"1569565907","weight":3},{"id":"1569566167","weight":3},{"id":"1569563981","weight":11},{"id":"1569566643","weight":7},{"id":"1569567665","weight":3},{"id":"1569561143","weight":3},{"id":"1569565535","weight":3},{"id":"1569561795","weight":3},{"id":"1569566851","weight":3},{"id":"1569553909","weight":3},{"id":"1569559111","weight":7},{"id":"1569552251","weight":3},{"id":"1569565559","weight":3},{"id":"1569565655","weight":3},{"id":"1569566909","weight":3},{"id":"1569566913","weight":3},{"id":"1569566809","weight":3},{"id":"1569565887","weight":3},{"id":"1569565633","weight":19},{"id":"1569565219","weight":7},{"id":"1569565595","weight":3},{"id":"1569565357","weight":11},{"id":"1569567029","weight":3},{"id":"1569565909","weight":3},{"id":"1569555787","weight":3},{"id":"1569566673","weight":3},{"id":"1569566233","weight":3},{"id":"1569566297","weight":3},{"id":"1569564097","weight":3},{"id":"1569565439","weight":11},{"id":"1569565415","weight":11},{"id":"1569565571","weight":3},{"id":"1569566983","weight":3},{"id":"1569565397","weight":3},{"id":"1569566873","weight":3},{"id":"1569565765","weight":3},{"id":"1569566129","weight":7},{"id":"1569565919","weight":3},{"id":"1569565661","weight":7},{"id":"1569561221","weight":3},{"id":"1569566823","weight":3},{"id":"1569566595","weight":7},{"id":"1569566137","weight":3},{"id":"1569565375","weight":3},{"id":"1569566639","weight":3},{"id":"1569565597","weight":3},{"id":"1569564437","weight":3},{"id":"1569561185","weight":15},{"id":"1569558779","weight":11},{"id":"1569566817","weight":7},{"id":"1569567483","weight":3},{"id":"1569564923","weight":3},{"id":"1569566299","weight":7},{"id":"1569561713","weight":3},{"id":"1569566933","weight":3},{"id":"1569557851","weight":7},{"id":"1569564961","weight":3},{"id":"1569564253","weight":3},{"id":"1569550425","weight":3},{"id":"1569564505","weight":3},{"id":"1569556327","weight":3},{"id":"1569565707","weight":3},{"id":"1569565113","weight":3},{"id":"1569564141","weight":3},{"id":"1569566449","weight":7},{"id":"1569564755","weight":7},{"id":"1569558697","weight":3},{"id":"1569565139","weight":7},{"id":"1569564419","weight":7},{"id":"1569566443","weight":3},{"id":"1569566417","weight":7},{"id":"1569560581","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T8.3","endtime":"17:40","authors":"Tatsuto Murayama, Peter Davis","date":"1341249600000","papertitle":"Rate Distortion Codes for the Collective Estimation from Independent Noisy Observations","starttime":"17:20","session":"S4.T8: Information and Estimation","room":"Stratton (491)","paperid":"1569565805"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
