{"id":"1569566303","paper":{"title":{"text":"Finding the Capacity of a Quantized Binary-Input DMC"},"authors":[{"name":"Brian M. Kurkoski"},{"name":"Hideki Yagi"}],"abstr":{"text":"Abstract\u2014Consider a binary-input, M-output discrete mem- oryless channel (DMC) where the outputs are quantized to K levels, with K < M. The subject of this paper is the maximization of mutual information between the input and quantizer output, over both the input distribution and channel quantizer. This can be regarded as ﬁnding the capacity of a quantized DMC. An algorithm is given, which either ﬁnds the optimal input distribution and corresponding quantizer, or declares a failure."},"body":{"text":"Consider a binary-input, M-output discrete memoryless channel (DMC) where the outputs are quantized to K levels, with K < M. The subject of this paper is the maximization of mutual information between the input and quantizer output, over both the input distribution and channel quantizer. This can be regarded as ﬁnding the capacity of a quantized DMC. An algorithm is given, which either ﬁnds the optimal input distribution and corresponding quantizer, or declares a failure.\nConcretely, let the DMC input be X, let the DMC output be Y, and let the quantized output be Z. The alphabet sizes of X, Y and Z are J, M and K, respectively. Here, K < M is of interest, since K ≥ M implies no reduction in mutual information due to quantization.\nLet C −1 (k) denote the subset of {1, . . . , M} that maps to channel quantizer k. Denote the channel input distribution as:\nThis paper makes the restriction that J = 2. Since J = 2, p 1 + p 2 = 1 , and in the sequel the input distribution is p = p 1 . The mutual information between X and Y is:\n, etc. Mutual information I(X; Y) is convex (lower convex) in P m |j , for ﬁxed p j . Similarly, it is concave (upper convex) in p j for ﬁxed P m |j [1, Theorem 2.7.4].\nmax p I(X; Y) \t (3) and clearly for any ﬁxed quantizer C the capacity of the quan- tized channel is max p I(X; Z) . Furthermore, the celebrated Arimoto-Blahut algorithm [2] [3] ﬁnds the capacity-achieving input distribution:\np ∗ = arg max p I(X; Z). \t (4) Since P and C uniquely determine T , the capacity-achieving input distribution of the quantized channel will be denoted as a function C APACITY : p ∗ = C APACITY (P, C) .\nOn the other hand, for any ﬁxed p, it is possible [4] [5] to ﬁnd the quantizer C ∗ which maximizes the mutual information,\nwhich will be denoted as a function Q UANT : C ∗ = Q UANT (P, p) .\nThe objective in this paper is to ﬁnd the jointly optimal input distribution p ∗ and channel quantizer C ∗ which maximizes the mutual information:\nThis expression is regarded as the capacity of a quantized DMC. The algorithm described in this paper either ﬁnds a jointly optimal p ∗ and C ∗ , or it declares a failure.\nThe rest of this paper is outlined as follows. Section II describes previous work on the capacity of quantized channels and summarizes the contribution of this paper. Section III\ndescribes two key concepts: partial mutual information, a partial sum of mutual information; and a certiﬁcate, the range of input distributions over which a particular quantizer is known to be optimal. Section IV describes the main algorithm which either gives the quantizer and channel input distribution which maximizes mutual information, or declares a failure. Section V gives some numerical results that illustrate the algorithm. Section VI is the conclusion.\nThe importance of designing channel quantizers has long been recognized as a topic of interest in information theory with practical applications. In the 1960s, Wozencraft and Kennedy suggested using the cut-off rate as a criteria for quan- tizer optimization [6], and design algorithms for both binary- input channels and non-binary inputs channels were described around that time [7] [8]. But the ﬁrst known reference to using mutual information to design channel quantizers came in 2002 [9]. An important application is the design of analog-to-digital converters for communication receivers, and codes for such systems.\nSingh et al. considered the capacity of quantized channels, but of continuous output channels, partticularly the AWGN channel [10]. For a ﬁxed quantizer, optimal input distributions can be found using a cutting-plane algorithm. Since certain two-bit quantizers can be characterized by one parameter (for symmetrical channels), joint optimization of the input distribution and quantizer can be performed in a brute-force manner. But for three-bit quantization, it was necessary to resort to an optimization approach that involves alternating between ﬁnding the capacity-achieving input and the optimal quantizer, but this was not proved globally optimal.\nBy considering a DMC rather than a continuous output channel, further progress can be made on this problem. For a ﬁxed input distribution, there exists a polynomial-time al- gorithm which gives the quantizer which maximizes mutual information [4] [5], for a binary input channel. When the channel outputs satisfy:\n, \t (7) then for the optimal quantizer, each quantizer output consists of a convex subset of channel outputs. This quantization problem is an example of impurity partitions from machine learning, where convex subsets are known to be optimal [11]. While restricted to binary-input DMCs, this approach ﬁnds the optimal quantizer for otherwise arbitrary channels.\nIt is also worth noting that the optimal quantizer is known to be deterministic. That is, for a continuous-output channel there is no advantage to using dithered quantization [12]. And for a DMC, probabilistic quantizers are suboptimal [4].\nThus, various optimization problems have been considered. The channel capacity is a straightforward convex minimiza- tion problem (Arimoto-Blahut). Finding the optimal channel quantizer is a concave minimization problem which is NP-hard in general, but has polynomial complexity when attention is\nrestricted to binary-input channels. So joint optimization of the input distribution and the quantizer is a convex-concave optimization problem, and provably optimal methods remain elusive.\nThe contribution of this paper is an algorithm which ﬁnds the jointly optimal input distribution and channel quantizer, for a given binary-input DMC, or declares a failure. The basic approach is to augment the quantization algorithm for a ﬁxed input distribution [4], by adding a \u201ccertiﬁcate\u201d property. The certiﬁcate is a range of input distributions over which the channel quantizer (or a partial quantization of the channel) is known to be optimal. The algorithm can be seen fom a dynamic programming perspective. Dynamic programming decompositions are an effective way to show the optimality of algorithms. Distinct from previous work, this approach can ﬁnd the capacity of arbitrary channels.\nThis section considers a partially quantized channel, by developing the concepts of partial mutual information and a certiﬁcate for a partially quantized channel. After these preliminary concepts are established, the algorithm to compute the DMC capacity is given in the following section.\nThe objective function in (6) is the mutual information between X and Z:\nA partial sum of mutual information is called partial mutual information in this paper. Partial mutual information is a func- tion of the input distribution p. The partial mutual information ι k for output k and quantizer C is:\nFor quantizer outputs 1 to k, quantized using C, the partial mutual information is:\nSo the total mutual information for some input distribution p is the sum of all the partial mutual information terms evaluated at p :\nConsider distinct quantizers C 1 , C 2 , . . ., C i , . . . and some ﬁxed k. The partial mutual information is: ι(p, C i , k). The input which maximizes mutual information can be found:\np ∗ i = arg max p ι(p, C i , k). \t (10) Without loss of generality, assume that the channels are ordered such that:\nι(p ∗ 1 , C 1 , k) ≥ ι(p ∗ 2 , C 2 , k) ≥ · · · \t (11) Let P i , i = 2, 3, . . . be the domain of p for which C 1 has higher partial mutual information than C i :\nP i = p ι(p, C 1 , k) > ι(p, C i , k) . \t (12) A certiﬁcate for C 1 , denoted L, is a domain of ι(p; C 1 , k)\nfor which C 1 achieves the maximum partial mutual informa- tion over all other quantizations:\nSince L is a line segment, it is sufﬁcient to represent L by the two values and r:\nThe following subsection gives an explicit method for ﬁnding P 2 , but can of course be applied for any P i . Then the certiﬁcate L is found as the intersection P 2 ∩ P 3 ∩ · · · .\nThe algorithm input is two partially quantized channels with partial mutual information ι(p; C 1 , k) and ι(p; C 2 , k) . Since C 1 , C 2 and k are ﬁxed, from here, write ι(p; C i , k) as ι i (p) since C and k are ﬁxed. The corresponding derivatives are\nι 1 (p) and ι 2 (p) . The derivative of partial mutual information is:\n+T k |1 log 2 T k |1 − T k |2 log 2 T k |2 , (15) where f(p) = pT k |1 + (1 − p)T k |2 .\n1) Input: two partially-quantized channels with partial mu- tual information ι 1 (p) and ι 2 (p) with ι 1 (p ∗ 1 ) > ι 2 (p ∗ 2 )\n3) For i = 1, 2, . . ., ﬁnd i+1 , the solution in p to: ι 2 ( i ) p − i + ι 2 ( i ) = ι 1 (p)\nfor which i+1 ≤ i . Repeat until a sufﬁciently accurate solution is obtained.\n4) For i = 1, 2, . . ., ﬁnd r i+1 , the solution in p to ι 2 (r i ) p − r i + ι 2 (r i ) = ι 1 (p)\nfor which r i+1 ≥ r i . Repeat until a sufﬁciently accurate solution r is obtained.\nThe operation of the algorithm is illustrated in Fig. 1. The key point is that for any r, the line tangent to ι 2 (p) at r is greater than or equal to ι 2 (p) :\nι 2 (p)(p − r) + ι 2 (r) ≥ ι 2 (p), \t (16) for 0 ≤ p ≤ 1, and equality at r = p. In the region where this line is less than ι 1 (p) :\n{p|ι 2 (p)(p − r) + ι 2 (r) ≤ ι 1 (p) }, \t (17) the inequality ι 2 (p) ≤ ι 1 (p) holds. D. Newton-Raphson Method\nThe Newton-Raphson method is an iterative technique for ﬁnding a root of f(p) which has a derivative f (p). Beginning with an initial value p 1 , compute:\n, \t (18) iteratively until a sufﬁciently accurate value is obtained.\nHere, f(p) is the difference between the partial mutual information function ι(p) and a line. Since ι(p) is strictly convex, the equality f(p) = 0 has at most two solutions. However, which of the two solution found by the Newton- Raphson method depends upon the initial value. For Step 3, use an initial value of 0. For Step 4, use an initial value of 1.\n, \t (19) and\nq if 0 ≤ q ≤ 1 1 if q > 1\nBecause f(x) is convex, this modiﬁcation does not change the convergence.\nThis section describes an algorithm which computes the capacity of the quantized DMC. A dynamic programming approach is used. In this way, it is possible to show the optimality of the algorithm. Note that the algorithm may fail, but if it does produce a solution, it is an optimal solution.\nIn dynamic programming, a problem exhibits optimal sub- structure if the optimal solution contains optimal solutions to subproblems [13]. The subproblem is as follows. For m channel outputs quantized to k quantizer outputs (with m ≤ M and k ≤ K and k ≤ m), ﬁnd C m,k with certiﬁcate L m,k (that is, C m,k is known to be optimal over input distributions in the set L m,k ). Assume that the optimal quantization:\nis known, and each has a corresponding certiﬁcate L k −1,k−1 , L k,k −1 , . . . , L n −1,k−1 . The solution to the subproblem, forming the iterative step of the algorithm, is as follows. For some ﬁxed m and k, and for some n < m, consider a candidate quantizer for channel outputs 1 to m, denoted C (n) m,k . This can be formed by combining the known-optimal quantizer C n,k −1 , with the quantization of channel outputs n+1 to m to the single output k . The candidate quantizer is given by:\nC (n) m,k (m ) = \t C n,k −1 (m ) if 1 ≤ m ≤ n k \t if n < m ≤ m . (22) For each n, compute the input distribution which achieves the maximum partial mutual information. Then select n ∗ for the quantizer C (n ∗) m,k which has maximum mutual information (here n ∗ corresponds to 1 of the previous section) with certiﬁcate K. Then, the optimal quantizer is C (n ∗) m,k with certiﬁcate:\nL m,k = L n ∗,k−1 ∩ K. \t (23) Note that if L m,k = ∅, then a valid certiﬁcate cannot be found with this method, and the algorithm declares a failure.\nThe algorithm to compute the capacity of the quantized DMC is as follows.\n\u2022 Binary-input discrete memoryless channel P m |j . If necessary, modify labels to satisfy (7).\n\u2022 The number of quantizer outputs K. 2) Initialize C 0,0 = ∅ and L 0,0 = [0, 1]\n3) For each k ∈ {1, . . . , K}, and for each m ∈ {k, . . . , k+ M − K}\n\u2022 Compute p ∗ n = C APACITY (P, C (n) m,k ) . b) Select n ∗ = arg max n ι(p ∗ n ; C (n) m,k , k)\n4) Outputs. The globally optimal quantizer C ∗ is C M,K . The capacity-achieving input distribution is:\nThe algorithm has polynomial complexity. In step 3, it can be seen there are three \u201cfor each\u201d loops which which contribute M 3 operations. For each of these, it is necessary to ﬁnd p ∗ , which is also polynomial complexity. Note that ﬁnding the certiﬁcate has complexity linear in M.\nThe relationship between the subproblems is illustrated in a trellis-type diagram in Fig. 2, for M = 5 and K = 3. For any node C m,k , lines indicate those C n,k −1 , n = k − 1, . . . , m − 1 which are used in step 3.\nAn alternating algorithm is presented, which is based upon the principles similar to Singh et al [10]. By alternating between the DMC Quantization algorithm [4] (for a ﬁxed p ) and the Arimoto-Blahut algorithm (for a ﬁxed C), this approach is straightforward:\n4) If C i = C i −1 and i > 1 then stop. Output quantizer C i and distribution p i+1 .\nThis algorithm is considerably simpler, but it is not guar- anteed to ﬁnd optimum p ∗ and C ∗ . In particular, the capacity maximization may ﬁnd an input distribution p which is locally optimal for all possible quantizers, but is distinct from the global optimal p ∗ .\nTo illustrate, the following test channel is used. A BPSK channel with data-dependent noise is used, where Gaussian noise with variance 4 is added to −1 and Gaussian noise with variance 0.1 is added to +1. A DMC is formed by uniformly quantizing this between −1 and +1 to M levels.\nThe quantized channel capacity of the test channel is shown in Fig. 3 for various values of M and K, with K < M. The unquantized channel capacity is also shown. Generally, a DMC with a larger number of outputs M has a greater channel capacity, for ﬁxed K. Note an exception for M = 5 and M = 8, where the later has greater channel capacity. This may be attributed to relatively coarse channel quantization, where the boundaries for the M = 5 test channel are more suitable for quantization to K = 2.\nFor this particular test channel, the alternating algorithm of Subsec. IV-C produced the same quantizer and input distribution.\nThe algorithm is able to certify the output. That is, if an output is produced it is known to be optimal. Otherwise, the algorithm declares a failure. Table I lists various combinations of M and K for the test channel. Cases where the algorithm failed are marked \u201cF\u201d (and success is marked \u201c-\u201d). The algo- rithm is more likely to fail when attempting to resolve small differences between competing quantizers. In these cases, the new certiﬁcate is relatively short, and has no overlap with the prior certiﬁcate, which may also be short. When there is no intersection, the capacity-achieving input distribution cannot be found.\nThis paper has presented an algorithm which computes the capacity of quantized discrete memoryless channels. For\na quantizer with K outputs, the algorithm may ﬁnd the input distribution and quantizer which maximizes the mutual information. If it does not ﬁnd these, then it declares a failure.\nJointly maximizing mutual information in both the input distribution and the quantizer is concave-convex optimization problem; this class of problem is NP-hard in general. However, by exploiting the properties of efﬁcient quantizers (described in II), polynomial complexity is possible. The optimality of the solution produced by the algorithm can be shown by dynamic programming techniques."},"refs":[{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory"}},{"authors":[{"name":"S. Arimoto"}],"title":{"text":"An algorithm for computing the capacity of arbitrary dis- crete memoryless channels"}},{"authors":[{"name":"R. E. Blahut"}],"title":{"text":"Computation of channel capacity and rate-distortion functions"}},{"authors":[{"name":"B. M. Kurkoski"},{"name":"H. Yagi"}],"title":{"text":"Quantization of binary-input dis- crete memoryless channels, with applications to LDPC decoding."}},{"authors":[{"name":"B. Kurkoski"},{"name":"H. Yagi"}],"title":{"text":"Concatenation of a discrete memoryless channel and a quantizer"}},{"authors":[{"name":"J. M. Wozencraft"},{"name":"R. S. Kennedy"}],"title":{"text":"Modulation and demodulation for probabilistic coding"}},{"authors":[{"name":"J. L. Massey"}],"title":{"text":"Coding and modulation in digital communications"}},{"authors":[{"name":"L. Lee"}],"title":{"text":"On optimal soft-decision demodulation"}},{"authors":[{"name":"X. Ma"},{"name":"X. Zhang"},{"name":"H. Yu"},{"name":"A. Kavcic"}],"title":{"text":"Optimal quantization for soft- decision decoding revisited"}},{"authors":[{"name":"J. Singh"},{"name":"O. Dabeer"},{"name":"U. Madhow"}],"title":{"text":"On the limits of communication with low-precision analog-to-digital conversion at the receiver"}},{"authors":[{"name":"D. Burshtein"},{"name":"V. D. Pietra"},{"name":"D. Kanevsky"},{"name":"A. Nadas"}],"title":{"text":"Minimum impurity partitions"}},{"authors":[{"name":"J. Singh"},{"name":"O. Dabeer"},{"name":"U. Madhow"}],"title":{"text":"Communication limits with low- precision analog-to-digital conversion at the receiver"}},{"authors":[{"name":"T. H. Corme"},{"name":"C. E. Leiserso"},{"name":"R. L. Rives"},{"name":"C. Stei"}],"title":{"text":"Introduction to Algorithms, Second Edition"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566303.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T7.2","endtime":"17:20","authors":"Brian Michael Kurkoski, Hideki Yagi","date":"1341248400000","papertitle":"Finding the Capacity of a Quantized Binary-Input DMC","starttime":"17:00","session":"S4.T7: Capacity of Finite-Alphabet Channels","room":"Stratton (407)","paperid":"1569566303"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
