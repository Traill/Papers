{"id":"1569566711","paper":{"title":{"text":"On Modulo-Sum Computation over an Erasure Multiple Access Channel"},"authors":[{"name":"Ashish Khisti"},{"name":"Brett Hern"},{"name":"Krishna Narayanan"}],"abstr":{"text":"Abstract\u2014We study computation of a modulo-sum of two binary source sequences over a two-user erasure multiple access channel. Each sender observes an independent and equiprobable binary sequence and the receiver is interested in computing the modulo-sum of these two sequences. The channel is modelled as a binary-input, erasure multiple access channel, which can be in one of three states \u2014 either the channel output is a modulo-sum of the two input symbols, or the channel output equals the input symbol on the ﬁrst link and an erasure on the second link, or it equals the input symbol on the second link and an erasure on the ﬁrst link. The associated state sequence is independent and identically distributed. We establish upper and lower bounds on the modulo-sum capacity. Our coding scheme uses either the compute-and-forward or the decode-and-forward techniques. The upper bound is obtained by a genie aided argument that reduces the setup to a compound multiple-access channel. It is in general is tighter than a simple upper bound obtained by revealing one of the messages to the decoders. We also brieﬂy consider the case when a strictly causal state feedback is available to the encoders and establish that such feedback can increase the modulo-sum capacity."},"body":{"text":"Many problems in network information theory require that the intermediate relay nodes only compute a function of the underlying source messages. One such example is the two-way relay channel where two users need to communicate to each other using a central relay node. It is natural that the relay node only compute a modulo-sum of the messages of the two users. In a classic paper, Korner and Marton [4] consider a multi- terminal source coding problem where the destination terminal is required to compute a modulo-sum of two binary sources. Each source is revealed to one encoder. The authors establish the optimality of a scheme using identical linear codebooks to separately compress the two source sequences. There has been a signiﬁcant interest in source and channel coding techniques for function computation in recent times; see e.g. [1]\u2013[3], [5]\u2013 [13].\nRecently Wilson et.al [11] and Nazer and Gastpar [6] study achievable rates for computation of the modulo-sum of two messages over a Gaussian multiple access channel (MAC).\nThe authors observe that for a wide range of signal-to-noise ratio (SNR), one can achieve higher rates using lattice codes instead of an i.i.d. random code ensemble. The additive nature, the Gaussian MAC channel is well suited for computing the modulo sum of two messages using lattice codes. A simple upper bound obtained by revealing one of the messages to the destination, is sufﬁciently close to the known lower bounds for a wide range of SNR. Several interesting extensions along these lines are also discussed in [6], [11].\nIn the present paper we introduce a MAC channel model that does not appear naturally matched for computing the modulo- sum function. Our model is an erasure multiple access channel with binary inputs. With a certain probability, the destination observes a modulo-sum of the two transmitted bits whereas with a certain probability the destination observes only one of the two bits and an erasure symbol associated with the other transmitted bit. As our main result, we establish that the cut- set upper bound is not tight and develop a new upper bound. We also brieﬂy consider the case when there is strictly causal feedback of the state sequence available from the destination (using e.g., ARQ) and show that the capacity can be increased compared to the case without such a feedback.\nWe study a multiple access channel with two transmitters and one receiver. The channel input symbols are denoted by x and y respectively and are binary valued. The channel output is denoted by z and is also binary valued. The channel transition probability is controlled by a state variable s ∈ {0, 1, 2}. In particular we have:\n⎧ ⎪ ⎨ ⎪ ⎩\nx ⊕ y, s = 0, x, \t s = 1, y, \t s = 2.\nWe assume that the receiver is revealed the pair (z, s). We assume that Pr(s = 1) = Pr(s = 2) = ε and Pr(s = 0) = 1− 2ε where ε satisﬁes 0 ≤ ε ≤ 1/2. The channel is memoryless i.e., Pr(s n = s n ) = n i=1 Pr(s i = s i ) is satisﬁed.\nA code of length n is deﬁned as follows. Sender i observes a message w i uniformly and independently distributed over the set [1, . . . , 2 nR ]. For sake of convenience we will represent\nmessage w i as a sequence b nR i consisting of nR indepen- dent and equiprobable bits. We deﬁne u = w 1 ⊕ w 2 as the exclusive-or of b nR 1 ⊕ b nR 2 .\nThe messages are mapped into codewords x n = f n (w 1 ) and y n = g n (w 2 ) respectively and the decoder is required to produce ˆ u = h n (z n , s n ). An error is declared if {u = ˆu}.\nA rate R is achievable if there is a sequence of encoders and decoders such that the error probability goes to zero as n approaches inﬁnity. The largest achievable rate is deﬁned as the modulo-sum capacity.\nRemark 1: In this paper we focus on the simpliﬁed model in (1) where the channel can only take one of three states. The results can be extended to the case when there are two additional states \u2014 either the decoder observes both (x, y) or it observes an erasure. Such extensions will be reported elsewhere.\nWe summarize the main results in the remainder of this section.\nWe propose the following lower bound on the modulo-sum capacity.\nProposition 1: The modulo-sum capacity of the multiple- access erasure channel (1) is lower bounded by the following expression:\nThe lower bound of R = 1 − 2ε is attained using a compute- and-forward technique [6]. The lower bound R = 1/2 is attained using a standard multiple-access channel code to transmit both w 1 and w 2 to the destination. This technique is referred to as decode-and-forward in this paper. The coding technique discussed in [3], which uses identical codebooks at the two transmitters also sufﬁces to achieve this rate.\nWe provide the following upper bound on the modulo-sum capacity.\nTheorem 1: The modulo-sum capacity of the multiple- access erasure channel (1) is upper bounded by the following expression:\n3 \t (3) where (·) + equals zero if the argument inside is negative.\nThe proposed upper bound is tighter than a genie-aided bound where one of the messages, say w 1 , is revealed to the decoder. We provide some intuition behind the upper bound below.\n1) Revealing Side Information to the Transmitters: Our key idea is to reveal part of the state sequence to the encoders. In particular deﬁne the sets A = {i : s i = 1}, B = {i : s i = 2} and C = {i : s i = 0}. We illustrate the technique when |A| = |B| = |C| = n 3 , which roughly corresponds to the case when ε = 1/3. We will use the notation z n C to denote the projection of z n onto the indices i ∈ C etc.\nIn our upper bound, we ﬁrst reveal the knowledge of B to the two encoders non-causally. However the encoders are not aware of the sets A and C. Note from (1) that z n B = y n B , z n A = x n A and z n C = x n C ⊕ y n C .\n2) Independence of Input Signals from w 1 ⊕ w 2 : Observe that y n B is sub-sequence transmitted by user 2 and hence independent of u = w 1 ⊕ w 2 . Using this property we have:\nnR = H(u) \t (4) = H(u|y n B ) \t (5) = H(u|y n B , x n A , z n C ) + I(x n A , z n C ; u|y n B , u) \t (6) ≤ n(1 − ε) − H(x n A , z n C |y n B , u) + n · o n (1), (7)\nwhere we use Fano\u2019s inequality in 1 n H(u|x n A , y n B , z n C ) ≤ o n (1) and o n (1) denotes a vanishing function in n.\n3) Compound MAC Channel: Observe that the same coding scheme must also work when the positions of sets A and C are interchanged. This results in\nCombining (7) and (8) and ignoring the o n (1) term, we obtain the following:\n(9) ≤ n(1 − ε) − 1 2 H(x n A , z n C |y n B , u)+H(x n C , z n A |y n B , u)\n(10) ≤ n(1 − ε) − 1 2 H(x n A , z n C , x n C , z n A |y n B , u) \t (11)\n= n(1 − ε) − 1 2 H(x n A , y n C , x n C , y n A |y n B , u) \t (12) ≤ n(1 − ε) − 1 2 H(y n A , y n C |y n B , u) \t (13) ≤ n(1 − ε) − 1 2 H(y n A , y n C |y n B ) \t (14)\nwhere (14) from the fact that the transmit sequence by user 2, y n is independent of w 1 and hence w 1 ⊕w 2 . Eq. (14) suggests that for the rate to be high (y n A , y n C ) and y n B must be strongly correlated. However as we show below, such a constraint can only reduce the upper bound obtained by revealing one of the messages to the destination.\n4) Penalty from Repetition Coding: Suppose that the se- quence x n is completely revealed to the destination. The receiver only needs to compute w 2 and hence we have:\nEliminating the joint entropy term between (14) and (15) we get\nBy using the simple upper bound H (y n B ) ≤ |B| = nε we get R ≤ 2−ε 3 which agrees with (3) for ε = 1/3.\nConsider the case when the encoders are revealed the state sequences in a strictly manner. The encoding functions at time i can depend on the state sequence up to time i − 1 i.e. x i = f i (w 1 , s i−1 1 ) and y i = g i (w 2 , s i−1 1 ).\nProposition 2: The modulo-sum capacity the multiple ac- cess channel with strictly causal state feedback is lower and upper bounded by R − FB ≤ C ≤ R + FB , where\nThe lower bound is achieved by a two-phase protocol where the users transmit uncoded bits in the ﬁrst phase and use a multiple-access code in the second phase. The upper bound is the genie-aided bound where one of the messages is revealed to the destination. The problem reduces to communicating the other message, say w 2 to the destination. Feedback in such a case is well known to not increase the point-to-point capacity.\nWe separately establish the achievability of R = 1 − 2ε and R = 1/2.\nWe use identical linear codebooks at the two transmitters in the compute and forward scheme to achieve R = 1 − 2ε. Recall that the messages w 1 and w 2 are assumed to be binary valued sequences of length nR bits i.e., we take\nwhere K = nR denote the number of information bits in the message. Let G be a matrix of dimensions K ×n, and let each entry in G be sampled independently from an equiprobable Bernoulli distribution. It is useful to express\nwhere each g i ∈ {0, 1} K is a length K binary valued column vector. The transmitted sequence x T = [x 1 , . . . , x K ] at receiver 1 is expressed as:\nThe transmitted sequence y T at user 2 is deﬁned in a similar manner.\nGiven our speciﬁc encoder, the received symbol can be ex- pressed as:\n⎧ ⎪ ⎨ ⎪ ⎩\n(b T 1 ⊕ b T 2 )g i , s i = 0, b T 1 g i , \t s i = 1, b T 2 g i , \t s i = 2.\nOur proposed decoder only uses the output of the channel when s i = 0 and declares erasures if s i = 0. Let ˆ G 0 = G |s i =0 be collection of column vectors in G when s i = 0. We use the following lemma regarding ˆ G 0 :\nLemma 1: For every δ > 0, there exists a function o n,δ (1) that goes to zero as n → ∞, such that following holds:\nThe proof of Lemma 1 is obtained by showing that, with high probability, each randomly selected column of ˆ G 0 is in a general position. We omit the proof. Clearly the receiver can uniquely recover (b T 1 ⊕ b T 2 ) from\nif ˆ G 0 has full row-rank, which holds if R ≤ 1 − 2ε − δ. Since δ > 0 is arbitrary this establishes our ﬁrst lower bound.\nB. Achievability of R = 1/2: Decode and Forward Approach The rate R = 1/2 is achieved by transmitting both w 1\nand w 2 to the destination instead of taking advantage of the fact that the destination only requires w 1 ⊕ w 2 . The multiple access capacity region is given by the convex hull of rate pairs (R 1 , R 2 ) that satisfy:\nR 1 ≤ I(x; z, s|y) \t (27) R 2 ≤ I(y; z, s|x) \t (28)\nTaking x and y to be independent equiprobable binary sym- bols we get that MAC Capacity region contains R 1 ≤ 1 − ε, R 2 ≤ 1 − ε and R 1 + R 2 ≤ 1. Since ε < 1/2 the rate pair R 1 = R 2 = 1 2 is achievable. Thus each user can transmit w i at a rate of R = 1/2 to the destination. The destination then computes w 1 ⊕ w 2 .\nRemark 2: The rate R = 1/2 can be achieved using a decode and forward scheme even when the two transmitters use identical codebooks. As established in [3], in addition to (27)-(29), an additional constraint\nmust be satisﬁed when identical codebooks are used. Thus the achievable rate now reduces to R = min(1/2, 2ε). Note that with with identical codebooks, the rate R = 1/2 is achievable for ε > 1/4, the region in which decode and forward dominates compute and forward discussed before.\nAchieving R = 1/2 with Compute and Forward: The rate R = 1/2 can also be achieved using identical linear codes if the receiver does not ignore the output when s i = 0. Let Let ˆ G 0 = G |s i =0 , ˆ G 1 = G |s i =1 and ˆ G 2 = G |s i =2 be the projections of G onto the indices where s i = 0, s i = 1 and s i = 2 respectively. Following (24), we let z T C = (b T 1 +b T 2 ) ˆ G 0 , z T A = b T 1 ˆ G 1 and z T B = b T 2 ˆ G 2 . Since the column vectors in ˆ G 1 and ˆ G 2 are independently sampled it can be shown that\nfor any δ > 0, with a probability that exceeds 1 − o n,δ (1), we have that\nwhere A is a full-matrix of dimension n × d 12 . The receiver ﬁrst computes\nand then needs to compute b 1 ⊕ b 2 from (b 1 ⊕ b 2 ) T [ ˆ G 0 A]. Since the entries in ˆ G 0 and A are independent the rank of [ ˆ G 0 A] is, with high probability at-least n(d 12 + 1 − 2ε − δ). From (30) we can show that R = max( 1 2 , 1−2ε) is achievable.\nWe begin with some notation. For a given sequence s n A(s n ) = {i : s i = 1} and B(s n ) = {i : s i = 2}. Let C(s n ) = {i : s i = 0}. We let x n A(s n ) to be the projection of the sequence x n on the indices where s i = 1 and use a similar notation for other indices.\nSince the receiver decodes u = w 1 ⊕ w 2 from its output, from Fano\u2019s inequality, we have that\nfor some sequence ε n that goes to zero as n → ∞. Now consider\nnR = H(u) \t (34) = H(u|s n ) \t (35) = H(u|s n , y n B(s n ) ) \t (36) = nδ n + I(u; x n A(s n ) , z n C(s n ) |s n , y n B(s n ) ) \t (37)\nwhere (35) follows from the fact that the message u is independent of the sequence s n , equation (36) follows from the fact that u = w 1 ⊕ w 2 is independent of w 2 and hence also independent of y n , and equation (37) follows from the chain rule of mutual information and the application of Fano\u2019s inequality,\nwhere (39) follows from the fact that conditioning reduces entropy whereas (40) follows from the fact that both x n and z n are binary sequences and (41) follows from the fact s n is\nsampled i.i.d. from a distribution with Pr(s = 0) = 1 − 2ε and Pr(s = 1) = Pr(s = 2) = ε.\n(42) In this paper we only provide the complete converse when\nPr(s = 0) = Pr(s = 1) = Pr(s = 2) = 1 3 . The proof for the general case also uses (42) as a starting point and will appear elsewhere.\nFor a sequence s n we ﬁrst consider a permutation function π(s n ) such that A(s n ) = C(π(s n )), B(s n ) = B(π(s n )) and C(s n ) = A(π(s n )). Note that Pr(s n = s n ) = Pr(s n = π(s n )) = 1 3 n . We establish the following lower bound:\nwhere (44) follows from the construction of the permutation function π (·), (45) follows from the fact that conditioning reduces entropy and the chain rule, (46) follows from the fact that z n = x n ⊕ y n and for any random variable A we have that H (A) = H(f (A)) if the function f (·) is one-to- one, while (48) follows from the fact that u = w 1 ⊕ w 2 is independent of w 2 and hence of y n .\nnR ≤ n(1 − ε) + nδ n − 1\nTo complete our analysis, we consider the upper bound by revealing w 1 to the decoder.\nnR = H(w 1 ⊕ w 2 ) = H(w 2 |w 1 ) \t (52) = H(w 2 |w 1 , s n ) \t (53) = H(w 2 |w 1 , s n , y n ) + I(y n ; w 2 |w 1 , s n ) \t (54) ≤ nδ n + H(y n |s n ) \t (55)\n(56) where (52) follows from the fact that w 1 and w 2 are mutually\nindependent, and (53) follows from the fact that s n is indepen- dent of (w 1 , w 2 ) and (55) follows since the message w 1 ⊕ w 2 can be recovered if the receiver is revealed (w 1 , y n , s n ) and hence Fano\u2019s inequality implies, H (w 2 |w 1 , s n , y n ) ≤ nδ n .\nCombining (56) with (51) we get 3\nSince δ n → 0 as n → ∞, this establishes that R ≤ 2−ε 3 , as required.\nWe provide a sketch of the achievable rate with feedback stated in Prop. 2. We use a two phase protocol. In the ﬁrst phase encoders 1 and 2 transmit b 1i and b 2i respectively for i = 1, 2 . . . , n. For those indices where s i = 0 the receiver obtains b 1i ⊕ b 2i . Among the remaining indices users 1 and 2 construct ˆ w 1 = {b 1j } j:s j =2 and ˆ w 2 = {b 2j } j:s j =1 . In the second phase, the messages ˆ w 1j and ˆ w 2j are transmitted to the destination using a multiple access channel code. By computing the capacity region of the associated multiple access channel (c.f. (27)-(29)), it can be veriﬁed that the number of channel uses in this phase is ≈ 2nε. Thus the total rate is ≈ n n+2nε = 1 1+2ε as required.\nFig. 1 provides a numerical computation of the upper and lower bounds for the Erasure MAC channel both with and without feedback. The upper-most dotted curve corresponds to R + FB = 1 − ε and is the upper bound on the capacity with feedback. The lowermost curve, marked with backward arrows, is the lower bound achieved by either the decode and forward or the compute and forward schemes. The other solid\ncurve is our new upper bound on the capacity without feedback (c.f. Theorem 1). The fourth curve is the lower bound with feedback in Prop. 2. Interestingly we see that it lies above the upper bound for certain values of ε, thus establishing that feedback helps in computation over the erasure multiple access channel.\nWe study computation of modulo-sum of two messages over a multiple access channel with erasures. Unlike the Gaussian model, our proposed this model does not have a suitable structure to directly compute the modulo-sum. Our main result is an upper bounding technique that reduces the setup to a compound multiple access channel. Using this bound we establish that a simple ARQ type feedback can increase the modulo-sum capacity. In the full paper we will report a complete derivation of the upper bound and other extensions such as the case of lossy recovery.\nWhile computation over Gaussian networks has received a signiﬁcant interest in recent years, we understand little about fundamental limits of computation over other channel models. We expect the ideas involved in this paper to be also applicable to other channel models."},"refs":[{"authors":[{"name":"S. Agrawal"},{"name":"S. Vishwanath"}],"title":{"text":"On the secrecy rate of interference networks using structured codes"}},{"authors":[{"name":"X. He"},{"name":"A. Yener"}],"title":{"text":"Providing secrecy with structured codes: Tools and applications to two-user Gaussian channels"}},{"authors":[{"name":"B. Hern"},{"name":"K. Narayanan"}],"title":{"text":"Multilevel coding schemes for compute- and-forward"}},{"authors":[{"name":"J. Korner"},{"name":"K. Marton"}],"title":{"text":"How to encode the modulo-two sum of binary sources (corresp.)"}},{"authors":[{"name":"D. Krithivasan"},{"name":"S. Pradhan"}],"title":{"text":"Lattices for distributed source coding: Jointly gaussian sources and reconstruction of a linear function"}},{"authors":[{"name":"B. Nazer"},{"name":"M. Gastpar"}],"title":{"text":"Computation over multiple-access channels"}},{"authors":[],"title":{"text":"Compute-and-forward: Harnessing interference through struc- tured codes"}},{"authors":[{"name":"T. Oechtering"},{"name":"E. Jorswieck"},{"name":"R. Wyrembelski"},{"name":"H. Boche"}],"title":{"text":"On the optimal transmit strategy for the MIMO bidirectional broadcast channel"}},{"authors":[{"name":"T. Philosof"},{"name":"R. Zamir"},{"name":"U. Erez"},{"name":"A. Khisti"}],"title":{"text":"Lattice strategies for the dirty multiple-access channel"}},{"authors":[{"name":"A. Sahebi"},{"name":"S. Pradhan"}],"title":{"text":"On the capacity of abelian group codes over discrete memoryless channels"}},{"authors":[{"name":"M. P. Wilson"},{"name":"K. Narayanan"},{"name":"H. Pﬁster"},{"name":"A. Sprintson"}],"title":{"text":"Joint physical layer coding and network coding for bi-directional relaying"}},{"authors":[{"name":"R. \t Zamir"}],"title":{"text":"Anti-structure \t problems"}},{"authors":[{"name":"J. Zhang"},{"name":"U. Erez"},{"name":"M. Gastpar"},{"name":"B. Nazer"}],"title":{"text":"MIMO compute-and- forward"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566711.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S17.T2.3","endtime":"16:00","authors":"Ashish Khisti, Brett Hern, Krishna Narayanan","date":"1341589200000","papertitle":"On Modulo-Sum Computation over an Erasure Multiple Access Channel","starttime":"15:40","session":"S17.T2: Communication and Computation over Multiple Access Channels","room":"Kresge Auditorium (109)","paperid":"1569566711"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
