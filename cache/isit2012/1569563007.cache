{"id":"1569563007","paper":{"title":{"text":"Quantization Effect On Second Moment Of Log-Likelihood Ratio And Its Application To Decentralized Sequential Detection"},"authors":[{"name":"Yan Wang"},{"name":"Yajun Mei"}],"abstr":{"text":"Abstract\u2014It is well known that quantization cannot increase the Kullback-Leibler divergence which can be thought of as the expected value or ﬁrst moment of the log-likelihood ratio. In this paper, we investigate the quantization effects on the second moment of the log-likelihood ratio. It is shown that quantization may result in an increase in the case of the second moment, but the increase is bounded above by 2/e. The result is then applied to decentralized sequential detection problems to provide a simpler sufﬁcient condition for asymptotic optimality theory, and the technique is also extended to investigate the quantization effects on other higher-order moments of the log-likelihood ratio and provide lower bounds on higher-order moments."},"body":{"text":"In information theory and statistics, the Kullback-Leibler divergence is a fundamental quantity that characterizes the difference between two probability distributions P 1 and P 0 of a random variable X. Denote by f 1 (x) and f 0 (x) the densities of P 1 and P 0 with respect to some common underlying probability measure µ, then the Kullback-Leibler divergence from P 1 to P 0 is\nf 1 (X) f 0 (X)\nand E 1 means taking the expectation over the distribution P 1 . In some applications, we need to deal with the quantized\nversion of X that is a function of X and often is required to belong to a ﬁnite alphabet. Denote by Y = ϕ(X) the quan- tized observation, and let P ϕ i and f i (y; ϕ) be the probability distribution and probability mass (or density) function of Y\nwhen X is distributed according to P i for i = 0 or 1. Then the Kullback-Leibler divergence of the quantized observation Y is\nAn important property is that quantization cannot increase the Kullback-Leibler divergence, that is,\nwith equality if and only if Y = ϕ(X) is a sufﬁcient statistic of X, see Theorem 4.1 of Kullback and Leibler [2]. This is consistent with our intuition that Y = ϕ(X) is generally less informative than the X itself. Note that the inequality (1), which will be referred to as Kullback-Leibler\u2019s inequality below, deals with the expected value or ﬁrst moment of the log-likelihood ratio.\nIn this paper, we extend the Kullback-Leibler inequality (1) to investigate the quantization effects on the second or other higher moments of the log-likelihood ratio. Our re- search is motivated by the decentralized sequential detec- tion problem where there are unobservable random variables X k,n ; k = 1, . . . , K; n = 1, 2, . . . , and where what is actually observed are their quantized versions Y k,n = ϕ k,n (X k,n ). In such a problem, one wants to design appropriate quantization functions ϕ k,n \u2019s so as to utilize the Y k,n \u2019s to make the best possible decision (under a proper criterion). Intuitively, it is natural to choose ϕ k,n to maximize the Kullback-Leibler divergence of the quantized observations I ϕ (f 1,k , f 0,k ) (or possibly I ϕ (f 0,k , f 1,k )) if the X k,n \u2019s are independent with density f 1,k or f 0,k for each k. In order to guarantee that the corresponding statistical procedures are indeed efﬁcient, one needs to verify that the distribution of quantized observations Y k,n satisﬁes some standard regularity conditions, and one of them is that the second moments of the log-likelihood ratios have a common upper bound over a class of allowable quan- tization functions ϕ k,n \u2019s, or equivalently, that the variances of quantized observations are bounded above.\nUnfortunately, it can be analytically challenging or in- tractable to verify the assumptions for quantized observations Y k,n directly, even if the distributions of the unobservable random variable X k,n are known to belong to some simple families of distributions, as one may have a large class of quantization functions ϕ k,n to work with when ﬁnding the optimal one. To overcome such a difﬁculty, in this paper we show that one could dispense with quantization and only look at the second moment associated with the raw data X k,n \u2019s since the boundedness of the unquantized version implies the boundedness of the quantized version regardless of the quantization function ϕ k,n . Also see Le Cam and Yang [4] for similar approaches in other contexts.\nThe remainder of the paper is organized as follows. In Section II we extend the Kullback-Leibler\u2019s inequality (1) to\nthe second moment of the log-likelihood ratio, and Section III discusses an application our result in the decentralized sequential detection problems. In Section IV, we extend our result to investigate the quantization effects on other higher- order moments of the log-likelihood ratio.\nFor the X and Y = ϕ(X), deﬁne their respective second moments of log-likelihood ratios as\nwhere Z and Z ϕ are the log-likelihood ratios of X and Y. Our main result of this section is the following theorem.\nProof: Let L = e Z = f 1 (X)/f 0 (X) and L ϕ = e Z ϕ = f 1 (Y ; ϕ)/f 0 (Y ; ϕ) be the likelihood ratios. To simplify no- tation, let E 1 {·|Y } denote the conditional expectation with respect to a given value of the quantization observation Y = ϕ(X), then it is easy to see that\nRecall that in the proof of the Kullback-Leibler\u2019s inequality (1), one uses heavily the fact that the function H(t) = − log t is convex when t > 0. Then, by Jensen\u2019s inequality,\nZ ϕ = log L ϕ = H(L −1 ϕ ) = H(E 1 (L −1 |Y )) ≤ E 1 (H(L −1 ) |Y ) = E 1 (Z |Y ),\nand the Kullback-Leibler\u2019s inequality (1) is proved by taking expectations under P 1 on both sides.\nUnfortunately, the above approach fails for the second mo- ment case since the function H 2 (t) = ( − log t) 2 = (log t) 2 is no longer convex (nor is it concave). Fortunately, this approach can be salvaged by what we call the \u201cconvex domination\u201d approach, i.e., by ﬁnding a convex function that is larger, but not too much larger, than H 2 (t) = (log t) 2 .\nTo do so, taking derivatives of the function H 2 (t) = (log t) 2 leads to\nThus H 2 (t) = (log t) 2 is convex on t ≤ e but is concave on t ≥ e. Hence, if we consider the following new function\nH 2 (t) = (log t) 2 , \t if 0 < t ≤ e H 2 (e) + H \u2032 2 (e)(t − e) = 2 e t − 1, if t > e\nthen it is clear that ˜ H 2 (t) is a continuous convex function of t when t ≥ 0. Moreover, the concavity of H 2 (t) on t ≥ e implies that ˜ H 2 (t) dominates H 2 (t).\nNow by the deﬁnitions of H 2 (t), ˜ H 2 (t) and V ϕ (f 1 , f 0 ), we have\nwhere the ﬁrst inequality follows from H 2 (t) ≤ ˜ H 2 (t), and the second inequality is to apply Jensen\u2019s inequality to the convex function ˜ H 2 (t).\nCombining the above inequalities yields (3), completing the proof of our theorem.\nIt is useful to provide some comments to better understand our theorem. First, the discrete version of the Kullback- Leibler\u2019s inequality (1) is the well-known log-sum inequality: for non-negative numbers a 1 , . . . , a n and b 1 , . . . , b n , denote the sum of all a i \u2019s by a and the sum of all b i \u2019s by b, and then we have\nwith equality if and only if a i /b i are constant. Meanwhile, the discrete version of our result in (3) becomes that\n( log a b\nNote that the extra term on the right side is 2b/e instead of 2/e as we do not put any normalization conditions on a or b.\nSecond, a comparison of (1) and (3) shows that we have an extra constant term 2/e for the second moment case, and thus it is natural to ask whether or not the term can be eliminated, i.e., whether it is always true that V ϕ (f 1 , f 0 ) ≤ V (f 1 , f 0 ). The following counterexample provides a negative answer. Suppose that the X takes three distinct values 0, 1, 2 with probabilities 29/36, 1/9, 1/12 under P 1 and equal probabil- ities 1/3 under P 0 . Let ϕ be a function with a binary range {0, 1} such that ϕ(0) = 0, ϕ(1) = ϕ(2) = 1. Then it is easy to verify that V (f 1 , f 0 ) = 0.9215 ≤ V ϕ (f 1 , f 0 ) = 0.9224. More generally, other counterexamples can be easily found by choosing two distributions P 1 and P 0 of X, both of\nwhich are supported on n + 1 (n ≥ 2) points x 0 , . . . , x n such that the likelihood ratio L 0 = f 0 (x 0 )/f 1 (x 0 ) < e and L i = f 0 (x i )/f 1 (x i ) > e for i = 1, . . . , n with L 1 , . . . , L n being n distinct values. If we consider a quantization function ϕ that maps all x 1 , . . . , x n to a single point y 1 but maps x 0 to another point y 0 , then V (f 1 , f 0 ) ≤ V ϕ (f 1 , f 0 ), since the function H 2 (t) = (log t) 2 is strictly concave on t ≥ e. In other words, unlike the case of Kullback-Leibler\u2019s inequality (1), a quantization indeed can increase the second moment of the log-likelihood ratio. Fortunately, our theorem shows that such an increase is at most 2/e.\nThe problem that motivated us to write the present paper arises from decentralized sequential detection problems, see, Veeravalli, Basar and Poor [17], and Veeravalli [13], [15]. Fig. 1 depicts a typical conﬁguration of a decentralized network system that consists of K geographically deployed local sensors S 1 , . . . , S K and a fusion center. At each time step n = 1, 2, . . . , each local sensor S k observes a raw data X k n and sends a quantized message U k n to the fusion center, which makes a ﬁnal decision when stopping taking observations. Due to constraints on communication bandwidths or requirements of reliability, the local sensors are required to compress the raw data to quantized sensor messages U k n \u2019s, which all belong to ﬁnite alphabets, say {0, 1, . . . , l k − 1} respectively. In other words, the fusion center has no direct access to the raw observations and has to make its decisions based on the quantized sensor messages. If necessary, the fusion center can send feedback, V k n , to the local sensors to adaptively adjust the local quantization so as to achieve maximum efﬁciency.\nThere are many possible useful setups for the decentralized network system, and one widely used setup is the system with limited local memory and full feedback, or Case E in Veeravalli, Basar and Poor [17]. Mathematically, in such a system, the quantized sensor message sent from the local sensor S k to the fusion center at time n is\nwhere F n −1 = {U k=1,...,K [1,n −1] } (here U k [1,n] = {U k 1 , . . . , U k n } ) denotes all past sensor messages at the fusion center.\nIn the simplest version of decentralized quickest change detection problems, we assume that an event occurs to the network system at some unknown time ν, and changes the probability measure of the raw data X k n from P 0 (with density f k 0 for observations X k n ) to P 1 (with density f k 1 ). Furthermore, we assume that the observations are independent over time and from sensor to sensor, conditional on each hypothesis on the possible change-time ν = 1, 2, 3, . . . or ν = ∞ (no change). The objective is to jointly optimize the policies at the local sensors and fusion center levels so as to detect the change as soon as possible subject to a constraint on the false alarm rate.\nA crucial challenge in decentralized quickest change detec- tion is which kind of local quantizers should be used at each local sensor. This is easy if one further assumes that each local sensor uses a stationary local quantizer, as the corresponding problem reduces to the classical centralized case and various well-developed optimal or asymptotic optimal theories are applicable, see for example Lorden [5], Moustakides [8], Page [9], Pollak [10], Shiryaev [11] and [12], etc. In fact, it is not difﬁcult to see that the optimal stationary quantizer ϕ ∗ for any local sensor S k is the one that maximizes the local Kullback-Leibler divergence I ϕ (f k 1 , f k 0 ), and it can be shown that such an optimal quantizer ϕ ∗ is a Monotone Likelihood Ratio Quantizer (MLRQ), see, for example, Tsitsiklis [14], Crow and Schwartz [1], Tartakovsky and Veeravalli [13].\nOn the other hand, the scenario becomes more complicated if the local quantizers are allowed to be non-stationary. By comparing with Bayes procedures, Veeravalli [16] conjectures that the schemes based on the optimal stationary MLRQ ϕ ∗ are asymptotically optimal regardless whether the quantizers are stationary or not. While this conjecture sounds reasonable as maximizing the Kullback-Leibler divergence seems to be natural to construct optimal local quantizers, it is very chal- lenging to prove or disprove it, partly because the regularity conditions of the quantized observations are needed to do any reasonable asymptotic analysis.\nSome sufﬁcient conditions under which this conjecture holds are available in the literature. By Lai [3], this conjecture is true under the following sufﬁcient condition:\nwhere P (ν) is the probability measure when the change occurs at time ν, Z k i,ϕ is the likelihood ratio for the quantized data U k i , i.e.,\nI k max with I k max = sup ϕ I(f k 1 , f k 0 ; ϕ). Here f k m (u; ϕ k i ) is probability mass function, i.e.,\nUnfortunately, condition (5) involves all possible non- stationary quantizers, and it is too complicated to be veriﬁed\nit directly. By using Kolmogorov\u2019s inequality for martingales, Mei [6] provides a simpler though stronger sufﬁcient condi- tion, and shows that the conjecture holds if there is a uniform bound on the second moments of the log-likelihood ratios of quantized observations. Speciﬁcally, Mei [6] showed that condition (5) holds if for all k = 1, . . . , K,\nwhere V ϕ is deﬁned in (2). While it was shown in [6] that re- lation (6) holds when the quantized messages belong to binary sensor messages with l = 2 and when f 0 and f 1 belong to the same one-parameter exponential family satisfying certain restrictions, it is still an open problem whether condition (6) holds in general or not, especially when the quantizers can have arbitrary forms and belong to the inﬁnite dimensional functional space.\nTo verify (6) in more general scenarios, given the inequality in Theorem 1, it is enough to consider the second moment associated with the raw data. To be more speciﬁc, by Theorem 1, if for all k = 1, . . . , K, for the raw observations, we have\n(x) f k 0 (x)\nthen condition (6) holds and so does (5). Note that condition (7) only deals with the densities of raw observations and does not involve the stationary or non-stationary quantizers. More- over, it is a standard assumption in the statistical literature as a regularity condition for the raw density functions. Therefore, condition (7) provides a simple but useful sufﬁcient condition under which the long-standing conjecture of asymptotic opti- mality of the schemes with the optimal stationary MLRQ ϕ ∗ is true regardless whether the quantizers are stationary or not.\nSimilarly, our results can also be applied to the problem of decentralized sequential hypotheses testing, see Veeravalli, Basar and Poor [17]. This problem is similar to the above- mentioned decentralized quickest change detection problem except that the distributions of the raw data do not change over time. In other words, we have two simple hypotheses H 0 and H 1 regarding the distributions of X k n \u2019s, and conditional on each of these two hypotheses, the raw observations {X k n } form i.i.d. sequences over time n and are independent among different sensors. The objective here is to use as few samples as possible to correctly decide which of these two simple hypotheses is true. An optimal sequential test is one that balances the trade-off between the average sample size under each hypothesis and the probabilities of making Type I and II errors, see Veeravalli, Basar and Poor [17], Veeravalli [15] and Mei [7] for more details.\nUnlike the quickest change detection problem, non- stationary quantizers are generally needed in order to develop asymptotically optimal decentralized sequential tests. This is because each local sensor will have two kinds of optimal stationary quantizers: one maximizes I ϕ (f k 0 , f k 1 ) (if H 0 is true) and the other maximizes I ϕ (f k 1 , f k 0 ) (if H 1 is true), due to the asymmetric properties of the Kullback-Leibler divergences.\nDenote them by ϕ k opt,0 and ϕ k opt,1 respectively. To develop a simple but asymptotically optimal decentralized sequential tests, Mei [7] introduces the concept of \u201ctandem quantizers\u201d where the test procedure is divided into two stages. In the ﬁrst stage, any reasonable stationary quantizer is used and the network system makes a preliminary decision about which of two hypothesis is likely to be true. Then at the second stage, each local sensor switches to one of two optimal stationary quantizers ϕ k opt,0 or ϕ k opt,1 , based on the preliminary decision.\nIt was shown in Mei [7] that under the condition (6) together with the symmetric condition that sup ϕ V ϕ (f k 0 , f k 1 ) < ∞, the proposed two-stage tests with the tandem quantizers are asymptotically optimal among all decentralized sequential tests with or without stationary quantizers. By Theorem 1, the asymptotic optimality theory in Mei [7] holds under a simpler sufﬁcient condition: the proposed two-stage tests are asymp- totically optimal as long as the distributions of raw sensor observations satisfy V (f k 1 , f k 0 ) < ∞ and V (f k 0 , f k 1 ) < ∞ for all k = 1, . . . , K. In particular, they are asymptotically optimal when the raw observations are normally or exponentially distributed.\nThe convex domination technique we developed in proving Theorem 1 can have a broad application. In this section, it is applied to deal with some properties of the higher-order moments of the log-likelihood ratios.\nFor a positive integer j = 1, 2, . . . , deﬁne the j-th moment of the log-likelihood ratios as\nlog f 1 (y; ϕ) f 0 (y; ϕ)\nwhere Z and Z ϕ are the log-likelihood ratios of X and Y. It turns out that we need to consider two different cases, depending on whether j is even or odd.\nWhen j ≥ 1 is even, it is straightforward to apply our technique in Theorem 1. Below we will present a more general result on the α-moments of the absolute values of the log- likelihood ratios Z and Z ϕ for any real number α ≥ 1.\nTheorem 2. Deﬁne ˜ W α (f 1 , f 0 ) = E 1 {|Z| α } and ˜ W ϕ,α (f 1 , f 0 ) = E 1 {|Z ϕ | α } . Then for any α ≥ 1,\ne α −1 \t > 0 \t (10) and C 1 = 1 by convention that 0 0 = 1.\nProof: The proof is identical to that of Theorem 1, except considering the convex function\nNote that Theorem 2 describes the quantization effects on the j-th moment of log-likelihood ratios when the integer j ≥ 1 is even, and includes Theorem 1 as a special case with α = 2. A simple calculation shows that C 2 = 2/e ≈ 0.7358 and C 4 = 108/e 3 ≈ 5.3770. In addition, it is interesting to see that E 1 {Z ϕ } ≤ E 1 {Z} (by the Kullback-Leibler\u2019s inequality) but E 1 |Z ϕ | ≤ E 1 |Z| + 1 (by Theorem 2 with α = 1). Moreover, it is generally not true that ˜ W ϕ,α (f 1 , f 0 ) ≤ ˜ W α (f 1 , f 0 ), and counterexamples can be found by exploring the fact that for any α ≥ 1, H α (t) = | log t| α is strictly concave when t ≥ t α .\nNow let us focus on the case when the integer j ≥ 1 is odd. In order to establish the relationship between W j (f 1 , f 0 ) and W ϕ,j (f 1 , f 0 ) deﬁned in (8), it turns out that when j ≥ 1 is odd, we need to deﬁne another constant C ∗ j to be the only real number x ≥ 0 that satisﬁes the equation\nwhere C j is deﬁned in (10). A simple numerical calculation shows that C ∗ 1 = 0 and C ∗ 3 ≈ 3.6518.\nThe following theorem describes the quantization effects on the higher-order moments of the log-likelihood ratios, and includes the Kullback-Leibler\u2019s inequality (1) as a special case.\nTheorem 3. For any measurable function ϕ, if the integer j ≥ 1 is odd, we have\nProof: Fix the odd integer j ≥ 1, by taking derivatives, it is easy to see that H(t) = ( − log t) j is convex on 0 < t ≤ 1 or t ≥ e j −1 but is concave when 1 ≤ t ≤ e j −1 . Let t 0 = e j −1 , then H(t) ≤ H(t 0 ) + H \u2032 (t 0 )(t −t 0 ) = −C j t + d j for 1 ≤ t ≤ t 0 , where C j > 0 is deﬁned in (10) and d j = (j − 1) j −1 ≥ 0. Now the line y = −C j t + d j intersects the curve y = H(t) = ( − log t) j at two points: one is t = t 0 = e j −1 ≥ 1 and the other is in the interval (0, 1] and denoted by t ∗ ≤ 1. Therefore, we can construct a convex function as follows:\n    \n−C j t + d j , \t if t ∗ ≤ t < t 0 = e j −1 ; H(t) = ( − log t) j , if t ≥ t 0 ( ≥ 1).\nClearly, by our construction, the convex function ˜ H(t) is continuous and dominates H(t) = ( − log t) j .\nNext, we claim that 0 ≤ ˜ H(t) − H(t) ≤ C ∗ j for all t > 0, where C ∗ j is deﬁned through (11). In other words, the dif- ference between ˜ H(t) and H(t) = ( − log t) j is insigniﬁcant. To prove this claim, ﬁrst note that C ∗ j = H(t ∗ ) ≥ 0 and it sufﬁces to prove the claim when t ∗ ≤ t < t 0 , i.e., when ˜ H(t) is decreasing as it is a linear function with negative slope. The proof needs to consider two scenarios, depending on whether t ≤ 1 or ≥ 1. If t ∗ ≤ t ≤ 1, then the claim clearly holds since ˜ H(t) ≤ ˜ H(t ∗ ) = C ∗ j and H(t) ≥ 0. On the other hand,\nif 1 ≤ t < t 0 , then by taking derivatives, ˜ H(t) − H(t) is a decreasing function and thus\nThus, for all t > 0 we have 0 ≤ ˜ H(t) − H(t) ≤ C ∗ j , and our claim is proved.\nThe remaining proof of Theorem 3 can then be proceeded along the same line as in Theorem 1 and thus omitted.\nClearly, the convex domination method we developed allows one to deal with non-convex functions and can have broad applications. As an illustration, it can also be used to establish lower bound for W j (f 1 , f 0 ) and W ϕ,j (f 1 , f 0 ) in (8) for a given odd integer j ≥ 1. The proof is omitted due to the page limit.\nTheorem 4. For an integer j ≥ 1, the j-th moments of log-likelihood ratios, W ϕ,j (f 1 , f 0 ) and W j (f 1 , f 0 ), have a lower bound 0 if j is even, and have a lower bound −j(j − 1) j −1 /e j −1 − (j − 1) j if j is odd.\nThis research was supported in part by grants from the AFOSR grant FA9550-08-1-0376 and the NSF grants CCF- 0830472 and DMS-0954704."},"refs":[{"authors":[{"name":"R. W. Crow"},{"name":"S. C. Schwartz"}],"title":{"text":"Quickest detection for sequential decen- tralized decision systems"}},{"authors":[{"name":"S. Kullback"},{"name":"R. A. Leibler"}],"title":{"text":"On information and sufﬁciency"}},{"authors":[{"name":"T. L. Lai"}],"title":{"text":"Information bounds and quick detection of parameter changes in stochastic systems"}},{"authors":[{"name":"L. Le Cam"},{"name":"G. Yang"}],"title":{"text":"On the preservation of local asymptotic normality under information loss"}},{"authors":[{"name":"G. Lorden"}],"title":{"text":"Procedures for reacting to a change in distribution"}},{"authors":[{"name":"Y. Mei"}],"title":{"text":"Information bounds and quickest change detection in decentral- ized decision systems"}},{"authors":[{"name":"Y. Mei"}],"title":{"text":"Asymptotic optimality theory for decentralized sequential hy- pothesis testing in sensor networks"}},{"authors":[{"name":"G. V. Moustakides"}],"title":{"text":"Optimal stopping times for detecting changes in distributions"}},{"authors":[{"name":"E. S. Page"}],"title":{"text":"Continuous inspection schemes"}},{"authors":[{"name":"M. Pollak"}],"title":{"text":"Optimal detection of a change in distribution"}},{"authors":[{"name":"A. N. Shiryayev"}],"title":{"text":"On optimum methods in quickest detection problems"}},{"authors":[],"title":{"text":"Optimal Stopping Rules"}},{"authors":[{"name":"A. G. Tartakovsky"},{"name":"V. V. Veeravalli"}],"title":{"text":"An efﬁcient sequential procedure for detecting changes in multichannel and distributed systems"}},{"authors":[{"name":"J. N. Tsitsiklis"}],"title":{"text":"Extremal properties of likelihood ratio quantizers"}},{"authors":[{"name":"V. V. Veeravalli"}],"title":{"text":"Sequential decision fusion: theory and applications"}},{"authors":[],"title":{"text":"Decentralized quickest change detection"}},{"authors":[{"name":"V. V. Veeravalli"},{"name":"T. Basar"},{"name":"H. V. Poor"}],"title":{"text":"Decentralized sequential detection with a fusion center performing the sequential test"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569563007.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S2.T8.1","endtime":"11:50","authors":"Yan Wang, Yajun Mei","date":"1341228600000","papertitle":"Quantization Effect On Second Moment of Log-Likelihood Ratio and Its Application to Decentralized Sequential Detection","starttime":"11:30","session":"S2.T8: Distributed Detection and Estimation","room":"Stratton (491)","paperid":"1569563007"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
