{"id":"1569566129","paper":{"title":{"text":"Lattice Coding and the Generalized Degrees of Freedom of the Interference Channel with Relay"},"authors":[{"name":"Anas Chaaban"},{"name":"Aydin Sezgin"}],"abstr":{"text":"Abstract\u2014The generalized degrees of freedom (GDoF) of the symmetric two-user Gaussian interference relay channel (IRC) is studied. While it is known that the relay does not increase the DoF of the IC, this is not known for the more general GDoF. For the characterization of the GDoF, new sum-capacity upper bounds and lower bounds are derived. The lower bounds are obtained by a new scheme, which is based on functional decode- and-forward (FDF). The GDoF is characterized for the regime in which the source-relay link is weaker than the interference link, which constitutes half the overall space of channel parameters. It is shown that the relay can indeed increase the GDoF of the IRC and that it is achieved by FDF."},"body":{"text":"The exact characterization of the capacity of interference networks is an open problem for several decades now. Given the difﬁculty of the problem, there is shift of paradigm to provide an approximate characterization of the capacity, referred to as the degrees of freedom (DoF), which gets asymp- totically tight for high signal-to-noise power ratios (SNR). While the DoF provides interesting insights into the behaviour of the system, the so-called generalized degrees of freedom, or GDoF [1], is a much more powerful metric, as it allows different signal strengths and thus captures a large variety of scenarios.\nThe setups gets even more interesting for cases, in which some of the nodes are dedicated relays. It is known that relaying in wireless networks can play a vital role in improving its performance in terms of coverage and achievable rates. As such, a relay can help the network by establishing cooperation between the nodes in the network. Interestingly enough, the ca- pacity of even the basic point-to-point (P2P) relay channel [2] (without interference) is an open problem, although there exist good approximations of the capacity of the Gaussian P2P relay channel within one bit [3].\nThe improvements obtained depend heavily on the capa- bility of and restrictions at the relay, such as cognition and causality. For example, for a network with two transmitters, two receivers, and a relay referred to as the interference relay channel (IRC), several capacity bounds have been derived with a causal or with a cognitive relay [4]\u2013[10]. As for the relay and the interference channel (IC) individually, the capacity of the IRC remains an open problem.\nSurprisingly, in characterizing the gains in terms of DoF by deploying a relay in a wireless interference network, it was shown in [11] that relaying does not increase the DoF. As a\nconsequence of this result, the DoF of the IRC is the same as that of the IC, i.e., DoF=1. One question which immediately arises with this result is whether this is also true in terms of GDoF.\nIn this paper, we investigate the GDoF for the symmet- ric Gaussian IRC. Shortly, our contribution includes deriv- ing new upper bounds on the sum-capacity, providing new achievable sum-rates by proposing a \u201cfunctional decode-and- forward\u201d [12] (FDF) scheme. The distinct feature of the achievable strategy is that the overall message is split in three parts, namely a private, a common and a cooperative public part. While the former two are in use already in the basic IC, the latter one, encoded using nested lattices, is of particular value to overcome the multiple-access-bottleneck at the relay. We characterize the GDoF of the IRC for all cases in which the interference link is stronger than the source-relay link. This characterized regime covers half the space of all possible channel parameters for the IRC, and is especially interesting for the IRC with weak interference. It turns out that while a relay does not increase the DoF of the IC, it does increase its GDoF. In the next section, we formally deﬁne the IRC and the notation used in the paper.\nIn the symmetric Gaussian IRC (Fig. 1), transmitter i, i ∈ {1, 2}, has a message m i uniformly distributed over the set M i {1, . . . , 2 nR i }, to be sent to receiver i. The message m i is encoded into an n-symbol codeword X n i , where X ik is a real valued random variable, and transmits this codeword. At time instant k, the input-output equations of this setup are given by\ny ik = h d x ik + h c x jk + h r x rk + z ik , y rk = h sr x 1k + h sr x 2k + z rk .\nfor i = j, i, j ∈ {1, 2}. The coefﬁcients h d , h c , h r , h sr ≥ 0 are real valued channel gains, and x rk is the transmit signal at the relay at time instant k. The relay is causal, which means that x rk is a function of the previous observations at the relay, i.e., x rk = f r (y k− 1 r ). The source and relay signals must satisfy a power constraint E [X 2 i ] ≤ P , i ∈ {1, 2, r}. The receivers\u2019 additive noise z 1 , z 2 , and z r is Gaussian with zero mean and unit variance.\nAfter receiving y n i , receiver i uses a decoder to detect m i by processing y n i . The messages set, encoders, and decoders de- ﬁne a code denoted (2 nR 1 , 2 nR 2 , n), with an error probability P e deﬁned by P e = P ( ˆ m 1 = m 1 or ˆ m 2 = m 2 ). A rate pair (R 1 , R 2 ) is said to be achievable if there exists a sequence of (2 nR 1 , 2 nR 2 , n) codes such that P e → 0 as n → ∞. The capacity region C of the IRC is deﬁned as the closure of the set of these achievable rate pairs, and the sum-capacity C Σ is the maximum achievable sum-rate R Σ = R 1 + R 2 , i.e., C Σ = max (R 1 ,R 2 )∈C R Σ . The GDoF of the IRC is deﬁned as follows.\nDeﬁnition 1. Let the following variables represent the channel strength (as in [1])\nlog(h 2 d P ) . \t (2) Throughout the paper, we use x n to denote the length- n\nsequence (x 1 , . . . , x n ), and we use C(x) = (1/2) log(1 + x), C + (x) = max {0, C(x)}.\nThe main statement of the paper is characterizing the GDoF of the IRC for all cases where h 2 sr ≤ h 2 c . The GDoF in this case is given in the following theorem.\n       \n      \n       \n      \nFigures 2 and 3 show the GDoF for two examples of the IRC. The GDoF of the IC is also shown (dash-dotted) for comparison. The characterization of the GDoF in the shaded area, where α < γ, is not considered in this paper. The proof of this theorem is provided in the next section. Namely, in Section IV we provide the sum-capacity upper bounds that translate to this GDoF, in Section V we describe the scheme\nα d\nα d\nthat is used to achieve it, and in Section VI we sketch the proof of Theorem 1.\nWe start by providing the following bounds that can be obtained from the cut-set bounds [13] applied to the IRC. The cut-set bounds for the IRC are expressed in [4], [14].\nUsing the deﬁnition of β in (1) and the ﬁrst bound in Theorem 2, we can write C Σ ≤ 2C((|h d | + |h r |) 2 P ) ≤ 2 max{C(h 2 d P ), C((h 2 d P ) β )+2, which, by using (2) translates to the ﬁrst argument in the min in (3). The second argument in (3) can be obtained similarly from the second bound in Theorem 2. Using a similar method, the third and ﬁfth arguments in (3) can be obtained from the bounds in [14, Theorems 1 and 2]. The remaining expressions in (3) are obtained from the following theorem.\nC Σ ≤ C(2h 2 sr P ) + C(h 2 d P + h 2 c P ) + C + h 2 d /h 2 c − 1 (4) C Σ ≤ 2C(h 2 c /h 2 sr + (1 − h d /h c ) 2 ) + 2C(2h 2 sr P ). \t (5)\nDue to space limitations, we only provide a sketch of the proof of (4) in Appendix A. In the next section, we provide a GDoF achieving scheme for the IRC.\nUsing nested-lattice coding and lattice alignment, we estab- lish a cooperation strategy between the relay and the users. This scheme is denoted \u201cFunctional Decode-and-Forward\u201d (FDF) using the terminology of [12]. We use three kinds of messages in FDF, private (P), common (C), and cooperative public (CP) messages. The private and the common messages are the same as those used by Etkin et al. in the IC [1]. The CP message itself is also split into K sub-messages. The superposition of the CP messages is decoded by the relay, and forwarded to the destinations. Using backward decoding, the sum-rates given in the following theorems are achievable.\nTheorem 4. The sum-rate R Σ = 2(R p + R c + R cp ) is achievable where\nR (k) cp , K ∈ N, and where the constraints (9)-(11) on the next page are satisﬁed.\nTheorem 5. The sum-rate R Σ = 2(R c + R cp ) is achievable where R c ≤ min C min{h 2 d , h 2 c }P c , 1 2 C h 2 d P c + h 2 c P c , and R cp satisﬁes (12)-(14) on the next page, such that P c +\nProof: Due to the lack of space, we refer the reader to [15]\u2013[17] for more details about nested-lattice coding. In this work, we need nested-lattice codes with a ﬁne lattice Λ f and a coarse lattice Λ c ⊆ Λ f denoted (Λ f , Λ c ). The nested-lattice codewords are constructed as x n = (λ − d) mod Λ c where λ ∈ Λ f ∩ V(Λ c ) (V(.) for fundamental Voronoi region) and d is a random dither.\nFor a transmission block b, user 1 splits its message m 1 (b) into three parts, a private (P), a common (C), and a cooperative public (CP) [18] part denoted m 1,p (b), m 1,c (b), and m 1,cp (b), respectively. Moreover, the CP message is divided into K CP sub-messages m (k) 1,cp (b), k = 1, . . . , K. The rates of these messages are denoted R p , R c , R (1) cp , R (2) cp , . . . , R (K) cp .\nBrieﬂy, m 1,p (b) and m 1,c (b) are encoded into x n 1,p (b) and x n 1,c (b), respectively, where X 1,p ∼ N (0, P p ) and X 1,c ∼ N (0, P c ). Each CP message m (k) 1,cp (b) is encoded into x (k),n 1,cp (b) = (λ (k) 1,cp (b) − d (k) 1,cp ) mod Λ (k) c using a nested- lattice code (Λ (k) f , Λ (k) c ) with power P (k) cp . In order to satisfy\nthe power constraint, we set P p + P c + K k =1 P (k) cp = P . Same is done at transmitter 2, using the same nested-lattices. This enables the relay to decode the sum [17] u (k) (b) = λ (k) 1,cp (b) + λ (k) 2,cp (b) modulo Λ (k) c . The transmitters then send the superposition of their codes as\nat each block b ∈ {1, . . . , B − 1}. No messages are sent in block B. This incurs a rate loss which, however, becomes negligible for large B.\nIn this scheme, the relay only decodes the CP messages. More precisely, the relay decodes the superposition of CP messages as follows. The relay starts decoding at the end of block b = 1 where the sum u (k) (1) = (λ (k) 1,cp (1) + λ (k) 2,cp (1)) mod Λ (k) c is decoded, starting with k = 1 and ending with k = K (see successive compute-and-forward [19]). Decoding this superposition of codewords is possible as long as the rate constraint (9) is satisﬁed.\nNotice that the set of all possible values of u (k) (1) ∈ U (k) has size U (k) = 2 nR (k) cp . The relay combines all u (k) (1), k = 1, . . . , K, into one message m r ∈ M r . Then the message set M r has a size which is equal to the size of the Cartesian product of all U (k) , i.e., |M r | =\nU (1) × U (2) × · · · × U (K) = 2 n K k =1 R (k) cp . The relay then maps the message tuple (u (1) (1), . . . , u (K) (1)) to a message m r (2) ∈ M r to be sent in block b = 2. This message is split into m (1) r (2) and m (2) r (2) with rates R (1) r and R (2) r , respectively. The relay messages are then encoded to x (1),n r (2) and x (2),n r (2), two Gaussian codewords with powers P (1) r and P (2) r , respectively, such that P (1) r + P (2) r ≤ P . The sum of these codewords is sent in block 2. This process is repeated for every block b = 1, . . . , B − 1. The relay sends in blocks b = 2, . . . , B and does not send any signal in block 1.\nThe receivers wait until the end of block B where decoding starts. Let us focus on receiver 1. At the end of block B where only the relay is active, receiver 1 has y n 1 (B) = h r (x (1),n r (B) + x (2),n r (B)) + z n 1 since the transmitters do not send in this block. Then, m (1) r (B − 1) and m (2) r (B − 1) are decoded successively in this order, which is reliable if\nNow, the receiver knows (u (1) (B − 1), . . . , u (K) (B − 1)). Decoding proceeds backwards to block B − 1 where y n 1 (B − 1)\n+ h c x n 2,p (B − 1) + h c x n 2,c (B − 1) + h c x (k),n 2,cp (B − 1) + h r x (1),n r (B − 1) + h r x (2),n r (B − 1) + z n 1 .\n(m 1,c , m 2,c ) → m 1,p . The message m (1) r (B − 1) is ﬁrst decoded while treating the other signals as noise, leading to the ﬁrst term in the rate constraint (11). Next, the receiver decodes m (1) 1,cp (B − 1) while treating the other signals as noise. Thus, we have the rate constraint in (10) with k = 1.\nRecall that u (1) (B − 1) is known at the receiver from the decoding process in block B. Now interference cancellation is performed. Since the receiver now knows both m (1) 1,cp (B − 1) and u (1) (B −1), then, it can extract m (1) 2,cp (B −1) (see [17]). It thus removes its contribution, h c x (1),n 2,cp (B−1), from y n 1 (B−1). Therefore, after decoding each m (k) 1,cp (B−1), interference from m (k) 2,cp (B−1) is cancelled. This continues until all CP messages are decoded, leading to the rate constraint (10). At this stage, the receiver can calculate\nh d x n 1,p (B − 1) + h d x n 1,c (B − 1) + h c x n 2,p (B − 1) + h c x n 2,c (B − 1) + h r x (2),n r (B − 1) + z n 1 .\nby subtracting the contribution of m (1) r (B − 1), m (k) 1,cp (B − 1), and m (k) 2,cp (B − 1), for k = 1, . . . , K, from y n 1 (B − 1). The receiver then decodes m (2) r (B−1), (m 1,c (B−1), m 2,c (B−1)) (jointly), and m 1,p (B − 1) successively in this order, each time treating the remaining signals as noise. This leads to the second term in the rate constraint (11), and the constraints (6)- (8). Notice that the ﬁrst and second terms in (11) are more binding than (15), thus the latter are ignored. Additionally, since we have R (1) r + R (2) r = R r = K k =1 R cp (k) = R cp , then, we can write the bound (11).\nDecoding then proceeds backwards till block 1 is reached and the same is done at the second receiver, which proves the achievability of Theorem 4. Theorem 5 can be proved\nsimilarly, except that the interfering CP messages are decoded ﬁrst at each receiver instead of the desired CP messages.\nAt this point, it is worth to remark that a lattice strategy for the IRC was also proposed in [8]. The ﬁrst difference between our scheme and the one in [8] is that we use P, C, and a set of CP messages, while in [8] each user sends only a CP message. The relay processing of the CP messages is the similar in both cases. The fundamental difference however is the decoding at the destination. We use interference cancellation described in Section V-D which is not used in [8].\nTo examine the performance of the FDF scheme, one has to carefully choose K (the inﬂuence of which is explained in the next section) and the power allocations, plug in the FDF rate constraints, and compare to the upper bounds. In this way, it is possible to prove that the GDoF in Theorem 1 is achievable. Due to space constraints, we use an example to illustrate the proof.\nConsider an IRC with β − 1 < γ ≤ α ≤ 1 ≤ β, and 2α > 1 + γ. In this case, from (3) we obtain d = min{2α, 1 + β − α}. Let us set the FDF parameters to P p = 1/h 2 c , P c = h 2 d P/h 2 r − P p , P (1) r = P , P (2) r = 0,K =\n, for k = 1, . . . , K − 1. Evaluating the expressions stated in Theorem 4, gives the achievable private GDoF d p = 1 − α. For the common messages we get d c = min {2α − β, (1 + α − β)/2} where we used γ > β − 1 and 2α > 1 + γ. For the cooperative public messages, by plugging the chosen parameters in (10)\nd (k) cp ≤ 1 − α, ∀k = 1, . . . , K − 1, \t (16) d (K) cp ≤ (K − 1)(α − 1) − 1 + β. \t (17)\nHere comes the importance of the choice of P (k) cp and K. The choice of the powers of the CP signals leads to h 2 d P (k+1) cp = h 2 c P (k) cp , i.e., while decoding the k-th CP message, the inter- ference power from the ( k +1)-th desired CP message is equal to that of the kth interfering CP message. Thus, the (k + 1)- th desired CP message does not affect d (k) cp . This allows the achievability of 1 − α. Now notice that without CP message splitting, that is all we could achieve. By splitting the CP messages, after decoding the k-th desired CP message, we can cancel the interference of the k-th interfering CP message, and then proceed to decode the ( k + 1)-th desired CP message where we have h 2 d P (k+2) cp \t = h 2 c P (k+1) cp , achieving another 1 − α. By an appropriate choice of K, the ﬁrst K − 1 CP messages have 1 − α GDoF, leading to (16). While decoding the K-th desired CP message, the strongest interferer is the desired C message since\nwhich follows from the choice of K. In fact, K is chosen as the largest number such that K(1 − α) ≥ β − 1 leading to the total CP GDoF d cp = β − 1. Interestingly, this is as if there were no CP interference at all, where β − 1 would be achievable by decoding the CP messages while treating only the C and the P messages as noise. CP message splitting and interference cancellation therefore provide d cp = β −1 instead of d cp = 1 − α. Similar CP GDoF expressions are obtained by evaluating the bounds (9) and (11). Consequently, (9) and (11) do not decrease the achievable CP GDoF, which is still β −1. By adding d p , d c , and d cp , we get the overall achievable GDoF of d ≤ min{2α, 1 + β − α}.\nThe ﬁrst bound in Theorem 3, i.e., (4) is obtained by giving Y n r and (Y n r , m 1 ) as side information to receivers 1 and 2, respectively. Using classical information theoretic procedures, it is possible to write\nn(R Σ − ǫ n ) ≤ I(m 1 , m 2 ; Y n r ) + h(h d X n 1 + h c X n 2 + Z n 1 |Y n r ) − h(S n c |S n sr ) + h(S n d |S n sr ) − h(Z n 2 )\nwith ǫ n → 0 as n → ∞, S sr = h sr X 2 +Z r , S c = h c X 2 +Z 1 , and S d = h d X 2 + Z 2 . We proceed by writing\nn(R Σ − ǫ n ) ≤ I(m 1 , m 2 ; Y n r ) + h(h d X n 1 + h c X n 2 + Z n 1 ) − h (S n c /h c |S n sr ) + h (S n d /h d |S n sr ) + (n/2) log h 2 d /h 2 c − h(Z n 2 ),\nwhich follows since conditioning does not increase entropy, and since h(aX) = h(X) + 1 2 log(a 2 ). Now if h 2 c ≤ h 2 d , then\nwe can write Z n 1 /h c = ˜ Z n + Z n 2 /h d , where ˜ Z n and Z n 2 are independent. Then we can write\nwhich is negative. As a result, by letting n → ∞, and using the Gaussian distribution for X 1 and X 2 to maximize the upper bound, we obtain (4). If h 2 c > h 2 d , then the bound (4) can be obtained by enhancing receiver 2 by replacing the noise Z 2 by h d Z 2 /h c , and proceeding as above."},"refs":[{"authors":[{"name":"R. H. Etkin"},{"name":"D. N. C. Tse"},{"name":"H. Wang"}],"title":{"text":"Gaussian interference channel capacity to within one bit"}},{"authors":[{"name":"T. M. Cover"},{"name":"A. El-Gamal"}],"title":{"text":"Capacity theorems for the relay channel"}},{"authors":[{"name":"A. S. Avestimehr"},{"name":"S. Diggavi"},{"name":"D. Tse"}],"title":{"text":"A deterministic approach to wireless relay networks"}},{"authors":[{"name":"I. Maric"},{"name":"R. Dabora"},{"name":"A. Goldsmith"}],"title":{"text":"An outer bound for the Gaussian interference channel with a relay"}},{"authors":[{"name":"O. Sahin"},{"name":"E. Erkip"}],"title":{"text":"Achievable rates for the Gaussian interference relay channel"}},{"authors":[],"title":{"text":"On achievable rates for interference relay channel with inter- ference cancellation"}},{"authors":[{"name":"S. Sridharan"},{"name":"S. Vishwanath"},{"name":"S. A. Jafar"},{"name":"S. Shamai"}],"title":{"text":"On the capacity of cognitive relay assisted Gaussian interference channel"}},{"authors":[{"name":"Y. Tian"},{"name":"A. Yener"}],"title":{"text":"The Gaussian interference relay channel: improved achievable rates and sum rate upper bounds using a potent relay"}},{"authors":[{"name":"S. Rini"},{"name":"D. Tuninetti"},{"name":"N. Devroye"}],"title":{"text":"Capacity to within 3 bits for a class of gaussian interference channels with a cognitive relay"}},{"authors":[],"title":{"text":"Outer bounds for the interference channel with a cognitive relay"}},{"authors":[{"name":"V. R. Cadambe"},{"name":"S. A. Jafar"}],"title":{"text":"Degrees of freedom of wireless networks with relays, feedback, cooperation and full duplex operation"}},{"authors":[{"name":"L. Ong"},{"name":"C. Kellett"},{"name":"S. Johnson"}],"title":{"text":"Capacity theorems for the AWGN multi-way relay channel"}},{"authors":[{"name":"T. Cove"},{"name":"J. Thoma"}],"title":{"text":"Elements of information theory"}},{"authors":[{"name":"A. Chaaban"},{"name":"A. Sezgin"}],"title":{"text":"Achievable rates and upper bounds for the interference relay channel"}},{"authors":[{"name":"B. Nazer"},{"name":"M. Gastpar"}],"title":{"text":"Compute-and-Forward: Harnessing interfer- ence through structured codes"}},{"authors":[{"name":"U. Erez"},{"name":"R. Zamir"}],"title":{"text":"Achieving 1/2 log(1 + SNR) on the AWGN channel with lattice encoding and decoding"}},{"authors":[{"name":"K. Narayanan"},{"name":"M. P. Wilson"},{"name":"A. Sprintson"}],"title":{"text":"Joint physical layer coding and network coding for bi-directional relaying"}},{"authors":[{"name":"V. M. Prabhakaran"},{"name":"P. Viswanath"}],"title":{"text":"Interference channels with source cooperation"}},{"authors":[{"name":"B. Nazer"}],"title":{"text":"Successive compute-and-forward"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566129.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T2.2","endtime":"10:30","authors":"Anas Chaaban, Aydin Sezgin","date":"1341396600000","papertitle":"Lattice Coding and the Generalized Degrees of Freedom of the Interference Channel with Relay","starttime":"10:10","session":"S9.T2: Relaying over Interference Networks","room":"Kresge Auditorium (109)","paperid":"1569566129"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
