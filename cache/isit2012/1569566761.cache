{"id":"1569566761","paper":{"title":{"text":"Message-Passing Algorithms for Channel Estimation and Decoding Using Approximate Inference"},"authors":[{"name":"Mihai-A. Badiu ∗\u2020"},{"name":"Gunvor E. Kirkelund ∗"},{"name":"Carles Navarro Manch ´on ∗"},{"name":"Erwin Riegler \u2021"},{"name":"Bernard H. Fleury ∗"}],"abstr":{"text":"Abstract\u2014We design iterative receiver schemes for a generic communication system by treating channel estimation and infor- mation decoding as an inference problem in graphical models. We introduce a recently proposed inference framework that combines belief propagation (BP) and the mean ﬁeld (MF) approximation and includes these algorithms as special cases. We also show that the expectation propagation and expectation maximization (EM) algorithms can be embedded in the BP-MF framework with slight modiﬁcations. By applying the considered inference algorithms to our probabilistic model, we derive four different message-passing receiver schemes. Our numerical evaluation in a wireless scenario demonstrates that the receiver based on the BP-MF framework and its variant based on BP-EM yield the best compromise between performance, computational complexity and numerical stability among all candidate algorithms."},"body":{"text":"The design of advanced receiver algorithms is crucial to meet the stringent requirements of modern communication systems. Motivated by the successful application of the \u201cturbo\u201d principle in the decoding of channel codes, a large number of works have been devoted to the design of turbo receivers (see [1] and the references therein). While in many of these works the receiver modules are individually designed and heuristically intercon- nected to exchange soft values, iterative receiver algorithms can be rigourously designed and better understood as instances of message-passing inference techniques (e.g., see [2]).\nIn this context, variational Bayesian inference in probabilistic models [3] have proven to be a very useful tool to design receivers where tasks like channel estimation, detection and decoding are jointly derived. Among the variational techniques, belief propagation (BP) [4], [5] has found the most widespread use. Originally applied to the decoding of channel codes, BP has been shown to be especially efﬁcient in discrete probabilistic models. An alternative to BP is the mean ﬁeld (MF) approxi- mation and its message-passing counterpart, usually referred to as variational message-passing [6]. MF inference has been suc- cessfully applied to continuous probabilistic models involving probability density functions (pdfs) belonging to an exponential family, in which BP suffers from numerical intractability. Other notable examples of general-purpose inference techniques are expectation-maximization (EM) [7] and expectation propagation (EP) [8]. EM is a special case of MF, where the approximate pdfs \u2013 referred to as beliefs \u2013 are Dirac delta functions; EP can be seen as an approximation of BP where some beliefs are approximated by pdfs in a speciﬁc exponential family. Some attempts to ﬁnd a uniﬁed framework encompassing all these techniques include the α-divergence interpretation in [9] and the region-based free energy approximations in [10]. Following\nthe latter approach, a novel hybrid message-passing inference framework combining BP and the MF approximation was recently proposed in [11].\nIn this paper, we investigate the design of receivers that perform joint channel estimation and data decoding in a generic communication system. For this purpose, we capitalize on the combined inference framework [11], which provides some de- gree of freedom in the choice of the parts of the factor graph in which either BP or MF is applied. We show that this framework can be modiﬁed to naturally embed EP, EM and BP with Gaussian approximation of some messages. Then, we apply these hybrid inference techniques to the underlying probabilistic model of the system and obtain four receiver algorithms, whose performance we assess by simulating a wireless system.\nNotation : we denote by |I| the cardinality of a ﬁnite set I; the relative complement of {i} in I is written as I \\ i; the set {i ∈ N | 1 ≤ i ≤ n} is denoted by [1 : n]. Boldface lowercase and uppercase letters are used to represent vectors and matrices, respectively; superscripts (·) T and (·) H denote transposition and Hermitian transposition, respectively. The Hadamard product of two vectors is denoted by ⊙. For a vector x = (x i | i ∈ I) T , we write x ¯i = (x j | j ∈ I \\ i) T ; for a matrix A ∈ C m ×n , [A] i,j denotes its (i, j)th entry, [A] ¯i,¯j is the matrix A with the ith row and jth column deleted, [A] ¯i,j denotes the column vector ([A] k,j | k ∈ [1 : m] \\ i) T , and [A] i,¯ j is the row vector ([A] i,k | k ∈ [1 : n] \\ j). The pdf of a multivariate complex Gaussian distribution with mean µ and covariance matrix Σ is denoted by CN (·; µ, Σ). We write f (x) ∝ g(x) when f (x) = cg(x) for some positive constant c. We denote by G[·] the approximation of the pdf in the argument with a Gaussian pdf with the same mean and covariance matrix. The Dirac delta function is denoted by δ(·).\nWe begin by concisely describing the uniﬁed message- passing algorithm that combines the BP and MF approaches (refer to [11]). Then, we brieﬂy show how other widespread inference algorithms can be obtained as particular instances or slight modiﬁcations of the uniﬁed framework.\nLet p(z) be an arbitrary pdf of a random vector z (z i | i ∈ I) T which factorizes as\nwhere z a is the vector of all variables z i that are arguments of the function f a .We have grouped the factors into two sets that partition A: A MF ∩ A BP = ∅ and A MF ∪ A BP = A. The\nfactorization in (1) can be visualized in a factor graph [4] representation. We deﬁne N (a) ⊆ I to be the set of indices of all variables z i that are arguments of function f a ; similarly, N (i) ⊆ A denotes the set of indices of all functions f a that depend on z i . The parts of the graph that correspond to\nf a (z a ) and to a ∈A MF f a (z a ) are referred to as \u201cBP part\u201d and \u201cMF part\u201d, respectively. We denote the variable nodes in the BP part by I BP \t a ∈A BP N (a) and those in the MF part by I MF \t a ∈A MF N (a).\nThe combined BP-MF inference algorithm approximates the marginals p(z i ) = p(z)dz ¯i , i ∈ I by auxiliary pdfs b i (z i ) called beliefs. They are computed as [11]\n(3) where ω i and ω a are constants that ensure normalized beliefs.\nBelief propagation is obtained as a particular case of BP-MF by setting A MF = ∅, since in this case the expressions in (3) reduce to the BP message computations. Similarly, mean ﬁeld is an instance of BP-MF when A BP = ∅.\nExpectation propagation is very similar to BP, the main difference being that it constrains the beliefs of some variables to be members of a speciﬁc exponential family. Assuming Gaus- sian approximations of the beliefs, EP can also be integrated in the BP-MF framework by modifying the messages\nThe expectation-maximization algorithm is a special case of MF when the beliefs of some variables are constrained to be Dirac delta functions [11]. Again, we include this approximation in the BP-MF framework. This leads to n i →a (z i ) = δ(z i − ˜ z i ) for all i ∈ I EM ⊆ I MF and a ∈ N (i)∩A MF , where ˜ z i maximizes the unconstrained belief (2). We refer to this modiﬁed algorithm as BP-EM.\nIn this section, we present the signal model of our inference problem and its graphical representation. These will establish the baseline for the derivation of message-passing receivers.\nWe analyze a system consisting of one transmitter and one receiver. A message represented by a vector u = (u k | k ∈ [1 : K]) T ∈ {0, 1} K of information bits is conveyed\nby sending N data and M pilot channel symbols having the sets of indices D ⊆ [1 : M + N ] and P ⊆ [1 : M + N ], respectively, such that D ∪ P = [1 : M + N ] and D ∩ P = ∅. Speciﬁcally, vector u is encoded and interleaved using a rate R = K/(N L) channel code and a random interleaver into the vector c = (c T n | c n ∈ {0, 1} L , n ∈ [1 : N ]) T of length N L. For each n ∈ [1 : N ], the subvector c n = (c (1) n , . . . , c (L) n ) T is mapped to a data symbol x i n ∈ S D with i n ∈ D, where S D is a discrete complex modulation alphabet of size 2 L . Symbols x D = (x i | i ∈ D) T are multiplexed with pilot symbols x P = (x j | j ∈ P) T , which are randomly selected from a QPSK modulation alphabet. Finally, the aggregate vector of channel symbols x = (x i | i ∈ D ∪ P) T is sent through a channel with the following input-output relationship:\nThe vector y = (y i | i ∈ [1 : M + N ]) T contains the received signal samples, h = (h i | i ∈ [1 : M + N ]) T is the vector of channel coefﬁcients, and w = (w i | i ∈ [1 : M + N ]) T contains the samples of additive noise and has the pdf p(w) = CN (w; 0, γ −1 I M +N ) for some positive component precision γ. Note that (5) can model any channel with a multiplicative effect that is not affected by inter-symbol interference, e.g., a time- varying frequency-ﬂat channel or the equivalent channel in the frequency domain in a multicarrier system.\nBased on the above signal model, we can state the probabilis- tic model which captures the dependencies between the system variables. The pdf of the collection of observed and unknown variables factorizes as\nwhere f D i (h i , x i ) \t p(y i |h i , x i ) and f P j (h j ) \t p(y j |h j ) incorporate the observations in y and are given by\nf D i (h i , x i ) = CN h i x i ; y i , γ −1 , ∀i ∈ D, \t (7) f P j (h j ) = CN h j x j ; y j , γ −1 , ∀j ∈ P, \t (8)\nf H (h) \t p(h) is the prior pdf of the vector of channel coefﬁcients for which we set\nf H (h) = CN h; µ p h , Σ p h , \t (9) f M n (x i n , c n ) p (x i n |c n ) stand for the modulation mapping, f C (c, u) p(c|u) accounts for the coding and interleaving operations and f U k (u k ) p(u k ) is the prior pmf of the kth information bit. To obtain (6), we used the fact that y is conditionally independent of c and u given x D , h is independent of x D , c and u, the noise samples w i are i.i.d., and each data symbol x i n is conditionally independent of all the other symbols given c n . The factorization in (6) can be visualized in the factor graph depicted in Fig. 1. The graph of the code and interleaver is not explicitly given, its structure being captured by f C .\nIn this section, we derive iterative receiver schemes by applying different inference algorithms to the factor graph in\nFig. 1. The receiver has to infer the beliefs of the information bits using the observed vector y and prior knowledge, i.e., the pilot symbols and their set of indices P, the noise precision γ, the channel statistics in (9), the modulation mapping and the structure of the channel code and interleaver.\nWe set A and I (deﬁned in Section II for a general probabilis- tic model) to be the sets of all factors and variables, respectively, contained in our probabilistic model. Next, we show that the BP algorithm resulting from setting A MF = ∅ yields messages of an intractable complexity. Assume that by running BP in the part of the graph containing the modulation and code constraints we obtain the messages\nwith i n ∈ D, ∀n ∈ [1 : N ], where β i n (s) represent extrinsic information on symbol x i n . These messages are further passed as n x in →f Din (x i n ) = m BP f Mn →x in (x i n ). Then, for each i ∈ D, compute the message\nγ|x j | 2 . (12) Note that the message in (11) is proportional to a mixture of Gaussian pdfs with |S D | = 2 L components. Then, after setting n h i →f H (h i ) = m BP f Di →h i (h i ) for all i ∈ D and n h i →f H (h i ) = m BP f Pi →h i (h i ) for all i ∈ P, the message from f H to h i reads\nUsing (9), (11) and (12), the message in (13) becomes a Gaussian mixture with 2 L (N −1) and 2 LN components for i ∈ D and i ∈ P, respectively. Clearly, the computation of such messages is intractable and one has to use approximations.\nA. Algorithm based on BP combined with Gaussian approxi- mation\nSince the intractability of the messages occurs due to the Gaussian mixture in (11), we approximate those messages as\n(14) with\nIn (15), we have deﬁned the normalized amplitudes of the Gaussian mixture α i (s) = β i (s)/(κ i |s| 2 ), where the constant κ i ensures s ∈S D α i (s) = 1. We also denote the mean and variance of the pdf in (12) by µ h j ,o and σ 2 h j ,o , j ∈ P, and we deﬁne the vector µ o h = (µ h i ,o | i ∈ [1 : M + N ]) T and the matrix Σ o h with entries [Σ o h ] i,j = σ 2 h i ,o if i = j and zero otherwise, for all i, j ∈ [1 : M + N ].\n∝ CN h i ; µ h i ,c , σ 2 h i ,c , \t (16) with\n(17) These messages are further passed as extrinsic values, i.e., n h i →f Di or Pi (h i ) = m BP f H →h i (h i ). For each i ∈ D, the following message is then computed:\nAfter passing the extrinsic messages n x in →f Mn (x i n ) = m BP f Din →x in (x i n ), i n ∈ D, n ∈ [1 : N ], we apply the BP update rule to compute the probabilities of the coded and interleaved bits (which is equivalent to MAP demapping), followed by BP decoding to obtain the beliefs of the information bits.\nWe set A MF = ∅ and I EP = {h i | i ∈ D}. The message m EP f Di →h i (h i ) computed with (4) is proportional to a Gaussian pdf; consequently, the EP rule for m EP f H →h i (h i ) reduces to the BP rule and outputs a Gaussian pdf as in (16), since the operator G[·] is an identity operator for Gaussian arguments.\nUnlike (15) in BP with Gaussian approximation, the values of µ h i ,o and σ 2 h i ,o , i ∈ D, computed with (18) depend on all µ h j ,o and σ 2 h j ,o , j ∈ D, j = i, through (17). The parameters of m EP f H →h i (h i ) are updated using (17) but with µ h i ,o and σ 2 h i ,o computed as above. Note that all messages that depend on the channel coefﬁcients need to be updated in a sequential manner. The rest of the messages are computed as in Section IV-A.\nThe factor graph is split into the MF and BP parts by setting A MF = {f D i | i ∈ D} and A BP = A \\ A MF . Such a splitting yields tractable and simple messages, takes advantage of the fact that BP works well with hard constraints and best exploits the correlation between the channel coefﬁcients for the graphical representation in Fig. 1 1 .\nAssuming we have obtained the messages n x i →f Di (x i ) (their expression will be given later), we can compute\nwith the deﬁnition µ x i \t n x i →f Di (x i ) x i dx i and σ 2 x i n x i →f Di (x i )|x i − µ x i | 2 dx i .\nThe messages n h i →f H (h i ) = m MF f Di →h i (h i ) are sent to the BP part and hence are extrinsic values. When computing m BP f H →h i (h i ) we get the same expression as (16), with the pa- rameters (17). Unlike in the previous algorithms, the following messages are beliefs, i.e., a posteriori probabilities (APP):\nand we pass n x in →f Mn (x i n ) = m MF f Din →x in (x i n ) to the modu- lation and coding part of the graph as extrinsic values, for all n ∈ [1 : N ]. After running BP, we obtain (10) and then pass the following APP values back to the MF part:\nWe now apply EM for channel estimation, so we constrain b h i (h i ) from the previous BP-MF scheme to be Dirac delta functions. The resulting messages are the same as in the previous subsection, except for n h i →f Di (h i ) = δ(h i −µ h i ) with µ h i computed as in (19). Note that this algorithm uses only point estimates of the channel weights; however, its complexity is basically still the same, since the computation of (19) actually includes the computation of the corresponding variance.\nAll algorithms employ the same message-passing scheduling: they start by sending messages m f Pj →h j (h j ) corresponding to pilots and by initializing m f Di →h i (h i ) ∝ CN (h i ; 0, ∞); messages (computed according to the corresponding algorithm) are passed on up to the information bit variables \u2013 this com- pletes the ﬁrst iteration; each following iteration consists in passing messages up to the channel prior factor node and back; messages are passed back and forth until a predeﬁned number of iterations is reached. All algorithms end by taking hard decisions on the beliefs of the information bits.\nWe consider a wireless OFDM system with the parameters given in Table I, and we evaluate by means of Monte Carlo simulations the bit error rate (BER) performance of the receiver algorithms derived in Section IV. We employ as a reference a scheme which has perfect channel state information (CSI), i.e., it has prior knowledge of the vector of channel coefﬁcients h.\nWe encountered numerical problems with the EP-based scheme due to the instability of EP in general, so we used the heuristic approach [9] to damp the updates of the beliefs b h i with a step-size ǫ = 0.5. Also, the EP-based scheme has higher computational complexity than the others due to its message deﬁnition \u2013 it requires multiplication of a Gaussian pdf with a mixture of Gaussian pdfs, the approximation G[·] and division of Gaussian pdfs \u2013 and to the sequentiality of the message updates for the channel coefﬁcients 2 .\nResults in terms of BER versus signal-to-noise ratio (SNR) are given in Fig. 2, while the convergence of the BER with the number of iterations is illustrated in Fig. 3. The receivers based on EP, combined BP-MF and BP-EM exhibit similar\nperformance. They signiﬁcantly outperform the receiver em- ploying BP with Gaussian approximation. Note that even with a high pilot spacing ∆ P ≈ 2.5W coh the performance of the former algorithms is close to that of the receiver having perfect CSI. These three algorithms converge in about 10\u201312 iterations, while BP with Gaussian approximation converges a little faster, but to a higher BER value. Other results not presented here show that for a higher pilot density the algorithms converge faster, as expected.\nNote that the results for the (essentially equally-complex) BP- EM and BP-MF receivers are nearly identical, even if the former discards the soft information in channel estimation. We noticed during our evaluations that σ 2 h i ≪ |µ h i | 2 even at low SNR values, so our explanation would be that accounting for σ 2 h i in the BP-MF receiver does not have a noticeable impact on the detection (20).\nWe formulated the problem of joint channel estimation and decoding in a communication system as inference in a graphical model. To solve the inference problem, we resorted to a re- cently proposed message-passing framework that uniﬁes the BP and MF algorithms and includes them as particular instances. Additionally, we illustrated how the combined framework can encompass the EP and EM inference algorithms.\nBased on the inference techniques considered, we derived four receiver algorithms. Since BP is not suitable for the studied problem, as it leads to intractable messages, we applied its variant which employs Gaussian approximation of the compu- tationally cumbersome messages instead. However, our results showed that it performs signiﬁcantly worse than the other pro- posed schemes. Considering the BER results, the computational complexity and stability of these schemes, we conclude that the receiver based on the combined BP-MF framework and its BP- EM variant are the most effective receiver algorithms.\nSix projects have supported this work: the Project SIDOC under contract no. POSDRU/88/1.5/S/60078; the Cooperative Research Project 4GMCT funded by Intel Mobile Commu- nications, Agilent Technologies, Aalborg University and the Danish National Advanced Technology Foundation; the PhD Project \u201cIterative Information Processing for Wireless Re- ceivers\u201d funded by Renesas Mobile Corporation; the Project ICT-248894 WHERE2; the WWTF Grant ICT10-066; and the\nFWF Grant S10603-N13 within the National Research Network SISE."},"refs":[{"authors":[{"name":"M. T ¨uchler"},{"name":"A. C. Singer"}],"title":{"text":"Turbo equalization: An overview"}},{"authors":[{"name":"J. Boutros"},{"name":"G. Caire"}],"title":{"text":"Iterative multiuser joint decoding: uniﬁed frame- work and asymptotic analysis"}},{"authors":[{"name":"M. J. Wainwright"},{"name":"M. I. Jordan"}],"title":{"text":"Graphical models, exponential families, and variational inference"}},{"authors":[{"name":"F. Kschischang"},{"name":"B. Frey"},{"name":"H.-A. Loeliger"}],"title":{"text":"Factor graphs and the sum- product algorithm"}},{"authors":[{"name":"H.-A. Loeliger"},{"name":"J. Dauwels"},{"name":"J. Hu"},{"name":"S. Korl"},{"name":"L. Ping"},{"name":"F. Kschischang"}],"title":{"text":"The factor graph approach to model-based signal processing"}},{"authors":[{"name":"J. Winn"},{"name":"C. Bishop"}],"title":{"text":"Variational message passing"}},{"authors":[{"name":"A. Dempster"},{"name":"N. Laird"},{"name":"D. Rubin"}],"title":{"text":"Maximum likelihood from incomplete data via the EM algorithm"}},{"authors":[{"name":"T. Minka"}],"title":{"text":"Expectation propagation for approximate bayesian inference"}},{"authors":[],"title":{"text":"Divergence measures and message passing"}},{"authors":[{"name":"J. Yedidia"},{"name":"W. Freeman"},{"name":"Y. Weiss"}],"title":{"text":"Constructing free-energy ap- proximations and generalized belief propagation algorithms"}},{"authors":[{"name":"E. Riegler"},{"name":"G. E. Kirkelund"},{"name":"C. N. Manch´on"},{"name":"M.-A. Badiu"},{"name":"B. H. Fleury"}],"title":{"text":"Merging belief propagation and the mean ﬁeld approximation: A free energy approach"}},{"authors":[{"name":"Z. Shi"},{"name":"T. Wo"},{"name":"P. Hoeher"},{"name":"G. Auer"}],"title":{"text":"Graph-based soft iterative receiver for higher-order modulation"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566761.pdf"},"links":[{"id":"1569565883","weight":8},{"id":"1569559665","weight":5},{"id":"1569566683","weight":5},{"id":"1569565495","weight":2},{"id":"1569564469","weight":2},{"id":"1569566739","weight":2},{"id":"1569562685","weight":2},{"id":"1569566157","weight":5},{"id":"1569565809","weight":2},{"id":"1569566895","weight":2},{"id":"1569566749","weight":2},{"id":"1569564195","weight":2},{"id":"1569564311","weight":2},{"id":"1569565785","weight":13},{"id":"1569566617","weight":2},{"id":"1569566311","weight":8},{"id":"1569566759","weight":2},{"id":"1569565213","weight":2},{"id":"1569561795","weight":2},{"id":"1569559805","weight":2},{"id":"1569566687","weight":2},{"id":"1569566513","weight":2},{"id":"1569566425","weight":2},{"id":"1569562821","weight":2},{"id":"1569566649","weight":2},{"id":"1569565559","weight":2},{"id":"1569564333","weight":2},{"id":"1569566809","weight":5},{"id":"1569566141","weight":2},{"id":"1569564677","weight":2},{"id":"1569565469","weight":2},{"id":"1569566853","weight":5},{"id":"1569555787","weight":2},{"id":"1569567235","weight":13},{"id":"1569565739","weight":2},{"id":"1569566949","weight":2},{"id":"1569566133","weight":2},{"id":"1569566383","weight":2},{"id":"1569565885","weight":2},{"id":"1569566177","weight":2},{"id":"1569559199","weight":11},{"id":"1569566929","weight":2},{"id":"1569565665","weight":8},{"id":"1569566983","weight":8},{"id":"1569565093","weight":11},{"id":"1569565661","weight":2},{"id":"1569566887","weight":2},{"id":"1569561221","weight":2},{"id":"1569566917","weight":5},{"id":"1569566651","weight":2},{"id":"1569566595","weight":2},{"id":"1569565013","weight":2},{"id":"1569565829","weight":2},{"id":"1569565375","weight":2},{"id":"1569566639","weight":2},{"id":"1569566819","weight":8},{"id":"1569565597","weight":2},{"id":"1569564787","weight":8},{"id":"1569566397","weight":5},{"id":"1569566435","weight":2},{"id":"1569565769","weight":2},{"id":"1569566171","weight":2},{"id":"1569565561","weight":2},{"id":"1569566847","weight":2},{"id":"1569564961","weight":2},{"id":"1569566583","weight":2},{"id":"1569566413","weight":2},{"id":"1569565143","weight":2},{"id":"1569561579","weight":2},{"id":"1569558697","weight":2},{"id":"1569566067","weight":2},{"id":"1569564807","weight":8}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T7.1","endtime":"15:00","authors":"Mihai Alin Badiu, Gunvor E Kirkelund, Carles Navarro Manchón, Erwin Riegler, Bernard Henri Fleury","date":"1341499200000","papertitle":"Message-Passing Algorithms for Channel Estimation and Decoding Using Approximate Inference","starttime":"14:40","session":"S13.T7: Communication Systems","room":"Stratton (407)","paperid":"1569566761"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
