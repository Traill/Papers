{"id":"1569564411","paper":{"title":{"text":"Continuous Time Channels with Interference"},"authors":[{"name":"Ioana Ivan ∗"},{"name":"Michael Mitzenmacher \u2020"},{"name":"Justin Thaler \u2020"},{"name":"Henry Yuen ∗"}],"abstr":{"text":"Abstract\u2014Khanna and Sudan [2] studied a natural model of continuous time channels where signals are corrupted by the effects of both noise and delay, and showed that, surprisingly, in some cases both are not enough to prevent such channels from achieving unbounded capacity. Inspired by their work, we consider channels that model continuous time communication with adversarial delay errors. The sender is allowed to subdivide time into an arbitrarily large number M of micro-units in which binary symbols may be sent, but the symbols are subject to unpredictable delays and may interfere with each other. We model interference by having symbols that land in the same micro-unit of time be summed, and we study k-interference channels, which allow receivers to distinguish sums up to the value k. We consider both a channel adversary that has a limit on the maximum number of steps it can delay each symbol, and a more powerful adversary that only has a bound on the average delay.\nWe give precise characterizations of the threshold be- tween ﬁnite and inﬁnite capacity depending on the in- terference behavior and on the type of channel adver- sary: for max-bounded delay, the threshold is at D max = Θ (M log (min{k, M })), and for average bounded delay the threshold is at D avg = Θ \t M min{k, M } ."},"body":{"text":"We study continuous time channels with adversarial delay errors in the presence of interference. Our models are inspired by recent work of Khanna and Sudan [2], who studied continuous-time channels in the presence of both delay errors and (signal) noise errors. In this model, the communicating parties can subdivide time as ﬁnely as they wish. In each subdivided unit of time a 0 or 1 can be sent, but the sent signals are subject to unpredictable delays . Khanna and Sudan found (suprisingly) that the channel capacity in their model is ﬁnitely bounded only if at least one of the two sources of error (delay or signal noise) is adversarial. However, they assumed that at any instant in time, the receiver observes the sum of the signals delivered.\nIn this paper, we observe that the behavior of the channel changes dramatically if one accounts for the possibility of interference, and that this holds even in the absence of signal noise. Our model of interference is very simple; the symbols received at each time unit are summed, and the receiver sees the exact sum if it is less than k, but values greater than k cannot be distinguished from each other.\nAt a high level, our results are two-fold. First, we show that delay errors in the presence of interference are surprisingly powerful. Second, in the context of delay errors with interference, we ﬁnd that seemingly innocuous modeling decisions can have large effects on channel behavior.\nRelated Work. Typically a communication channel is modeled as follows. The channel takes as input a signal f , modeled as a function from some domain T to some range R, and the channel outputs a received signal ˜ f : T → R, which is a noisy version of f . For discrete time channels, T is a ﬁnite domain {0, . . . , T − 1} where T is the time duration, and for continuous-time channels, T is a continuous domain such as the interval [0, T ]. For discrete signal channels, R is a ﬁnite set such as {0, 1}, and for continuous signal channels, R is an inﬁnite set such as the interval [0, 1].\nShannon showed in the discrete time setting, the capacity of the channel is ﬁnite even if the signal is continuous, as long as there is signal noise [4]. Nyquist [3] and Hartley [1] showed that even in the continuous time setting, the capacity is ﬁnite if one places certain restrictions on the Fourier spectrum of the signal.\nMost relevant to us is recent work by Khanna and Sudan [2], which introduced continuous-time channels with signal noise and delay errors. They modeled their channel as the limit of a discrete process, and found that the capacity of their channel is inﬁnite unless at least one of the error sources is adversarial.\nOur work differs from previous work in several ways. We consider channels which introduce delays adversar- ially, but we additionally consider a very simple model of interference. We also consider two limitations on the adversary: one where maximum delay for any symbol is bounded, and one where the average delay over all symbols is bounded. In both cases, we ﬁnd that our channels display a clean threshold behavior.\nWe believe that the adversarial setting presented here offers a clean initial analysis of the interference model, already with surprising results. A next natural step would be to analyze the effect of random delays in the presence of interference, and we leave this question as an interesting direction for future work.\nModeling Time. Following [2], we model continuous time as the limit of a discrete process. More speciﬁcally, the sender and receiver may send messages that last a duration of T units of time, but also can divide every unit of time into M subintervals called micro-intervals, and the sender may send one bit per micro-interval. We refer to M as the granularity of time, and refer to a sequence of M micro-intervals as a macro-interval. We call T the message duration of the channel. A codeword c sent over the channel is therefore represented as c ∈ {0, 1} M T .\nModeling Delays. The effect of the channel on a sent codeword c ∈ {0, 1} M T is to delay symbols of c by some amount, e.g. the ith symbol of c may be moved to the jth timestep of the received codeword, where j ≥ i. The delay process is adversarial, where we assume that the adversary knows the encoding/decoding scheme of the sender and receiver, and both the symbols that get delayed and the amount they are delayed can depend on the codeword that is sent. We formalize the notions of max-bounded delay and average-bounded delay below.\nModeling Interference. If multiple symbols are deliv- ered at the same time step, there are several natural ways the channel could behave. In [2], the receiver observes the sum of all bits delivered at that instant of time; we call this the sum channel. Another obvious choice is for the receiver to see the OR of all bits delivered at that instant in time; we call this the OR channel.\nWe generalize these two models to what we call the k-interference channel. If there are fewer than k 1s delivered at an instant in time, the receiver will see the exact number of 1s delivered; otherwise the receiver will only see that at least k 1s have arrived. Thus, the sum channel can be viewed as the ∞-interference channel, and the OR channel as the 1-interference channel. We consider k-interference channels as k varies between the extremes of 1 and ∞, and may depend on the granularity of time M . We call the parameter k the collision resolution of the channel.\nValid Codebooks. For any ﬁxed channel and codeword c, we let B(c) denote the set of possible received strings corresponding to c. For any time T , we say a codebook C ⊆ {0, 1} M T is valid for a channel if for any c = c\nin C, B(c) ∩ B(c ) = ∅. Informally, this means that the adversary cannot cause the decoder to confuse c with c for any other codeword c .\nRate and Capacity. For any ﬁxed granularity of time M and time T , let s M,T := log |C(M, T )|, where |C(M, T )| denotes the size of largest valid codebook C(M, T ) ⊆ {0, 1} M T for the channel. The capacity of the channel at granularity M is deﬁned as R(M ) = lim sup T →∞ {s M,T /T }. The capacity of the channel is deﬁned as lim sup M →∞ R(M ) = lim sup M →∞ [lim sup T →∞ {s M,T /T }]. We stress that\nthe order of the limits in the deﬁnition of the channel capacity is crucial, as we show in Section V.\nEncoding: For every T and M , the sender encodes s T ,M bits as M T bits by applying an encoding function E T : {0, 1} s T ,M → {0, 1} M T . The encoded sequence is denoted X 1 , . . . , X M T .\nDelay: The delay is modeled by a delay function ∆ : [M T ] → Z ≥0 , where Z ≥0 denotes the non-negative integers. The delay function has to satisfy a constraint depending on the type of delay channel we have:\n\u2022 Average-bounded delay : \t i ∆(i) ≤ M T · D avg , where D avg is the bound on the average delay.\nReceived Sequence . The ﬁnal sequence seen by the receiver given delay ∆, is Y 1 , . . . , Y M T ∈ Z ≥0 , where Y i := min{k, j≤i s.t. j+∆(j)=i X j } and k is the colli- sion resolution parameter of the channel. We will ignore the symbols that get delayed past timestep M T .\nFor brevity, we use the shorthand AVG-k channel and MAX-k channel, where the meaning is clear.\nWe prove that in the case of max-bounded delay, the capacity is ﬁnite if D max = Ω (M log (min{k, M })), and inﬁnite otherwise. In contrast, we prove that in the case of average-bounded delay, the capacity is ﬁnite if D avg = Ω \t M · min{k, M } , and inﬁnite otherwise.\nWe also consider a number of variant channels and observe that seemingly innocuous modeling choices cause the behavior to change drastically. In particular, we consider settings where the granularity of time is allowed to grow with the message duration, and where adversarial signal noise can also be added. For brevity, we provide a few speciﬁc interesting results.\nWe give a precise characterization of the inﬁnite/ﬁnite capacity threshold of the MAX-k channel. Here and throughout, k refers to the collision resolution paramater, and M to the granularity of time.\nTheorem III.1. If D max is the max-delay bound for the MAX- k channel, then then the capacity of the channel is inﬁnite when D max = o (M log (min{k, M })), and the capacity is ﬁnite when D max = Ω (M log (min{k, M })).\nProof: Inﬁnite capacity regime. Suppose D max = cM log (min{k, M }) for c = o(1) (here, c denotes a function of M that is subconstant in M ).\nAssume for simplicity that 1/c is an integer. Also assume that c ≥ 1 log k and k ≤ M , as smaller values of c and larger values of k only make communication easier.\nWe give a valid codebook of size s = 2 T /2c , showing R(M ) = ω(1), and thus the capacity is inﬁnite. Given a message x ∈ {0, 1} T /2c , the sender breaks the message x into blocks of length log k. The sender then encodes each block independently, using 2cM log k bits for each block as described below. The resulting codeword has length T 2c log k · 2cM log k = T M as desired.\nA block is encoded as follows. Since each block is log k bits long, we interpret the block as an integer y, 1 ≤ y ≤ k. The sender encodes the block as a string of 2cM log k bits, where the ﬁrst y ≤ k bits in the string are 1s, and all remaining bits are 0s. To decode the j\u2019th block of the sent message, the receiver simply looks at the j\u2019th set of 2cM log k bits in the received string, and decodes the block to the binary representation of y, where y is the total count of 1s received in those 2cM log k bits.\nSince the maximum delay is bounded by cM log k, and 1s only occur as the ﬁrst k ≤ M ≤ cM log k locations of each sent block, any 1-bit must be delivered within its block. Furthermore, the count of 1 bits is preserved, because at most k 1 bits collide within a block. Correctness of the decoding algorithm follows.\nFinite Capacity Regime. Suppose the delays have bounded maximum D max = cM log (min{k, M }), with c = Ω(1). We give an adversary who ensures that there at most O(log k) bits of information are transmitted every c log k macro-timesteps. Thus, for c = Ω(1), the rate is bounded above by O( 1 c ) = O(1) for all values of M , and hence the capacity is ﬁnite.\nAssume ﬁrst that k ≤ M . The adversary breaks the sent string into blocks of length D max , and delays every sent symbol to the end of its block. The adversary clearly never introduces a delay longer than D max micro- timesteps. Each received block can only take k + 1 values: all bits of the received block will be 0, except for the last symbol which can take any integer value between 0 and k. Thus, only O(log k) bits of information are transmitted every D max = cM log k micro-timesteps, or c log k macro-timesteps, demonstrating ﬁnite capacity.\nIf k > M , then the adversary is the same as above, where the block size is D max = cM log M . Each received block can only take one of cM log M +1 values, since all bits of the block are 0, except for the last symbol which may vary between 0 and cM log M . Thus, only log(cM log M ) = O(c log M ) bits of information are transmitted every c log M macro-timesteps, completing the proof.\nTheorem IV.1. If D avg is the average-delay bound for the AVG- k channel, then then the capacity of the channel\nis inﬁnite when D avg = o( M min{k, M }), and the capacity is ﬁnite when D avg = Ω( M min{k, M }).\nProof: Inﬁnite capacity regime. Suppose D avg = c\nM k, where c = o(1) (that is, again, c is a function of M that is subconstant in M ). Let T be the message duration. Assume without loss of generality that c ≥ 1 M k and k ≤ M (smaller values of c and larger values of k only make communication easier).\nSuppose the sender wants to send a message x ∈ {0, 1} s T ,M with s T ,M = T /c. As in [2, Lemma 4.1], we use a concatenated code: we assume that x has already been encoded under a classical error-correcting code C that corrects a 1/5-fraction of adversarial errors (or any other constant less than 1/4), as this will only affect the rate achieved by our scheme by a constant factor. C is then concatenated with the following inner code, which is tailored for resilience against delay errors: each bit of x gets encoded into a block of length 2 = 2cM : 0\u2019s map to 2 0\u2019s (called a 0-block), and 1\u2019s map to\n1\u2019s followed by 0\u2019s (called a 1-block). The resulting codeword is thus M T symbols long as required.\nFor decoding, let Y = Y 1 , . . . , Y M T be the received word. The receiver divides Y into blocks of length . Let γ(i) = j∈[iM T ,...,(i+1)M T −1] Y j denote the number of 1s encountered in the ith block. The receiver decodes Y as a message y ∈ {0, 1} s T ,M where y i is declared to be 1 if γ(i) ≥\nk ≥ 1. Finally, the receiver will decode y using the outer decoder to obtain the original message. By the error-correcting properties of the outer code C, it sufﬁces to show that at least 4/5ths of the inner-code blocks get decoded correctly.\nWe use a potential argument to demonstrate that the adversary can afford to corrupt a vanishingly small frac- tion of the blocks. We maintain a potential function Φ(i) that measures the total amount of delay the adversary can apply after performing the ith action (where an action is delaying a single symbol some distance). Initially, Φ(0) = M T D avg .\nTurning a 0-block into a 1-block requires the adversary to delay at least\nk 1 symbols from some previous block at least a distance /2, so this requires reducing Φ by Ω( 3/2\nk). To turn a 1-block into a 0-block, the adversary can either 1) move 1 symbols out of the 1- block (evicting 1s), or 2) collide 1s within the 1-block, or 3) a combination of both. We show that any combination requires reducing Φ by Ω( 3/2\nSuppose the adversary chooses to corrupt a 1-block by evicting δ 1 symbols, and colliding the remaining 1 symbols so that at most\nk 1s remain. The adversary minimizes the amount of delays it spends to do this by evicting the last δ 1s from a block, and choosing α equally spaced \u201ccollision points\u201d (CPs) within the remaining 1s, where each remaining 1 symbol is delayed\nto the nearest CP ahead of it. Evicting δ 1s out of the block requires the adversary to spend at least δ delays. Each CP receives ( − δ)/α 1 symbols, and the amount of delays spent per CP is 1 + 2 + · · · + ( − δ)/α = Θ ( −δ) 2 α 2 . Thus, the total amount of delay spent by\nthe adversary to corrupt the 1-block is Ω ( −δ) 2 α + δ . This is minimized when δ = 0, i.e. when no symbols are evicted. Since αk ≤\nk (because each CP will have value k in the received string if at least k 1s are delivered at that index), the adversary needs to use Ω( 3/2\nk) units of potential in order to corrupt a 1-block.\nk) accounts for corrupting at most a block and its adjacent neighbor. Thus, the maximum num- ber of blocks corruptable is 2Φ(0)/Ω( 3/2\nk) = O(c(M/ ) 3/2 T ) = O(T /\nword had a total of T /c blocks, the maximum fraction of blocks corruptable is O(\nc). However, c = o(1), so a vanishingly small fraction of blocks are corrupted, and the original message can be recovered. Thus, we have constructed a valid codebook of size 2 Ω(T /c) , and this implies that the capacity is inﬁnite.\nFinite capacity regime. Suppose the delays have bounded average D avg = c M min{k, M }, for some constant c. We will assume for simplicity that c = 1 and k ≤ M , and explain how to handle smaller values of c and larger values of k later. We show the capacity is ﬁnite by specifying an adversary who ensures that there are a constant number of possible received strings for almost every macro-timestep.\nTo accomplish this, the adversary will break the sent string into blocks of length M . It scans the blocks sequentially, and adds and removes 1s so that each block will have 1s only at indices that are multiples of D avg , or at the very last index of the block. The adversary ensures that it can always add 1s when it needs to by maintaining a \u201cbank\u201d of delayed 1s from previous blocks that will have size between D avg and 2D avg 1s whenever possible. The bank will always be small enough so that it does not contribute too many delays to the average. Once the bank reaches size D avg , its size never falls below this level again. We show that the amount of information transmitted before the bank reaches this size is negligible for large T .\nThe adversary considers each block in turn, and its actions falls into four cases. Let denote the number of 1s in the block, and let s denote the size of the bank at the start of the block.\na) If s ≥ D avg + k − , the adversary will delay all 1s in the block until the ﬁnal index within the block. If < k, the adversary will also deliver k − 1s from the bank at the ﬁnal index to\nensure that the value of the ﬁnal index is k. When this step completes, the size of the bank will be between D avg and s .\nb) If s < D avg + k − , the adversary adds all 1s in the block to the bank, ensuring that the received block consists entirely of 0s. When this step completes, the bank has size least s and at most D avg + k ≤ 2D avg .\na) If s ≤ D avg , the adversary adds D avg − s < of the new 1s to the bank, and it delays the rest of the 1s to the nearest integer multiple of D avg .\nb) Otherwise, s will be at least D avg . The adversary will place k 1s at every location which is an integer multiple of D avg using bits from its bank (this requires at most kM/D avg = kM/\nkM = D avg bits), and delays the ﬁrst −D avg 1s within the current block to the nearest integer multiple of D avg . The last D avg 1s get added to the bank to replace the 1s lost from the bank, so the bank stays at size s.\nM k log k + O(T ) bits of information are transmitted over T blocks by the above scheme. Once the bank reaches size D avg , there are only three possible values for each received block: the all- zeros vector; the vector that is all 0s except for the ﬁnal index which has value k; and the vector that is all 0s except for indices which are integer multiples of D avg , which have value exactly k. Before the bank reaches size D avg , any light block is still received as either the ﬁrst or second possibility just described. Finally, at most one heavy block is encountered before the bank reaches size D avg , and this block can take on at most k M/D avg ≤ k\nM k possible values. Thus, over all T blocks, at most\nM k log k + O(T ) bits of information are transmitted, and hence the capacity is ﬁnite.\nFinally, we bound the average delay incurred by the adversary. For each block, we separately bound the total delays incurred by the symbols banked at the beginning of the block and symbols within the block. The symbols within any light block are responsible for total delay at most M D avg , since at most D avg symbols are delayed at most M . The symbols within in any heavy block are responsible for total delay at most 2M D avg , since all but at most D avg 1s are delayed only until the nearest integer multiple of D avg , and the rest are delayed at most M . As the bank contains at most 2D avg 1s, banked symbols contribute at most 2M D avg total delays per block. The adversary therfore spends at most 4M D avg total delays per block, for an average delay of 4D avg . To reduce this to D avg , we modify the above construction to use a block length of M/16 micro-timesteps, decreasing the average delay appropriately while increasing the rate by only a\nIt remains to explain how to handle cases c < 1 and k > M . If c < 1, we simply decrease the block size further, from M/16 to M c 2 /16. This decreases the average delay by a factor of c and increases the rate by only a constant factor. For k > M , we note the adversary described above never delivers more than M 1s at any particular micro-timestep. Thus, even if k > M , the the received string is the same as it would be if k = M .\nUnder the deﬁnition of capacity used in the sections above and in [2], lim sup M →∞ lim sup T →∞ {k M,T /T }, the sender and receiver are not allowed to let the granu- larity of time grow with T . If we instead deﬁne the ca- pacity to be lim sup T →∞ lim sup M →∞ {k M,T /T }, then the channel would behave very differently. Conceptually, the reason is that if M is allowed to grow with T , the sender and receiver can choose M to be so much larger than T that a vast amount of information (relative to T ) can be encoded in just the ﬁrst macro-timestep, avoiding interference issues.\nTo demonstrate one place where this interchange of limits alters the channel capacity, we show the AVG-1 channel behaves differently under this deﬁnition.\nTheorem V.1. If one interchanges the order of limits in the deﬁnition of channel capacity, then the capacity of the AVG-1 channel with D avg = o(M ) is inﬁnite.\nProof: The idea is that the sender encodes ω(1) bits of information via the location of the ﬁrst 1 in the entire codeword. More formally, suppose D avg = cM − 1 with c = o(1), and let c = c/2. Assume for simplicity that M c is an integer. We will construct a valid codebook C ⊆ {0, 1} M T with |C| = Ω(1/c ) = ω(1) such that for each message x ∈ C, the last T − 1 macro-timesteps consist only of 0s. Thus, we only specify the ﬁrst macro- timestep in each codeword x. In the ﬁrst codeword, the ﬁrst macro-timestep will simply be M c 0s followed by M − M c 1s. In the second codeword, the ﬁrst macro- timestep will be 2M c 0s followed by M − 2M c 1s. In general, in the ith codeword, the ﬁrst macro-timestep will be iM c 0s followed by M − iM c 1s.\nThe decoder will look at the position L of the left- most 1 in the received string and output the largest i such that iM c ≤ L.\nIn order for the adversary to force the decoder to decode incorrectly, the decoder has to make the ﬁrst 1 appear at least c · M positions later than it does in the sent string. For this to happen, the adversary has to spend at least 1 + 2 + · · · + M c ≥ M 2 c 2 /2 delays in total. So the average delay has to be at least M c 2 2T = M c 4T . For ﬁxed T , this is Ω(M ).\nIn this section we note that the combination of inter- ference with noise yields a max-bounded adversary that is surprisingly potent. We omit the proof of the statement for lack of space (it appears in the arXiv version).\nTheorem V.2. Suppose the adversary is allowed to ﬂip t bits per macro-timestep, and delay each bit a maximum of D max micro-timesteps. Then the capacity of the 1- interference channel is ﬁnite if D max · t = Ω(M ), and is inﬁnite if D max · t = o(M ). In particular, the capacity is ﬁnite if t = D max = Ω(\nWe studied a variety of natural models for continuous- time channels with delay errors in the presence of interference. Our results show that these channels exhibit a clean threshold behavior. Further, the results can be viewed as a counterweight to those of Khanna and Sudan [2], by showing that other natural additional restrictions can lead to ﬁnite capacity in their model.\nMany questions remain for future work. Our results only address adversarial delays; random delays under our interference model remains open. One might also consider different models of interference, or different limitations on the delays introduced by the adversary.\nAcknowledgments: Michael Mitzenmacher was sup- ported by NSF grants CCF-0915922 and IIS-0964473. Justin Thaler was supported by the Department of De- fense through the National Defense Science & Engi- neering Graduate Fellowship (NDSEG) Program, and by NSF grant CCF-0915922. Henry Yuen was supported by an MIT Presidential Fellowship."},"refs":[{"authors":[{"name":"R. V. L. Hartley"}],"title":{"text":"Transmission of information"}},{"authors":[{"name":"S. Khann"},{"name":"M. Sudan"},{"name":"R. Ostrovsk"}],"title":{"text":"Delays and the capacity of continuous- time channels"}},{"authors":[{"name":"H. Nyquist"}],"title":{"text":"Certain Factors Affecting Telegraph Speed"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"Communication in the Presence of Noise"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564411.pdf"},"links":[{"id":"1569565663","weight":10},{"id":"1569559617","weight":10},{"id":"1569566981","weight":10},{"id":"1569565551","weight":10},{"id":"1569552245","weight":10},{"id":"1569560833","weight":10},{"id":"1569566469","weight":10},{"id":"1569565355","weight":10},{"id":"1569564469","weight":10},{"id":"1569560427","weight":10},{"id":"1569566941","weight":10},{"id":"1569556713","weight":10},{"id":"1569566467","weight":10},{"id":"1569566843","weight":10},{"id":"1569565455","weight":10},{"id":"1569566269","weight":10},{"id":"1569566095","weight":10},{"id":"1569559565","weight":10},{"id":"1569565213","weight":10},{"id":"1569566437","weight":10},{"id":"1569565915","weight":10},{"id":"1569566425","weight":10},{"id":"1569554881","weight":10},{"id":"1569565055","weight":10},{"id":"1569555879","weight":10},{"id":"1569565219","weight":10},{"id":"1569566553","weight":10},{"id":"1569566603","weight":10},{"id":"1569566275","weight":10},{"id":"1569560503","weight":10},{"id":"1569565439","weight":10},{"id":"1569562551","weight":10},{"id":"1569563395","weight":10},{"id":"1569566901","weight":10},{"id":"1569565571","weight":10},{"id":"1569565665","weight":10},{"id":"1569566873","weight":10},{"id":"1569565435","weight":10},{"id":"1569566253","weight":10},{"id":"1569566813","weight":10},{"id":"1569564437","weight":10},{"id":"1569563975","weight":10},{"id":"1569565271","weight":10},{"id":"1569566397","weight":10},{"id":"1569563919","weight":10},{"id":"1569557851","weight":10},{"id":"1569565389","weight":10},{"id":"1569565853","weight":10},{"id":"1569565165","weight":10},{"id":"1569566797","weight":10},{"id":"1569565113","weight":10},{"id":"1569566375","weight":10},{"id":"1569564257","weight":10},{"id":"1569564509","weight":10},{"id":"1569566067","weight":10},{"id":"1569566443","weight":10},{"id":"1569560581","weight":10}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T7.2","endtime":"10:30","authors":"Ioana Ivan, Michael Mitzenmacher, Justin Thaler, Henry Yuen","date":"1341310200000","papertitle":"Continuous Time Channels with Interference","starttime":"10:10","session":"S5.T7: Continuous Time Channels and Shannon Ordering","room":"Stratton (407)","paperid":"1569564411"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
