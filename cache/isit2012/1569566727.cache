{"id":"1569566727","paper":{"title":{"text":"Polar Codes: Robustness of the Successive Cancellation Decoder with Respect to Quantization"},"authors":[{"name":"S. Hamed Hassani"},{"name":"R¨udiger Urbanke"}],"abstr":{"text":"Abstract\u2014Polar codes provably achieve the capacity of a wide array of channels under successive decoding. This assumes inﬁnite precision arithmetic. Given the successive nature of the decoding algorithm, one might worry about the sensitivity of the performance to the precision of the computation.\nWe show that even very coarsely quantized decoding algo- rithms lead to excellent performance. More concretely, we show that under successive decoding with an alphabet of cardinality only three, the decoder still has a threshold and this threshold is a sizable fraction of capacity. More generally, we show that if we are willing to transmit at a rate δ below capacity, then we need only c log(1/δ) bits of precision, where c is a universal constant."},"body":{"text":"Since the invention of polar codes by Arikan, [1], a large body of work has been done to investigate the pros and cons of polar codes in different practical scenarios (for a partial list see [2]-[9]).\nWe address one further aspect of polar codes using succes- sive decoding. We ask whether such a coding scheme is robust. More precisely, the standard analysis of polar codes under suc- cessive decoding assumes inﬁnite precision arithmetic. Given the successive nature of the decoder, one might worry how well such a scheme performs under a ﬁnite precision decoder. A priori it is not clear whether such a coding scheme still shows any threshold behavior and, even if it does, how the threshold scales in the number of bits of the decoder.\nWe show that in fact polar coding is extremely robust with respect to the quantization of the decoder. In Figure 1, we show the achievable rate using a simple successive decoder with only three messages, called the decoder with erasures, when transmission takes place over several important channel families. As one can see from this ﬁgure, in particular for channels with high capacity, the fraction of the capacity that is achieved by this simple decoder is close to 1, i.e., even this extremely simple decoder almost achieves capacity. We further show that, more generally, if we want to achieve a rate which is δ below capacity by δ > 0, then we need at most c log(1/δ) bits of precision (all the logarithms in this paper are in base 2).\nThe signiﬁcance of our observations goes beyond the pure computational complexity which is required. Typically, the main bottleneck in the implementation of large high speed coding systems is memory. Therefore, if one can ﬁnd de- coders which work with only a few bits per message then\nthis can make the difference whether a coding scheme is implementable or not.\nLet W : X → Y be a binary memoryless symmetric (BMS) channel, with input alphabet X = {0, 1}, output alphabet Y, and the transition probabilities {W (y | x) : x ∈ X , y ∈ Y}. Also, let I(W ) denote the capacity of W .\nLet G 2 = [ 1 0 1 1 ]. The generator matrix of polar codes is deﬁned through the Kronecker powers of G 2 , denoted by G N = G ⊗n 2 . Throughout the paper, the variables N and n are related as N = 2 n . Let us review very brieﬂy how the generator matrix of polar codes is constructed. Consider the N × N matrix G N and let us label the rows of the matrix G N from top to bottom by 0, 1, · · · , N − 1. Now assume that we desire to transmit binary data over the channel W at rate R < I(W ) with block-length N . One way to accomplish this is to choose a subset I ⊆ {0, · · · , N − 1} of size N R and to construct a vector U N −1 0 \t = (U 0 , · · · , U N −1 ) in a way that it contains our N R bits of data at positions in I and contains, at positions not in I, some ﬁxed value (for example 0) which is known to both the encoder and decoder. We then send the codeword X N −1 0 \t = U N −1 0 G N through the channel W . We refer to the set I as the set of chosen indices or information\nindices and the set I c is called the set of frozen indices. We explain in Section II-A how the good indices are chosen. At the decoder, the bits u 0 , · · · , u N −1 are decoded one by one. That is, the bit u i is decoded after u 0 , · · · u i−1 . If i is a frozen index, its value is known to the decoder. If not, the decoder estimates the value of u i by using the output y N −1 0 \t and the estimates of u 0 , · · · , u i−1 .\nLet R ∗ = R ∪ {±∞} and consider a function Q(x) : R ∗ → R ∗ that is anti-symmetric (i.e., Q(x) = −Q(−x)). We deﬁne the Q-quantized SC decoder as a version of the SC decoder in which the function Q is applied to the output of any computation that the SC decoder does. We denote such a decoder by SCD Q .\nTypically, the purpose of the function Q is to model the case where we only have ﬁnite precision in our computations perhaps due to limited available memory or due to other hardware limitations. Hence, the computations are correct within a certain level of accuracy which the function Q models. Thus, let us assume that the range of Q is a ﬁnite set Q with cardinality | Q | . As a result, all the messages passed through the decoder SCD Q belong to the set Q.\nIn this paper we consider a simple choice of the function Q that is speciﬁed by two parameters: The distance between levels ∆, and truncation threshold M . Given a speciﬁc choice of M and ∆, we deﬁne Q as follows:\n \nTheorem 1 (Main Statement): Consider transmission over a BMS channel W of capacity I(W ) using polar codes and a SCD Q with message alphabet Q. Let C(W, Q) denote the maximum rate at which reliable transmission is possible for this setup.\n(i) Let |Q| = 3. Then there exists a computable decreas- ing sequence {U n } n∈N (see (19)) and a computable increasing sequence {L n } n∈N (see (20)), so that L n ≤ C(W, Q) ≤ U n and\nIn other words, U n is an upper bound and L n is a lower bound on the maximum achievable rate C(W, Q) and for increasing n these two bounds converge to C(W, Q).\n(ii) To achieve an additive gap δ > 0 to capacity I(W ), it sufﬁces to choose log |Q| = c log(1/δ).\nDiscussion: In Figure 1 the value of C(W, Q), |Q| = 3, is plotted as a function of I(W ) for different channel families (for more details see Section II-D2). A universal lower bound for the maximum achievable rate is also given in Figure 1. This suggests that even for small values of |Q| polar codes are very robust to quantization. In particular for channels with capacity\nclose to 1, very little is lost by quantizing. The methods used here are extendable to other quantized decoders.\nThe rest of the paper is devoted to proving the ﬁrst part of Theorem 1. Due to space limitation, we have omitted the proof of the second part of theorem 1 as well as the proofs of the lemmas stated in the sequel and we refer the reader to [10] for more details.\nA. Equivalent tree channel model and analysis of the proba- bility of error for the original SC decoder\nSince we are dealing with a linear code, a symmetric chan- nel and symmetric decoders throughout this paper, without loss of generality we conﬁne ourselves to the all-zero codeword (i.e., we assume that all the u i \u2019s are equal to 0). In order to better visualize the decoding process, the following deﬁnition is handy.\nDeﬁnition 2 (Tree Channels of Height n): For each i ∈ {0, 1, · · · , N − 1}, we introduce the notion of the i-th tree channel of height n which is denoted by T (i). Let b 1 . . . b n be the n-bit binary expansion of i. E.g., we have for n = 3, 0 = 000, 1 = 001, . . . , 7 = 111. With a slight abuse of notation we use i and b 1 · · · b n interchangeably. Note that for our purpose it is slightly more convenient to denote the least (most) signiﬁcant bit as b n (b 1 ). Each tree channel consists of n + 1 levels, namely 0, . . . , n. It is a complete binary tree. The root is at level n. At level j we have 2 n−j nodes. For 1 ≤ j ≤ n, if b j = 0 then all nodes on level j are check nodes; if b j = 1 then all nodes on level j are variable nodes. Finally, we give a label for each node in the tree T (i): For each level j, we label the 2 n−j nodes at this level respectively from left to right by (j, 0), (j, 1), · · · , (j, 2 n−j − 1).\nAll nodes at level 0 correspond to independent observations of the output of the channel W , assuming that the input is 0.\nAn example for T (3) (that is n = 3, b = 011 and i = 3) is shown in Fig. 2.\nGiven the channel output vector y N −1 0 \t and assuming that the values of the bits prior to u i are given, i.e., u 0 = 0, · · · , u i−1 = 0, we now compute the probabilities p(y N −1 0 , u i−1 0 | u i = 0) and p(y N −1 0 , u i−1 0 | u i = 1) via a simple message passing procedure on the equivalent tree\nchannel T (i). We attach to each node in T (i) with label (j, k) a message 1 m j,k and we update the messages as we go up towards the root node. We start with initializing the messages at the leaf nodes of T (i). For this purpose, it is convenient to represent the channel in the log-likelihood domain; i.e., for the node with label (0, k) at the bottom of the tree which corresponds to an independent realization of W , we plug in the log-likelihood ratio (llr) log( W (y k | 0) W (y\n) as the initial message m 0,k . That is,\nW (y k | 1) ). \t (2) Next, the SC decoder recursively computes the messages\n(llr\u2019s) at each level via the following operations: If the nodes at level j are variable nodes (i.e., b j = 1), we have\nand if the nodes at level j are check nodes (i.e., b j = 0), the message that is passed up is\nm j,k = 2 tanh −1 (tanh( m j−1,2k 2\n) tanh( m j−1,2k+1 2\nIn this way, it can be shown that ([1]) the message that we obtain at the root node is precisely the value\np(y N −1 0 , u i−1 0 | u i = 0) p(y N −1 0 , u i−1 0 | u i = 1)\nNow, given (y N −1 0 , u i−1 0 ), the value of u i is estimated as follows. If m n,0 > 0 we let u i = 0. If m n,0 < 0 we let u i = 1. Finally, if m n,0 = 0 we choose the value of u i to be either 0 or 1 with probability 1 2 . Thus, denoting E i as the event that we make an error on the i-th bit within the above setting, we obtain\n2 Pr(m n,0 = 0). \t (6) Given the description of m n,0 in terms of a tree channel, it\nis now clear that we can use density evolution [2] to compute the probability density function of m n,0 . In this regard, at each level j, the random variables m j,k are i.i.d. for k ∈ {0, 1, · · · , 2 n−j − 1}. The distribution of the leaf messages m 0,k is the distribution of the variable log( W (Y | 0) W (Y | 1) ), where Y ∼ W (y | 0). One can recursively compute the distribution of m j,k in terms of the distribution of m j−1,2k , m j−1,2k+1 and the type of the nodes at level j (variable or check) by using the relations (3), (4) with the fact that the random variables m j−1,2k and m j−1,2k+1 are i.i.d.\nLet us now analyze the density evolution procedure for the quantized decoder. For each label (j, k) in T (i), let ˆ m j,k represent the messages at this label. The messages ˆ m j,k take their values in the discrete set Q (range of the function Q). It is now easy to see that for the decoder SCD Q the messages\nevolve via the following relations. At the leaf nodes of the tree we plug in the message ˆ m 0,k = Q(log( W (y k | 0) W (y\n)), and the update equation for ˆ m (j,k) is\n(8) if the node (j, k) is a check node. One can use the density evolution procedure to recursively obtain the densities of the messages ˆ m j,k .\nFinally, let ˆ E i denote the event that we make an error in decoding the i-th bit, with a further assumption that we have correctly decoded the previous bits u 0 , · · · , u i−1 . In a similar way as in the analysis of the original SC decoder, we get\nHence, one way to choose the information bits for the algo- rithm SCD Q is to choose the bits u i according to the least values of Pr( ˆ E i ).\nAn important point to note here is that with the decoder SCD Q , the distribution of the messages in the trees T (i) is different than the corresponding ones that result from the original SC decoder. Hence, the choice of the information indices is also speciﬁed by the choice of the function Q as well as the channel W .\nNote here that, since all of the densities takes their value in the ﬁnite alphabet Q, the construction of such polar codes can be efﬁciently done in time O( | Q | 2 N log N ). We refer the reader to [1] for more details.\nSince our aim is to show that polar codes under successive decoding are robust against quantization, let us investigate an extreme case. The perhaps simplest message-passing type decoder one can envision is the Gallager algorithm. It works with single-bit messages. Does this simple decoder have a non- zero threshold? Unfortunately it does not, and this is easy to see. We start with the equivalent tree-channel model. Consider an arbitrary tree-channel T (i). Since messages are only a single bit, the \u201cstate\u201d of the decoder at level j of T (i) can be described by a single non-negative number, namely the probability that the message at level j is incorrect. It is an easy exercise to show that at a level with check nodes the state becomes worse and at a level with variable nodes the state stays unchanged and hence no progress in the decoding is achieved, irrespective of the given tree. In other words, this decoder has a threshold of zero. The problem is the processing at the variable nodes since no progress is achieved there. But since we only have two possible incoming messages there is not much degree of freedom in the processing rules.\nMotivated by the previous example, let us now add one message to the alphabet of the Gallager decoder, i.e., we also add the possibility of having erasures. In this case Q(x) becomes the sign function 2 , i.e.,\n \n∞, x > 0, 0, x = 0,\nAs a result, all messages passed by the algorithm SCD Q take on only three possible values: {−∞, 0, ∞}. In this regard, the decoding procedure takes a very simple form. The algorithm starts by quantizing the channel output to one of the three values in the set Q = {−∞, 0, ∞}. At a check node we take the product of the signs of the incoming messages and at a variable node we have the natural addition rule (0 ← ∞ + −∞, 0 ← 0 + 0 and ∞ ← ∞ + ∞, ∞ ← ∞ + 0 and −∞ ← −∞ + −∞, −∞ ← −∞ + 0 ). Note that on the binary erasure channel, this algorithm is equivalent to the original SC decoder.\nOur objective is now to compute the maximum reliable rate that the decoder SCD Q can achieve for a BMS channel W . We denote this quantity by C(W, Q). The analysis is done in three steps:\n1) The density evolution procedure: To analyze the perfor- mance of this algorithm, ﬁrst note that since all our messages take their values in the set Q, then all the random variables that we consider have the following form\n \n∞, \t w.p. p, 0, \t w.p. e,\nHere, the numbers p, e, m are probability values and p + e + m = 1. Let us now see how the density evolves through the tree-channels. For this purpose, one should trace the output distribution of (7) and (8) when the input messages are two i.i.d. copies of a r.v. D with pdf as in (11).\nLemma 3: Given two i.i.d. versions of a r.v. D with distri- bution as in (11), the output of a variable node operation (7), denoted by D + , has the following form\n \n∞, \t w.p. p 2 + 2pe, 0, \t w.p. e 2 + 2pm, −∞, w.p. m 2 + 2em.\n \n∞, \t w.p. p 2 + m 2 , 0, w.p. 1 − (1 − e) 2 ,\nIn order to compute the distribution of the messages ˆ m n,0 at a given level n, we use the method of [1] and deﬁne the polarization process D n as follows. Consider the random variable L(Y ) = log( W (Y | 0) W (Y | 1) ), where Y ∼ W (y | 0). The\nstochastic process D n starts from the r.v. D 0 = Q(L(Y )) deﬁned as\n \n∞, \t w.p. p = Pr(L(Y ) > 0), 0, \t w.p. e = Pr(L(Y ) = 0), −∞, w.p. m = Pr(L(Y ) < 0),\nD − n , w.p. 1 2 , \t (15) where the plus and minus operations are given in (12), (13).\n2) Analysis of the process D n : Note that the output of process D n is itself a random variable of the form given in (11). Hence, we can equivalently represent the process D n with a triple (m n , e n , p n ), where the coupled processes m n , e n and p n are evolved using the relations (12) and (13) and we always have m n + e n + p n = 1. Following along the same lines as the analysis of the original SC decoder in [1], we ﬁrst claim that as n grows large, the process D n will become polarized, i.e., the output of the process D n will almost surely be a completely noiseless or a completely erasure channel.\nLemma 4: The random sequence {D n = (p n , e n , m n ), n ≥ 0} converges almost surely to a random variable D ∞ such that D ∞ takes its value in the set {(1, 0, 0), (0, 1, 0)}.\nWe now aim to compute the value of C(W, Q) = Pr(D ∞ = (1, 0, 0)), i.e., the highest rate that we can achieve with the 1-Bit Decoder with Erasures. In this regard, a convenient approach is to ﬁnd a function f : D → R such that f ((0, 1, 0)) = 0 and f (1, 0, 0) = 1 and for any D ∈ D\nWith such a function f , the process {f (D n )} n≥0 is a martin- gale and consequently we have Pr(D ∞ = (1, 0, 0)) = f (D 0 ). Therefore, by computing the deterministic quantity f (D 0 ) we obtain the value of C(W, Q). However, ﬁnding a closed form for such a function seems to be a difﬁcult task 3 . Instead, the idea is to look for alternative functions, denoted by g : D → R, such that the process g(D n ) is a super-martingale (sub- martingale) and hence we can get a sequence of upper (lower) bounds on the value of Pr(D ∞ = (1, 0, 0)) as follows. Assume we have a function g : D → R such that g((0, 1, 0)) = 0 and g(1, 0, 0) = 1 and for any D ∈ D,\n1 2\nThen, the process {g(D n )} n≥0 is a super-martingale and for n ≥ 0 we have\nThe quantity E[g(D n )] decreases by n and by using Lemma 4 we have\nIn a similar way, on can search for a function h : D → R such that for h with the same properties as g except that the inequality (16) holds in opposite direction and in a similar way this leads us to computable lower bounds on C(W, Q). It remain to ﬁnd some suitable candidates for g and h. Let us ﬁrst note that a density D as in (11) can be equivalently represented as a simple BMS channel given in Fig. 3. This equivalence\nstems from the fact that for such a channel, conditioned on the event that the symbol +1 has been sent, the distribution of the output is precisely D. With a slight abuse of notation, we also denote the corresponding BMS channel by D. In particular, it is an easy exercise to show that the capacity (I(D)), the Bhattacharyya parameter (Z(D)) and the error probability (E(D)) of the density D are given as\nZ(D) = 2 √ mp + e, E(D) = 1 − p − e 2\nwhere h 2 (·) denotes the binary entropy function. Since the function Q is a not an injective function, we have\n≤ I(D). This implies that the process I n = I(D n ) is a bounded supermartingale. Furthermore, since I(D = (1, 0, 0)) = 1 and I(D = (0, 1, 0)) = 0, we deduce from Lemma 4 that I n converges a.s. to a 0 − 1 valued r.v. I ∞ and hence\nfor n ∈ N. In a similar way, one can obtain a sequence of lower bounds for C(W, Q).\nLemma 5: Deﬁne the function F (D) as F (D) = p−4 √ pm for D ∈ D. We have F (D = (1, 0, 0)) = 1, F (D = (0, 1, 0)) = 0 and F (D + )+F (D − ) 2 \t ≥ F (D).\nHence, the process F n = F (D n ) is a submartingale and for n ∈ N we have\nGiven a BMS channel W , one can numerically compute C(W, Q) with arbitrary accuracy using the sequences L n and U n (see Figure 1). Also, for a channel W with capacity I(W ) and error probability E(W ), we have\nExample 6: Let the channel W be a BSC channel with cross over probability = 0.11 (hence I(W ) ≈ 0.5). Using (22) we obtain\nTherefore, we get L 0 = F (D 0 ) = −0.361 and U 0 = I(D 0 ) = 0.5. We can also compute L 1 = F (D + 0 )+F (D − 0 ) 2 \t = −0.191,\nContinuing this way, one can ﬁnd L 10 = 0.264, U 10 = 0.474 and L 20 = 0.398, U 20 = 0.465 and so on. \t ♦\n3) Scaling behavior and error exponent: In the last step, we need to show that for rates below C(W, Q) the block-error probability decays to 0 for large block-lengths.\nHence, for transmission rate R < C(W, Q) and block-length N = 2 n , the probability of error of SCD Q , denoted by P e,Q (N, R) satisﬁes P e,Q (N, R) = o(2 −N β ) for β < log 3 2 2 .\nThe authors wish to thank anonymous reviewers for their valuable comments on an earlier version of this manuscript. The work of Hamed Hassani was supported by Swiss National Science Foundation Grant no 200021-121903."},"refs":[{"authors":[{"name":"E. Arıkan"}],"title":{"text":"Channel polarization: A method for constructing capacity- achieving codes for symmetric binary-input memoryless channels"}},{"authors":[{"name":"R. Mori"},{"name":"T. Tanaka"}],"title":{"text":"Performance and construction of polar codes on symmetric binary-input memoryless channels"}},{"authors":[{"name":"I. Tal"},{"name":"A. Vardy"}],"title":{"text":"How to construct polar codes"}},{"authors":[{"name":"C. Leroux"},{"name":"I. Tal"},{"name":"A. Vardy"},{"name":"W. J. Gross"}],"title":{"text":"Hardware architectures for successive cancellation decoding of polar codes"}},{"authors":[{"name":"R. Pedarsani"},{"name":"S. H. Hassani"},{"name":"I. Tal"},{"name":"E. Telatar"}],"title":{"text":"On the construction of polar codes"}},{"authors":[{"name":"I. Tal"},{"name":"A. Vardy"}],"title":{"text":"List decoding of polar codes"}},{"authors":[{"name":"S. B. Korada"},{"name":"A. Montanari"},{"name":"E. Telatar"},{"name":"R. Urbanke"}],"title":{"text":"An empirical scaling law for polar codes"}},{"authors":[{"name":"S. H. Hassani"},{"name":"K. Alishahi"},{"name":"R. Urbanke"}],"title":{"text":"On the scaling of polar codes: II. The behavior of un-polarized channels"}},{"authors":[{"name":"S. H. Hassani"},{"name":"S. B. Korada"},{"name":"R. Urbanke"}],"title":{"text":"The compound capacity of polar codes"}},{"authors":[{"name":"S. H. Hassani"},{"name":"R. Urbanke"}],"title":{"text":"Polar Codes: Robustness of the Succes- sive Cancellation Decoder with Respect to Quantization"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566727.pdf"},"links":[{"id":"1569565883","weight":25},{"id":"1569565867","weight":12},{"id":"1569566605","weight":18},{"id":"1569566683","weight":6},{"id":"1569565551","weight":12},{"id":"1569566871","weight":6},{"id":"1569565461","weight":6},{"id":"1569566207","weight":18},{"id":"1569560427","weight":6},{"id":"1569565317","weight":6},{"id":"1569564249","weight":6},{"id":"1569564613","weight":6},{"id":"1569566617","weight":6},{"id":"1569563307","weight":25},{"id":"1569555999","weight":12},{"id":"1569566657","weight":6},{"id":"1569566369","weight":6},{"id":"1569566581","weight":12},{"id":"1569566423","weight":6},{"id":"1569565257","weight":6},{"id":"1569558901","weight":6},{"id":"1569565735","weight":12},{"id":"1569553909","weight":6},{"id":"1569559111","weight":50},{"id":"1569564441","weight":6},{"id":"1569566425","weight":6},{"id":"1569554971","weight":6},{"id":"1569566809","weight":6},{"id":"1569554759","weight":6},{"id":"1569566223","weight":12},{"id":"1569562207","weight":12},{"id":"1569566191","weight":6},{"id":"1569566655","weight":6},{"id":"1569566245","weight":12},{"id":"1569560503","weight":6},{"id":"1569565885","weight":12},{"id":"1569566805","weight":6},{"id":"1569566293","weight":43},{"id":"1569566983","weight":6},{"id":"1569565765","weight":31},{"id":"1569565215","weight":6},{"id":"1569565093","weight":12},{"id":"1569565241","weight":6},{"id":"1569565661","weight":6},{"id":"1569566737","weight":68},{"id":"1569564595","weight":6},{"id":"1569565353","weight":6},{"id":"1569564305","weight":31},{"id":"1569566547","weight":6},{"id":"1569565177","weight":6},{"id":"1569566595","weight":6},{"id":"1569566639","weight":6},{"id":"1569566755","weight":12},{"id":"1569566713","weight":6},{"id":"1569564437","weight":6},{"id":"1569565529","weight":12},{"id":"1569565271","weight":6},{"id":"1569566075","weight":6},{"id":"1569566397","weight":6},{"id":"1569564281","weight":6},{"id":"1569565769","weight":6},{"id":"1569567691","weight":12},{"id":"1569565861","weight":12},{"id":"1569566147","weight":6},{"id":"1569562367","weight":6},{"id":"1569566847","weight":6},{"id":"1569565997","weight":31},{"id":"1569565731","weight":6},{"id":"1569566797","weight":6},{"id":"1569565707","weight":18},{"id":"1569565143","weight":37},{"id":"1569565031","weight":6},{"id":"1569566067","weight":6},{"id":"1569566825","weight":12},{"id":"1569566443","weight":12},{"id":"1569565315","weight":18}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T5.2","endtime":"10:30","authors":"S. Hamed Hassani, Ruediger L Urbanke","date":"1341483000000","papertitle":"Polar Codes: Robustness of the Successive Cancellation Decoder with Respect to Quantization","starttime":"10:10","session":"S11.T5: Polar Codes:  Theory and Practice","room":"Kresge Little Theatre (035)","paperid":"1569566727"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
