{"id":"1569564305","paper":{"title":{"text":"Polar Coding without Alphabet Extension for Asymmetric Channels"},"authors":[{"name":"Junya Honda"},{"name":"Hirosuke Yamamoto"}],"abstr":{"text":"Abstract\u2014We consider channel coding of binary asymmet- ric memoryless channels with polar codes. The difﬁculty for asymmetric channels comes from the fact that the optimal input probability distributions are not always uniform. S¸as¸o˘glu et al. realized a nonuniform input distribution by mapping multiple auxiliary symbols distributed uniformly to an actual input symbol. However, the complexity of the scheme increases considerably for the case that the input distribution cannot be approximated by simple rational numbers. To overcome this problem, we propose another polar coding scheme for asymmetric channels, which realizes the optimal nonuniform input distribution by randomizing symbols in the frozen bits with an appropriate probability distribution."},"body":{"text":"Recently polar coding is attracting much attention for its achievability of Shannon bound with polynomial complexity. Polar codes are originally proposed by Arikan [1] for binary memoryless symmetric sources and generalized for Galois ﬁelds [2] and arbitrary q-ary alphabets [3]. The idea of polar codes is also extended to lossless and lossy source coding and some multiterminal problems [4].\nWe consider channel coding with polar codes for asymmet- ric memoryless channels. For asymmetric channels, the ideal input probability distribution to achieve the capacity is not always uniform. In the previous works on the polar coding for asymmetric channels, a nonuniform input distribution is generated based on the technique of nonlinear mapping of symbols discussed in [5]. This technique is illustrated by the following example. Consider the case that the optimal input distribution is P X (0) = 2/3 and P X (1) = 1/3 with alphabet X = {0, 1}. This input distribution can be realized by considering a ternary polar code with an extended alphabet X \u2032 = {0, 1, 2}. Mapping symbols 0, 1 ∈ X \u2032 to 0 ∈ X and 2 ∈ X \u2032 to 1 ∈ X in codewords, we obtain codewords on X with the desired distribution.\nAlthough this technique is simple and applicable widely, a large-sized extended alphabet is required if the optimal distribution P X (·) cannot be approximated by simple rational numbers. In such cases, the complexity of decoding increases considerably. The mapping from X \u2032 to X is equivalent to the case that the auxiliary channel W \u2032 : X \u2032 → X with W \u2032 (0|0) = W \u2032 (0|1) = W \u2032 (1|2) = 1 is pre-cascaded to the original channel W . This auxiliary channel does not decrease the channel capacity since we have I(X \u2032 ; Y ) = I(X; Y )−I(X; Y |X \u2032 ) = I(X; Y ) for the cascade of channels W \u2032 (x|x \u2032 ) and W (y|x). But, in the case of ﬁnite code length,\nthe decoding error probability may worsen because the input probability of X \u2032 is restricted to the uniform probability distribution.\nTo overcome these defects, we need to generate the given input distribution P X (·) without any extended alphabet. A key idea to generate a desired input distribution can be found in the lossless compression by polar codes [4]. In this case an original message X n 1 = (X 1 , · · · , X n ) with nonuniform distribution is transformed to U n 1 = X n 1 G n by the generator matrix G n of polar codes. It is shown that the elements of U n 1 polarize into two groups, F and F c . Each bit U i in F is almost uniformly distributed and independent of previous bits U 1 , · · · , U i −1 , and each bit in F c is determined from the previous bits almost surely.\nThis result implies that when we have a uniform source, we can obtain a nonuniform input for a given channel in the following way: (a) choose a value of U i for i ∈ F uniformly, (b) determine each bit U i for i ∈ F c appropri- ately from U 1 · · · , U i −1 and (c) transform U n 1 to X n 1 by X n 1 = U n 1 G (−1) n = U n 1 G n . In the case of the channel coding, the bits of F polarize further into I ⊂ F and F \\ I. Here I is the set of information bits such that each bit of I is almost independent of the previous bits but determined from the channel output almost surely. F \\I is the set of bits almost independent of the previous bits and the channel output. By assigning a message to the bits of I, we can send it with small decoding error probability.\nIn the above scheme, we have to share symbols of I c between the encoder and the decoder. In our scheme we choose symbols in I c with a randomized algorithm, using common randomness or practically pseudo random numbers shared between the encoder and the decoder. The idea of this randomized algorithm comes from the polar coding for lossy compression [6]. As in the case of the lossy coding, the randomization makes the theoretical analysis much easier.\nThis paper is organized as follows. In Sect. II, we give nota- tion and derive basic inequalities on conditional entropies. We examine the channel polarization of the asymmetric channels in Sect. III. In Sect. IV we propose a new polar coding scheme for asymmetric channels and show that it achieves the channel capacity asymptotically. In Sect. V, we give a simulation result of the proposed scheme. Finally we prove the main result of this paper in Sect. VI.\nWe consider a discrete memoryless channel with transition probability W (y|x) for input x ∈ X and output y ∈ Y. Note that W (y|x) is not necessarily symmetric. For simplicity, we assume in the theoretical analysis that X is binary, i.e., X = {0, 1}, although the result can be extended to the nonbinary case, i.e., |X | ≥ 3 in the same way as the case of symmetric channels [2][3].\nThe channel capacity is given by C(W ) = max X I(X; Y ) where Y is a random variable such that the joint probability is given by P X,Y (x, y) = P X (x)W (y|x). In the following of this paper, X and Y always denote the random variables that achieve the channel capacity. We assume without loss of generality that 0 < P X (0) < 1.\nLet X n 1 = (X 1 , · · · , X n ) and Y n 1 = (Y 1 , · · · , Y n ) denote i.i.d. copies of X and Y , respectively. For n = 2 k , the generator matrix for polar codes is given by G n = F ⊗k where F = 1 0 1 1 and ⊗ denotes the Kronecker power. Then U n 1 is deﬁned as U n 1 ≡ X n 1 G (−1) n \t = X n 1 G n . Let X j i , i < j, stand for subvector (X i , · · · , X j ) of X n 1 . Similarly, let X A , A ⊂ {1, · · · , n}, represent subvector {X i } i ∈A .\nIn the case of symmetric channels, Bhattacharyya parameter Z B (W ), which is deﬁned by\nis used to evaluate the error probability. But, in this paper, we use another parameter Z(S; T ) deﬁned by\nfor random variables S ∈ {0, 1} and T . Note that Z(S; T ) coincides with the Bhattacharyya parameter Z B (P T |S ) when S is uniformly distributed.\nIn the case of asymmetric channels, it is convenient to use conditional entropy H(S|T ) rather than mutual information I(S; T ) = H(S) − H(S|T ). For any random variables S ∈ {0, 1} and T , H(S|T ) can be lower and upper bounded by functions of Z(S; T ) as follows.\nWe omit the proof but it can be shown in a similar way to [1, Prop. 1]. From this lemma, H(S|T ) → 1 as Z(S; T ) → 1. Further, H(S|T ) → 0 as Z(S; T ) → 0 and P S (0) → 1 2 .\nIn this section, we show that elements U i of U n 1 = X n 1 G n polarize into three groups, which satisfy (a) H(U i | U i −1 1 ) ≈ H(U i |U i −1 1 , Y n 1 ) ≈ 0, (b) H(U i |U i −1 1 ) ≈ 1,\nH(U i |U i −1 1 , Y n 1 ) ≈ 0 and (c) H(U i |U i −1 1 ) ≈ H(U i |U i −1 1 , Y n 1 ) ≈ 1, respectively. We show in the next section that we\ncan achieve the channel capacity by using the bits of group (b) to transmit a message. Note that the probability distribution of each U i depends on block length n = 2 k . Superscript \u201c( k)\u201d, e.g. Z (k) (U i ; U i −1 1 ) or P (k) U\n, is used to represent the length.\nNow we consider the evolution of joint probabilities for our setting. First, the joint probability of U and Y for n = 1 is given by P (1) U 1\nwhere ⊕ represents the addition on GF(2), and u j 1,e and u j 1,o denote the subsequence of u j 1 with even and odd in- dices, respectively. For marginal probabilities P (k+1) U\nwe obtain similar expressions by taking the sum over all y 2n 1 .\nRemark 1. In the case of symmetric channel [1], the evolution of channel probabilities is given by\nSubstituting (5) into (3) and (4), we have the same expression as the case of the asymmetric channel, (1) and (2). In this way we can develop the argument on the polarization of the parameters Z and H similarly to the symmetric case.\nand substituting 1 − P (k) U i (0) for P (k) U i (1). It is obvious that (ii) follows from the chain rule of the entropy. Further, (iii) and (iv) can be proved by the same argument as for the symmetric channel [1, Prop. 7][6, Lemma 20] with the relation given by (5) in Remark 1.\nFrom this lemma we can obtain the following theorem on the polarization of asymmetric channels. Theorem 1. For any β < 1/2,\nThis theorem asserts that the fraction of the bits U i that are almost noisy given U i −1 1 but almost deterministic given (U i −1 1 , Y n 1 ) approaches the channel capacity.\nSketch of Proof: First we can show from Lemma 2 (i) that ¯ P (k) i → 0, that is, U i is uniformly distributed for almost\nall i as k → ∞. Then, from Lemma 1, the convergence of Z (k) i to 0 or 1 implies the convergence of H (k) i to 0 or 1, respectively. Similarly the convergence of ˜ Z (k) i implies the convergence of ˜ H (k) i . From these relations and Lemma 2 (ii)\u2013 (iv), we can develop the same argument as the symmetric case, which shows that\nfrom the martingale convergence theorem. Combining these two inequalities we obtain the theorem.\nIn this section we construct polar codes which achieve the capacity for asymmetric memoryless channels. A. Code Construction\nAssume that an information set I ⊂ {1, · · · , n} and a frozen set I c = {1, · · · , n} \\ I is given. We use information bits u I to send a message.\nIn the case of symmetric channels, the values of frozen bits u I c are chosen randomly with the uniform distribution on {0, 1} in the code construction but are ﬁxed when the code is used. In our scheme, the values of the frozen bits u I c are deterministic but dependent on the value of previous bits u i −1 1 . Furthermore, unlike the symmetric case, we choose the value of u i given u i −1 1 not uniformly in the code construction.\nLet L i be the family of functions λ i : {0, 1} i −1 → {0, 1}. Now we consider a polar code with frozen set I c and maps\nλ I c ≡ {λ i } i ∈I c . The maps λ I c are used to determine the frozen bits and are shared between the encoder and the decoder.\nLet M denote a message uniformly distributed on {0, 1} |I| . Then the encoder determines a codeword from M in the following way. First, the encoder determines the information bits by u I = M . Next, for the frozen bits I c , the encoder determines the value u i , i ∈ I c , in the ascending order by u i = λ i (u i −1 1 ). We represent the resulting sequence of u i by u n 1 (M, λ I c ). Third, the encoder sends the codeword x n 1 = G n u n 1 = G n u n 1 (M, λ I c ) with code length n. Thus the coding rate is given by R = |I|/n.\nThe decoder receives a sequence y n 1 according to the channel probability W n (y n 1 |x n 1 ). The decoder estimates u n 1 by ˆ u n 1 = ˆ u n 1 (y n 1 , λ I c ) as follow:\nλ i (ˆ u i −1 1 ) \t i ∈ I c . \t (6) The decoding is successful if ˆ u I = u I which means ˆ u n 1 = u n 1 . We deﬁne the decoding error probability by P e (λ I c ) ≡ Pr [ˆ u n 1 (Y n 1 , λ I c ) = u n 1 (M, λ I c )].\nNow consider the choice of maps λ I c . Let Λ I c ≡ {Λ i ∈ L i } i ∈I c be random variables which are independent of each other and of X n 1 , Y n 1 , and satisfy P Λ i [Λ i (u i −1 1 ) = 1] =\nFrom Theorem 1 there exists a subset I of {1, · · · , n} such that |I| = nR and\nZ (k) i ≤ 2 −n β , \t ˜ Z (k) i ≥ 1 − 2 −n β . \t (7) for all i ∈ I if R < I(X; Y ), β < 1/2, and n is sufﬁciently large. For this I the following theorem holds.\nTheorem 2. Let M be a message chosen uniformly from {0, 1} |I| and I ⊂ {1, · · · , n} be a set satisfying (7). Then the expectation of the decoding error probability over the maps Λ I c satisﬁes E Λ Ic [P e (Λ I c )] ≤ 2 −n β for any β < 1/2 and sufﬁciently large n. Consequently, there exists a deterministic map λ I c = {λ i ∈ L i } i ∈I c such that P e (λ I c ) ≤ 2 −n β .\nWe can easily construct a stochastic encoder Λ I c rather than a deterministic encoder λ I c if we share |I c | i.i.d. random vari- ables V I c ≡ {V i } i ∈I c between the encoder and the decoder, where each V i is distributed uniformly on [0, 1]. We obtain Λ I c with desired properties by letting Λ i (u i −1 1 ) = 1 1 [V i ≤ P U i |U i−1\n(0|u i −1 1 , y n 1 )} i ∈I are com- puted. We can compute them with complexity O(n log n) by a dynamic programming technique from the recursive formulae\nRemark 2. The randomized map Λ I c is equivalent to the following randomized mapping of u i , i ∈ I c .\n1 ) , 1 with probability P U i |U i−1\nPractically this randomized mapping can be realized by pseudo random numbers shared between the encoder and the decoder.\nIn this section we compare the proposed scheme with the scheme based on the alphabet extension [5]. In Fig. 1 we used binary asymmetric erasure channel such that erasure probabilities for input 0 and 1 are ǫ 0 = 0.4 and ǫ 1 = 0.8159, respectively. The ideal input distribution of this channel is given by (P X (0), P X (1)) = (7/16, 9/16) and the capacity is C(W ) = 0.4498. In the scheme with the alphabet extension, we used 16-ary polar codes with generator matrix G n = F ⊗k where F = 1 0 γ 1 for a primitive element γ ∈ GF(16). In the proposed scheme we used the binary polar code with generator matrix F ⊗k for F = 1 0 1 1 . Then, the complexity of the proposed scheme is at least 16/2 = 8 times less than that of the scheme with alphabet extension.\nIn the proposed scheme frozen bits have to be chosen so that H(U i |U i −1 1 , Y n 1 ) is small and H(U i |U i −1 1 ) is large. To determine the frozen bits we estimated\nby Monte Carlo simulation on U i −1 1 , Y n 1 , where D(· ·) is the relative entropy. We chose frozen bits in the order that the estimated mutual information is small.\nIn Fig. 1, \u201cAE\u201d denotes the result of the known scheme [5] with the alphabet extension. The plots \u201crandom\u201d denote the scheme proposed in Sect. IV. \u201cMAP\u201d denotes the scheme obtained by replacing the randomization (8) with MAP assign- ment\nAs reported in the case of lossy coding [6], the MAP scheme works better than the randomized scheme although the the- oretical analysis is difﬁcult. Further, in spite of the small complexity, both proposed schemes using (8) and (9) perform comparably to the scheme AE, especially for the region that the redundancy of the rate is small. Note that if the same complexity as the scheme AE is allowed then we can further improve the performance of the proposed scheme by using the same F as the scheme AE and the binary expression of elements of the Galois ﬁeld.\nNote that the channel used in this simulation is somewhat artiﬁcially designed so that the ideal input distribution is a simple rational number. However, in general cases, the ideal input distribution may require a large alphabet size for the alphabet extension. In such cases, the proposed scheme has much advantage since it can deal with any input distribution in the same manner.\nLet E i be the the set of codewords u n 1 = u n 1 (M, Λ I c ) and received words y n 1 such that decoding error occurs at the ith bit. The block decoding error event is given by E ≡ i ∈I E i .\nUnder decoding given in (6) with an arbitrary tie-breaking rule, each E i satisﬁes\nConsider the block decoding error probability P e (Λ I c ) for the map Λ I c . Since each codeword u n 1 (M, Λ I c ) appears with probability\nHence, the expectation of the decoding error probability is bounded as\n· P Y n 1 |U n 1 (y n 1 |u n 1 ) 1 1 [(u n 1 , y n 1 ) ∈ E] . (10) Then, for probability distribution Q U n 1 ,Y n 1 deﬁned as\n(10) can be rewritten as E[P e (Λ I c )] = Q U n 1 ,Y n 1 (E). Let F − G be the variational distance deﬁned by\nfor probability distributions F, G. The variational distance between Q U n 1 ,Y n 1 and P U n 1 ,Y n 1 satisﬁes the following lemma.\nProof: We use an argument similar to [6, Lemma 4] based on the expression\nwhere A j k and B j k denote the products k i =j A i and k i =j B i , respectively.\nFor simplicity, we omit the symbols of random variables, e.g. P (u n 1 , y n 1 ) and Q(u i |u i −1 1 , y n 1 ) for P U n 1 ,Y n 1 (u n 1 , y n 1 ) and\n(u i |u i −1 1 , y n 1 ) in the following. Now P U n 1 ,Y n 1 − Q U n 1 ,Y n 1 is bounded by (14).\n(See below for Eq. (14).) Hence, we obtain\nwhere β \u2032 ∈ (β, 1/2) is arbitrary. Eq. (11) follows since n 2 · 2 −n β\u2032 < 2 −n β for sufﬁciently large n.\nProof of Theorem 2: First we have E[P e (Λ I c )] = Q U n 1 ,Y n 1 (E)"},"refs":[{"authors":[{"name":"E. Arikan"}],"title":{"text":"Channel polarization: a method for constructing capacity- achieving codes for symmetric binary-input memoryless channels"}},{"authors":[{"name":"R. Mori"},{"name":"T. Tanaka"}],"title":{"text":"Channel polarization on q-ary discrete memory- less channels by arbitrary kernels"}},{"authors":[{"name":"E. S¸as¸o˘glu"},{"name":"E. Telatar"},{"name":"E. Arikan"}],"title":{"text":"Polarization for arbitrary discrete memoryless channels"}},{"authors":[{"name":"S. B. Korada"}],"title":{"text":"Polar codes for channel and source coding"}},{"authors":[{"name":"R. G. Gallage"}],"title":{"text":"Information Theory and Reliable Communication"}},{"authors":[{"name":"S. Korada"},{"name":"R. Urbanke"}],"title":{"text":"Polar codes are optimal for lossy source coding"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564305.pdf"},"links":[{"id":"1569565883","weight":18},{"id":"1569565867","weight":9},{"id":"1569566605","weight":18},{"id":"1569566683","weight":18},{"id":"1569565551","weight":9},{"id":"1569564481","weight":9},{"id":"1569565355","weight":9},{"id":"1569551535","weight":9},{"id":"1569565461","weight":18},{"id":"1569566207","weight":27},{"id":"1569564613","weight":18},{"id":"1569566095","weight":9},{"id":"1569566617","weight":9},{"id":"1569563307","weight":36},{"id":"1569555999","weight":18},{"id":"1569566581","weight":18},{"id":"1569553909","weight":9},{"id":"1569559111","weight":54},{"id":"1569565915","weight":9},{"id":"1569567051","weight":9},{"id":"1569566425","weight":9},{"id":"1569554971","weight":9},{"id":"1569566909","weight":9},{"id":"1569554759","weight":18},{"id":"1569566223","weight":9},{"id":"1569565469","weight":9},{"id":"1569562207","weight":18},{"id":"1569566191","weight":9},{"id":"1569567033","weight":9},{"id":"1569566655","weight":9},{"id":"1569566245","weight":9},{"id":"1569560503","weight":18},{"id":"1569565463","weight":9},{"id":"1569566901","weight":9},{"id":"1569565885","weight":18},{"id":"1569566805","weight":18},{"id":"1569566293","weight":27},{"id":"1569565765","weight":54},{"id":"1569565215","weight":18},{"id":"1569565093","weight":9},{"id":"1569565661","weight":9},{"id":"1569565865","weight":9},{"id":"1569566737","weight":54},{"id":"1569566547","weight":9},{"id":"1569566755","weight":9},{"id":"1569566713","weight":9},{"id":"1569565529","weight":9},{"id":"1569565271","weight":9},{"id":"1569565367","weight":9},{"id":"1569564281","weight":9},{"id":"1569565769","weight":9},{"id":"1569567691","weight":9},{"id":"1569565861","weight":9},{"id":"1569566147","weight":9},{"id":"1569566847","weight":9},{"id":"1569565997","weight":18},{"id":"1569565035","weight":9},{"id":"1569565889","weight":9},{"id":"1569566413","weight":9},{"id":"1569565707","weight":27},{"id":"1569566375","weight":9},{"id":"1569565143","weight":27},{"id":"1569564755","weight":9},{"id":"1569551541","weight":9},{"id":"1569565895","weight":9},{"id":"1569566067","weight":9},{"id":"1569566825","weight":18},{"id":"1569566443","weight":9},{"id":"1569566727","weight":45},{"id":"1569565315","weight":9}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T5.4","endtime":"12:50","authors":"Junya Honda, Hirosuke Yamamoto","date":"1341491400000","papertitle":"Polar Coding without Alphabet Extension for Asymmetric Channels","starttime":"12:30","session":"S12.T5: Polar Codes over Non-Binary Alphabets","room":"Kresge Little Theatre (035)","paperid":"1569564305"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
