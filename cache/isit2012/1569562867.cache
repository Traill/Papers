{"id":"1569562867","paper":{"title":{"text":"Max-Product Algorithm for Low Density Lattice Codes"},"authors":[{"name":"Yair Yona"},{"name":"Meir Feder"}],"abstr":{"text":"Abstract\u2014A max-product algorithm for approximating maximum-likelihood lattice decoding of low density lattice codes is derived, operating directly in the Euclidean space. First we derive the max-product algorithm for continuous channels by taking a factor graph based approach. Then, for the additive white Gaussian noise channel we show the relation between the sum-product and max-product algorithms for low density lattice codes. In both algorithms the messages consist of the same Gaussians. While in the sum-product algorithm we sum the Gaussians in each message, for the max-product we take the maximal envelope of these Gaussians. Finally, we extend the parametric approach to efﬁciently implement the max-product algorithm, and show decrease in the word error rate (WER)."},"body":{"text":"Low density lattice codes (LDLC\u2019s) [1] are lattices char- acterized by the sparseness of the inverse of their generator matrix. Under the tree assumption a sum-product algorithm was derived [1] directly in the Euclidean space. For LDLC\u2019s of dimension n = 100, 000, this message passing decod- ing algorithm attains up to 0.6 dB from channel capacity. In addition to its good performance, the iterative decoding algorithm has linear complexity as a function of the block length. However, the decoder presented in [1] samples and quantizes the passed messages, which result in a large storage requirement and relatively large (although linear in the block length) computational complexity. Efﬁcient implementations of the sum-product algorithm, that signiﬁcantly reduce both the computational complexity and the storage requirement were presented in [2], [3]. These works take parametric approach in the representation of the passed messages.\nThe max-product algorithm, presented in [4] for algebraic codes, is a message passing decoding algorithm aimed at minimizing the WER. Under the tree assumption this decoding algorithm yields blockwise maximum-likelihood (ML) decod- ing. For lattices, ML lattice decoding gives the most likely lattice point in the inﬁnite lattice for a ceratin observation, i.e. decoding without taking into consideration any shaping region boundaries. In this work we take factor graph approach to derive a max-product algorithm for LDLC\u2019s directly in the Euclidean space. For lattices that hold the tree assumption we get the exact ML lattice decoding, and for general LDLC\u2019s we get approximation of the blokwise ML lattice decoding.\nFor the additive white Gaussian noise (AWGN) channel we reveal an interesting connection between the sum-product\nand max-product algorithms of LDLC\u2019s. For both algorithms, in each iteration, the passed messages consist of Gaussians. Interestingly, the messages Gaussians in both algorithms are identical . However, the messages are different. While in the sum-product algorithm we sum those Gaussians to get a Gaussian-mixture, in the max-product algorithm for each message we take the maximal value between those Gaussians in each point, i.e. the passed message is the maximal envelope of those Gaussians.\nThe parametric approach provides an efﬁcient way to im- plement the sum-product algorithm [2], while maintaining the same performance as the decoder presented in [1]. In this case each Gaussian is represented by three parameters: mean value, variance and amplitude. Each message is represented by a list of its Gaussians parameters. As there is an inﬁnite number of Gaussians in each message, one of the cornerstones in the parametric algorithm is consolidating those Gaussians into a ﬁnite parametric list of Gaussians in an efﬁcient way, while retaining good performance. In this work we also adapt the parametric algorithm to approximate the max-product algorithm. We show that extending the parametric approach to the max-product algorithm does not increase the computational complexity compared to the parametric sum-product.\nFinally, we provide numerical results and show that for small dimensional LDLC\u2019s the max-product algorithm im- proves the WER compared to the sum-product algorithm. Improving the WER for small dimensional LDLC\u2019s is desired since in practical systems it may reduce the number of packet retransmissions. As expected, we show that for either large di- mensions or very small noise variance values, the performance of both algorithms is similar.\nThe outline of the paper is as follows. In section II basic deﬁnitions are given. In Section III we derive the max-product algorithm for continuous channels. Section IV presents the max-product algorithm for the AWGN channel and its relation to the sum-product algorithm. Extension of the parametric decoder to the max-product algorithm is presented in section V, followed by numerical results in Section VI.\nA lattice Λ is a discrete set in the Euclidean space R n , closed under addition. n-dimensional lattice can be represented by a squared generating matrix G. In this case we can write\nx = G · b, where x ∈ Λ and b ∈ Z n . The Voronoi region of a lattice point x ∈ Λ is the set of points in R n that are closer to x than to any other lattice point. The Voronoi regions are identical up to a translate. The Voronoi region volume equals | det(G)|, i.e. the absolute value of the determinant.\nSimilarly to low-density parity-check codes (LDPC), LDLC\u2019s are lattices that have a sparse \u201cparity-check matrix\u201d H = G −1 , i.e. the inverse of the lattice generator matrix. In this case we get for x ∈ Λ that H · x ∈ Z n . A Latin square LDLC has the same non-zero values in each row and each column of its parity check matrix, H, up to a permutation and sign. In this case we denote the absolute values of the non-zero entries of the rows and columns of H by h = {|h 1 |, · · · , |h d |}, where d is the LDLC degree, and we assume |h 1 | ≥ · · · ≥ |h d | > 0. Note that we can represent each LDLC by a bipartite graph, where the variable nodes represent the lattice symbols and the check nodes represent the parity check equations.\nFor the AWGN channel we can write y = x + z, where x ∈ Λ and z ∼ N(0, σ 2 · I) is the n-dimensional AWGN with variance σ 2 . In this paper we consider lattice decoding, i.e. we consider AWGN channel with no power constraint, for which the classic channel capacity is meaningless. This channel was analyzed in [5]. For lattices, this channel generalized capacity as presented in [5] gives σ 2 < | det(G)| 2 n 2πe .\nWe would like to calculate the following marginal ψ j (x j ) = max\nwhere {x ∈ Λ, ∼ x j } means the set of lattice points in which the j\u2019th component equals x j , and f Y |X (y|x) =\nf Y k |X k (y k |x k ) is the probability of receiving the ob- servation y given that x was transmitted. Note that if we take for each function in (1) the argument x j , j = 1, . . . , n that maximizes it, we get exactly the ML lattice decoding decision. We can factorize the problem and rewrite it as\n(h i · x ∈ Z) equals 1 if h i · x equals an integer and zero else. The multiplication n i =1 ½ (h i · x ∈ Z) in (2) takes into account all of the lattice check equations, hence this product equals 1 if and only if x ∈ Λ. We can translate the factorized function in (2) to a factor graph, for which the variable nodes represent the symbols x j , j = 1, . . . , n, and the check nodes represent the check equations indication functions ½ (h i · x ∈ Z), i = 1, . . . , n. Edges in the factor graph are stretched from each check equation indication function to the variable nodes that take place in it, i.e. the variables corresponding to the non-zero entries in the relevant row in H. In addition, each variable node x k is connected to the function that represents its channel observation f Y k |X k (y x |x k ), k = 1, . . . , n.\nbroken into similar independent marginalization subproblems corresponding to the subtrees. Without loss of generality let us observe the marginalization of x 1 . In this case under the tree assumption we get that ψ 1 (x 1 ) equals\nwhere we assumed that x 1 takes place in the ﬁrst l check equations in H, and x (i) are the variables that take place in the i\u2019th check equation with x 1 . c i (x 1 , x (i) ) · ½ (h i · x ∈ Z) is related to the subtree of the i\u2019th check equation that x 1 takes place in. Note that due to the tree assumption the elements in x (i) i = 1, . . . , l are different. Hence we can rewrite (3) as\nIn order to calculate the marginalization in (4) we can divide the operation into two phases. The ﬁrst phase takes place in the check nodes connected to x 1 , and the second phase takes place in the variable node of x 1 . For the i\u2019th check equation let us denote the check node message by\nAssuming h i has m + 1 non-zero values, we would like to break the calculation of c (i) (x 1 ) into m maximization operations. Hence, let us assume that h i ·x = m k =1 h k i ·x (i) k + h m +1 i ·x 1 , where h k i , k = 1, . . . , m+1 are the non-zero entries of h i and x (i) k , x 1 , k = 1, . . . , m are their corresponding variable nodes. By assuming t 1 = h 1 i · x (i) 1 , t 2 = h 2 i x (i) 2 + t 1 and in general t k = h k i x (i) k + t k −1 , k = 2, . . . , m, we write\n(·) is the message sent from variable x (i) k , k = 1, . . . , m. Note that due to the tree assumption v x (i)\n(·) is the marginalization of x (i) k over its subtree excluding its edge with x 1 i\u2019th check equation, i.e. to calculate x (i) k message we need to take the same marginalization operation over its subtree. We can break the maximization in (5) as follows\nHence, by assigning t m = b − h m +1 i x 1 , where b ∈ Z, we get that the check node message equals\nIn the second phase, that takes place in the variable node, we simply multiply the messages related to variable node x 1\nIn general the tree assumption does not necessarily hold. In this case we take the following steps to get the max-product message passing algorithm.\n\u2022 Check node: Consider a certain message sent to variable node x j from a certain check equation. First we calculate the maximization over the messages sent from the other variable nodes that take place in this check equation, after expanding these messages. This maximization was described in (6)-(8). Then we calculate the maximization on the replications of P (t m ), after contracting it. This operation was described in (9).\n\u2022 Variable Node: In the variable node we multiply the incoming check node messages with the variable node channel observation in a similar manner to (10). However, this time we exclude in the multiplication the message from the check node for which we send the message.\n\u2022 Final Decision: We multiply all the variable node in- coming messages with its channel observation to get ψ j (x j ). Then we ﬁnd ˆ x j = argmax x j ψ j (x j ) and take ˆb = ⌊H · ˆx⌉\nNote that in order to get the sum-product algorithm derived in [1], we only need to replace the maximization in (5) by an integral, i.e. we get convolution between the expanded messages (6)-(8), and we replace the maximization operation in (9) by summation.\nIn this section we analyze the max-product algorithm for the AWGN channel. We prove that the maximization of the passed messages (5) still enables us to represent the passed messages with Gaussians, just like in the sum-product algorithm where the passed messages are Gaussian-mixtures. Moreover, we show that the Gaussians representing the passed messages in both algorithms are identical in each iteration. The only difference between the algorithms passed messages is how we process these Gaussians. In the sum-product algorithm we sum these Gaussians. In the max-product algorithm we take the maximal value in each point between these Gaussians.\nWe characterize each Gaussian by three parameters: ampli- tude a ≥ 0, mean value µ, and variance v. We deﬁne the Gaussian function N (x; µ, v) as follows\nFor the AWGN channel we get that f Y j |X j (y j |x j ) = N (x j ; y j , σ 2 ), j = 1, . . . , n.\nNext we prove several properties of Gaussians combined with multiplication and maximization operations. This prop- erties enable us to prove that the passed messages can be represented via Gaussians, and also to prove the relation to the sum-product algorithm. The following lemma relates to the operations presented in (6)-(8).\nLemma 1. Assume v 1 > 0, v 2 > 0. sup\n. Taking the ﬁrst derivative according to x \u2032 and ﬁnding x \u2032 that zeros it gives us x \u2032 = v 1 ·v 2 v 1 +v 2 · ( µ 1 v 1 + x −µ 2 v 2 ). Assigning this value in the exponent gives us\nInterestingly, the expression received in Lemma 1 is iden- tical to the convolution result between this Gaussians up to a coefﬁcient that depends on the variance values. Now let us deﬁne an inﬁnite set of Gaussians {a k · N(x; µ k , v k )}, k ≥ 1 (for a ﬁnite set we can zero the irrelevant amplitudes). The maximal envelope of these Gaussians is deﬁned as follows\ni.e., for each point x we take the maximal value between the Gaussians. Next we prove a property of Gaussian-envelopes that relates to the multiplication in the variable node (10).\nProposition 1. Consider two Gaussian-envelopes sup k ≥1 a (1) k · N (x; µ (1) k , v (1) k ) and sup k \u2032 ≥1 a (2) k \u2032 ·N(x; µ (2) k \u2032 , v (2) k \u2032 ). In this case their multiplication yields\nProof: This proposition states that multiplying the Gaussians-envelopes is equivalent to ﬁrst multiplying the Gaussians constituting these Gaussian-envelopes, and then taking the maximization over this multiplication. The proof is straight forward. The Gaussians and their coefﬁcients are positive. Hence the maximal value must be the multiplication of the maximal values of each envelope. Considering the other multiplications does not effect the result. The result is a Gaussian-envelope since the multiplication of Gaussians yields a Gaussian [1]. Hence the maximization over the Gaussians multiplication yields a Gaussian-envelope.\nProposition 2. Consider two Gaussian-envelopes consist- ing of Gaussians with the same variance: G (1) Env (x) =\nN (x; µ (2) k \u2032 , v (2) ). Assume v (1) , v (2) > 0, and that these functions are bounded. We get\nProof: From Proposition 1 we can write the Gaussian- envelopes multiplication as sup x \u2032 sup k ≥1,k \u2032 ≥1 a (1) k · a (2) k \u2032 · N (x \u2032 ; µ (1) k , v (1) ) · N(x − x \u2032 ; µ (2) k \u2032 , v (2) ). Since both functions are bounded we can reverse the maximization order, and then in the inner maximization we need to ﬁnd for each k ≥ 1 and k \u2032 ≥ 1, the sup x \u2032 a (1) k · a (2) k \u2032 · N(x \u2032 ; µ (1) k , v (1) ) · N(x − x \u2032 ; µ (2) k \u2032 , v (2) ). From Lemma 1 we know that this maximization\nThe variance values do not depend on k, k \u2032 , and so they can be taken out of the maximization.\nTheorem 1. For each message in each iteration there exists a set of Gaussians (set of mean values, variance values and amplitudes) for which the maximal envelope of these Gaus- sians gives the max-product algorithm message, and the sum of these Gaussians gives the sum-product algorithm message.\nProof: We prove this theorem by induction. We begin with the initialization step. In this case the variable node messages consist of a Gaussian that equals to the channel observation, i.e. variable node x j sends N (x; y j , σ 2 ). This messages are bounded functions. In the check node we begin by analyzing the calculation of P (t m ) presented in (6)-(8). We keep the notations used in these equations, and without loss of generality consider a check node message sent to x 1 . In this case if we denote the mean values of the incoming messages to the check node by µ k , k = 1, . . . , m + 1, and the variance values by σ 2 , based on Lemma 1 we get that in the ﬁrst iteration P (t m ) ∝ N t m ; m k =1 h k i · µ k , σ 2 m k =1 (h k i ) 2 . Note that since the incoming messages are bounded, P (t m ) is also a bounded function. For the calculation of c (i) (x 1 ) in (9), ﬁrst we take the set of Gaussians P (b − h m +1 i \t · x 1 ) ∝ N x 1 ; b − m k =1 h k i ·µ k h m +1\n, where b ∈ Z. These Gaussians are all replications of the Gaussian P (−h m +1 i x 1 ), and so they all have the same variance and the same coefﬁcient. Hence we get that\nwhich is a Gaussian-envelope consisting of Gaussians with the same variance. Based on this operation we can see that c (i) (x 1 ) is also a bounded function. In the sum-product algorithm [1] the initialization is the same. This time in the ﬁrst iteration in each check node we perform convolution between the incoming Gaussians, which gives the same result as P (t m ) up to a factor. We perform the same replication of the Gaussian as in the max-product algorithm, only this time we sum the replications. Hence, the check node messages in the ﬁrst iteration consist of the same Gaussians in both algorithms. Now we turn to analyze the max-product operations in the\nﬁrst iteration in the variable node. In this case we calcu- late ψ 1 (x 1 ) according to (10) by multiplying the incoming Gaussian-envelopes messages, with the channel observation. The incoming messages are Gaussian-envelopes consisting of Gaussians with the same variance. Based on Proposition 1, the multiplication is equivalent to ﬁrst multiplying the Gaussians from the different messages, and then taking a Gaussian- envelope on the multiplication. Multiplying two Gaussians with variance values v a and v b gives a Gaussian with variance\n) −1 [1]. Hence, since the Gaussians in each message have the same variance, the Gaussians product also yields Gaussians that have the same variance. Since the incoming messages are bounded, their multiplication ψ 1 (x 1 ) is also bounded. For the sum-product algorithm this multiplication is performed between Gaussian-mixtures and yields the same Gaussians as in the max-product, only this time those Gaus- sians are summed.\nNext, assume that in iterations 1, . . . , L the check and variable nodes messages are Gaussian-envelopes, where the Gaussians in each message have the same variance. Also assume that these Gaussians are identical to the Gaussians in the sum-product messages, and that the messages are bounded. In the check node, since the incoming messages are bounded we can use Proposition 2. We get that P (t m ) is a Gaussian- envelope consisting of Gaussians that equal to the convolution between the Gaussians of the incoming messages. Calculating P (t m ) requires multiplications and maximization of bounded functions, hence P (t m ) is bounded. In the sum-product al- gorithm, these Gaussians undergo convolution. Hence the Gaussians are identical in both algorithms. The calculation of c (i) (x 1 ) (9) is equivalent to replicating each Gaussian in P (−h m +1 i x 1 ) and taking the Gaussian-envelope on these Gaussians. This operation keeps c (i) (x 1 ) bounded, and also the replicated Gaussians variance values are identical. For the sum-product algorithm we take the same replications as in the max-product algorithm. Hence the Gaussians are identical between both algorithms. In the variable node the arguments are identical to the arguments given for the ﬁrst iteration.\nIn [1] the authors formulated necessary conditions for convergence of the mean and variance values of the Gaussians\nin the passed messages. A partial convergence analysis of the amplitudes was also given. As the Gaussians in each iteration are identical to the sum-product Gaussians, the conditions and analysis also hold for the max-product algorithm.\nThe Gaussians in the passed messages are identical in both algorithms. Hence, the difference in each iteration comes from the processing each algorithm performs on these Gaussians to obtain the messages. In some cases, the different processing leads to different ﬁnal decisions made by the algorithms. For instance it may occur when there is rather \u201ctall\u201d Gaussian concentrated around a certain point, and also several smaller Gaussians concentrated around another point, whose sum yields larger value than the tall Gaussian. In this case the decisions may be different. For illustration see Figure 1.\nIn [2] a parametric algorithm for the sum-product case was presented. In this section we adapt this algorithm to the max- product case. We will brieﬂy go over the parametric algorithm [2] and highlight the required changes to adapt it to the max- product case. The parametric approach uses the fact that the passed messages consist of Gaussians. In this approach the Gaussians in the passed messages are represented by lists of their means, variance values and amplitudes. The operations in the check nodes and variable nodes can be done by calculating the Gaussians parameters. However, the number of Gaussians in each message is inﬁnite. Hence, a key component of the parametric algorithm is to efﬁciently approximate the inﬁnite parametric Gaussians list of each message, by a ﬁnite list of M Gaussians. The Gaussians in each message are consolidated in each iteration by ﬁrst choosing the Gaussian with largest coefﬁcient in the list, and consolidating it with Gaussians that fall within a certain range around its mean value. Assume {a k , µ k , v k , 1 ≤ k ≤ L} are the Gaussians to be consol- idated. In this case we approximate these Gaussians by a single Gaussian with mean ˆ µ k = L k =1 a \u2032 k µ k and variance ˆ v k = L k =1 a \u2032 k v k , where a \u2032 k = a k L\n. In [2] the amplitude of this Gaussian equals ˆ a = L k =1 a k . In the max-product case we take ˆ a = max 1≤k≤L a k . This is the ﬁrst difference between the algorithms . After consolidating these Gaussians, we erase them from the message, and ﬁnd the message next Gaussian with largest coefﬁcient. We repeat this process M times at most. The second difference between the algorithms is the amplitudes calculation in the check nodes. Lemma 1 proves that in the check node, both algorithms operations yield the same Gaussians up to a coefﬁcient that depends on the variance. In the theoretical algorithm the variance values of the Gaussians in each message are identical. However the parametric approximation gives Gaussians with different variance values. Hence, when calculating the convolution between Gaussians pairs in the sum-product algorithm [2], the result Gaussian amplitude is a 1 · a 2 , where a 1 and a 2 are the Gaussians amplitudes, and for the max-product algorithm we\n· a 1 · a 2 , where v 1 and v 2 are the Gaussians variance values. Besides that both algorithms are identical. The parametric algorithms complexity is the same.\nWe compare the performance of both algorithms. In all cases we took Gaussian lists of length M = 10. Also, we normalize the Voronoi region volume to one. We begin by comparing the performance for Latin square LDLC of dimension n = 8, degree d = 3, and generating sequence |h| = { 1 2.31 , 1 3.17 , 1 5.11 }. We normalize the WER by a factor of\n. For normalized WER (NWER) of 2 ·10 −5 the max-product algorithm improves the performance by 0.2 dB compared to the sum-product algorithm. Next, we compared the algorithms performance for Latin square LDLC of dimension n = 16, d = 3, and the same generating sequence as for n = 8. In this case for NWER of 2 · 10 −5 we can see that the max- product algorithm is 0.15 dB closer to the channel capacity than the sum-product algorithm. In [6], the smallest gap from channel capacity for a certain dimension and a certain error probability was presented. For dimensions n = 8, 16, we also compared the performance at NWER 2 · 10 −5 to the smallest gap from channel capacity. In accordance with [6] we choose ǫ 1 = 10 −5 which gives NWER 2 n (1 − ǫ 1 ) n ≈ 2 · 10 −5 for n = 8, 16. We can see that for n = 8 the max-product algorithm is 1.5 dB from the smallest gap from channel capacity. For n = 16 the max-product algorithm has gap of 1.35 dB from the smallest gap from channel capacity. Finally, we take Latin square LDLC of dimension n = 100, d = 5 and |h| = { 1 2.31 , 1 3.17 , 1 5.11 , 1 7.33 , 1 11.71 }. In this case the per- formance of both algorithms is essentially the same. Indeed we can see that for rather small dimensions, and at moderate gap from channel capacity, the max-product algorithm improves the performance. As the gap from channel capacity increases the improvement decreases. Also, for large dimensions both algorithms attain essentially the same performance."},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569562867.pdf"},"links":[{"id":"1569564989","weight":50},{"id":"1569567665","weight":50},{"id":"1569564281","weight":50}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T4.1","endtime":"11:50","authors":"Yair Yona, Meir Feder","date":"1341401400000","papertitle":"Max-Product Algorithm for Low Density Lattice Codes","starttime":"11:30","session":"S10.T4: Coding with Lattices","room":"Stratton 20 Chimneys (306)","paperid":"1569562867"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
