{"id":"1569566095","paper":{"title":{"text":"A Strong Converse for Joint Source-Channel Coding"},"authors":[{"name":"Da Wang"},{"name":"Amir Ingber"},{"name":"Yuval Kochman"}],"abstr":{"text":"Abstract\u2014We consider a discrete memoryless joint source- channel setting. In this setting, if a source sequence is re- constructed with distortion below some threshold, we declare a success event. We prove that for any joint source-channel scheme, if this threshold lower (better) than the optimum average distortion, then the success probability approaches zero as the block length increases. Furthermore, we show that the probability has an exponential behavior, and evaluate the optimal exponent. Surprisingly, the best exponential behavior is attainable by a separation-based scheme."},"body":{"text":"Information Theory produces sharp results: if the required performance is below a threshold, set by the problem param- eters, then it may be achieved reliably, i.e., with probability that approaches one; otherwise, it can not. Converse results may be divided into categories, according to their strength, measured asymptotically as the block length goes to inﬁnity. A weak converse indicates it is impossible for the probability of success to approach one. A strong converse further states that the success probability must approach zero. Beyond that, one may be interested in the rate in which this happens; speciﬁcally, an \u201cexponentially-strong\u201d converse states that the probability of success must approach zero exponentially. Indeed, for source and channel coding exponentially strong converses are known.\nFor a discrete memoryless channel (DMC) W , when the required channel rate R is above the channel capacity C, the probability of correct decoding decays with the following exponent [1] (Earlier in [2] it appears in a different form and proven to be a lower bound on the exponent): 1\n(1) This exponent is closely related the sphere-packing exponent, which is an upper bound on the exponent of the error proba- bility when R < C [3] (in this form, [4]):\nIn lossy source coding, dual results can be obtained where the excess-distortion probability plays the role of error prob- ability. Speciﬁcally, for some single-letter distortion measure\nd(·, ·) and distortion threshold D, we say that the scheme is successful if\n1 n\nwhere S and ˆ S are the source and reconstruction sequences respectively.\nFor a discrete memoryless source (DMS) P , when the rate R is below the rate-distortion function (RDF) R(P, D) 2 , the success probability has exponent [5, Problem 9.6]:\nSimilarly, this exponent is closely related to the lossy source coding excess-distortion exponent for R > R(P, D) [6]:\nIn joint source-channel coding (JSCC), an average distortion D is achievable if R(D) < ρC and not achievable if the opposite holds, where ρ is the bandwidth expansion factor (number of channel uses per source sample). This result, due to Shannon [7], immediately implies a weak converse for the excess-distortion probability.\nA (non-exponentially) strong converse for JSCC has been shown for special cases: Quadratic-Gaussian [8], channels with additive noise [9], and lossless JSCC [10]. However, it has not been explicitly shown in general. We note that the strong converse for JSCC may be derived using previously known results. One approach is to use equivalence to channel coding [11] and the strong channel converse. Alternatively, JSCC dispersion [12] implies a strong converse. Another approach is the information spectrum method, which is used by Han in deriving the lossless JSCC case [10].\nIn this work we formally prove the strong converse for general lossy JSCC, and furthermore our converse is \u201cexponentially-strong\u201d. We do that by showing that whenever R(D) > ρC, the probability of not having excess distortion for the optimal JSCC scheme decays with exponent\nwhere the channel and source exponents are given by (1) and (2), respectively. This is analogous to the exponent of\nthe excess distortion: the JSCC excess-distortion exponent for R(D) < ρC is upper-bounded by [13], [14]\nThis paper uses lower case letters (e.g. x) to denote a particular value of the corresponding random variable denoted in capital letters (e.g. X). Vectors are denoted in bold (e.g. x or X). Calligraphic fonts (e.g. X ) represent a set and P (X ) denotes all the probability distributions on the alphabet X . We use Z + and R + to denote the set of non-negative integer and real numbers respectively.\nOur proofs make use of the method of types, and follow the notations in [5]. Speciﬁcally, the type of a sequence x with length n is denoted by P x , where the type is the empirical distribution of this sequence, i.e., P x (a) = N (a|x)/n ∀a ∈ X , where N (a|x) is the number of occurrences of a in sequence x. The subset of the probability distributions P (X ) that can be types of n-sequences is denoted as\nand sometimes P n is used to emphasize the fact that P n ∈ P n (X ). A type class T n P\nis deﬁned as the set of sequences that have type P x . Given some sequence x, a sequence y of the same length has conditional type P y|x if N (a, b|x, y) = P y|x (a|b)N (a|x), and we call the set of all y that have conditional type V given x the V -shell of x, and denote it by T n V (x). Furthermore, the random variable corresponding to the conditional type of a random vector Y given x is denoted as P Y|x . In addition, the possible conditional type given an input distribution P x is denoted as\nGiven two distributions P and Q, their KL-divergence is deﬁned as\nP (x) log P (x) Q(x)\nA discrete memoryless channel W : X → Y is deﬁned with its input alphabet X , output alphabet Y, and conditional distribution W ( · | x) of output letter Y when the channel input letter X equals x ∈ X . Also, we abbreviate W ( · | x) as W x (·) for notational simplicity. Given a channel input distribution Φ, we denote the corresponding channel output distribution, and mutual information by\nAnd we denote channel capacity and the set of capacity- achieving distributions by C(W ) \t max Φ I (Φ, W ) and Π(W ) {Φ : I (Φ, W ) = C(W )} respectively.\nA discrete memoryless source is deﬁned with source alpha- bet S, reproduction alphabet ˆ S, source distribution P and a distortion measure d : S × ˆ S → R + . Without loss of generality, we assume that for any s ∈ S there is ˆ s ∈ ˆ S such that\nd(s, ˆ s) = 0. The rate-distortion function (RDF) of a DMS (S, ˆ S, P, d) is given by\n1 n\nis the distortion between the source and reproduction words s and ˆ s and I(P, Λ) is the mutual information over a channel with input distribution P (S) and conditional distribution Λ : S → ˆ S.\nA discrete memoryless joint source-channel coding problem consists of a DMS (S, ˆ S, P, d), a DMC W : X → Y and a bandwidth expansion factor ρ ∈ R + . A JSCC scheme C (n) JSCC is comprised of an encoder mapping f J ;n : S n → X ρn and decoder mapping g J ;n : Y ρn → ˆ S n . Given a source block s, the encoder maps it to a sequence x = f J ;n (s) ∈ X ρn and transmits this sequence through the channel. The decoder re- ceives a sequence y ∈ Y ρn distributed according to W (·|x), and maps it to a source reconstruction ˆ s. The corresponding distortion is given by (8). For a given JSCC scheme, we deﬁne the error event E (D) as\nE(D) E(D, f J ;n , g J ;n ) E D, C (n) JSCC {d(S, ˆ S) > D},\nand the correct event ¯ E(D) \t E(D) c = {d(S, ˆ S) ≤ D}. Finally, for block length n, we deﬁne the best correct event ¯ E n (D) as an event that corresponds to the JSCC scheme that produces the minimum error probability, i.e.,\nThe following formally states the exponential decay rate of the probability of success at distortion thresholds R(D) > ρC, thus also serves as a strong converse for JSCC coding.\nTheorem 1 (Strong Converse for JSCC). Given a dis- crete memoryless JSCC problem with DMS (S, ˆ S, P, d), DMC (X , Y, W ) and bandwidth expansion factor ρ, let ¯ E (P, D, W, ρ) be the exponent of the success probability for the best sequence of JSCC schemes:\nThen ¯ E (P, D, W, ρ) exists and is given by ¯ E JSCC (P, D, W, ρ) (3).\nRemark 1 (Direct part of the theorem). The achievability of the exponent ¯ E JSCC (P, D, W, ρ) can be proven by a separation scheme, noting that the probability of no excess distortion is lower-bounded by the product of the probability of no excess distortion in source coding and the probability of no channel error, for any chosen digital rate R. These proba- bilities have exponents ¯ E S (R, D, P ) and ρ ¯ E sp (R/ρ, W ), re- spectively (see Section I). Thus this proves separation theorem for the (rather strange from a practical point of view) case\nρC \t R(P, D) ¯ E JSCC (P, D, W, ρ)\nwhere one is interested in achieving a distortion threshold with exponentially small probability.\nRemark \t 2 \t (Alternative form). The \t exponent ¯ E JSCC (P, D, W, ρ) may be written explicitly as a function of\nthe source and channel parameters as follows: ¯ E JSCC (P, D, W, ρ)\nWe use this form in the proof of Theorem 1, rather than the form (3). The equivalence proof can be shown by letting Φ ∗ , V ∗ and Q ∗ be the optimizing distributions for (9), and noting that regardless of the | · | + operation taking effect, (9) can be decomposed into two parts that correspond to\n¯ E S (R, D, P ) and ρ ¯ E sp (R/ρ, W ). We omit the detailed proof due to space limits.\nRemark 3 (Minimizing rate). When R(P, D) ≤ ρC, proving Theorem 1 is trivial as ¯ E (P, D, W, ρ) = 0. When R(P, D) > ρC, the rate minimizing (3) satisﬁes R(P, D) > R > ρC, as shown in Fig. 1. This is parallel to the excess-distortion exponent where R(P, D) < ρC, where (4) is minimized by a rate R(P, D) < R < ρC.\nThe proof of our main result builds on the following key lemma, which shows for any JSCC scheme, when the source, channel and channel codebook are all constant-composition, the probability of a no-excess distortion event vanishes expo- nentially with the exponent in (11).\nLemma 2 (Joint source channel coding converse with ﬁxed types). For a JSCC problem, we deﬁne all the channel outputs that covers some source sequence s with distortion D as\nˆ B(s, D) {y ∈ Y m : d(s, g J ;n (y)) ≤ D} \t (10) where m = ρn and g J ;n is the JSCC decoder. Then given a source type Q ∈ P n (S) and a channel input type Φ ∈ P n (X ), for a distortion D and a channel with constant composition conditional distribution V ∈ P m (Y|Φ), we have\n0.2 0.3 0.9\nis the probability of having channel input type Φ given that the source type is Q, G(Q, Φ) is the set of source sequences in T n Q that are mapped (via JSCC encoder f J ;n ) to channel codewords with type Φ, i.e.,\nG(Q, Φ) \t s ∈ T n Q : x = f J ;n (s) ∈ T m Φ , \t (13) and p(n) is a polynomial that depends only on the source, channel and reconstruction alphabet sizes and ρ.\nThe proof of this lemma is based upon the exponential channel coding converse [1], combined with the following.\nLemma 3 (Restricted D-ball size). Given source type P and a reconstruction sequence ˆ s, deﬁne restricted D-ball as\nB(ˆ s, P, D) {s ∈ T n P : d(s, ˆ s) ≤ D} . Then\nThis result is similar to the bound over the size of the restricted D-ball presented in Lemma 3 in [15]. However, This bound is uniform over source types and does not depend on the reconstruction alphabet. We prove this lemma, as well as Lemma 2 and the converse part of Theorem 1 in the next section 3 . We conclude this section by presenting the following example.\nExample 1. Transmitting a binary symmetric source (BSS) over a binary symmetric channel (BSC) subject to the Ham- ming distortion with the bandwidth expansion factor ρ = 1.\nFor a BSS, the RDF is given by R(P, D) = 1 − H b (D), where H b (·) is the binary entropy function. It can be shown that (1) is always minimized by a uniform distribution and:\nFor a BSC with cross over probability ε, the capacity is given by C(W ) = 1 − H b (ε). it can be shown that the optimizing Φ and V in (1) are always symmetric and:\nTherefore, when R(P, D) > C(W ), i.e. D < ε, ¯ E JSCC (P, D, W ) = inf\n0 \t 0.02 \t 0.04 \t ε 0.06 0\n0.1 0.2\nD ¯ E JSCC (P, D, W )\nFor the case of H b (D) = 0.1 and H b (ε) = 0.3, we plot the ¯ E S (R, D, P ), ¯ E sp (R, W ) and ¯ E JSCC (P, D, W ) in Fig. 2.\nFinally, we show ¯ E JSCC (P, D, W ) as a function of D when H b (ε) = 0.3 in Fig. 3.\nProof of Lemma 3: Let P ∈ P n (S) be a given type and let Q be the type of ˆ s. Then the size of the set of source codewords with type P that are D-covered by ˆ s is\nNote there are at most (n + 1) |S| | ˆ S | joint types, and {s ∈ T n P : P s,ˆ s = P × Λ} = T n ˜ Λ (ˆ s) ,\nwhere ˜ Λ is the reverse channel from ˆ S to S such that Q× ˜ Λ = P × Λ. Therefore,\nProof for Lemma 2: In our proof, we ﬁrst bound the denominator in (11) uniformly for all s i , and then bound the sum of the numerator over all s i , as done in [1] for the channel error exponent.\na) Bounding the denominator: Standard results in method of types [5] show for f (s) ∈ T n Φ ,\nb) Bounding the sum of numerator: Since s ∈ G(Q, Φ), y ∈ T V (f (s)) ∩ ˆ B(s, D)\nTherefore, any y will be counted at most |B(g J ;n (y), Q, D) ∩ G(Q, Φ)| \t times. \t According to Lemma 3, this is upper bounded by B u = (n + 1) |S| | ˆ S | exp {n [H(Q) − R(Q, D)]} . In addition, it is obvious that\nwhere Ψ = ΦV is the channel output distribution correspond- ing to Φ. Therefore,\nn log(n + 1) + H(Q) − R(Q, D) ≤ρH(Ψ) − R(Q, D)\nCombining the bounds for both numerator and denominator, we have\n1 n\nn log(n + 1). Note m = ρn ≤ ρn, let\nProof for Theorem 1 (converse part): As discussed in Remark 3, we only need to consider the case that ρC(W ) < R(P, D).\nLet P ¯ E(D) = 1 − P [E(D)]. By following a similar argument in [1, Proof of Lemma 5], clearly,\nLet m \t ρn and let A m denote the channel codebook in this JSCC scheme. Then conditioning again, we have\nRecall the deﬁnitions for G(Q, Φ) and α(Q, Φ) in (12) and (13), noting that given a source type, all strings within a type class are equally likely, hence\nwhere p (n) is some polynomial in n. Therefore, ¯ E (P, D, W, ρ)\nFinally, it can be shown that the exponents in (9) and (3) are equivalent, whose proof is omitted due to space limit. This completes the proof."},"refs":[{"authors":[{"name":"G. Duec"},{"name":"J. K¨orner"}],"title":{"text":"Reliability function of a discrete memoryless channel at rates above capacity"}},{"authors":[{"name":"S. Arimoto"}],"title":{"text":"On the converse to the coding theorem for discrete memoryless channels (corresp"}},{"authors":[{"name":"C. E. Shanno"},{"name":"R. G. Gallage"},{"name":"E. R. Berlekamp"},{"name":"I. Inform"}],"title":{"text":"Lower bounds to error probability for coding on discrete memoryless channels, part I-I Contr"}},{"authors":[{"name":"E. A. Haroutunian"}],"title":{"text":"Estimates of the error exponent for the semi- continuous memoryless channel"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orner"}],"title":{"text":"Information Theory - Coding Theorems for Discrete Memoryless Systems "}},{"authors":[{"name":"K. Marton"}],"title":{"text":"Error exponent for source coding with a ﬁdelity criterion"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"Coding theorems for a discrete source with a ﬁdelity criterion"}},{"authors":[{"name":"Y. Zhon"},{"name":"F. Alajaj"},{"name":"L. Campbell"}],"title":{"text":"L"}},{"authors":[{"name":"Y. Zhon"},{"name":"F. Alajaj"},{"name":"L. Campbell"}],"title":{"text":"L"}},{"authors":[{"name":"T. S. Han"}],"title":{"text":"Information-Spectrum Method in Information Theory "}},{"authors":[{"name":"M. Agarwa"},{"name":"A. Saha"},{"name":"S. Mitter"}],"title":{"text":"Coding into a source: a direct inverse rate-distortion theorem"}},{"authors":[{"name":"D. Wan"},{"name":"A. Ingbe"},{"name":"Y. Kochman"}],"title":{"text":"The dispersion of joint source- channel coding"}},{"authors":[],"title":{"text":"I Csisz´ar"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"On the error exponent of source-channel transmission with a distortion threshold"}},{"authors":[{"name":"Z. Zhan"},{"name":"H. Yan"},{"name":"V. Wei"}],"title":{"text":"E"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566095.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T4.2","endtime":"12:10","authors":"Da Wang, Amir Ingber, Yuval Kochman","date":"1341489000000","papertitle":"A Strong Converse for Joint Source-Channel Coding","starttime":"11:50","session":"S12.T4: Classical and Adversarial Joint Source-Channel Coding","room":"Stratton 20 Chimneys (306)","paperid":"1569566095"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
