{"id":"1569565737","paper":{"title":{"text":"Low Complexity Syndrome Based Decoding of Turbo Codes"},"authors":[{"name":"Jan Geldmacher"},{"name":"Klaus Hueske"},{"name":"J¨urgen G¨otze"},{"name":"Martin Kosakowski"}],"abstr":{"text":"Abstract\u2014A new syndrome trellis based decoding approach for Turbo coded data is described in this paper. Based on estimating error symbols instead of code symbols, it inherently features options for reducing the computational complexity of the decoding process. After deriving required transition metrics, numerical results in terms of block error rate and required equivalent iterations are presented to demonstrate the efﬁciency of the approach."},"body":{"text":"Forward error correction based on Turbo coding has been adopted in numerous applications since its initial presentation [1]. Especially in mobile communication systems like the Long Term Evolution (LTE) system their error correcting perfor- mance is an integral part for enabling high data throughput. On the downside, however, is the high computational complexity involved with the decoder\u2019s iterative implementation, which makes it one of the most complex and power consuming parts in the receiver baseband signal processing. Therefore it is important to keep the computational effort as small as possible and to avoid unnecessary iterations of the decoder. This can be achieved by using early termination (ET) schemes, which stop the iteration process if a block is not decodable (Low SNR ET) or already successfully decoded (High SNR ET). Several methods have been proposed to achieve this, see for example [2]\u2013[4]. An effective ET scheme leaves the highest number of iterations to the medium SNR range, the waterfall region of the code. Unfortunately this is also often the desired working point of the system \u2013 in the LTE system for example, the CQI reporting adaptively keeps the system at a target block error rate (BLER) of about 10%, which corresponds to the waterfall region for common block lengths.\nIn this paper a syndrome trellis based MAP decoding algorithm is proposed. While its trellis complexity and perfor- mance in terms of the generated soft information is equivalent to the conventional encoder trellis based MAP algorithm, it can be modiﬁed to reduce the computational effort during Turbo decoding. Especially in the medium SNR range, where many iterations are required to decode a received block, a signiﬁcant reduction can be achieved, such that the proposed approach can complement High and Low SNR ET schemes.\nA key property of the underlying syndrome based decoding algorithm are the unbalanced probabilities of the trellis states, which tend to the all-zero state if the input sequence contains\nfewer errors. Combining this with a syndrome based state iden- tiﬁcation [5] allows to estimate possible error-free subblocks in the input sequence. The MAP algorithm then only needs to process the remaining erroneous blocks.\nThe basic idea of this so-called block syndrome decoder (BSD) principle has been described previously for Viterbi de- coding [6] and Turbo equalization [7]. The extension to Turbo decoding as presented in this work requires a modiﬁcation of the original syndrome based MAP decoding algorithm: In order to achieve a meaningful syndrome based state es- timation, we propose a precorrection scheme, which corrects hard decision errors in the systematic and parity part with ongoing iterations. The transition metric involved with this modiﬁcation is derived in the following Sec. II. The resulting Turbo decoding framework is described in Sec. III. Sec. IV brieﬂy reviews ET and the BSD principle and its application in context of Turbo decoding. And ﬁnally numerical results and conclusions are given in Sec. V and Sec. VI, respectively.\nGiven a k/n-rate binary convolutional code C, then H T is a syndrome former of C, if GH T = 0, where G represents an encoder of C. Let r denote the binary hard decision of a received sequence,\nwhere u denotes the original information sequence and ǫ c the hard decision of the channel error. Applying an arbitrary sequence x to r and using H T yields the syndrome b,\nThen each sequence e corresponding to each path in the trellis representation of H T subject to b represents an admissible error sequence for (r ⊕ x), i.e. (r ⊕ x ⊕ e) ∈ C, and x ⊕ e = ǫ denotes an admissible error sequence for r. The objective here is to ﬁnd an estimate for ǫ, given r, an arbitrary x and the trellis of H T subject to b.\nMore speciﬁcally, the proposed syndrome based MAP de- coder maximizes the a posteriori probability P (e t |˜r, x) of an error symbol e t given the received soft decision sequence ˜r and a binary sequence x. e t is an element of e at time instant\nt. The probability P (e t = e l |˜r, x) for some error symbol e l , l = 1 . . . 2 n , can be expressed using all transitions T e l t in the trellis of H T that lead to an error symbol e l at time instant t:\nIn (3), Ψ t = p and Ψ t +1 = q denote trellis states at time instants t and t + 1, respectively. It is well known that P (Ψ t = p, Ψ t +1 = q|˜r, x) can be efﬁciently computed by performing forward and backward recursions on the trellis [8]. Thus, using the forward and backward state probabilities α t (p) and β t +1 (q) of states p and q yields\nThe probability γ t (p, q) of a transition from state p to state q at time instant t is given as\nThe error symbol associated with the transition from p to q is denoted by e (p,q) and p(˜r t |e (p,q) , x t ) is determined by the channel, while P (e t = e (p,q) , x t ) denotes the a priori probability of e t . The \u201cprecorrection\u201d symbol is denoted by x t . Further factorization yields\nwhere we assume that an error symbol consists of n bit, and that a priori information is available for the ﬁrst k bit.\nIn the following, a transmission over an AWGN channel with noise power σ 2 n and energy per coded bit E c is assumed. Also, to simplify the notation, p(˜ r (i) t |e (i) (p,q) , x (i) t ) =: p(˜ r|e, x) is used in the following. In order to ﬁnd p(˜ r|e, x), the probabilities of the different combinations of ˜ r > 0, ˜ r ≤ 0, e = 0, e = 1, x = 0 and x = 1 have to be combined to one expression. For example, letting e = 0 yields the probabilities of the corresponding four combinations:\np(˜ r > 0|e = 0, x = 0) ∼ exp(−(˜r − E c ) 2 /(2σ 2 n )) (7) p(˜ r ≤ 0|e = 0, x = 0) ∼ exp(−(˜r + E c ) 2 /(2σ 2 n )) (8) p(˜ r > 0|e = 0, x = 1) ∼ exp(−(˜r + E c ) 2 /(2σ 2 n )) (9) p(˜ r ≤ 0|e = 0, x = 1) ∼ exp(−(˜r − E c ) 2 /(2σ 2 n )) (10)\np(˜ r|e = 0, x = 0) ∼ exp(−(−|˜r| + E c ) 2 /(2σ 2 n )) (11) p(˜ r|e = 0, x = 1) ∼ exp(−(|˜r| + E c ) 2 /(2σ 2 n )). (12)\nIn the same way, the probability p(˜ r|e = 1, x) of recieving ˜r given e = 1 and x, can be derived as\np(˜ r|e, x) ∼ exp(−(˜x|˜r| − E c ˜ e) 2 /(2σ 2 n )). \t (15) Under the assumption, that the a priori probability P (ǫ) of\nthe a priori probabilities for the cases e = 0, e = 1, x = 0 and x = 1 can be combined to the following expression\n∼ exp (−˜x˜eL A (ǫ)/2) . \t (17) Putting (15) and (17) into (6) yields the transition probability\nNote that some scaling factors have been skipped in the derivation such that suitable normalization would be required in order to get true probabilities.\nhave no inﬂuence on a maximization, the ﬁnal transition metric Γ t (p, q) from state p to state q at time instant t becomes\n(20) B. Relation to Other Work\nSchalkwijk et. al. have proposed syndrome decoding for convolutional codes [9], where the trellis of H T is constructed subject to b = rH T . Tajima et. al. have derived soft decision transition metrics for this approach [10], [11]. On the other hand, constructing the trellis of H T subject to b = 0 [12]\u2013 [14], results in a trellis where each path represents a element of C, and where the conventional MAP transition metric can be adopted. The main objective of this approach is a possible reduction of trellis complexity for high code rates.\nBoth methods can be found as special cases of the approach proposed in this paper, by selecting x = 0 or x = r. However, instead of selecting a ﬁxed value for x, we propose to select x as an estimate of the channel error ǫ c . Therefor, if x is a suitable \u201cprecorrection\u201d for r, the trellis state probabilities are dominated by the all zero state. This property can be exploited to achieve a reduction of computational complexity as will be described in Sec. IV-B.\nIn the following subsection the operation of one constituent decoder is summarized, while Subsection III-B provides an overview of the resulting Turbo decoding framework with precorrection. A binary Turbo code based on two parallel con- catenated recursive systematic convolutional codes is assumed. Puncturing may be employed on transmitter side. In this case a depuncturing is realized on receiver side by placing zero reliability softbits at the punctured positions. The energy per transmitted bit is set to E c = 1, for simplicity of notation.\nA constituent decoder takes the received soft decision se- quence ˜ r, a precorrection sequence x and a priori LLRs of the errors L A (ǫ) in the systematic part as input. For the systematic part, it generates an a posteriori hard decision estimate ˆ ǫ of the error in the hard decision r of ˜ r and extrinsic LLRs L E (ǫ). The decoder performs the following steps:\n1) Precorrection and syndrome computation The precor- rection sequence x is applied to the hard decision r of ˜ r, and the syndrome b is computed as\n2) Trellis operation Given the trellis of H T subject to b, the LogMAP or MaxLog algorithm is applied to generate an a posteriori estimate L(ǫ):\nThe initial state for the forward recursion is set to the zero state, while the initial state for the backward recursion is selected according to the ﬁnal state of the syndrome former from (21). The LogMAP or MaxLog algorithm based on the transition metric (20) is applied, resulting in the a posteriori LLR L(e). Note that in case of the MaxLog algorithm, the noise power σ 2 n can be neglected in (20), because the result of the MaxLog algorithm is independent of any scaling factor.\nIn order to generate the LLR L(ǫ) of the absolute error in r, the sign of L(e), which is an estimate of the error in r ⊕ x, has to be ﬂipped according to x,\nThe hard decision ˆ ǫ of L(ǫ) delivers the a posteriori estimate of the channel error ǫ c . Applying the systematic part of ˆ ǫ to the systematic part of r results in the estimate ˆ u of the original information bits u.\n3) Generation of Extrinsic LLR Extrinsic LLRs L E (ǫ) are generated by removing the a priori information L A (ǫ) and the received softbits from L(ǫ (j) t ),\nThe systematic extrinsic LLRs are (de-)interleaved and become the a priori LLR of the other constituent de- coder.\nLike in the conventional Turbo decoder, both constituent decoders use the systematic and their according parity part of the received softbits, and, as priors, the interleaved extrinsic LLRs of the systematic part generated by the other decoder. Additionally, the precorrection sequence x is used.\nGenerally an arbitrary sequence can be selected for x, without changing the absolute values of the generated LLRs. However, as mentioned before, here it is desired to make b a function of the remaining errors in r. This can be achieved by selecting x as an extrinsic estimate of the channel error\nǫ c , which in context of the Turbo decoding framework can be done as follows:\n\u2022 The systematic part of x is set to the hard decision of the a priori values L A (ǫ). This estimate stems from the other constituent decoder.\n\u2022 The parity part of x is set to the extrinsic estimate of the parity error from the previous iteration, from the same constituent decoder.\nAs a result the sequence x depends on both constituent decoders. In case of convergence, it therefore leads to a decreasing hamming weight of the syndrome sequence b with ongoing iterations.\nIt is important to note that up to this point the described syndrome based Turbo decoder is identical to the conventional decoder in terms of decoding performance and trellis complex- ity. Additional effort is required to compute the sequence x and the syndrome b. However, both are low complexity, binary operations.\nIn order to reduce the computational complexity of the decoder, ET for high and low SNR scenarios can be adopted. Furthermore, we propose the BSD approach to reduce the average number of iterations for medium to high SNR. Both approaches will now be reviewed brieﬂy in context of the syndrome based Turbo decoder.\nSeveral ET schemes have been proposed, which may be classiﬁed into\nIn practice a low complexity criterion capable of Low and High SNR ET is preferred. One might also want to avoid a threshold, because it can be difﬁcult to select suitable values for all possible states of a system.\nIn this work a modiﬁed version of the improved hard decision aided (IHDA) criterion [15] is applied. Although originally only described for High SNR ET, it can be easily extended to also cover Low SNR ET. For the syndrome based Turbo decoder it may be implemented in the following way:\n\u2022 Count the sign differences ∆ i between the a posteriori LLRs L 1 (ǫ) and L 2 (ǫ) of the ﬁrst and second constituent decoder, respectively, after each full iteration i.\n\u2022 Terminate the decoding process after the i-th iteration for i > 1 if\n\u2013 i = i max (max. number of iterations i max reached). This criterion performs well for low and high SNR, is inde- pendent of a threshold and keeps the implementation overhead small. Note however, that any other criterion, which has been proposed for conventional MAP decoding can be adopted for syndrome based decoding as well.\nBased on the described syndrome based Turbo decoder, the BSD concept [6], [7] can be applied to achieve a reduction of decoding effort. Because of the precorrection, the syndrome sequence b of a constituent decoder shows subsequences of consecutive zeros, whose length and number increase with ongoing iterations in case of convergence. As described in Sec. I and in [6], [7], this can be exploited to separate the input sequence into subblocks that are considered to be erroneous and subblocks that are considered to be error-free. Consequently, a reduction of decoding effort can be achieved by only processing the erroneous subblocks and neglecting the supposedly error-free subblocks.\nMore precisely the syndrome based Turbo decoding algo- rithm from Sec. III is extended as follows:\n1) Preprocessing of syndrome Identify subsequences of length ≥ ℓ min zeros in b. Consider the corresponding subblocks in ˜ r, except a padding of ⌊ℓ min /2⌋ at the beginning and end of each subblock, as error-free and the remaining subblocks as erroneous.\na) Erroneous blocks The erroneous subblocks are processed by the syndrome based MAP decoder, which generates extrinsic values L E (ǫ (j) t ) and the estimated error ˆ ǫ (j) t for all t in these subblocks as described in Sec. III-A. Note that these blocks can be considered to be terminated in the zero state, because the zero state is the most likely state in the preceding and succeeding error-free blocks.\nb) Error-free blocks No processing is required for the supposedly error-free blocks. Instead, the extrinsic LLR is set to a sufﬁciently large value c > 0,\nand the estimated error ˆ ǫ is set according to the precorrection sequence\nˆ ǫ (j) t = x (j) t , j = 1 . . . n. \t (24) A reasonable choice for c is the largest value in the quantization range of the LLR values.\nThe choice of the design parameter ℓ min affects the achiev- able reduction of decoding effort and the possible loss in de- coding performance due to falsely classiﬁed error-free blocks. Given an acceptable loss in BLER or BER, it may be selected\nheuristically. It is important to note that ℓ min is dependent on the underlying code (and puncturing scheme), and less dependent on other system parameters like block length or modulation type. Thus it is not prohibitive to select it in advance for the considered system and its code rates.\nThis section shows simulation results for performance and computational complexity in terms of BLER and the average number of required iterations.\nThe following simulation results are based on a binary Turbo code using parallel concatenation of two UMTS/LTE compatible recursive systematic encoders G (D) = 1, 1+D+D 3 1+D 2 +D 3 . A corresponding syndrome former 1 is H T (D) = 1 + D + D 3 , 1 + D 2 + D 3 T . Two code\nrates are evaluated: R = 1/3 and R = 1/2, where the latter is generated by puncturing odd and even parity bits of the ﬁrst and second encoder, respectively. A pseudo- random interleaver of length 6144 is used, along with BPSK modulated transmission over an AWGN channel. The decoder is based on the MaxLog algorithm and maximum number of i max = 8 iterations.\nFor R = 1/2 and R = 1/3 Fig. 1 compares BLER and the average number of iterations for the following four cases:\n\u2022 Reference / Genie ET This setting serves as a reference. The reference BLER is shown in Fig. 1(a) for the case where no ET is done and where the decoder always executes i max = 8 iterations. The lowest possible number of iterations without BLER degradation is plotted in Figs. 1(b) and 1(c) and termed Genie ET. Here it is assumed that the decoder could perfectly detect unde- codable blocks after the ﬁrst full iteration (Low SNR ET scenario) and successfully decoded blocks after each full iteration (High SNR ET scenario).\n\u2022 ET In this case only Low and High SNR ET is applied as described in Sec. IV-A.\n\u2022 BSD & ET This is the result of complementing Low and High SNR ET with the BSD approach.\nThe design parameter ℓ min has been chosen individually for the different code rates as ℓ min = 31 and ℓ min = 25 for R = 1/2 and R = 1/3, respectively. The choice of ℓ min is a tradeoff between the required BLER performance and the resulting reduction of computational complexity. In this case, is has been selected such that there is negligible degradation for BLERs around 10%. This is a typical working point in the LTE system, but has also been shown to be a reasonable choice in general if automatic retransmissions are involved [17].\nAs shown in Fig. 1(a) the BLER around the 10% BLER working point is practically identical in all cases. For smaller BLERs there is a visible degradation: For example at 1% BLER the combination of BSD & ET shows a loss of about 0.2dB to the reference.\nFigs. 1(b) and 1(c) show the average number of iterations. In case of ET this is measured by just averaging over the iterations carried out by the decoder until termination or until i max is reached. In case of BSD, each iteration is weighted by the percentage of erroneous blocks before the average is computed. For both code rates and BSD&ET a signiﬁcant reduction of the number of iterations is visible: For example at the working point, the reduction against using only ET is about 1 full iteration (around 20%) without loss in BLER performance. For higher SNR the reduction is even larger than for Genie ET, because the BSD processes subblocks and the conventional ET scheme always processes the whole block.\nIt can also be observed, that for High SNR, the number of iterations is determined by the BSD. On the other hand in the low SNR range the average number of iterations is dominated by the ET scheme only. The reason for the latter is clearly that the constituent decoders do not converge to a common solution and thus no suitable precorrection can be found. In the medium range, both schemes complement each other such that the combination of both yields more reduction than each approach can achieve separately.\nA syndrome based MAP decoding approach for convolu- tional codes has been presented in this paper and its application to Turbo codes has been described. The extension with the so called BSD principle for reduction of computational com- plexity has been described. Further it has been demonstrated that the combination with an ET scheme can further reduce the average number of iterations, with negligible performance loss around a given typical working point.\nBesides the BSD approach it should be noted that the described syndrome decoder with precorrection also yields scarce state transitions, which cannot be directly realized using the conventional MAP decoding approach [18]. Moreover, in case of true high rate codes, well-known trellis complexity reduction methods can be realized as well (cf. [19])."},"refs":[{"authors":[{"name":"C. Berrou"},{"name":"A. Glavieux"},{"name":"P. Thitimajshima"}],"title":{"text":"Near shannon limit error- correcting coding and decoding: Turbo-codes"}},{"authors":[{"name":"A. Worm"},{"name":"H. Michel"},{"name":"F. Gilbert"},{"name":"G. Kreiselmaier"},{"name":"M. Thul"},{"name":"N. Wehn"}],"title":{"text":"Advanced implementation issues of turbo-decoders"}},{"authors":[{"name":"F. Gilbert"},{"name":"F. Kienle"},{"name":"N. Wehn"}],"title":{"text":"Low complexity stopping criteria for UMTS turbo-decoders"}},{"authors":[{"name":"F.-M. Li"},{"name":"A.-Y. Wu"}],"title":{"text":"On the new stopping criteria of iterative turbo decoding by using decoding threshold"}},{"authors":[{"name":"A. H. Vinck"},{"name":"P. Dolezal"},{"name":"Y. Kim"}],"title":{"text":"Convolutional encoder state estimation"}},{"authors":[{"name":"J. Geldmacher"},{"name":"K. Hueske"},{"name":"J. G¨otze"}],"title":{"text":"An adaptive and complexity reduced decoding algorithm for convolutional codes and its application to digital broadcasting systems"}},{"authors":[{"name":"J. Geldmacher"},{"name":"K. Hueske"},{"name":"S. Bialas"},{"name":"J. G¨otze"}],"title":{"text":"Adaptive low complexity MAP decoding for turbo equalization"}},{"authors":[{"name":"L. Bahl"},{"name":"J. Cocke"},{"name":"F. Jelinek"},{"name":"J. Raviv"}],"title":{"text":"Optimal decoding of linear codes for minimizing symbol error rate"}},{"authors":[{"name":"J. Schalkwijk"},{"name":"A. Vinck"}],"title":{"text":"Syndrome decoding of binary rate-1/2 convolutional codes"}},{"authors":[{"name":"M. Tajima"}],"title":{"text":"Metrics for syndrome decoding of convolutional codes"}},{"authors":[{"name":"M. Tajima"},{"name":"K. Shibata"},{"name":"Z. Kawasaki"}],"title":{"text":"Relation between encoder and syndrome former variables and symbol reliability estimation using a syndrome trellis"}},{"authors":[{"name":"T. Yamada"},{"name":"H. Harashima"},{"name":"H. Miyakawa"}],"title":{"text":"A new maximum likelihood decoding of high rate convolutional codes using a trellis"}},{"authors":[{"name":"L. Lee"},{"name":"D. Tait"},{"name":"P. Farrell"},{"name":"P. Leung"}],"title":{"text":"Novel scarce-state-transition syndrome-former error-trellis decoding of (n, n-1) convolutional codes"}},{"authors":[{"name":"T. Minowa"},{"name":"H. Imai"}],"title":{"text":"Decoding of high-rate turbo codes using a syn- drome trellis"}},{"authors":[{"name":"T. Ngatched"},{"name":"F. Takawira"}],"title":{"text":"Simple stopping criterion for turbo decoding"}},{"authors":[{"name":"R. Johannesso"},{"name":"K. S. Zigangiro"}],"title":{"text":"Fundamentals of Convolutional Coding "}},{"authors":[{"name":"P. Wu"},{"name":"N. Jindal"}],"title":{"text":"Coding versus arq in fading channels: How reliable should the phy be?"}},{"authors":[{"name":"H. Liu"},{"name":"C. Jego"},{"name":"E. Boutillon"},{"name":"M. Jezequel"},{"name":"J.-P. Diguet"}],"title":{"text":"Scarce state transition turbo decoding based on re-encoding combined with a dummy insertion"}},{"authors":[{"name":"M. Tajima"},{"name":"K. Okino"},{"name":"T. Miyagoshi"}],"title":{"text":"Minimal code(error)-trellis module construction for rate-k/n convolutional codes: Extension of yamada-harashima-miyakawas construction"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565737.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T6.5","endtime":"16:20","authors":"Jan Geldmacher, Klaus Hueske, Jürgen Götze, Martin Kosakowski","date":"1341504000000","papertitle":"Low Complexity Syndrome Based Decoding of Turbo Codes","starttime":"16:00","session":"S13.T6: Convolutional and Turbo Codes","room":"Kresge Rehearsal A (033)","paperid":"1569565737"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
