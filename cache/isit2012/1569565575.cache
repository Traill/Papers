{"id":"1569565575","paper":{"title":{"text":"Active Sequential Hypothesis Testing with Application to a Visual Search Problem"},"authors":[{"name":"Nidhin Koshy Vaidhiyan"},{"name":"S. P. Arun"},{"name":"Rajesh Sundaresan"}],"abstr":{"text":"Abstract\u2014We consider a visual search problem studied by Sripati and Olson where the objective is to identify an oddball image embedded among multiple distractor images as quickly as possible. We model this visual search task as an active sequential hypothesis testing problem (ASHT problem). Chernoff in 1959 proposed a policy in which the expected delay to decision is asymptotically optimal. The asymptotics is under vanishing error probabilities. We ﬁrst prove a stronger property on the moments of the delay until a decision, under the same asymptotics. Applying the result to the visual search problem, we then propose a \u201cneuronal metric\u201d on the measured neuronal responses that captures the discriminability between images. From empirical study we obtain a remarkable correlation (r = 0.90) between the proposed neuronal metric and speed of discrimination between the images. Although this correlation is lower than with the L 1 metric used by Sripati and Olson, this metric has the advantage of being ﬁrmly grounded in formal decision theory."},"body":{"text":"Sripati and Olson [1] studied the correlation between the speed of discrimination of images and a measure of dis- similarity of neuronal representations of the images in the inferotemporal (IT) cortex of the brain. They conducted two sets of experiments, one on human subjects and the other on macaque monkeys. The experiments were the following.\n(1) In the experiment on humans, subjects were shown a picture as in Figure 1(a) having six images placed at the vertices of a regular hexagon, with one image being different from the others. In particular, let I 1 and I 2 be two images. One of these two was picked randomly with equal probability and was placed at one of the six locations randomly, again with equal probability. The remaining ﬁve locations contained the copies of the other image. See Figure 1(a) and 1(b). The subjects were required to identify the correct half (left or right) of the plane where the odd image was located. The subjects were advised to indicate their decision \u201das quickly as possible without guessing\u201d [1]. The time taken to make a decision after the onset of the image was recorded.\n(2) In the other set of experiments on macaque monkeys, the images I 1 and I 2 were displayed on the screen, and the neuronal ﬁrings elicited by I 1 separately and I 2 separately on a set of IT neurons were recorded. The neuronal representation of an image was taken to be a vector of average ﬁring rates across the neurons. The two experiments were done on several pairs ( I 1 , I 2 ) of images. The authors observed a\nremarkably high correlation (r = 0.95) between the speed of discrimination in humans and the L 1 distance between the ﬁring rate vectors of the two images. They concluded that the more similar the neuronal representation (in monkey IT cortex), the tougher it is for humans to distinguish the images.\nIn this paper, we model the visual search problem as an active sequential multiple hypothesis testing problem (ASHT problem), ﬁrst studied by Chernoff [2]. In sequential hypoth- esis testing [3]\u2013[9], the only decision a controller makes at each stage is to either stop and make a decision or continue to draw another sample and trade off the cost of delay for a decision of better quality. In active sequential hypothesis testing, the controller is additionally capable of controlling the quality of the sample drawn. In our experiment the human subject can actively choose the location to sample based on past observations and actions.\na) Prior Work: Chernoff [2] formulated the ASHT prob- lem, proved an asymptotic lower bound under vanishing error probabilities and further devised the so-called Procedure A that he went on to show was asymptotically optimal under vanishing error probabilities. Recently Naghshvar and Javidi [10]\u2013[12], also motivated by a visual search problem, cast Chernoff\u2019s problem into the framework of dynamic program- ming and attempted to derive structural properties on the value functions and the decision regions. A related but different line of research was carried out by Rao [13] who studied visual search as a partially observed Markov decision problem over ﬁxed time horizons. In contrast, the works of Chernoff [2], Naghshvar and Javidi [10]\u2013[12], and this work model the\nb) Our contribution: We ﬁrst provide a mild extension of Chernoff\u2019s asymptotic optimality of expected delay for his Procedure A by showing asymptotic optimality for all moments. We then recognize that Chernoff\u2019s solution suggests a natural \u201cneuronal metric\u201d for the visual search problem that is more appropriate than the L 1 distance between the vectors of ﬁring rates used in Sripati and Olson [1]. This metric is closely related to the relative entropy between the Poisson point processes associated with ﬁring rate vectors for the two images. We do see a high correlation (r = 0.9) between speed of discrimination and the new neuronal metric. While this is lower than the correlation (r = 0.95) with the L 1 metric as observed by Sripati and Olson [1], we remark that our abstraction is overly simplistic, and emphasize that our work provides a more logical basis for the proposed neuronal metric than the L 1 distance.\nc) Organization: In Section II we study the ASHT problem. We ﬁrst set up the notation, state the assumptions, and identify the performance criterion in sections II-A through II-C. In section II-D we give an asymptotic lower bound for the moments of the stopping time for all policies that satisfy a constraint on the maximum possible conditional error cost. In section II-E we show that a class of policies proposed by Chernoff [2] and Naghshvar and Javidi [12] asymptotically attain the lower bounds for general moments. In Section III, we return to the visual search problem, and obtain the appropriate metric for comparison with delays and identify the degree of correlation.\nIn this section we study the active sequential multiple hypothesis testing problem (ASHT problem).\nLet H i , i = 1, 2, . . . , M denote the M hypotheses of which only one holds true. Let A be the set of all possible actions which we take as ﬁnite |A| = K < ∞. Let X be the observation space. Let (X n ) n≥1 and (A n ) n≥1 denote the observation process and the control process respectively. We assume that (A n ) n≥1 is a random or deterministic function of the past observations and actions. Conditioned on action A n and the true hypothesis H, we assume that X n is conditionally independent of previous actions A n−1 = (A 1 , A 2 , . . . A n−1 ) and observations X n−1 = (X 1 , X 2 , . . . , X n−1 ). Let q a i be the probability density function with respect to some reference measure µ for observation X under action a when H i is the true hypothesis. Let q i (X n , A n ) be the probability density, with respect to a common reference measure µ ⊗n ×unif(A) ⊗n , of observations and actions till time n, where unif(A) is the uniform distribution on A. Let Z ij (n) denote the log- likelihood ratio (LLR) process of hypothesis H i with respect to hypothesis H j , i.e.,\nLet E i denote the conditional expectation and let P i de- note the conditional probability measure under the hypothesis H i . Let D(q a i q a j ) denote the relative entropy between the probability measures associated with the observations under hypothesis H i and hypothesis H j under action a.\nLet p i (0) denote the prior probability that hypothesis H i is true. The posterior probability that hypothesis H i is true, given the observations and actions till time n is denoted p i (n). The beliefs p (n) = (p 1 (n), . . . , p M (n)) admit the sequential update\nA policy π is a sequence of action plans that at time n looks at the history X n−1 , A n−1 and prescribes a composite action that could be either (stop, d) or (continue, λ). If the composite action is (stop, d), then d is the decision on the hypothesis at the stopping time, and so d ∈ {1, 2, . . . , M }. If the composite action plan is (continue, λ), then λ ∈ P(A) is the distribution with which the next control is picked. Let τ be the stopping time. Recall that (A n ) n≥1 is the control process until the stopping time.\nGiven an error tolerance vector α = (α 1 , α 2 , . . . , α M ) with 0 < α i < 1, let ∆(α) be the set of policies\nThese are policies that meet a speciﬁed conditional error probability tolerance criterion. Let || α|| = max i α i .\nWe deﬁne λ i to be the mixed action under hypothesis H i that guards against the nearest alternative, i.e., λ i ∈ P(A) such that\nLet A ij = {a ∈ A : D(q a i q a j ) > 0} be the set of all actions that can differentiate hypothesis H i from hypothesis H j . It is easy to see that A ij = A ji . Finally, let β k ij = a∈A\nλ k (a), i.e., β k ij is the probability of choosing some action that can distinguish hypothesis H i from hypothesis H j when the actions are picked according to λ k .\nAssumption (I) implies that D(q a i ||q a j ) < ∞ and ensures that no single observation can result in a reliable decision. Assump- tion (II) is an important technical assumption in our work. A crucial exponential boundedness property of the stopping time\nfor Chernoffs Procedure A and Naghshvar and Javidi\u2019s Policy- NJ (described in section II-E), which is required in many proofs, is based on this assumption. Further it, ensures that under all beliefs, there is a positive probability of choosing an action that can distinguish hypothesis H i from hypothesis H j (i = j). In particular, for any distinct i and j, there is at least one control that can help distinguish the hypotheses H i from H j .\nWe study the ASHT problem from the perspective of mini- mizing the expected stopping time delay subject to constraints on the conditional probability of error, i.e., policies belonging to ∆(α).\nThe following proposition gives a lower bound for the mth moment of the conditional stopping time given hypothesis H i for all policies belonging to ∆(α).\nProposition 2.1: Assume (I). Then for any π ∈ ∆(α) and any m ≥ 1, we have\n(6) where D i is given in (5).\nProof: The proof for the case m = 1 follows from the proof of [2, Th. 2, p. 768] with minor modiﬁcations to account for the possibly different α j . (See also proof of [8, Lem 2.1, Th 2.2]). For m > 1, the results follows from the fact that E i [τ m ] ≥ (E i [τ ]) m .\nE. Achievability - Chernoff \u2019s Procedure A and related policies Chernoff [2] proposed a policy termed Procedure A and\nshowed that it has asymptotically optimal expected decision delay. Naghshvar and Javidi [12] proposed a class of policies (hereafter referred to as Policy-NJ), which are minor variants of Procedure A. We shall now argue that both the above poli- cies are asymptotically optimal for all moments of stopping time. We consider only Procedure A. The result for Policy-NJ follows from a similar analysis.\nPolicy Procedure A: π P A (L) Fix L > 0. At time n:\n\u2022 If p θ(n) (n) < L 1+ L , then A n+1 (p) is chosen according to λ θ(n) , i.e., P (A n+1 (p) = a) = λ θ(n) (a).\n\u2022 If p θ(n) (n) ≥ L 1+ L , then the test retires and declares H θ(n) as the true hypothesis.\nFix 0.5 < ˜ p < 1. Fix L > 0. At time n:\n\u2022 If 0 ≤ p i (n) < ˜ p for every i, then A n+1 (p) is chosen uniformly, i.e., P (A n+1 (p) = a) = 1 K .\n\u2022 If ˜ p ≤ p i (n) < L 1+ L for some i, then A n+1 (p) is chosen according to λ i , i.e., P (A n+1 (p) = a) = λ i (a).\n\u2022 If p i (n) ≥ L 1+ L for some i, then the test retires and declares H i as the true hypothesis.\nAs mentioned earlier, we focus only on Procedure A. However, we also consider the following variant of Procedure A for analysis. Policy π j P A (L) is same as π P A (L) except that it stops only when p j (n) ≥ L 1+ L .\n1) τ j P A (L) := inf{n : p j (n) ≥ L 1+ L } is the stopping time at which the posterior belief of hypothesis H j crosses the detection threshold for policy π j P A .\n2) τ P A (L) := min j τ j P A (L) is the stopping time for policy π P A (L).\nPolicy π P A is deﬁned to stop only when the posteriors suggest a reliable decision. This is formalized now.\nProposition 2.3: For Policy π P A , the conditional probabil- ity of error under hypothesis H i is upper bounded by\nWe now proceed towards identifying the time-delay perfor- mance of the policy π P A . Towards this, we ﬁrst consider the easier to analyze policy π i P A . Note that as L → ∞, we have α i → 0, and therefore the policy has vanishing error cost. However, the time taken scales to inﬁnity as given next.\nProposition 2.4: Assume (I) and (II). Consider policy π i P A . The following convergences then hold as L → ∞:\nA sandwiching argument on the likelihood ratio about the stopping time τ i P A (L) gives the ﬁrst result. To show conver- gence in L p we prove and use an exponentially boundedness property of the stopping time τ i P A (L). We omit the complete proof due to lack of space.\nTheorem 2.5: Choose α as per (8). Assume (I) and (III). The family of policies (π P A (L)) L>0 is asymptotically optimal in the sense that, for each m ≥ 1, we have\nand by Proposition 2.4, we have the achievability of the above lower bound by policy π P A (L) ∈ ∆(α).\nWe now return to the visual search problem. In the visual search task, a subject has to identify an oddball image from amongst W images displayed on a screen (W = 6 in Figures 1(a) and 1(b)). With equal probability, the odd image can either be image I 1 or image I 2 . When image I 1 is the odd image, the other W − 1 images are of type I 2 and vice-versa. For the purpose of modeling, we make the following assumptions. The subject can focus attention on only one of the W positions, and that the ﬁeld of view is restricted to that image alone. Further, we assume that time is slotted and of duration T . The subject can change the focus of his attention to any of the W image locations, but only at the slot boundaries. We assume that the subject would have indeed found the exact location of the odd image and the image identity before mapping it to a \u201cleft\u201d or \u201cright\u201d decision. These are clearly oversimplifying assumptions, yet our model and analysis provide some insights into the visual search problem.\nIf the image in the focused location is I i , we assume that a set of d neurons sensitive to these images produce spike trains, which constitute the observations. These are modeled as d Poisson point processes of duration T with rates R i = (R i (1), R i (2), . . . , R i (d)).\nThe visual search problem can be formulated as a 2W hypothesis testing problem:\nAs the subject can change the point of focus at any slot boundary we have an ASHT problem.\nWe now calculate the optimal λ i and the optimal neuronal metric D i for the visual search problem. Recall that the set of controls A = {1, 2, . . . , W }. For notational simplicity let f i denote the probability density on the observations when focusing on image I i , i = 1, 2 . The conditional probability density function q a i under hypothesis H i when action a is chosen is :\nIndeed, under Hypothesis H i with i ≤ W , the odd image is I 1 and is at location i. If the control is to focus on location i, i.e., a = i, then the subject views image I 1 and so the observations have density f 1 corresponding to I 1 . The others are explained similarly.\nThe relative entropy between the probability densities for the various combinations of hypotheses and actions are:\n  \nD(f 1 f 2 ) a = i D(f 2 f 1 ) a = j\n0 \t a = i, a = j, (ii) \t i ≤ W , j = i + W\n2 f 1 ) a = i, (iii) \t i ≤ W , j > W , j = i + W\n  \nOur aim is to ﬁnd the optimum λ i that maximizes D i = max\nThe corresponding optimum D i thus obtained will yield the required \u201cneuronal metric\u201d (after a further scaling). Due to the symmetry in the problem, λ i and D i will be the same under all hypotheses H i , i ≤ W , and similarly it will be the same under all hypotheses H i , i > W . Without loss of generality we consider the case i ≤ W . Solution to the case when i > W will have the same structure.\nTheorem 3.1: Let i ≤ W . The optimum λ i and D i are as follows\nf 2 ) + (W − 3)D(f 2 f 1 ) \t ∀j = i, D i = \t (W − 2)D(f 1 f 2 )D(f 2 f 1 ) (W − 1)D(f\nThe proof is omitted due to lack of space. Note that the lower bound in Naghshvar and Javidi [12] would have max{D(f 1 f 2 ), D(f 2 f 1 )} as an upper bound for the neu- ronal metric, but our bound tightens it by a factor of two.\nd) Empirical Study on Neuronal Data: We now apply the results obtained in the previous section to the empirical data obtained from the experiments of Sripati and Olson [1]. Let T be the slot duration, during which one focuses attention on a particular image. X is the space of counting processes in [0, T ] with an associated σ-algebra. Let P 1 ,T be the standard Poisson point process and let P ⊗d 1 ,T its d-fold product measure. Let P R j ,T denote the probability measure P j , so that f j = dP Rj ,T dP ⊗d\nmetric when f 1 and f 2 are vector Poisson processes of duration T with rates R 1 = (R 1 (1), R 1 (2), . . . , R 1 (d)) and R 2 = (R 2 (1), R 2 (2), . . . , R 2 (d)). Under an independence assumption, the relative entropy between the vector Poisson processes becomes the sum of relative entropies between the individual processes. Then\nSripati and Olson [1] conducted the experiment with W = 6. The normalized per-unit-time per-neuron neuronal metric when image I 1 is the target image is\nThe experimental data used in the empirical study consisted of the following. 1) Neuronal ﬁring rate vectors were obtained from the IT cortex of rhesus macaque monkeys for twelve image pairs with the number of neurons ranging from 78 to 174 for different image pairs. 2) Reaction times statistics for detection of the odd image were obtained from experiments on human subjects. The neuronal behavioral index for an ordered pair of images (i, j) is the inverse of average decision delay minus a baseline delay. In Figure 2, we plot the behavioral discrimination index (speed of discrimination or inverse of time taken to decide) against the normalized per-unit-time per- neuron neuronal metric. The correlation between behavioral discrimination index and the neuronal metric was 0.90. This is smaller than the correlation of 0.95 between the behavioral discrimination index and the L 1 distance between the neuronal ﬁring rate vectors. The discrepancy might arise from the highly simpliﬁed theoretical formulation, or from computational con- straints in the brain. Nonetheless the close correspondence with the data suggests that the neuronal metric proposed here is a step in the right direction.\nThis work is supported by the DST and UGC, Government of India. The authors would like to thank an anonymous reviewer for pointing our attention to [2] which greatly sim- pliﬁed our presentation."},"refs":[{"authors":[{"name":"A. P. Sripati"},{"name":"C. R. Olson"}],"title":{"text":"Global image dissimilarity in macaque inferotemporal cortex predicts human visual search efﬁciency"}},{"authors":[{"name":"H. Chernoff"}],"title":{"text":"Sequential design of experiments"}},{"authors":[{"name":"A. Wald"}],"title":{"text":"Sequential tests of statistical hypotheses"}},{"authors":[{"name":"A. Wald"},{"name":"J. Wolfowitz"}],"title":{"text":"Optimum character of the sequential probability ratio test"}},{"authors":[{"name":"A. Tartakovsky"}],"title":{"text":"Sequential testing of many simple hypotheses with independednt observations"}},{"authors":[{"name":"C. Baum"},{"name":"V. Veeravalli"}],"title":{"text":"A sequential procedure for multihypothesis testing"}},{"authors":[{"name":"V. Veeravalli"},{"name":"C. Baum"}],"title":{"text":"Asymptotic efﬁciency of a sequential multihypothesis test"}},{"authors":[{"name":"A. Tartakovsky"}],"title":{"text":"Asymptotic optimality of certain multihypothesis se- quential tests: Non - i.i.d. case"}},{"authors":[{"name":"V. Draglia"},{"name":"A. Tartakovsky"},{"name":"V. Veeravalli"}],"title":{"text":"Multihypothesis sequen- tial probability ratio tests - part I. Asymptotic optimality"}},{"authors":[{"name":"M. Naghshvar"},{"name":"T. Javidi"}],"title":{"text":"Active M -ary sequential hypothesis testing"}},{"authors":[],"title":{"text":"Information utility in active sequential hypothesis testing"}},{"authors":[],"title":{"text":"Performance bounds for active sequential hypothesis testing"}},{"authors":[{"name":"R. P. N. Rao"}],"title":{"text":"Decision making under uncertainty: A neural model based on partially observable markov decision processes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565575.pdf"},"links":[{"id":"1569567049","weight":6},{"id":"1569565931","weight":6},{"id":"1569564401","weight":6},{"id":"1569566821","weight":6},{"id":"1569556713","weight":6},{"id":"1569562685","weight":6},{"id":"1569566709","weight":13},{"id":"1569561513","weight":6},{"id":"1569565841","weight":6},{"id":"1569566649","weight":6},{"id":"1569565549","weight":33},{"id":"1569566823","weight":6},{"id":"1569565349","weight":6},{"id":"1569564769","weight":6}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T8.3","endtime":"12:30","authors":"Nidhin Vaidhiyan, S. P. Arun, Rajesh Sundaresan","date":"1341490200000","papertitle":"Active Sequential Hypothesis Testing with Application to a Visual Search Problem","starttime":"12:10","session":"S12.T8: Hypothesis Testing","room":"Stratton (491)","paperid":"1569565575"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
