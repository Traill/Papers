{"id":"1569566617","paper":{"title":{"text":"Ripple Design of LT Codes for AWGN Channel"},"authors":[{"name":"Jesper H. Sørensen ∗\u2020"},{"name":"Toshiaki Koike-Akino \u2020"},{"name":"Philip Orlik \u2020"},{"name":"Jan Østergaard ∗"},{"name":"Petar Popovski ∗"}],"abstr":{"text":"Abstract\u2014In this paper, we present an analytical framework for designing LT codes in additive white Gaussian noise (AWGN) channels. We show that some of analytical results from binary erasure channels (BEC) also hold in AWGN channels with slight modiﬁcations. This enables us to apply a ripple-based design approach, which until now has only been used in the BEC. LT codes designed by this way show promising performance which is near the Shannon limit even with short codewords."},"body":{"text":"LT codes [1] were the ﬁrst practical examples of a rateless erasure correcting code which approaches capacity for increas- ing message length. Raptor codes [2] were later developed as an extension of LT codes. Such rateless codes may potentially generate an inﬁnite amount of encoded symbols from ﬁnite input symbols. An important element in the design of both LT and Raptor codes for the binary erasure channel (BEC) is a parameter called the ripple. The performance depends signiﬁcantly on how this parameter evolves during decoding, and thus successful designs have mostly focused on this. Although LT codes were originally developed for the BEC, several works have recently focused on designing such codes for noisy channels.\nDesign of Raptor codes for the binary symmetric channel (BSC) and binary-input additive white Gaussian noise (Bi- AWGN) channel is treated in [3]. This design is based on the Gaussian approximation method [4], which is used to derive constraints for the degree distribution of the LT code. In [5, 6], approaches based on EXIT charts are applied to the design of LT and Raptor codes, respectively. Another design of Raptor codes for arbitrary symmetric channels is presented in [7], where an analogue to the ripple is deﬁned as the increase in correct bit estimates in a given decoding round. The design is based on ﬁnding an optimal value for this measure.\nIn this paper, we present an analytical framework for the design of LT codes in the AWGN channel. It has strong analogies to the framework presented in [1] for the BEC. Interestingly, we show that key analytical results in the BEC also hold even in the AWGN channel. This enables us to make a ripple-based design for the AWGN channel with slight modiﬁcations, which exploit characteristics unique in AWGN. The main contribution of this work is a bridge between the work in the BEC and the AWGN channel, and it can help further design extensions for such noisy channels.\nIn this section, an overview of LT codes is presented. Assume we wish to transmit a given amount of data, which\nis divided into k input symbols. From these input symbols a potentially inﬁnite amount of encoded symbols, called output symbols, are generated. Output symbols are exclusive- or (XOR) combinations of input symbols. The number of input symbols used in the XOR is called the degree of the output symbol, and all input symbols contained in an output symbol are called neighbors of the output symbol. The output symbols follow a certain degree distribution, Ω(d). The encoding process can be broken down into three steps: 1) Randomly choose a degree d by sampling Ω(d). 2) Choose uniformly at random d of the k input symbols. 3) Perform XOR of the d chosen input symbols. The resulting symbol is the output symbol. This process can be iterated as many times as needed, that results in a rateless code.\nA widely used decoder for LT codes is a belief propagation (BP) decoder. For the BP decoder, messages are passed between neighboring symbols, i.e. from output symbols to input symbols or vice versa. Such a message reﬂects the current belief of the sender on the value of an input symbol. The belief is quantiﬁed by the log-likelihood ratio (LLR), deﬁned as ln Pr(X i =1 |Y ) Pr(X i =0 |Y ) , where X i is the i-th binary input symbol and Y is the AWGN channel output symbols vector of potentially inﬁnite length. The o-th output symbol is denoted as Y o . The -th round of the BP decoder starts with all output symbols passing a message, m o,i to all their neighboring input symbols. Based on those messages, the input symbols pass a message, m i,o , back to all their neighboring output symbols. These rounds continue until a speciﬁed stop criterion has been reached, e.g. a certain number of rounds or a target error rate.\nwhere Z o is the LLR of Y o based only on the channel output. The product (sum) is taken over all neighboring input (output) symbols other than the message recipient i (o) itself.\nIn the BEC, the BP decoder can be signiﬁcantly simpliﬁed since all received symbols in decoding are completely reliable.\nThis implies that the decoder can perform the logical XOR op- erations inversely from the encoding process. First, all degree- 1 output symbols are identiﬁed and moved to a storage referred to as the ripple. Symbols in the ripple are processed one by one, in which they are XOR\u2019ed with all output symbols who have them as neighbors. Once a symbol has been processed, it is removed from the ripple and considered decoded. The processing of symbols in the ripple will potentially reduce some of the buffered symbols to degree one, in which case they are moved to the ripple. This is called a symbol release. The decoder can then process symbols in a successive fashion.\nThe ripple is an important parameter in the BEC. In [1], a trade-off was described. If a new symbol is released every time, there is a risk that it is already in the ripple, that causes redundancy. It suggests that the ripple size should be kept low. However, in order to decrease the risk of decoding failure, which occurs when the ripple size is zero, the ripple size should be kept high enough. A good solution to this trade- off is the main design problem in the BEC. In the following analysis, we use information-theoretic tools to present an ana- lytical framework which generalizes the ripple-based approach towards the AWGN channel.\nWe consider the BP decoder described in section II-A, with a slight modiﬁcation, in order to facilitate the analysis. Instead of letting all input symbols pass a new message in a round, we allow only one randomly chosen input symbol to do so. All other input symbols pass the message in the previous round. This modiﬁed BP decoder for input-to-output message updating is expressed as\nThis modiﬁcation is known as a random scheduling for BP. It is known that such a sequential scheduling offers better convergence performance than a standard parallel scheduling. Note that a code designed by this analysis can be decoded by any BP decoder scheduling.\nWe ﬁrst express the entropy, H(Z i ) , of the i-th input symbol after decoding rounds as follows:\nWhen an input symbol passes a new message to its neigh- bors, we refer to the information it holds as processed informa- tion. After the -th decoding round, the processed information,\nI P , is deﬁned as the total amount of information passed from input symbols to output symbols. It is given as\nwhere H(Z p i ) is interpreted as the entropy of the i-th input symbol at the point of its last message passing. Directly following from (4), we have the deﬁnition of unprocessed information, I L = k i=1 H(Z p i ) .\nWhen deciding which input symbol should be allowed to pass a new message, a uniform random selection is performed among the input symbols, which hold information not yet passed to its neighbors. We say that these candidates contribute to the information ripple. The information ripple, I R , after the -th decoding round, is deﬁned as the total amount of information, held by the input symbols which have not yet been passed to the output symbols. We have\nAfter the input symbol has passed its message, the output symbols obtain a chance to pass messages back to their neighboring input symbols. Only output symbols, which are neighbors to the last message passing input symbol, have new information to pass. This new information is referred to as released information, denoted I Q . It is expressed as\nwhere H(Z p+ i ) is interpreted as the entropy of the i-th input symbol when processed and newly released information is taken into account.\nHere, I Q is deﬁned by H(Z p i ) as reference, which is the information known by the output symbols. Hence, I Q can be regarded as the amount of new information passed to the input symbols, as seen from an output symbol perspective. In fact, this is not the true amount of new information since it might be combined with information in the ripple. For this case, the actual reference is H(Z −1 i ) and we can deﬁne the actual amount of information added to the ripple, I A , as follows:\nThe quantities deﬁned in (3) through (7) are illustrated in Fig. 1, where the entropy of a single input symbol has been plotted as a function of its LLR. Due to the convexity of the entropy function (except at very low LLR), we have I A < I Q , which means loss of information. This is analogous to the risk of redundancy for nonzero ripple in the BEC.\nIn general, there is a strong relation between the quantities in (3) through (7) and the terms deﬁned in section II-B for the BEC decoder. They are essentially continuous entropy counterparts of the discrete symbol based versions from the BEC. One interesting quantity in a ripple-based design is the expected amount of released information from an output symbol of degree d as a function of the amount of unprocessed information. It expresses the universal connection between the degree distribution, which is the design parameter, and the rate of recovery of new information during the decoding process. It was derived in [1] for the BEC, more speciﬁcally\nq(d, L) = 0, for all other d and L, \t (8) where L is the amount of unprocessed input symbols. Deriving the AWGN channel equivalent to (8) is outside the scope of this paper. However, we can easily obtain an understanding of its behavior by simulating an LT code in an AWGN channel and logging I Q versus I L for different degrees. In order to compare with (8), we quantize this data to integer values of I L and normalize such that it sums up to one. We thus have the fraction of the released information, which is released when I L bits remain unprocessed. This is denoted as I q (d, I L ) for output symbols of degree d and is deﬁned as\nwhere only information from output symbols of degree d is included. Fig. 2 shows a plot of the results at a signal-to-noise power ratio (SNR) of 5 dB. It reveals a clear correspondence between the theory derived for the BEC and what is observed in the AWGN channel.\nHowever, as described in connection with (7), not all released information is added to the ripple. In order to de-\n, the ratio of released information which is added to the ripple. The BEC counterpart is A(L) Q(L) = L −R(L) L , where A(L), Q(L) and R(L) are symbols added to the ripple, released symbols and symbols in the ripple after the (k − L)-th decoding round, respectively.\nhas been determined for 0 < R ≤ 20 and 0 < L ≤ 64 through a simulation similar to the one used for determining I q (d, I L ) . Fig. 3 shows a histogram of I A I\n− A(L) Q(L) , which illustrates that the AWGN channel behaves as the BEC with a small perturbation.\nBased on q(d, L) and the fact that A(L) Q(L) = L −R(L) L holds in the BEC, it is possible to control the expected amount of symbols added to the ripple in each decoding round, through the choice of degree distribution. We can now conclude that this theory also holds in the AWGN channel. Hence, the same ripple-based design approach can be used for the AWGN channel, in order to control the ﬁrst moment of the ripple. This suggests that degree distributions designed for the BEC should also perform well in the AWGN channel, which was also concluded in [8]. However, it was also mentioned that there is still a room for improvement, and thus behavioral differences between the BEC and the AWGN channel. These differences should be evident in the higher moments of the ripple. We take this into account in our design, which is described next.\nA typical approach to design LT codes is to choose design criterion for the ﬁrst moment of the ripple and meet it through proper choice of the degree distribution. The ﬁrst moment criterion is chosen based on heuristic assumptions about the second moment behavior. This approach was taken in both\n[1] and [2] for the BEC. Using the analytical framework presented in the previous section, we will apply the ripple- based approach to designing LT codes in the AWGN channel.\nIn [2], the assumptions about the second moment behavior of the ripple in the BEC was based on a random walk model. It was assumed that the ripple size either increases or decreases by one, with equal probabilities, in each decoding round, i.e., R +1 = R ± 1. Since one symbol is processed in each round in the BEC, L rounds will remain, and we can calculate the expected distance from the origin, E |R +L − R | (E[·] denotes an expectation), after the random walk as follows:\nBased on this it was argued that the expected ripple should be kept at c √ L , for a properly chosen constant c, when L symbols remain unprocessed, i.e. in the (k − L)-th decoding round. We will adopt this approach, for our choice of expected information ripple for the AWGN channel with a slight mod- iﬁcation which deals with the characteristics of this channel.\nFirst, we note that a decoding round does not necessarily result in the processing of 1 bit, i.e. H(Z p i ) − H(Z i ) ≤ 1. Hence, we assume that the ripple will increase or decrease by α with equal probabilities. Moreover, it is possible that H(Z i ) > H(Z p i ) , i.e. the messages passed from output symbols to input symbol i, since last time it was allowed to pass, has increased the entropy of that input symbol. In this case, we actually have negative decoding progress. Numerical experiments show that this happens more frequently later in the decoding progress, which results in an increasing number of decoding rounds per one bit of processed information. We assume a linear increase, such that the number of steps per processed bit is θ = β(1 − I L k ) + 1 . Hence, we obtain\nIt determines a desired information ripple evolution with properly chosen constants, α and β. Note that this model is based on heuristics, as in [1] and [2]. It has been conﬁrmed that a higher-order polynomial model of θ had almost no gains.\nThe AWGN channel differs from the BEC in another signiﬁcant way in BP decoding. Messages passed from output symbols may be misleading, in the sense that they contribute with an LLR of an opposite sign of the true value of the bit. If one or more bits are in error, more output symbols will need to be collected. In this case, the errors may propagate and make future output symbols misleading as well.\nConsider the example where 2 out of k input symbols are in error and a new output symbol of degree 3 is received. The three neighbors of the new symbol are the two erroneous\ninput symbols and one correct input symbol. When this new output symbol passes a message to an input symbol, it is based on a product of the messages from the two other neighbors. Hence, for the erroneous input symbols, yet another misleading message will be passed, while for the correct input symbol, the errors cancel out. If only one erroneous input symbol had been a neighbor, the output symbol would have made a helpful contribution. Similar examples can be made with higher numbers of errors. In general, we can say that if it is more likely that an even number of erroneous input symbols are neighbors to a new output symbol, compared to an odd number, then the errors will be self-perpetuating.\nThis observation can be used to create a design criterion, where we focus on the cases of 1 and 2 erroneous neighbors, since these are most likely. We deﬁne γ(d) = p 1 (d) − p 2 (d) , where p e (d) is the probability that a new output symbol of degree d has e and only e erroneous neighbor(s) for e = 1, 2. They are expressed as p 1 (d) = ( d 1 )( k −d 1 ) ( k\n( k 2 ) and plotted as a function of d in Fig. 4. The plots illustrate that high degree symbols should be avoided, whereas degrees in the lower half of the spectrum are more useful. In our design, we seek to maximize E[γ] = d γ(d)Ω(d) , under the constraints given by our choice of ripple.\nWe now summarize the analysis and the ripple-based design method for noisy channels. In section III-B, we showed that (8), which was originally derived for the BEC, also holds in the AWGN channel. Moreover, we showed that I A I\nL is also analogous to the BEC. If we quantize the decoding process to integer values of I L , we can express the expected evolution of the information ripple for each processed bit:\nwhere I(X; Y ) is the mutual information of the AWGN chan- nel and n is the number of symbols collected for decoding.\nobtain the system of equations as follows:     \nq(1, k) 0 \t 0 ... ... 0\nq(1, 1) · · · q(k, 1) ωγ(1) · · · ωγ(k)\n    \n      \n    \n. Here we have also added E[γ] = d γ(d)Ω(d) , which we wish to maximize. This equation has a multiplier ω 1 since this is a ﬁrm constraint. Note that n acts as a normalization factor, which ensures a valid degree distribution.\nmax E[γ] s.t. |An Ω(d) − b| 2 < t, \t (14) where A is the matrix in (13), b is the vector on the RHS of (13) and t is an appropriately chosen tolerance level of the deviation from the target information ripple.\nSimulations have been performed in order to compare the proposed design, Ω(d), with existing degree distributions. A tolerance level of t = 0.1 has been chosen for the optimization problem in (14). A numerical optimization of the parameters α and β has been performed, which revealed that parameter pairs where 1.0 ≤ α ≤ 1.4 and 0 ≤ β ≤ 3 perform well, depending on the SNR. In general α should increase for increasing SNR, while β should decrease. References in the ﬁrst simulation are the Robust Soliton Distribution (RSD), with parameters c = 0.1 and δ = 1, and a degree distribution designed by the proposed approach presented in this work, but with the ripple target in (10), which was suggested in [2]. We refer to this distribution as Γ(d). A numerical optimization resulted in c = 1.3, which has been used. All distributions have been evaluated at k = 64 for an SNR of 0, 2, ..., 10 dB using BPSK modulations. They are compared with respect to average overhead necessary in order to successfully decode all input symbols. An input symbol is considered successfully decoded if its error probability is below 10 −12 . Fig. 5 shows the results, where it is evident that the approach presented in this work signiﬁcantly outperforms the distributions designed for the BEC. Especially at low SNR, where the difference between the AWGN channel and the BEC is more signiﬁcant, the proposed design excels. In this ﬁgure, we also present the lower bound of the overhead calculated by the Shannon limit in AWGN (for BPSK-constrained codebooks with an inﬁnite length). As we can see, the proposed degree distribution approaches the Shannon limit in the low SNR regimes within 1 dB even for such a very short message length of k = 64.\nAnother simulation has been performed, where the purpose is to evaluate the degree distributions with respect to block error rate at speciﬁc overheads. A block error occurs when at least one symbol has an error probability above 10 −12 after 2000 decoding rounds on (1 + )k output symbols. The\nsimulation is performed at an SNR of 0 dB using the same parameters as in the ﬁrst simulation. The results are shown in Fig. 6 and it is seen that the proposed design outperforms the other degree distributions at any overhead.\nWe have presented an analytical framework for LT codes in AWGN channels. Surprisingly, key analytical results known in the BEC can be applied to AWGN channels within this framework. This enables us to introduce a ripple-based design scheme in AWGN channels with a few modiﬁcations. LT codes based on this design show promising results compared to standard designs. In particular, the designed codes can approach the capacity in the low SNR regimes even for a very short message length. Our analytical framework is a strong tool for the ripple-based design in any noisy channels."},"refs":[{"authors":[{"name":"M. Luby"}],"title":{"text":"LT codes"}},{"authors":[{"name":"A. Shokrollahi"}],"title":{"text":"Raptor codes"}},{"authors":[{"name":"O. Etesami"},{"name":"A. Shokrollahi"}],"title":{"text":"Raptor codes on binary memoryless symmetric channels"}},{"authors":[{"name":"S.-Y. Chung"},{"name":"T. Richardson"},{"name":"R. Urbanke"}],"title":{"text":"Analysis of sum-product decoding of low-density parity-check codes using a Gaussian approxima- tion"}},{"authors":[{"name":"Z. Cheng"},{"name":"J. Castura"},{"name":"Y. Mao"}],"title":{"text":"On the design of Raptor codes for binary-input Gaussian channels"}},{"authors":[{"name":"I. Hussain"},{"name":"M. Xiao"},{"name":"L. Rasmussen"}],"title":{"text":"LT coded MSK over AWGN channels"}},{"authors":[{"name":"P. Pakzad"},{"name":"A. Shokrollahi"}],"title":{"text":"Design principles for Raptor codes"}},{"authors":[{"name":"R. Palanki"},{"name":"J. Yedidia"}],"title":{"text":"Rateless codes on noisy channels"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566617.pdf"},"links":[{"id":"1569566567","weight":25},{"id":"1569566381","weight":5},{"id":"1569565883","weight":10},{"id":"1569565867","weight":5},{"id":"1569564605","weight":5},{"id":"1569566605","weight":10},{"id":"1569566683","weight":5},{"id":"1569565551","weight":10},{"id":"1569566761","weight":5},{"id":"1569565091","weight":10},{"id":"1569566571","weight":5},{"id":"1569565461","weight":5},{"id":"1569566207","weight":5},{"id":"1569566319","weight":5},{"id":"1569566941","weight":10},{"id":"1569566739","weight":5},{"id":"1569564203","weight":5},{"id":"1569566157","weight":5},{"id":"1569565455","weight":5},{"id":"1569565953","weight":5},{"id":"1569564613","weight":5},{"id":"1569564311","weight":5},{"id":"1569566679","weight":10},{"id":"1569555999","weight":15},{"id":"1569565199","weight":5},{"id":"1569566511","weight":5},{"id":"1569566531","weight":5},{"id":"1569559111","weight":5},{"id":"1569566809","weight":5},{"id":"1569565033","weight":5},{"id":"1569564353","weight":5},{"id":"1569566003","weight":10},{"id":"1569565185","weight":10},{"id":"1569566223","weight":5},{"id":"1569566553","weight":5},{"id":"1569565469","weight":35},{"id":"1569566191","weight":5},{"id":"1569566853","weight":5},{"id":"1569566501","weight":5},{"id":"1569565885","weight":5},{"id":"1569566805","weight":10},{"id":"1569559199","weight":5},{"id":"1569566779","weight":10},{"id":"1569566479","weight":5},{"id":"1569565765","weight":5},{"id":"1569565093","weight":25},{"id":"1569566927","weight":10},{"id":"1569566737","weight":5},{"id":"1569565353","weight":5},{"id":"1569564305","weight":5},{"id":"1569566547","weight":15},{"id":"1569565177","weight":5},{"id":"1569566823","weight":5},{"id":"1569566533","weight":5},{"id":"1569565457","weight":5},{"id":"1569566487","weight":5},{"id":"1569565529","weight":5},{"id":"1569565271","weight":10},{"id":"1569564769","weight":10},{"id":"1569565769","weight":10},{"id":"1569566601","weight":15},{"id":"1569565861","weight":5},{"id":"1569566147","weight":5},{"id":"1569565997","weight":10},{"id":"1569565707","weight":10},{"id":"1569565143","weight":5},{"id":"1569566973","weight":5},{"id":"1569565031","weight":5},{"id":"1569566727","weight":5},{"id":"1569565315","weight":10}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T5.2","endtime":"12:10","authors":"Jesper H Sørensen, Toshiaki Koike-Akino, Philip Orlik, Jan Østergaard, Petar Popovski","date":"1341402600000","papertitle":"Ripple Design of LT Codes for AWGN Channel","starttime":"11:50","session":"S10.T5: Rateless Codes","room":"Kresge Little Theatre (035)","paperid":"1569566617"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
