{"id":"1569565571","paper":{"title":{"text":"The Adversarial Joint Source-Channel Problem"},"authors":[{"name":"Yuval Kochman"},{"name":"Arya Mazumdar"},{"name":"Yury Polyanskiy"}],"abstr":{"text":"Abstract\u2014This paper introduces the problem of joint source- channel coding in the setup where channel errors are adversarial and the distortion is worst case. Unlike the situation in the case of stochastic source-channel model, the separation principle does not hold in adversarial setup. This surprising observation demonstrates that designing good distortion-correcting codes cannot be done by serially concatenating good covering codes with good error-correcting codes. The problem of the joint code design is addressed and some initial results are offered."},"body":{"text":"One of the great contributions of Shannon [1] was creation of tractable and highly descriptive stochastic models for the signal sources and communication systems. Shortly after, his work was followed up by Hamming [2], who proposed a combinatorial variation of the channel coding part. This com- binatorial formulation has become universally accepted in the coding-theoretic community. Similarly, for the case of lossy compression Shannon [3] proposed a stochastic model and the rate-distortion formula, while shortly after Kolmogorov fol- lowed up with a non-stochastic deﬁnition of the -entropy [4]. The research that followed demonstrated how both ways of thinking, stochastic and combinatorial, naturally complement each other, reinforcing intuition and yielding new results.\nTo the best of our knowledge, in the setup of joint source- channel coding, however, only the stochastic approach has been investigated so far, starting with [1], [3]. This paper aims to ﬁll in this omission.\nIn Section II we deﬁne the adversarial separate source and channel coding problems and present known results about them. Then, we build on these deﬁnitions to deﬁne the adversarial joint source channel coding (JSCC) problem. Next, in Section III we prove asymptotic bounds on the performance limits of adversarial JSSC codes. It turns out that the celebrated separation principle [1], [3] does not hold in the adversarial model. Therefore, the problem of constructing asymptotically optimal adversarial JSSC codes requires a joint approach and cannot be solved by combining good compressors with good error-correcting codes. In Section IV we focus on the binary case and propose methods for designing such codes and analyzing their performance.\nA source problem is speciﬁed by a source and reproduction alphabets S, ˆ S, a distribution P on S and a distortion metric\nd : S × ˆ S → R + . The distortion between a source string s k and a reproduction ˆ s k is given by:\nIn the stochastic setting, an (k, M k , D)-source code is speciﬁed by a surjective map φ : S k → C for some C ⊆ ˆ S k such that |C| = M k and the expected distortion is at most D, where the mean is taken with S k ∼ P k (memoryless source). The rate of the source code is deﬁned by 1 / k · log M k and asymptotically, the best possible rate for the distortion D is given by [3]:\nIn the adversarial setting, a source set F ⊆ S k is selected and then the smallest cardinality of a covering of F upto distortion D is sought; cf. [4]. Here we restrict ourselves to the case of F being the set of all source sequences that are strongly typical 1 with respect to the source distribution P .\nThe adversarial (k, M k , D) source code is deﬁned by a collection of M k points C ⊂ ˆ S k such that for any P - typical source sequence s k there exists a point ˆ s k in C such that d(s k , ˆ s k ) ≤ D. The asymptotic fundamental limit of adversarial source coding is deﬁned to be\nk log max {M k : ∃(k, M k , D) -adversarial source code } .\nNot only does this limit exist, but remarkably it coincides with R(P, D):\nAs an example, take S = ˆ S = F 2 and P is the uniform distribution, with the Hamming distortion measure. It is known that\nwhere h 2 (x) = −x log x − (1 − x) log(1 − x) is the binary entropy function. Indeed the same rate is achievable even if the source set is entire F k 2 [7].\nA channel problem is speciﬁed by input 2 and output alpha- bets X , Y, and a conditional distribution W : X → Y.\nIn the stochastic setting, an (n, M, )-channel code is speciﬁed by a pair of maps f : {1, . . . , M} → X n and g : Y n → {1, . . . , M} such that\nachievable rate is given by Shannon capacity [1]: C(W ) = max\nIn the adversarial setting, for each input sequence x n ∈ X n the channel output may be arbitrary within a subset of Y n . We choose this set to be A(x n ) ⊆ S n , the set of strongly typical sequences y n given x n with respect to W. The adversarial (n, M n ) channel code is deﬁned as a collection of M n points C ⊂ X n such that for any pair of different points x n , z n ∈ C, A(x n ) ∩ A(z n ) = ∅. The asymptotic fundamental limits of adversarial channel coding are deﬁned to be\n-adversarial channel code } \t (2) C ad (W ) = lim inf n →∞ 1 n log max {M n : ∃(n, M n )\n-adversarial channel code } . \t (3) Note that because the limits are not known to coincide for most channels of interest, we have to deﬁne both upper and lower limits.\nIt is known that C ad (W ) ≤ C(W ). Furthermore, in contrast to source coding, this inequality is known to be strict in the next example.\nThe most studied case of the adversarial channel coding is that of a binary symmetric channel with crossover probability δ, BSC(δ). Let A(n, d) be the cardinality of a largest set in F n 2 with minimal Hamming distance between any pair of elements not smaller than d. We have 3 :\nand similarly for C ad . Therefore, by the classical results on A(n, d):\nwhere the MRRW II bound [8] is R M RRW (δ) = min\nwith ˆh(x) = h 2 (1/2 − 1/2 √ 1 − x), and the Gilbert- Varshamov bound [9] is\n\u2022 Adversarial source: S, ˆ S, P S , d( ·, ·) \u2022 Adversarial channel: X , Y, W Y |X .\nAt source and channel blocklengths (k, n), a JSCC scheme is speciﬁed by:\n\u2022 an encoder map S k → X n from the source to channel input: x n = f (s k ).\n\u2022 a decoder map Y n → ˆ S k from the channel output to reconstruction: ˆ s k = g(y n ).\nWe say that a JSSC scheme is (k, n, D) adversarial if for all P -typical source sequence s k and corresponding channel outputs y n ∈ A(f(s k )), d(s k , g(y n )) ≤ D.\nThe asymptotically optimal tradeoff between the achievable distortion and the bandwidth expansion factor ρ = n k is given by\nadversarial JSCC } , \t (7) D ∗ ad (ρ) = lim inf\nadversarial JSCC } . \t (8) As in the source and channel cases, we use the stochastic\nsetting performance as a benchmark. In this setting, the source and channel are i.i.d. according to P = P S and W = W Y |X , and the requirement is for expected distortion to be at most D. It is well known that any k-to-n stochastic JSCC must satisfy [3],\nIn the asymptotic limit this can be approached, yielding the asymptotic fundamental limit:\nWe say that an (k, n) JSCC scheme is separation-based if for some space M (\u201cthe message space\u201d) the encoder consists of a source encoder f S : S k → M and a channel encoder f C : M → X n . The decoder consists of a channel decoder g C : Y n → M and a source decoder M → ˆ S k . Furthermore, following e.g. [10] we introduce a bijection σ : M → M that is applied at the encoder and reversed in the decoder, which is meant to ensure that there the mapping of source messages to channel ones is arbitrary. The encoder and decoder are thus given by\nf = f S ◦ σ ◦ f C ; g = g C ◦ σ −1 ◦ g S \t (10) where performance is required to hold for any bijection σ.\nThe asymptotic performance limits of the separation schemes are denoted as D ∗ ad,sep (ρ) and D ∗ ad,sep (ρ) and deﬁned\nin complete analogy with (7) and (8). In the stochastic setting, the asymptotic performance of the optimal separation scheme coincides with D ∗ (ρ) and thus does not need a special notation.\nWe start this section with an immediate lower bound on the fundamental limit of adversarial asymptotic distortion.\nProof: Any adversarial JSCC can be used as a usual (probabilistic) JSCC, in which case by typicality arguments it will achieve (maximal) distortion D with vanishing excess probability (namely, we assume excess distortion whenever the source or channel behavior are not strongly typical). Thus D must not be smaller than D ∗ (ρ).\nTheorem 3 (Separated schemes): If R(P, D) > ρC ad (W ) then\nWe will show shortly, that (11) demonstrates (in special cases) that D ∗ ad,sep > D ∗ ad .\nAnother special class of JSCC schemes is single-letter codes. In that case, the mappings f ( ·) and g(·) are scalar, and when applied to a block they are computed in parallel for each entry. Some examples where single-letter schemes yield the optimum D ∗ have been known for a long time, and Gastpar et al. [11] give the sufﬁcient and necessary conditions for that to hold.\nTheorem 4: If in the stochastic setting a single-letter scheme achieves some D sl , then\nWe omit a simple proof of this result, but its essence will be clear from the example in the next section.\nCorollary 5: Whenever single-letter codes are optimal in the stochastic setting, i.e., D sl = D ∗ (1) we have\nUsing Theorems 3 and 4, one may ﬁnd examples in which single-letter schemes achieve D ∗ while separation- based scheme do not, leading to the surprising conclusion that separation is not optimal in the adversarial setting.\nWe now combine the binary examples presented in sections II-A and II-B: the source is binary symmetric with Ham- ming distortion, and the channel is BSC(δ). The information- theoretic optimum D ∗ (ρ) is given by the solution D to:\nwhere again the bounds are zero for r.h.s. above one. Since R M RRW < 1 − h 2 (δ) for all δ > 0, it follows that D ∗ ad,sep (ρ) > D ∗ (ρ) strictly whenever ρR M RRW (δ) < 1.\nFor ρ = 1 the optimum D ∗ (1) is achievable by a trivial single-letter scheme (namely, the identity encoder and de- coder). Therefore, for ρ = 1 and any δ > 0,\nProposition 6: For any positive integer ρ, repetition coding (i.e., x n is constructed by ρ repetitions of s k ) achieves asymptotically:\nBy (4) and Theorem 3, it is easy to see that D ∗ ad,sep (ρ) = D ∗ ad,sep (ρ) = 1 / 2 whenever δ = 1 / 4 . Thus, comparing with (14) and by continuity for any positive integer ρ there is an interval of δ for which simple repetition coding outperforms any separation-based scheme.\nIn this section we slightly change the problem deﬁnition, in order to make it closer in spirit to that of traditional approach taken in the coding-theoretic literature for the Hamming space. Namely, we drop the strong typicality constraints on the source and the channel. Instead, we let the source outputs be any binary sequences in F k 2 , while the (adversarial) channel is allowed to ﬂip up to δn bits.\nDeﬁnition 1: A (k, n, D) adversarial JSSC code for the BSSC(δ) is a pair of maps f : F k 2 → F n 2 , g : F n 2 → F k 2 such that\nNote that while in channel coding the two deﬁnitions lead to similar results (recall Footnote 3), it is not clear whether the same holds for JSCC. For example, in Proposition 6, for even ρ the decoding relies on the fact that the adversary must ﬂip approximately δn bits, and if this assumption does not hold, repetition with even expansion ρ is equivalent to repetition with expansion ρ − 1 followed by channel uses that can be ignored.\nNote that by Theorem 2, we have that any asymptotically achievable distortion D over BSSC(δ) satisﬁes\nIn fact, if there exists a JSCC that achieves distortion D, then any ball of radius δn in F n 2 must not contain more than T k Dk codewords, where T m r is the volume of a ball of radius r in F m 2 . However there exists a ball of radius δn in F n 2 that contains at least 2 k −n T n δn codewords. Hence D must satisfy\nAsymptotically (16) coincides with (15), but otherwise is tighter.\nThe above lower bound on achievable distortion D can be improved for a region of δ if we consider the fact that any JSCC also gives rise to an error-correcting code. Recalling the cardinality A(n, δ) deﬁned in Section II-B, we have the following.\nTheorem 7: If a k-to-n JSCC achieves the distortion D over BSSC(δ), then\nProof: Suppose there is a code D ⊂ F k 2 that corrects up to any Dk errors. Let ˆ D be the image of this code in F n 2 under the JSCC encoding. We claim that ˆ D is a code in F n 2 that corrects any up to δn errors. Indeed, up to δn errors can be reduced to at most Dk errors in F k 2 with the JSCC decoding. These errors are then correctable with the decoding of D.\nCorollary 8: For the BSSC(δ) the distortion D ∗ ad (ρ) satis- ﬁes:\nAs explained in Footnote 3, the limits for channel coding are the same for strongly typical channel and for maximum number of ﬂips. Thus, by Theorem 3, the asymptotic perfor- mance of the separation schemes must satisfy\nRemark: Note that, although the exact value of C ad or C ad is unknown, the argument in Theorem 7 demonstrates that in the regime of distortion D → 0, separation yields an optimal (but unknown) performance.\nJust as in Section III-C it is clear that in the case ρ = 1 separation is strictly suboptimal for all δ > 0. Comparison of the different bounds for this case is shown in Fig. 1. Next, we show examples of codes that beat separation for other ρ = 1.\nLet B n (x, r) denote a ball of radius r centered at x in F n 2 . For any set S ∈ F n 2 , the radius of the set rad( S) is deﬁned to be the smallest r such that S ⊆ B n (x, r) for some x ∈ F n 2 , with the optimal x\u2019s called the Chebyshev center(s) of S.\nConsider some JSSC encoder f : F k 2 → F n 2 for the BSSC(δ). There exists a decoder achieving distortion D for this if and only if\nIn other words, the distortion achievable by the encoder f is given by\nIn contrast to channel coding, repetition of a single code of small block length leads to a non-trivial asymptotic perfor- mance.\nFix an arbitrary encoder given by the mapping f : F u 2 → F v 2 . If there are t errors in the block of length v, t = 0, . . . , v the performance of the optimal decoder (knowing t) is given by\nConsider also an arbitrary decoder g : F v 2 → F u 2 and its performance curve:\nand the decoder g achieving this bound with equality is called a universal decoder. Some trivial properties: r 0 (0) = 0 if and only if f is injective, r g (0) = 0 if and only if g is a left inverse of f , r 0 (v) = r g (v) = u.\nExample: Any repetition code F 2 → F v 2 is universally decodable with a majority-vote decoder g (resolving ties arbitrarily):\nFrom a given code f we may construct a longer code by repetition to obtain an F k 2 → F n 2 code as follows, where Lu = k, Lv = n:\nThis yields a sequence of codes with ρ = n / k = v / u . We want to ﬁnd out the achieved distortion D(δ) as a function of the maximum crossover portion δ of the adversarial channel.\nTheorem 9: The asymptotic distortion achievable by f L (repetition construction) satisﬁes\nwhere r ∗∗ 0 and r ∗∗ g are upper convex envelopes of r 0 and r g respectively.\nExample: Repetition code: Consider using a [v, 1, v] repeti- tion code. Since for such a code r g (t) = r 0 (t), the upper and lower bounds of Theorem 9 coincide. For odd v we have:\n(Compare this with Proposition 6 for the strong-typicality model of Section II-C.) In Fig. 2 the performance of the\n3-repetition code is contrasted with that of the separation schemes. In the same plot the converse bounds (17) and (15) are plotted. For δ > 0.23 it is clear that 3-repetition achieves better performance than any separation scheme.\nExample: [5,2,3] linear code for ρ = 5/2: Consider the linear map f : F 2 2 → F 5 2 given by the generator matrix\nIt can be shown that r 0 (t) = {0, 0, 1, 2, 2, 2} for t = {0, 1, 2, 3, 4, 5} and there exists a universal decoder g. Thus by Theorem 9 this code achieves D = 5δ / 3 . For δ > 0.22, this is better than what any separation scheme can achieve. This example demonstrates that in the JSSC setup one should not always use a simple decoder that maps to the closest codeword. In fact, further analysis demonstrates that perfect codes, Golay and Hamming, are among the worst in terms of distortion tradeoff.\nRemark: Note that there exist [12] linear codes of rate ρ −1 decodable with ﬁnite list size and capable of correcting all errors up to the information theoretic limit n h 2 −1 (1 − ρ −1 ). However, by the converse bound (17) it follows that the radius of the list in F k 2 must be Ω(k) regardless of the map between F k 2 and the codewords. This provides some interesting complement to the study of the properties of lists of codes achieving the information theoretic limit [13], [14]."},"refs":[{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[{"name":"W. Hamming"}],"title":{"text":"R"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"Coding theorems for a discrete source with a ﬁdelity criterion"}},{"authors":[{"name":"A. N. Kolmogoro"},{"name":"V. M. Tikhomirov"}],"title":{"text":"Theory of transmission of information"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orner"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems"}},{"authors":[{"name":"T. Berger"}],"title":{"text":"Rate-Distortion Theory: A Mathematical Basis for Data Compression"}},{"authors":[{"name":"G. Cohe"},{"name":"I. Honkal"},{"name":"S. Litsy"},{"name":"A. Lobstein"}],"title":{"text":"Covering Codes"}},{"authors":[{"name":"R. McEliec"},{"name":"E. Rodemic"},{"name":"H. Rumse"},{"name":"L. Welch"}],"title":{"text":"New upper bounds on the rate of a code via the Delsarte-MacWilliams inequalities"}},{"authors":[{"name":"F. J. MacWilliam"},{"name":"N. J. A. Sloane"}],"title":{"text":"The Theory of Error-Correcting Codes"}},{"authors":[{"name":"D. Wan"},{"name":"A. Ingbe"},{"name":"Y. Kochman"}],"title":{"text":"The dispersion of joint source- channel coding"}},{"authors":[{"name":"M. Gastpa"},{"name":"B. Rimold"},{"name":"M. Vetterli"}],"title":{"text":"To code, or not to code: Lossy source-channel communication revisited"}},{"authors":[{"name":"P. Elias"}],"title":{"text":"Error-correcting codes for list decoding"}},{"authors":[{"name":"V. M. Blinovsky"}],"title":{"text":"Bounds for codes in the case of list decoding of ﬁnite volume"}},{"authors":[{"name":"V. Guruswam"},{"name":"S. Vadhan"}],"title":{"text":"A lower bound on list size for list decoding"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565571.pdf"},"links":[{"id":"1569566725","weight":3},{"id":"1569565663","weight":3},{"id":"1569564669","weight":3},{"id":"1569559617","weight":3},{"id":"1569566981","weight":6},{"id":"1569566683","weight":3},{"id":"1569566597","weight":3},{"id":"1569565551","weight":3},{"id":"1569565711","weight":6},{"id":"1569566571","weight":3},{"id":"1569552245","weight":10},{"id":"1569559967","weight":3},{"id":"1569564481","weight":6},{"id":"1569566415","weight":6},{"id":"1569564805","weight":3},{"id":"1569566469","weight":3},{"id":"1569565355","weight":3},{"id":"1569564469","weight":3},{"id":"1569565931","weight":6},{"id":"1569565775","weight":6},{"id":"1569564227","weight":3},{"id":"1569560427","weight":3},{"id":"1569559541","weight":3},{"id":"1569566941","weight":3},{"id":"1569566739","weight":6},{"id":"1569565609","weight":10},{"id":"1569565291","weight":3},{"id":"1569566821","weight":3},{"id":"1569556713","weight":3},{"id":"1569566467","weight":6},{"id":"1569566903","weight":3},{"id":"1569565859","weight":3},{"id":"1569566843","weight":3},{"id":"1569564387","weight":3},{"id":"1569565455","weight":3},{"id":"1569566523","weight":3},{"id":"1569565953","weight":3},{"id":"1569566269","weight":3},{"id":"1569566985","weight":3},{"id":"1569567009","weight":6},{"id":"1569566865","weight":3},{"id":"1569566095","weight":23},{"id":"1569566239","weight":3},{"id":"1569563981","weight":3},{"id":"1569559565","weight":3},{"id":"1569565213","weight":3},{"id":"1569566369","weight":6},{"id":"1569566531","weight":3},{"id":"1569567665","weight":3},{"id":"1569561143","weight":6},{"id":"1569561795","weight":3},{"id":"1569566437","weight":6},{"id":"1569566851","weight":3},{"id":"1569553909","weight":3},{"id":"1569565915","weight":3},{"id":"1569552251","weight":6},{"id":"1569554881","weight":3},{"id":"1569566909","weight":6},{"id":"1569565151","weight":3},{"id":"1569566257","weight":3},{"id":"1569557083","weight":3},{"id":"1569565055","weight":6},{"id":"1569555879","weight":3},{"id":"1569565219","weight":3},{"id":"1569558401","weight":6},{"id":"1569566553","weight":3},{"id":"1569565357","weight":3},{"id":"1569565393","weight":6},{"id":"1569566603","weight":6},{"id":"1569566673","weight":3},{"id":"1569566233","weight":3},{"id":"1569560997","weight":3},{"id":"1569566407","weight":3},{"id":"1569566275","weight":3},{"id":"1569560503","weight":3},{"id":"1569565463","weight":3},{"id":"1569565439","weight":6},{"id":"1569566229","weight":3},{"id":"1569562551","weight":3},{"id":"1569563395","weight":3},{"id":"1569566901","weight":3},{"id":"1569551347","weight":3},{"id":"1569565415","weight":3},{"id":"1569566631","weight":3},{"id":"1569557633","weight":3},{"id":"1569564411","weight":3},{"id":"1569566805","weight":3},{"id":"1569565665","weight":3},{"id":"1569566779","weight":6},{"id":"1569556361","weight":3},{"id":"1569565397","weight":3},{"id":"1569566873","weight":3},{"id":"1569565765","weight":3},{"id":"1569565435","weight":3},{"id":"1569557275","weight":3},{"id":"1569565919","weight":6},{"id":"1569566267","weight":10},{"id":"1569566253","weight":6},{"id":"1569564283","weight":3},{"id":"1569564291","weight":3},{"id":"1569566823","weight":3},{"id":"1569566813","weight":3},{"id":"1569563975","weight":3},{"id":"1569556759","weight":3},{"id":"1569566619","weight":3},{"id":"1569565271","weight":3},{"id":"1569561185","weight":3},{"id":"1569566397","weight":3},{"id":"1569558779","weight":3},{"id":"1569566817","weight":3},{"id":"1569564923","weight":3},{"id":"1569566299","weight":3},{"id":"1569565805","weight":3},{"id":"1569563919","weight":3},{"id":"1569566577","weight":3},{"id":"1569557851","weight":3},{"id":"1569565389","weight":6},{"id":"1569565537","weight":3},{"id":"1569566057","weight":3},{"id":"1569560213","weight":3},{"id":"1569565853","weight":3},{"id":"1569550425","weight":3},{"id":"1569566273","weight":3},{"id":"1569565889","weight":3},{"id":"1569565165","weight":3},{"id":"1569561397","weight":3},{"id":"1569566797","weight":3},{"id":"1569565113","weight":3},{"id":"1569566375","weight":10},{"id":"1569564257","weight":3},{"id":"1569566555","weight":3},{"id":"1569564141","weight":6},{"id":"1569564509","weight":3},{"id":"1569565139","weight":3},{"id":"1569566067","weight":3},{"id":"1569566615","weight":3},{"id":"1569566113","weight":3},{"id":"1569566443","weight":3},{"id":"1569560581","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T4.1","endtime":"11:50","authors":"Yuval Kochman, Arya Mazumdar, Yury Polyanskiy","date":"1341487800000","papertitle":"The Adversarial Joint Source-Channel Problem","starttime":"11:30","session":"S12.T4: Classical and Adversarial Joint Source-Channel Coding","room":"Stratton 20 Chimneys (306)","paperid":"1569565571"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
