{"id":"1569565421","paper":{"title":{"text":"Bounds on estimated Markov orders of individual sequences ∗"},"authors":[{"name":"´ Alvaro Mart´ın"}],"abstr":{"text":"Abstract\u2014We study the maximal values estimated by com- monly used Markov model order estimators on individual se- quences. We start with penalized maximum likelihood (PML) estimators with cost functions of the form − log ˆ P k (x n )+f (n)α k , where ˆ P k (x n ) is the ML probability of the input sequence x n under a Markov model of order k, α is the size of the input alphabet, and f (n) is an increasing (penalization) function of n (the popular BIC estimator corresponds to f (n) = α−1 2 log n). Comparison with a memoryless model yields a known upper bound k(n) on the maximum order that x n can estimate. We show that, under mild conditions on f that are satisﬁed by commonly used penalization functions, this simple bound is not far from tight, in the following sense: for sufﬁciently large n, and any k<k(n), there are sequences x n that estimate order k; moreover, for all but a vanishing fraction of the values of n such that k = k(n), there are sequences x n that estimate order k. We also study KT-based MDL Markov order estimators, and show that in this case, there are sequences x n that estimate order n 1/2− , which is much larger than the maximum log n/ log α(1 + o(1)) attainable by BIC, or the order o(log n) required for consistency of the KT estimator. In fact, for these sequences, limiting the allowed estimated order might incur in a signiﬁcant asymptotic penalty in description length. All the results are constructive, and in each case we exhibit explicit sequences that attain the claimed estimated orders."},"body":{"text":"Initially, we consider penalized maximum likelihood (PML) Markov model order estimators, where, given a sequence x n over a ﬁnite alphabet A, of size α = |A|, and a candidate Markov order k, we deﬁne a cost 1\nHere, ˆ P k (x n ) is the maximum likelihood (ML) probability of x n under a kth order Markov model (with appropriate conventions on the initial states), and f (n) is a positive penalization function satisfying some mild conditions to be detailed later. The order estimated for x n is\nDifferent variants of PML estimators have been extensively studied (see, e.g., [1] and citations therein). When f (n) =\nis usually regarded as an asymptotic approximation of a Minimum Description Length (MDL) estimator of the Markov order. We will also be interested in the latter type of order estimators, and, in particular, in the variant based on the Krichevskii-Troﬁmov (KT) probability assignment [2]. The cost function in this case does not include an explicit penal- ization term; instead, the contribution of the model size to the cost is amortized across actual occurrences of model states in the sequence under evaluation.\nThe range of values of the estimate ˆ k has played an important role in the theoretical analysis of the above men- tioned estimators. The ﬁrst consistency results for the BIC estimator [3], for example, assumed a known bound on the Markov order. This assumption was removed in [1], where it is also shown that, if no bound is assumed, pure MDL Markov order estimators, be it in the KT or the normalized maximum likelihood (NML) versions, are not consistent. The consistency of the latter two was shown in [4], when the range of the Markov order k for the minimization in the estimation is bounded by o(log n) and c log n, respectively, with c < 1/ log α. Similarly, in the case of the estimation of context trees [5], the consistency of BIC and (KT-based) MDL estimators was proved in [6], under the assumption of an upper bound of o(log n) on the depth of the candidate context trees considered for the minimization.\nImposing a-priori bounds on the estimated order may be useful in some cases to guarantee consistency, but might not be desirable in other applications. For example, in universal lossless data compression, we are interested in choosing the estimated order that yields the shortest description length for the given input sequence, regardless of where the input originated from. Similarly, in the universal simulation results of [7], given a sequence x n , a Markov order ˆ k is estimated using a PML estimator, and a \u201csimulated\u201d sequence y n is obtained by drawing uniformly from the set of sequences in the ˆ kth order Markov type class of x n that also estimate order ˆ k. No assumptions are made on the range of ˆ k, and, in the individual sequence setting, the choice of penalization function f (n) governs a trade-off between the statistical similarity of y n to x n , and the \u201crichness\u201d (entropy) of the space from which y n is drawn. On the other hand, aside from the theoretical interest, obtaining inherent bounds on the possible outcome of the\nestimation procedure has practical computational implications, as a bound on the estimated order translates to a bound on the memory requirements of an algorithmic implementation of the estimator. Thus, in this paper, we study the maximum possible value of ˆ k(x n ), for any sequence x n , when no a-priori bounds are imposed on the candidate orders.\nFor k = ˆ k(x), writing C 0 (x n ) ≥ C k (x n ), trivially bounding the ML probability, and rearranging terms, yields\nfrom which a uniform upper bound (log n− log f (n))/ log α+ O(1) on ˆ k(x n ), which we denote by k(n), was obtained in [7]. We will show that, under mild conditions on f (n), which are satisﬁed by commonly used penalization functions, the bound k(n) is not far from tight, in the following sense: for any sufﬁciently large n, and any k < k(n), there are sequences of length n that estimate order k; moreover, for all sufﬁciently large k and all but a vanishing fraction of the values of n such that k = k(n), there are sequences of length n that estimate order k. After some preliminaries in Section II, these results are presented in Section III, by showing explicit constructions of sequences that attain the claimed estimated orders. The constructions rely on properties of de Bruijn sequences [8]. We initially present results for arbitrary values of α, and then show that these results can be tightened in the case α = 2 by exploiting properties of a special kind of binary de Bruijn sequence, the so-called Ford sequence [9]. In Section IV, we extend our study to the MDL estimator based on the KT probability assignment. We show that in this case, there exist sequences x n that estimate order n 1/2− for any ∈ (0, 1 2 ). This order is much larger than the maximum possible order, k(n) = log n/ log α + o(log n), attainable by a BIC estimator, and also of the order o(log n) required for consistency of the KT-based estimator [4]. In fact, we show that, in a universal lossless compression setting, for the constructed sequences, imposing an artiﬁcial upper-bound on the allowed estimated order could incur a signiﬁcant asymptotic penalty in overall description length. Similar results (with the same individual sequences) are obtained when a context tree [5], [6], rather than a plain Markov order, is estimated, using either the KT or the (tree) BIC estimator.\nWe denote by u j i the string u i u i+1 . . . u j over A, with u j i = λ, the empty string, when i > j. We omit the subscript when i = 1. We let |u| denote the length of a string u, and uv the concatenation of strings u and v. The terms string and sequence are used interchangeably.\nWe model a sequence x n as the realization of a generic kth order Markov process, where k is unknown. We regard a string s ∈ A k as a state of the Markov process and we say that a sequence y selects state s whenever s is a sufﬁx of y. When k is not clear from the context, we explicitly refer to s as a k-state. For the purpose of selecting states, we assume\nthat x n is preceded by an arbitrary ﬁxed semi-inﬁnite string x 0 −∞ . This convention uniquely determines a state selected by x i , for each i, 0 ≤ i ≤ n, and for any order k. If x i selects state s, 0 ≤ i < n, we say that x i+1 is emitted in state s and that s occurs (in position i) in x n . We denote by n s (x n ) the number of occurrences of s in x n , and, for a ∈ A, we denote by n (a) s (x n ) the number of times a symbol x i = a is emitted in state s. We omit the dependence on x n of n (a) s , n s , and other notations, when clear from the context.\nThe kth order ML probability of a sequence x n is deter- mined by the ﬁxed initial state and the empirical probabilities ˆ P k (·|s) conditioned on k-states s,\nk , a ∈ A , \t (4) so that\nThe class of PML estimators of interest is deﬁned by (1)\u2013(2), where we assume that f (n) is positive and nondecreasing, with f (n) n −→∞ and f (n) n n −→ 0. 2 We refer to the ﬁrst and second terms on the right-hand side of (1), respectively, as the ML term (speciﬁed in (5)) and the penalty term of order k.\nThe upper bound k(n) on ˆ k is deﬁned as the largest value of k satisfying (3) for a given n. Reciprocally, given k, the smallest integer n satisfying (3), denoted n(k), is a lower bound on the length of sequences that can estimate order k. In particular, from the deﬁnition of n(k), we have\nlog α f (n(k) − 1) + 1 . (6) The following lemma follows readily from the foregoing\nLemma 1. Given a value of n, the inequality (3) holds for all k, 0 ≤ k ≤ k(n). We have k(n) n −→∞, n(k) k −→∞, n(k) is nondecreasing, and, moreover, for sufﬁciently large k,\nlog α f (n(k + 1)) . (7)\nIn this section we exhibit sequences of length n that get very close to, or even precisely attain, the bound k(n) of the previous section. The constructions will be based on de Bruijn sequences, whose properties we review next.\nA kth order de Bruijn sequence [8] is a sequence b α k , of length α k , k ≥ 0, such that the sliding window b i+1 b i+2 . . . b i+k , with indices taken modulo α k , exhausts all distinct k-tuples over A. De Bruijn sequences exist for every order k, and every cyclic permutation of a de Bruijn sequence is itself a de Bruijn sequence of the same order. We denote by B k the (nonempty) set of de Bruijn sequences of order k that have x 0 −k+1 as a sufﬁx (i.e., they match, cyclically, the\nassumed ﬁxed initial condition). For a sequence u, we denote by (u) ∗ the concatenation of an inﬁnite number of copies of u, and, when |u| ≥ n, we denote by [ u ] n the truncation of u to length n. Let B n k denote the set of sequences\nB n k = (b α k ) ∗ n b α k ∈ B k . \t (8) The following lemma follows immediately from the deﬁni- tion (8) and the properties of de Bruijn sequences.\nLemma 2. Let x n ∈ B n k . If k ≥ k, then a k -state, when it occurs in x n , always emits the same symbol. In particular, when n = mα k for some integer m ≥ 0, then\n(i) each k-state s occurs m times in x n , and we have m = n s = n (a) s for some a∈A (which depends on s);\n(ii) if j < k, each possible j-state s occurs mα k−j times in x n and each symbol of A is emitted mα k−j−1 times in s, i.e., n (a) s = mα k−j−1 and n s = mα k−j for all a ∈ A and all s ∈ A j .\nTheorem 1. For sufﬁciently large n, if k < k(n) and x n ∈ B n k , then ˆ k(x n ) = k.\nProof: By Lemma 2, if k ≥ k, then the ML term of order k of x n is zero. Thus, since the penalty term grows with the order, we must have ˆ k(x n ) ≤ k. Let m = n α k . If j < k, then by Lemma 2 (ii), we have, for all a ∈ A and all s ∈ A j , n (a) s (x n ) ≤ (m + 1)α k−j−1 and n s (x n ) ≥ mα k−j , where at least one inequality is strict, which implies that n (a) s n\n(9) Using (9) and recalling that − log ˆ P k (x n ) = 0, we obtain, for 0 ≤ j < k,\nα ≥ n log mα m + 1 − n log α α\nwhere the third inequality follows from the ﬁrst claim of Lemma 1 and the fact that k+1 ≤ k(n), and the last inequality holds since α ≥ 2. It follows from (10) that C j (x n ) > C k (x n ) when m ≥ 3. The latter condition, in turn, holds for all sufﬁciently large n, since, by (3), and with k > 0, we have n/α k ≥ (1 − α −k )f (n)/ log α ≥ f (n)/(2 log α), which is unbounded by our assumptions on f .\nTheorem 1 shows that for sufﬁciently large n, we can construct sequences that estimate any order k up to k(n)−1. We next show that, with additional mild assumptions on f (n), for most values of n we can construct sequences that estimate precisely order k(n). We say that the function f is nice if it is deﬁned over the positive reals, f is concave and differentiable\nzf (z) < f (z) − α 2 for all z ∈ (˜ z, ∞) . \t (11) It is readily veriﬁed that commonly used penalization functions are nice. In particular, this includes functions of the form f (n) = c log n, f (n) = c log log n, and f (n) = cn β for positive constants c and β < 1. The following lemma is an immediate consequence of (11).\nLemma 3. If f is nice, then n/f (n) is strictly increasing with n in (˜ z, ∞).\nIn the sequel, for a real number z and a positive integer N , we write z N as shorthand for N z/N , i.e., the smallest multiple of N that is not smaller than z.\nTheorem 2. Assume f is nice. Then, for sufﬁciently large k, if n > n(k) α k and x n ∈ B n k , then ˆ k(x n ) = k.\nRemark: To interpret Theorem 2, we observe that for a given value of k, by Lemma 3, the set of integers n such that k(n) = k is given by the range n(k) ≤ n < n(k + 1). The fraction of values of n in this range for which the theorem does not provide a sequence of length n that estimates order k(n) is upper-bounded by\nwhere the inequality follows from the leftmost inequality in (7), and the limit follows from the unboundedness of n(k) and f (n). Thus, Theorem 2 guarantees that for all but a vanishing fraction of values of n such that k(n) = k, there are sequences of length n that estimate order k.\nTo prove Theorem 2, we rely on a series of lemmas. Lemma 4 below follows immediately from [4, Lemma 4]. For a probability vector P = (p 1 , . . . , p α ), we denote by H(P ) = − α i=1 p i log p i the entropy of P .\nLemma 4. If P = (p 1 , . . . , p α ) is a probability vector satisfying 1 2α ≤ p i ≤ 2 α for all i, 1 ≤ i ≤ α, then\np i − 1 α\nProof: Since − log ˆ P j (x n ) is non-increasing with j, it sufﬁces to consider j = k−1. For a (k−1)-state s, let R s be the set of symbols of A that are emitted in state s in x n mα k +1 , the truncated (possibly empty) copy of a sequence from B k at the end of x n . Let r s = |R s |, and deﬁne T = { s ∈ A k−1 | r s > 0 }. Clearly, s r s = r, and |T | ≤ r. By Lemma 2 (ii), we have n (a) s = m + 1 if a ∈ R s and n (a) s = m otherwise, so that n s = mα+r s . Thus, with m ≥ 1, we have, for all a ∈ A,\nand Lemma 4 applied to ˆ P s = ˆ P k−1 (·|s) yields, together with some algebraic manipulations,\nNow, writing the ML term of order k−1 in terms of state- conditioned empirical entropies, and applying (13), we obtain\nn s (α − r s )r s (mα + r s ) 2\n(α − r s )r s (mα + r s )\nwhere we recall that r s = 0 for s∈T , and, for the last equality, that n s = mα + r s . We claim that g(r s ) ∆ = (α−r s )r s mα+r\nis upper- bounded by α 4m for all s, which, by (14), would sufﬁce to prove (12). Indeed, elementary analysis of the function g(ρ) for ρ ≥ 0 reveals that it has a global maximum at ρ ∗ = α( m(m + 1) − m), with\nProof of Theorem 2: Let k be large enough so that n(k) ≥ ˜ z. By Lemma 3, n and k satisfy (3) for all n ≥ n(k). Now, for x n ∈ B n k , with n > n(k) α k , Lemma 5 and (1) yield\nwith m and r as deﬁned in the lemma. Thus, for 0 ≤ j < k, recalling that − log ˆ P k (x n ) = 0, we have,\nWrite µ = mα k = n − r. Since µ ≥ n(k) α k , µ and k satisfy (3) with µ in the role of n, i.e., we have α k − 1 ≤\nrα 4m\n= − f (n) − f (µ) f (µ)\nµ log α − rα 4m\nwhere the last inequality follows from the fact that f is concave in (˜ z, ∞) and µ ≥ n(k) α k ≥ ˜ z. Now, since n and k satisfy (3), for k > 0 we have m ≥ f (n)/(2 log α), so, recalling also the monotonicity of f , it follows from (16) that\nSince µ≥˜ z, by (11), the right-hand side of (17) is positive if r > 0. If r = 0, since n > n(k) α k ≥ n(k), by (3) and Lemma 3, the right-hand side of (15) is positive. Hence, C j (x n )>C k (x n ) for j<k, and, thus, ˆ k(x n )≥k. Furthermore, since − log ˆ P k (x n )=0 and the penalty term increases with k, we must have, in fact, ˆ k(x n ) = k.\nLet n(k) denote the least integer n in the interval n(k) ≤ n < n(k+1) such that for all n ≥ n(k) in that interval, there are sequences of length n that estimate order k. By Theorem 2, we have n(k) − n(k) ≤ α k . We next show that, in the special case α = 2, we can exploit known properties of special binary de Bruijn sequences to reduce this gap to n(k)−n(k) = o(2 k ). Speciﬁcally, the Ford sequence of order k ≥ 0, which will be denoted F k = a 2 k 1 , is constructed as follows: start with a k−1 1 = 0 k−1 , and extend the sequence using the least-ﬁrst greedy algorithm [9], where, for k < i ≤ 2 k , given s = a i−1 i−k+1 , we set a i = 0 if n s = 0 and a i = 1 otherwise (i.e., of the sibling k-tuples s0 and s1 we always choose s0 ﬁrst). It is readily veriﬁed that a de Bruijn sequence of order k is indeed constructed this way, and that the sequence is lexicographically ﬁrst among all binary de Bruijn sequence of order k. We denote by F n k the sequence [(F k ) ∗ ] n .\nThe following lemma is an immediate consequence of [10, Theorem 1] (n 0 and n 1 are interpreted as special cases of n s ). Lemma 6. Let x n = F n k . We have n 0 (x j ) − n 1 (x j ) = O 2 k log k k \t for all j, 1 ≤ j ≤ n.\nTheorem 3. Assume f is nice and x n = F n k . For sufﬁciently large k and a well-characterized function g(k) = O(1), if\nk , \t (18) then ˆ k(x n ) = k.\nRemark: It will turn out in the proof of Theorem 3 (given in the full paper) that for some penalization functions of interest we have, in fact, g(k) = o(1). In particular, if f (z) = c log z with c > 0, the second term on the right-hand side of (18) is O 2 k log k k 2 , whereas for f (z) = cz β , with 0 < β < 1, it is\nIn this section, we consider the MDL Markov order esti- mator based on the KT probability assignment. We construct sequences x n that estimate orders that are much larger than those attainable by a BIC estimator, or than the bound on the order required for consistency of the KT estimator. For nota- tional simplicity, we focus on the case α = 2 (A = {0, 1}).\nThe KT probability [2] of order 0 of a binary sequence x n is deﬁned as KT 0 (λ) = 1 (for n = 0), and\nΓ(n + 1) \t , n > 0, (19) where Γ is the Gamma function. The KT probability of order k ≥ 0, in turn, is deﬁned as\nwhere x n [s] denotes the subsequence of symbols from x n that occur in state s. Using this distribution, one can construct a lossless description of x n of length\nC KT ,k (x n ) = − log KT k (x n ) + c(k) , \t (21) where c(k)=O(log k) is the (non-decreasing) length of an efﬁcient encoding of k, and we ignore integer constraints on code lengths. The estimated Markov order for a sequence x n is the value of k that yields the shortest description, namely,\nWe will make use of the following relation, which follows from Stirling\u2019s approximation (see, e.g., [11]). For every sequence x n , we have\nDeﬁne the sequence U n k = [(10 k ) ∗ ] n , where 10 k is a string consisting of a 1 followed by k 0\u2019s. For simplicity, we assume that the sequence x 0 −∞ used to determine initial states is all 0\u2019s, and that n is a multiple of k + 1. These constraints can be easily removed.\nTheorem 4. Let n = (k+1)m for some m ≥ 1, and x n = U n k . If k and n satisfy\n≥ 1 2\n+ ν + (2k+1)(k+1)γ n\nRemark. It is readily veriﬁed that, for sufﬁciently large n, the condition (24) is satisﬁed by values of k as large as k = n 1/2−\nfor any ∈(0, 1 2 ). Notice that, since the KT cost penalizes only states that do occur in x n , the estimated order for the sequence x n =U n k would be the same if the model under estimation was a context tree [5]. In fact, it will be shown in the full paper that a similar result (with the same sequence x n ) holds also for context trees under the BIC estimator.\nProof of Theorem 4 (outline): For 0 ≤ j ≤ k, x n contains occurrences of exactly j + 1 states, namely 0 j and 0 10 j−1− , 0 ≤ \t < j. When j = k, each such state occurs exactly m times, always followed by the same symbol. Thus, the conditional distribution for each occurring k-state is deterministic, and, from (20) and (23), we obtain\nWhen j < k, states of the form 0 10 j−1− occur m times each, always followed by a 0. The state 0 j occurs (k −j +1)m times, m of them followed by a 1, and the rest followed by a 0. From (20) and (23), writing δ = k−j+1, and denoting the binary entropy function by h(·), we obtain\nwhere the second inequality follows by applying the uniform bound h(p)≥4p(1 − p), recalling that 0≤j<k, and dropping some nonnegative terms. Now, by (21), (25), and (26), recall- ing that m = n/(k + 1) and c(j) > 0, we obtain\n−(δ−1)ν − (2k+1)(k+1)γ n\nRecalling that 1 < δ ≤ k+1, and factoring out δ−1, we verify that, for 0 ≤ j < k, we have C KT ,j (x n ) > C KT ,k (x n ) whenever (24) holds. It is readily shown in the full paper that C KT ,j (x n ) > C KT ,k (x n ) also when j > k. Therefore, under the conditions of the theorem, we have ˆ k KT (x n ) = k.\nRemark. Consider the case where an upper bound K(n) = o(n 1/2− ) is imposed on the allowed estimated order of the KT-based MDL estimator (or the tree BIC estimator). Choose k = (1 + ξ)K(n) for some ξ > 0, and x n = U n k . By (25), recalling that m = n/(k + 1), we obtain C KT ,k (x n ) = O(K(n) log n) for this choice of k. On the other hand, if j ≤ K(n), then, by (26), we have C KT ,j (x n ) = Ω(n/K(n)), and, hence C KT ,k (x n )/C KT ,j (x n ) n −→ 0. We conclude that limiting the allowed estimated order as assumed incurs a signiﬁcant asymptotic penalty in the description length of the individual sequence x n . The gap is more pronounced the smaller the upper bound K(n) is. In particular, with K(n) = O(log n) (as required for consistency of the MDL or tree estimator [4]), we obtain a code length Ω(n/ log n) with the restricted estimator, as compared to O(log 2 n) with an unrestricted one."},"refs":[{"authors":[{"name":"I. Csisz´ar"},{"name":"P. C. Shields"}],"title":{"text":"The consistency of the BIC Markov order estimator."}},{"authors":[{"name":"R. E. Krichevskii"},{"name":"V. K. Troﬁmov"}],"title":{"text":"The performance of universal encoding"}},{"authors":[{"name":"L. Finesso"}],"title":{"text":"Estimation of the order of a ﬁnite markov chain"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"Large-scale typicality of Markov sample paths and con- sistency of MDL order estimators."}},{"authors":[{"name":"J. Rissanen"}],"title":{"text":"A universal data compression system"}},{"authors":[{"name":"I. Csisz´ar"},{"name":"Z. Talata"}],"title":{"text":"Context tree estimation for not necessarily ﬁnite memory processes, via BIC and MDL"}},{"authors":[{"name":"A. Mart´ın"},{"name":"N. Merhav"},{"name":"G. Seroussi"},{"name":"M. J. Weinberger"}],"title":{"text":"Twice- universal simulation of Markov sources and individual sequences"}},{"authors":[{"name":"N. G. de Bruijn"}],"title":{"text":"A combinatorial problem"}},{"authors":[{"name":"H. Fredricksen"}],"title":{"text":"A survey of full length nonlinear shift register cycle algorithms"}},{"authors":[],"title":{"text":"The discrepancy of the lex-least de Bruijn sequence"}},{"authors":[{"name":"N. Cesa-Bianch"},{"name":"G. Lugos"}],"title":{"text":"Prediction, learning, and games"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565421.pdf"},"links":[{"id":"1569566527","weight":18},{"id":"1569566725","weight":6},{"id":"1569559259","weight":6},{"id":"1569566597","weight":6},{"id":"1569552245","weight":6},{"id":"1569565931","weight":6},{"id":"1569551535","weight":12},{"id":"1569564731","weight":6},{"id":"1569564401","weight":12},{"id":"1569565291","weight":6},{"id":"1569566579","weight":37},{"id":"1569566095","weight":6},{"id":"1569561085","weight":6},{"id":"1569558681","weight":6},{"id":"1569567665","weight":6},{"id":"1569561795","weight":6},{"id":"1569566437","weight":6},{"id":"1569566939","weight":25},{"id":"1569553537","weight":6},{"id":"1569553519","weight":6},{"id":"1569554881","weight":6},{"id":"1569565151","weight":6},{"id":"1569565055","weight":6},{"id":"1569565633","weight":6},{"id":"1569565219","weight":6},{"id":"1569566233","weight":6},{"id":"1569560997","weight":6},{"id":"1569565463","weight":6},{"id":"1569562551","weight":6},{"id":"1569565665","weight":6},{"id":"1569557275","weight":6},{"id":"1569565919","weight":6},{"id":"1569566267","weight":6},{"id":"1569565013","weight":6},{"id":"1569551905","weight":25},{"id":"1569565457","weight":6},{"id":"1569558779","weight":12},{"id":"1569565669","weight":6},{"id":"1569566933","weight":6},{"id":"1569565389","weight":12},{"id":"1569555891","weight":12},{"id":"1569565889","weight":6},{"id":"1569551539","weight":12},{"id":"1569565165","weight":6},{"id":"1569565113","weight":18},{"id":"1569564257","weight":6},{"id":"1569566555","weight":6},{"id":"1569564141","weight":6},{"id":"1569566973","weight":6},{"id":"1569565031","weight":6},{"id":"1569551541","weight":12},{"id":"1569566113","weight":6}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T1.4","endtime":"16:00","authors":"Luciana Vitale, Álvaro Martín, Gadiel Seroussi","date":"1341330000000","papertitle":"Bounds on estimated Markov orders of individual sequences","starttime":"15:40","session":"S7.T1: Lossless and Universal Source Coding","room":"Kresge Rehearsal B (030)","paperid":"1569565421"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
