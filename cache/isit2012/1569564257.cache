{"id":"1569564257","paper":{"title":{"text":"Data Processing Lower Bounds for Scalar Lossy Source Codes with Side Information at the Decoder"},"authors":[{"name":"Avraham Reani"},{"name":"Neri Merhav"}],"abstr":{"text":"Abstract\u2014In this paper, we derive lower bounds on the distortion of scalar ﬁxed-rate codes for lossy compression with side information available at the receiver. These bounds are derived by presenting the relevant random variables as a Markov chain and applying generalized data processing inequalities a la Ziv and Zakai."},"body":{"text":"The Wyner\u2013Ziv (WZ) problem has received very much attention during the last three decades. There were several attempts to develop practical schemes for lossy coding in the WZ setting, by using codes with certain structures that facilitate the encoding and the decoding. Most notably, these studies include nested structures of linear coset codes (in the role of bins) for discrete sources, and nested lattice structures for continuous valued sources, see e.g., [2], [3]. Other direc- tions of introducing structure into WZ coding are associated with trellis/turbo/LDPC designs ([4] and references therein) and with progressive coding, i.e., successive reﬁnement with layered code design [5], [6]. The case of scalar source codes for the WZ problem was also handled in several papers, e.g. [7] and [8]. Zero-delay coding strategies for the WZ problem were also introduced in [9]. In [10] and [11] it was shown that under high-resolution assumptions, the optimal quantization level density is conjectured to be periodic. In addition, zero- delay schemes for speciﬁc source-side information correlation were presented in [10], [11] and [12].\nIn this paper, we develop lower bounds for the distortion in the scalar WZ setting. We generalize the results of [13] and [14], concerning functionals satisfying a data-processing theorem, to this setting. In [13] it was shown that the rate- distortion (RD) bound (R(d) ≤ C) remains true when − log x, in the deﬁnition of mutual information, is replaced by an arbitrary convex, non\u2013increasing function satisfying some technical conditions. For certain choices of the convex function, the bounds obtained were better than the classical RD bounds. These results were substantially generalized in [14] to apply to even more general information measures. The methods of [13] were used in [15], [16] and [17]. In these papers, lower bounds for the distortion of delay constrained joint source-channel coding were given. These bounds were obtained by combining the R´enyi information measure [18] with the generalized data processing theorem of [13], and under high-resolution and high SNR approximations. Another\nrelated work is [19], where certain degrees of freedom of the Ziv-Zakai generalized mutual information were exploited in order to get better bounds.\nWe start by presenting the relevant random variables of the WZ problem as a Markov chain. Then, using a data processing theorem, we get lower bounds on the distortion in this case. We show that replacing the logarithmic function with other functions, may give better bounds for the distortion of delay\u2013 limited coding (in particular, for scalar coding) in the WZ setting. Examples of non-trivial lower bounds for scalar coding in this setting, were obtained using the convex function Q(t) = t −s , s > 0, which is equivalent to using the R´enyi information measure. These results will be given in detail in Section II. The importance of such bounds stems from the fact that ﬁnding the optimal scalar code in the WZ setting is in general an hard problem. In fact, it is a problem of ﬁnding an optimal partition of the source alphabet and this partition does not necessarily correspond to intervals. A main objective will be to use these bounds for studying the performance of concrete coding schemes. In the next step, a possible direction will be to extend our setting to more general scenarios, for example, to scenarios where the output of the encoder is transmitted over a noisy channel (instead of the noiseless one in the WZ setting), and to scenarios of coding with memory.\nIn this section, we present the relevant random variables of the WZ problem as a Markov chain and establish a generalized Data Processing Theorem (DPT) for this setting, using the method of [13]. We begin with notation conven- tions. Capital letters represent scalar random variables (RV), speciﬁc realization of them are denoted by the correspond- ing lower case letters and their alphabets - by calligraphic letters. For k ≥ 1 (k is a positive integer), x k will denote the vector (x 1 , . . . , x k ). The logarithms are to base 2. We consider a memoryless source producing a random sequence X 1 , X 2 , . . . , X n , X i ∈ X , i = 1, 2, . . . , n, where X is a ﬁnite alphabet with cardinality |X |. Without loss of generality, we deﬁne this alphabet to be the set {1, 2, . . . , |X |}. The probability mass function of X, P X (x), is known. The encoder maps X n into a channel symbol Z which takes on values in the set {1, 2, . . . , M }, M ≤ |X | n . The decoder, in addition to Z, has access to a sequence Y n , dependent on X n via a known\nDMC, deﬁned by the single-letter transition probability matrix P Y |X (y|x) , whose entries are the conditional probabilities\nof the different channel output symbols given the channel input symbols. Based on Z and Y n , the decoder produces the reconstructed sequence ˆ X n . This setting is depicted in Figure 1. For simplicity, we assume that X i , Y i and ˆ X i , all take on values in the same ﬁnite alphabet X . Let Q(t), 0 ≤ t < ∞, be a real-valued convex function, where lim t · Q(1/t) = 0 as t → 0. This requirement implies that Q(t) is non-increasing, as was shown in [13]. We deﬁne 0 · Q(r/0) = 0, for all 0 ≤ r < ∞. The generalized mutual information relative to the function Q(t) is deﬁned by:\nwhere we used the fact that X n ↔ (Y n , Z) ↔ ˆ X n is a Markov chain. Since Z ↔ X n ↔ Y n is also a Markov chain, we have:\n  \n  \nAssuming the encoder is given by a deterministic function f : X n → {1, . . . , M }, we get:\n  \n  \nwhere z = f (x n ) and A z ≡ {˜ x n : f (˜ x n ) = z}. Using Q(t) = − log(t) in (5), thus turning back to the classical DPT, we get the following result:\nwhere the supremum is taken over all partitions of X n into M disjoint subsets. The proof is given in [21]. This inequality stems from the speciﬁc Markovity of the WZ problem. We see that given some ﬁxed-rate R = log M , we should ﬁnd the encoder that maximizes H(Z|Y n ). In Subsection II.C, we show some examples of scalar coding, i.e., n = 1, where this result gives us lower bounds for the distortion, which are better than the bounds obtained from the trivial inequality R W Z (d) ≤ log M , where R W Z (d) is the WZ RD function.\nA ﬁxed-rate scalar source code with rate R = log M , partitions X into M disjoint subsets (A 1 , A 2 , . . . , A M ). The encoder f is given by a function f : X →{1, 2, . . . , M }, that is, z i = f (x i ). The decoder g receives z i , together with the side information y i , and generates ˆ x i , using a decoding function g : {1, 2, . . . , M } × X → X , i.e., ˆ x i = g(z i , y i ). This is the setting described in Figure 1, with block length n = 1. We deﬁne the following vectors {p z } M z=1 :\nwhere 1 B is the indicator function for the event B. The jth coordinate of p z is 1 if j ∈ A z and 0 else. By deﬁnition of {p z } M z=1 , we have the following property:\np(y|x 1 ) \t , p(x 2 |y)Q\nNotice that the vector p ˜ x|y depends only on y and that the dot product [p z · p ˜ x|y ] is a function of z and y. Applying the RD bound [13, Theorem 4], we get:\nThe inﬁmum is taken over all conditional distributions P (ˆ x|x) that satisfy the distortion constraint Ed(X, ˆ X) ≤ d, where d(x, ˆ x) is a distortion measure. The supremum should be taken over all scalar encoders with a ﬁxed-rate R = log M . Alternatively, we can carry out a continuous optimization by taking the supremum over all sets of positive vectors {p z } M z=1 that satisfy (8), i.e., over all conditional distributions P (z|x). The result will be of course greater than or equal to C Q . We end this subsection with an upper bound on C Q for a speciﬁc convex function Q(t). Using Q(t) is equivalent to using the R´enyi information measure, for which the logarithmic measure is a special case (α → 1). In addition, Q(t) is relatively convenient to work with.\nLemma 1: For the convex function Q(t) = t 1−α , 1 < α < 2, we have the following upper bound:\nThe proof is given in [21]. The usefulness of this result stems from its generality. It holds for any source distribution and any transition probability matrix {P Y |X (y|x)}. This result is used in Subsection II.C, along with tighter bounds on the capacity that can be achieved in several special cases.\nB. The generalized rate-distortion function for uniform source distribution\nIn this subsection, we introduce the generalized rate- distortion function of uniformly distributed sources w.r.t gen- eral weakly symmetric distortion measures. We refer to a distortion measure d(x, ˆ x) as weakly symmetric if the rows of the distortion matrix, {d(x, ˆ x)}, are permutations of each\nother. A uniformly distributed source is a source for which P X (x) =\nLemma 2: Consider a discrete source X, uniformly dis- tributed over a ﬁnite alphabet X , and let Q(t), 0 ≤ t < ∞ be any real-valued convex function. Then, R Q (d) w.r.t to any weakly symmetric distortion measure is given by:\nwhere {p k } |X | k=1 is a probability measure which is given by the following equations (k = 1, . . . , |X |):\nwhere {d k } |X | k=1 are the elements of each row of the matrix {d(x, ˆ x)} (the same elements appear in each row) and λ 1 , λ 2 , {µ k } |X | k=1 are constants that are chosen such that:\nCorollary 1: Consider a discrete source X, uniformly dis- tributed over a ﬁnite alphabet X , and let Q(t), 0 ≤ t < ∞ be any real-valued convex function. Then, R Q (d) w.r.t to the Hamming distortion measure is given by:\nThe proof is given in [21]. The general form of R Q (d) enables the use of any convex function Q(t). These results make the Ziv-Zakai mechanism much more useful, at least for the case of uniform sources.\nIn this subsection, we use the results of the previous subsections to get lower bounds on the distortion in several cases. Non-trivial bounds were obtained using the convex function Q(t) = t 1−α , α > 1, which was mentioned above. We assume that the source is uniformly distributed, and that the DMC is symmetric. A channel is said to be symmetric if the rows of the channel transition matrix, P Y |X (y|x) , are permutations of each other, and the columns are permutations of each other. Since the source is uniformly distributed and the channel is symmetric, Y is also uniformly distributed. Under these conditions, Eq. (11) becomes:\nwhere we have deﬁned the following |X |-vectors: p y|˜ x = [p(y|x 1 ), p(y|x 2 ), . . .]\nwhere the supremum is taken over all sets of positive vectors {p z } M z=1 that satisfy (8). Notice that optimizing over α will produce the best lower bound for the distortion. Generally, the functions {G y (p z )} in this case are neither convex nor con- cave. However, at least for some cases, C Q can be calculated directly, as shown in the following examples. We can also upper bound C Q as was done in (14). This upper bound may give us non-trivial bounds, as shown in example 2. Additional examples, with more general distortion measures, will be given in [21].\nwhere µ, ∈ [0, 1], µ > , and µ + (|X | − 1) = 1. The distortion measure we use, is the Hamming distortion, deﬁned in (18).\nLemma 3: Consider the WZ setting with uniformly dis- tributed source and the DMC deﬁned in (23). Then, the minimal achievable distortion w.r.t to the Hamming distortion measure, of a scalar source code with a ﬁxed-rate R = log M , is:\nThis result, fully given in [21], follows directly from simple calculation of the distortion. Having the exact solution in this case, we can compare it to our bound to examine its quality. The generalized capacity (13) for this channel is given by:\n(M z + µ α / α − 1) (M z + µ/ − 1) α−1\n(25) where M z , M z ∈ {1, . . . , |X | − M + 1}, is the cardinality of A z , i.e., the number of source symbols that are encoded to z.\nObviously, z M z = |X |. Notice that the supremum is taken over all feasible encoders, where each encoder is represented by a speciﬁc set {p z } M z=1 as deﬁned in (7). The second equality is explained in [21]. The function q α (M z ) is concave for 1 < α ≤ 2, as shown in [21], and may be concave also out of this range, with dependence on the channel parameters. When q α (M z ) is concave, we can bound the supremum by taking equal M z \u2019s, i.e., M z = |X |/M , ∀z, and we get:\nα / α − 1) (|X |/M + µ/ − 1) α−1\n|X | M\nIf M divides |X |, this bound is achieved by any feasible encoder that partitions the source alphabet into equally-sized subsets, thus the optimization is exact. An example for speciﬁc values of µ and is presented in Figure 2. The bound is com- pared with the bound obtained from the classical DPT (6), the bound obtained from the trivial inequality R W Z (d) ≤ log M , the bound obtained by using (14) and the exact solution of Lemma 3. R W Z (d) was calculated using the Blahut-Arimoto- type algorithm presented in [20]. Eq. (22) was optimized over α, for each M ≤ |X |, as to get the best lower bound on the distortion. We see that even the classical DPT gives us\nnon-trivial lower bounds and that the lower bound obtained from (26) is much better than the trivial bound obtained from R W Z (d). The lower bound obtained from (14) is not useful in this case. There is a gap between the exact solution and the best bound, even for M = 2, where the optimization (13) is exact.\n(27) where l is an integer, 0 < l < |X |, and |X | mod |X | is deﬁned to be |X |. Given an input x, the channel produces one of l\nvalues with equal probability. The generalized capacity (13) for this channel is given by:\nwhere M y,z = l · [p z · p y|˜ x ]. It is easy to see that z M y,z = l. For 1 < α < 2, the function M 2−α y,z is concave in M z . Thus, the supremum is achieved by setting M y,z = l/M , ∀{y, z}:\n(29) If M divides l, equal M y,z \u2019s can be obtained by the following feasible encoder:\nTherefore, in this case, the optimization is exact. For α > 2, C Q is inﬁnite, because we can always set some of the M y,z \u2019s to 0 by appropriate choice of encoder. Thus, this range of α does not lead to a useful bound. An example for speciﬁc values of |X | and l is presented in Figure 3. The lower bound for the distortion, which coincides with the bound obtained from (14) (the upper bound on C Q is tight for this channel), is compared with the bound obtained from the classical DPT (6) and the bound obtained by the trivial inequality R W Z (d) ≤ log M . Eq. (22) was optimized over α, for each M ≤ |X |, as to get the best lower bound on the distortion. We see that in this case, the generalized DPT leads to bounds that are better than the trivial bound, whereas the classical DPT does not lead to a useful bound. We also present the exact distortion of the encoder deﬁned in (30), which is of course an upper bound on the distortion. Thus, the distortion of the optimal encoder must be in the range between this upper bound and our highest lower bound. For M = l, zero distortion can indeed be achieved using the encoder deﬁned in (30), thus our lower bound in this point is tight.\nThis research is supported by the Israeli Science Foundation (ISF), grant no. 208/08."},"refs":[{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[{"name":"J. Kusuma"}],"title":{"text":"Slepian\u2013Wolf coding and related problems"}},{"authors":[{"name":"S. D. Servetto"}],"title":{"text":"Quantization with side information: lattice codes, asymp- totics, and applications to sensor networks"}},{"authors":[{"name":"Z. Xiong"},{"name":"A. D. Liveris"},{"name":"S. Cheng"}],"title":{"text":"Distributed source coding for sensor networks"}},{"authors":[{"name":"Y. Steinberg"},{"name":"N. Merhav"}],"title":{"text":"On successive reﬁnement for the Wyner\u2013 Ziv problem"}},{"authors":[{"name":"S. Cheng"},{"name":"Z. Xiong"}],"title":{"text":"Successive reﬁnement for the Wyner\u2013Ziv problem and layered code design"}},{"authors":[{"name":"J. Kusuma"},{"name":"L. Doherty"},{"name":"K. Ramchandran"}],"title":{"text":"Distributed compression for sensor networks"}},{"authors":[{"name":"D. Muresan"}],"title":{"text":"Quantization as histogram segmentation: Optimal scalar quantizer design in network systems"}},{"authors":[{"name":"D. Teneketzis"}],"title":{"text":"On the structure of optimal real-time encoders and decoders in noisy communication"}},{"authors":[{"name":"J. Nayak"},{"name":"E. Tuncel"}],"title":{"text":"Low-delay quantization for source coding with side information"}},{"authors":[{"name":"X. Chen"},{"name":"E. Tuncel"}],"title":{"text":"Low-delay prediction and transform-based Wyner- Ziv Coding"}},{"authors":[{"name":"X. Chen"},{"name":"E. Tuncel"}],"title":{"text":"High-resolution predictive Wyner-Ziv coding of Gaussian sources"}},{"authors":[{"name":"J. Ziv"},{"name":"M. Zakai"}],"title":{"text":"On functionals satisfying a data-processing theorem"}},{"authors":[{"name":"M. Zakai"},{"name":"J. Ziv"}],"title":{"text":"A generalization of the rate-distortion theory and applications"}},{"authors":[{"name":"I. Leibowitz"},{"name":"R. Zamir"}],"title":{"text":"A Ziv-Zakai-R´enyi lower bound on distortion at high resolution"}},{"authors":[{"name":"S. Tridenski"},{"name":"R. Zamir"}],"title":{"text":"Bounds for joint source-channel coding at high SNR"}},{"authors":[{"name":"A. Ingber"},{"name":"I. Leibowitz"},{"name":"R. Zamir"},{"name":"M. Feder"}],"title":{"text":"Distortion lower bounds for ﬁnite dimensional joint source-channel coding"}},{"authors":[{"name":"A. R´enyi"}],"title":{"text":"On measures of entropy and information"}},{"authors":[{"name":"N. Merhav"}],"title":{"text":"Data Processing Theorems and the Second Law of Thermo- dynamics"}},{"authors":[{"name":"F. Dupuis"},{"name":"W. Yu"},{"name":"J. Willems"}],"title":{"text":"Blahut-Arimoto algorithms for computing channel capacity and rate-distortion with side information"}},{"authors":[{"name":"A. Reani"},{"name":"N. Merhav"}],"title":{"text":"Data processing lower bounds for scalar lossy source codes with side information at the decoder"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564257.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S1.T1.1","endtime":"10:10","authors":"Avraham Reani, Neri Merhav","date":"1341222600000","papertitle":"Data Processing Lower Bounds for Scalar Lossy Source Codes with Side Information at the Decoder","starttime":"09:50","session":"S1.T1: Source Coding with Side Information","room":"Kresge Rehearsal B (030)","paperid":"1569564257"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
