{"id":"1569551905","paper":{"title":{"text":"Perfectly Secure Encryption of Individual Sequences"},"authors":[{"name":"Neri Merhav"}],"abstr":{"text":"Abstract\u2014In analogy to the well\u2013known notion of ﬁnite\u2013state compressibility of individual sequences, due to Lempel and Ziv, we deﬁne a similar notion of \u201cﬁnite\u2013state encryptability\u201d of an individual plaintext sequence, as the minimum asymptotic key rate that must be consumed by ﬁnite\u2013state encrypters so as to guarantee perfect secrecy in a well\u2013deﬁned sense. Our main basic result is that the ﬁnite\u2013state encryptability is equal to the ﬁnite\u2013state compressibility for every individual sequence. This is in parallelism to Shannon\u2019s classical probabilistic counterpart result, asserting that the minimum required key rate is equal to the entropy rate of the source. However, the redundancy, deﬁned as the gap between the upper bound (direct part) and the lower bound (converse part) in the encryption problem, turns out to decay at a different rate (in fact, much slower) than the analogous redundancy associated with the compression problem. We also extend our main theorem, allowing: (i) availability of side information (SI) at the encrypter/decrypter/eavesdropper, (ii) lossy reconstruction at the decrypter."},"body":{"text":"The paradigm of individual sequences and ﬁnite\u2013state ma- chines (FSMs), as an alternative to the traditional probabilistic modeling of sources and channels, has been studied and explored quite extensively in several information\u2013theoretic problem areas, including data compression [9], [13], [15], [16], [19], source/channel simulation [5], [10], classiﬁcation [18], [20], prediction [2], [21], denoising [11], and even channel coding [4], just to name a few representative references. On the other hand, it is fairly safe to say that the entire literature on information\u2014theoretic security (see, e.g., [3], [6], [12] for surveys as well as references therein), is based exclusively on the probabilistic setting.\nTo the best of our knowledge, the only exception to this rule is [14]. In that work, the plaintext source to be encrypted, using a secret key, is an individual sequence, the encrypter is a general encoder, and the eavesdropper employs an FSM as a message discriminator. Speciﬁcally, it is postulated in [14] that the eavesdropper has prior knowledge that can be expressed in terms of the existence of some set of \u201cacceptable messages\u201d that constitutes the a-priori level of uncertainty that the eavesdropper has concerning the plaintext message. Next, it is assumed that there exists an FSM that can test whether a given candidate plaintext message is acceptable: Iff the FSM produces the all\u2013zero sequence in response to that message, then this message is acceptable. Perfect security is then deﬁned as a situation where the size of the acceptance set is not reduced in the presence of the cryptogram. The main result in [14] is that the asymptotic key rate needed for\nperfectly secure encryption in that sense, cannot be smaller (up to asymptotically vanishing terms) than the Lempel\u2013Ziv (LZ) complexity of the plaintext source [19]. This lower bound is obviously asymptotically achieved by one\u2013time pad encryption of the bit-stream obtained by LZ data compression of the plaintext source.\nIn this work, we also consider encryption of individual sequences, but our modeling approach and the deﬁnition of perfect secrecy are different. Rather than assuming that the encrypter and decrypter have unlimited resources, and that it is the eavesdropper which has limited resources, modeled in terms of FSMs, in our setting, the converse is true. We adopt a model of a ﬁnite\u2013state encrypter, which receives as inputs the plaintext stream and the secret key bitstream, and it produces a ciphertext. Based on this model, we deﬁne a notion of ﬁnite\u2013 state encryptability (in analogy to the notions of ﬁnite\u2013state compressibility [19] and the ﬁnite\u2013state predictability [2]), as the minimum achievable rate at which key bits must be consumed by any ﬁnite\u2013state encrypter in order to guarantee perfect security, while keeping the cryptogram decipherable at the legitimate receiver. Our main result is that the ﬁnite\u2013 state encryptability is equal to the ﬁnite\u2013state compressibility, similarly as in [14].\nMore precisely, denoting by c(x n ) the number of LZ phrases associated with the plaintext x n = (x 1 , . . . , x n ), we show that the number of key bits required by any encrypter with s states, normalized by n cannot be smaller than [c(x n ) log c(x n )]/n − δ s (n), where δ s (n) = O(s log(log n)/ log n). On the other hand, this bound is obviously essentially achievable by ap- plying the LZ \u201878 algorithm [19], followed by one\u2013time pad encryption, since the compression ratio of the LZ \u201878 algorithm is also [c(x n ) log c(x n )]/n, up to vanishingly small terms. It follows then that the ﬁnite\u2013state encryptability of every (inﬁnite) individual sequence is equal to its ﬁnite\u2013state compressibility.\nWhile the idea of LZ data compression, followed by one\u2013 time padding is rather straightforward, our main result, that no ﬁnite\u2013state encrypter can do better than that for any given individual sequence, may not be quite obvious since the operations of compression and encryption are basically different \u2013 secret key encryption need not necessarily be based on compression followed by one\u2013time padding, deﬁnitely not if both operations are formalized in the framework of ﬁnite\u2013 state machines.\nthe upper bound (of the direct part) and the lower bound (of the converse part), which can be thought of as some notion of redundancy, is again O(s log(log n)/ log n), which decays much more slowly than the corresponding redundancy in data compression [19, Theorems 1, 2], which is roughly O((log s)/ log n).\nFinally, we extend our main basic theorem in two direc- tions. The ﬁrst is in allowing availability of side information (SI) at all three parties (encrypter, legitimate decrypter and eavesdropper) or at the decrypter and the eavesdropper only. We assume that the SI sequence is an individual sequence as well. We initially assume that it is the same SI that is available to all three parties in the ﬁrst case or to both the legitimate decrypter and the eavesdropper, in the second case. Our main result is essentially unaltered, except that the LZ complexity, ρ LZ (x n ) ∆ = [c(x n ) log c(x n )]/n, is replaced by the conditional LZ complexity given the SI, to be deﬁned later (see also [7], [17]). Our second extension is to the case where lossy reconstruction is allowed at the legitimate receiver (ﬁrst, without SI). Here the LZ complexity is replaced by a notion of \u201cLZ rate\u2013distortion function,\u201d r LZ (D; x n ), which means the smallest LZ complexity among all sequences that are within the allowed distortion relative to the input plaintext sequence. While our framework allows randomized reconstruction sequences (that may depend on the random key), we ﬁnd that at least asymptotically, there is nothing to gain from this degree of freedom, as optimum performance can be achieved by a scheme that generates deterministic reproductions. Finally, we make a few short comments on the case where both SI and lossy reconstruction are allowed at the same time.\nIt should be pointed out that throughout the entire paper, most of our emphasis is on converse theorems (lower bounds). The compatible direct parts (upper bounds) will always be attainable by a straightforward application of the suitable data compression scheme, followed by one\u2013time padding.\nThroughout this paper, scalar random variables (RV\u2019s) will be denoted by capital letters, their sample values will be denoted by the respective lower case letters, and their al- phabets will be denoted by the respective calligraphic letters. A similar convention will apply to random vectors and their sample values, which will be denoted with same symbols superscripted by the dimension. Thus, for example, A m (m \u2013 positive integer) will denote a random m-vector (A 1 , ..., A m ), and a m = (a 1 , ..., a m ) is a speciﬁc vector value in A m , the m\u2013 th Cartesian power of A. The notations a j i and A j i , where i and j are integers and i ≤ j, will designate segments (a i , . . . , a j ) and (A i , . . . , A j ), respectively, where for i = 1, the subscript will be omitted (as above). For i > j, a j i (or A j i ) will be understood as the null string.\nSources and channels will be denoted generically by the letter P or Q, subscripted by the name of the RV and its conditioning, if applicable, exactly like in ordinary textbook\nnotation standards. Information theoretic quantities, like en- tropies and mutual informations, will be denoted following the usual conventions of the information theory literature, e.g., H(K m ), I(W ; X m |S m ), and so on.\nA ﬁnite\u2013state encrypter is deﬁned by a sixtuplet E = (X , Y, Z, f, g, ∆), where X is a ﬁnite input alphabet of size |X | = α, Y is a ﬁnite set of binary words, Z is a ﬁnite set of states, f : Z × X × {0, 1} ∗ → Y is the output function, g : Z × X → Z is the next\u2013state function, ∆ : Z × X → {0, 1, 2, . . .}, and {0, 1} ∗ is the set of all binary strings of ﬁnite length. The set Y is allowed to contain binary strings of various lengths, including the null word λ (whose length is zero). When two inﬁnite sequences, x = x 1 , x 2 , . . ., x i ∈ X , and u = u 1 , u 2 , . . ., u i ∈ {0, 1}, i = 1, 2, . . . are fed into E, it produces an inﬁnite output sequence y = y 1 , y 2 , . . ., y i ∈ Y, while passing through an inﬁnite sequence of states z = z 1 , z 2 , . . ., z i ∈ Z, according to the following recursive equations, implemented for i = 1, 2, . . .\nt i = t i−1 + ∆(z i , x i ), \t t 0 ∆ = 0 \t (1) k i = (u t i−1 +1 , u t i−1 +2 , . . . , u t i ) \t (2) y i = f (z i , x i , k i ) \t (3)\nwhere it is understood that if ∆(z i , x i ) = 0, then k i = λ, the null word of length zero, namely, no key bits are used in the i\u2013th step. By the same token, if y i = λ, no output is produced at this step, i.e., the system is idling and only the state evolves in response to the input. An encrypter with s states E, is one with |Z| = s. It is assumed that x is deterministic (i.e., an individual sequence), whereas u is purely random, i.e., for every positive integer n, P U n (u n ) = 2 −n .\nBy f (z 1 , x n , k n ), we refer to the vector y n produced by E in response to the inputs x n and k n when the initial state is z 1 . Similarly, the notation g(z 1 , x n ) will mean the state z n+1 and ∆(z 1 , x n ) will designate n i=1 ∆(z i , x i ) under the same circumstances. An encrypter E deﬁned as perfectly secure if for every positive integer n and for every x ∈ X ∞ and y n ∈ Y n , the probability Pr{Y n = y n |x} is independent of x.\nAn encrypter is deﬁned as information lossless (IL) if for every z 1 ∈ Z, every sufﬁciently large 1 n and all x n ∈ X n and k n ∈ K n , the quadruple (z 1 , k n , f (z 1 , x n , k n ), g(z 1 , x n )) uniquely determines x n . Given E and x n , the encryption key rate of x n w.r.t. E is deﬁned as\n1 n\nwhere (k i ) = ∆(z i , x i ) is the length of the binary string k i and (k n ) = n i=1 (k i ) is the total length of k n .\nOur purpose it to characterize these quantities and to point out how they can be achieved in principle.\nIncremental parsing [19] of a string x n is a sequential procedure of parsing x n into distinct phrases, where each new phrase is the shortest string that has not been encountered before as a phrase of x n , with the possible exception of the last phrase that might be incomplete. Let c(x n ) denote the number of phrases in LZ incremental parsing of x n . The LZ complexity of x n is deﬁned as\nn ) log c(x n ) n\nThe ﬁnite\u2013state compressibility, ρ(x), of x = (x 1 , x 2 , . . .) is deﬁned, in [19], as the best compression ratio achieved by IL ﬁnite\u2013state encoders, analogously to the above deﬁnition of ﬁnite\u2013state encryptability. From Theorems 1, 2 and 3 of [19], it follows that ρ LZ (x) ∆ = lim sup n→∞ ρ LZ (x n ) is equal to ρ(x).\nThe following theorem establishes a lower bound on σ s (x n ) in terms of ρ LZ (x n ) and hence a lower bound of σ(x) in terms of ρ(x).\nTheorem 1: For every x n , σ s (x n ) ≥ ρ LZ (x n ) − δ s (n), where δ s (n) is independent of x n and behaves according to δ s (n) = O(s log(log n)/ log n). Consequently, σ(x) ≥ ρ(x).\n1. It is readily observed that a compatible direct theorem holds, simply by applying the LZ \u201878 algorithm followed by one\u2013time pad encryption of the compressed bits. The resulting key\u2013rate needed is then upper bounded by 1 n [c(x n )+ 1] log[2α(c(x n ) + 1)], following [19, Theorem 2], which is, within negligible terms, equal to ρ LZ (x n ). Thus, σ(x) = ρ(x).\n2. Consider the difference between the upper bound per- taining to the direct part (as mentioned in item no. 1 above) and the lower bound of the converse part. The be- havior of this difference is O(αs log(log n)/\nlog n). This behavior is different from the behavior of the corresponding gap in compression (Theorems 1 and 2 in [19]), which is O([log(2α)] log(8αs 2 )/ log n). The guaranteed convergence to optimality is therefore considerably slower in the encryption problem.\n3. We already mentioned that the deﬁnition of the IL property here is somewhat more relaxed than in [19] (see footnote no.\n1). Moreover, it is possible to relax this requirement even further by allowing a relatively small uncertainty in x n given (z 1 , k n , f (z 1 , x n , k n ), g(z 1 , x n )) (see Subsection IV-B), at the possible cost of further slowing down the convergence of δ s (n).\nPartial Proof. Let m divide n and consider the partition of x n into n/m non\u2013overlapping m\u2013vectors x 1 , x 2 , . . . , x n/m , where x i = x im (i−1)m+1 . Recall that for a given z (i−1)m+1 and x i , the length l i of k i = k im (i−1)m+1 is uniquely de- termined as l i = ∆(z i , x im (i−1)m+1 ). Let us now deﬁne a joint empirical distribution of several variables. For every a m ∈ X m , z, z ∈ Z, and every positive integer l, let us deﬁne P X m ZZ L (a m , z, z , l) as the relative frequency of the combination x im (i−1)m+1 = a m , z (i−1)m+1 = z, z im+1 = z\nThroughout this proof, all information measures are deﬁned w.r.t. P K m X m Y m ZZ L . Consider the following chain of equal- ities for the given x n and an arbitrary encrypter E ∈ E (s):\nn ) n\n= 1 m\n= 1 m\nm |L) m\nNote that the length of the key for the i-th m\u2013 block, l i = (k i ) = ∆(z (i−1)m+1 , x im (i−1)m+1 ) =\n∆(z t , x t ), is a variable that may take on no more than (m + 1) αs−1 different values, 2 and hence the same is true concerning the random variable L, and so, H(L) ≤ (αs − 1) log(m + 1). Thus,\n= H(X m |Y m ) − H(X m |Y m , K m ) = H(X m ) − H(X m |Y m , K m )\n= H(X m ) − H(X m |Y m , K m , Z, Z ) − I(Z, Z ; X m |Y m , K m )\n= H(X m ) − 0 − I(Z, Z ; X m |Y m , K m ) ≥ H(X m ) − H(Z, Z |Y m , K m )\nwhere the second equality is due to the perfect security assumption and the third equality is due to the IL property, assuming that m is sufﬁciently large. Thus, combining eqs. (11) and (12), we obtain\nm ) m\nNow, the main term, H(X m )/m, is nothing but the normalized m\u2013th order empirical entropy associated with x n . Next, in order to get rid from the dependence of this main term on m, we further lower bound it in terms of ρ LZ (x n ) at the (small) price of reducing the bound further by additional terms that will be shown later to be negligible. In particular, in [8], we prove the following inequality:\n≥ c(x n ) log c(x n ) n\n− 2m(log α + 1) 2 (1 − n ) log n\nwhere n → 0 as n → ∞. Combining this with eq. (13), we get\nn ) log c(x n ) n\nWe now have the freedom to let m = m n grow slowly enough as a function of n such that δ s (n) = δ s (n, m n ) will vanish for every ﬁxed s. By letting m n be proportional to\n(log n) log(log n), δ s (n) becomes O(s log(log n)/ log n). Note that the ﬁrst two terms of δ s (n, m) come from con- siderations pertaining to encryption, whereas the other terms appear also in compression. The second term turns out to be the dominant one, which means that in the encryption problem we end up with slower decay of the redundancy. If we compare the difference between the upper bound and the lower bound in compression (coding them and converse in [19]), this difference is dominated by a term that is O(([log(2α)] log(8αs 2 )/ log n), whereas in encryption the dif- ference is O(αs log(log n)/ log n), namely, a signiﬁcantly slower decay rate.\nConsider the case where SI is available at the en- crypter/decrypter/eavesdropper. Suppose that, in addition to the source sequence x, there is an (individual) SI sequence\ns = (s 1 , s 2 , . . .), s i ∈ S, i = 1, 2, . . ., where S is a ﬁnite alphabet. Assume ﬁrst that all three parties (encoder, decoder, and eavesdropper) have access to s. In the formal model deﬁnition, a few modiﬁcations are needed: (i) In eqs. (1), (3), and (4), the functions ∆, f and g should be allowed to depend on the additional argument s i , (ii) The deﬁnition of perfect security should allow conditioning on s, in addition to the present conditioning on x. I.e., Pr{Y n = y n |x, s} is independent of x for every positive integer n (but it is allowed to depend on s). (iii) In the deﬁnition of an IL encrypter, the quadruple (z, k n , f (z, x n , k n ), g(z, x n )) should be extended to be the quintuple (z, k n , s n , f (z, x n , k n ), g(z, x n )).\nIn Theorem 1, the LZ complexity of x n , should be replaced by the conditional LZ complexity of x n given s n , denoted ρ LZ (x n |s n ), which is an empirical measure of conditional entropy (or conditional compressibility), that is deﬁned as follows (see also [7], [17]): Given x n and s n , let us apply the incremental parsing procedure of the LZ algorithm to the sequence of pairs ((x 1 , s 1 ), (x 2 , s 2 ), . . . , (x n , s n )). According to this procedure, all phrases are distinct with a possible exception of the last phrase, which might be incomplete. Let c(x n , s n ) denote the number of distinct phrases. For example, 3 if\nx 6 = 0 | 1 | 0 0 | 0 1| s 6 = 0 | 1 | 0 1 | 0 1|\nthen c(x 6 , s 6 ) = 4. Let c(s n ) denote the resulting number of distinct phrases of s n , and let s(l) denote the lth distinct s\u2013 phrase, l = 1, 2, ..., c(s n ). In the above example, c(s 6 ) = 3. Denote by c l (x n |s n ) the number of occurrences of s(l) in the parsing of s n , or equivalently, the number of distinct x-phrases that jointly appear with s(l). Clearly, c(s n ) l=1 c l (x n |s n ) = c(x n , s n ). In the above example, s(1) = 0, s(2) = 1, s(3) = 01, c 1 (x 6 |s 6 ) = c 2 (x 6 |s 6 ) = 1, and c 3 (x 6 |s 6 ) = 2. Now, the conditional LZ complexity of x n given s n is deﬁned as\nThe direct is obtained by ﬁrst, compressing x n to about n · ρ LZ (x n |s n ) bits using the conditional parsing scheme [17, Lemma 2, eq. (A.11)] and then applying one\u2013time pad encryption.\nThe same performance can be achieved even if the encrypter does not have access to s n , by using a scheme in the spirit of Slepian\u2013Wolf coding: Randomly assign to each member of X n a bin, selected independently at random across the set {1, 2, . . . , 2 nR }. The encrypter applies one\u2013time pad to the (nR)\u2013bit binary representation of the bin index of x n . The decrypter, ﬁrst decrypts the bin index using the key and then seeks a sequence ˆ x n within the given bin, which satisﬁes ρ LZ (ˆ x n |s n ) < R − . If there is one and only one such sequence, then it becomes the decoded message,\notherwise an error is declared. This scheme works, just like the ordinary SW coding scheme, because the number of {ˆ x n } for which ρ LZ (ˆ x n |s n ) < R − does not exceed 2 n[R− +O(log(log n)/ log n)] [17, Lemma 2]. The weakness of this is that prior knowledge of (a tight upper bound on) ρ LZ (x n |s n ) is required. If, for example, it is known that x n is a noisy version of s n , generated, say, by a known additive channel, then R should be essentially the entropy rate of the noise.\nSuppose that we are content with a lossy reconstruction, ˆ x n , at the legitimate receiver. In general, this reconstruction may be a random vector due to possible dependence on the random key bits. It is required, however, that d(x n , ˆ x n ) ≤ nD with probability one, for some distortion measure d. Then, in Theorem 1, ρ LZ (x n ) should be replaced by the \u201cLZ rate\u2013 distortion function\u201d of x n , which is deﬁned as\nThe deﬁnition of the IL property can be slightly re- laxed to a notion of \u201cnearly IL\u201d (NIL) property, which allows recovery with small uncertainty for all large enough n. In particular, we shall assume that given w ∆ = (z i , k i+n i , f (z i , x n+i−1 i \t , k n+i−1 i \t ), g(z i , x n+i−1 i \t )), ˆ x n+i−1 i\nmust lie, with probability one, in a subset A n (w) ⊂ ˆ X n , where 4\n1 n\nPerfect security should be deﬁned as statistical independence between the cryptogram and both the source and reconstruc- tion, i.e., the probability of any segment of {y i } should not depend on either x or ˆ x.\nAgain, the direct is obvious, and it implies that at least asymptotically, there is nothing to gain from randomizing the reconstruction: The best choice of ˆ x n is the one with minimum LZ complexity within the sphere of radius nD around x n . This conclusion is not obvious a\u2013priori as one might speculate that a randomized reconstruction, depending on the key, may potentially be more secure than a deterministic one.\nNote that we have not assumed anything on the distortion measure d, not even additivity. Another difference between Theorem 1 of the lossless case and its present extension to the lossy case, is that we are know longer able to characterize the rate of convergence of δ s (n), as it depends on the rate of decay of η m . In fact, we could have replaced the IL property we assumed in the lossless case by the NIL property there too, but again, the cost would be the loss of the ability to specify the behavior of δ n .\nThe simultaneous extension of Theorem 1, allowing both distortion D and SI s n leads, with the obvious modiﬁcations, to min{ρ LZ (ˆ x n |s n ) : d(x n , ˆ x n ) ≤ nD}, whose achievability is conceptually straightforward when all parties have access to s n , including the encrypter. But what if the encrypter does not have access to s n ? Due to space limitations, we will not provide detailed results concerning this case, except for a comment that for the case where the legitimate decrypter is also based on an FSM, it can be handled using the same line of thought as in [9] (and references therein), showing that the best general block code of length m is at least as good as any s\u2013state encrypter\u2013decrypter, up to a redundancy term that tends to zero as m → ∞ for every ﬁxed s (see [8])."},"refs":[{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, John Wiley & Sons, Hoboken, New Jersey, U"}},{"authors":[{"name":"M. Feder"},{"name":"N. Merhav"},{"name":"M. Gutman"}],"title":{"text":"Universal prediction of individual sequences"}},{"authors":[{"name":"Y. Liang"},{"name":"H. V. Poor"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"Information theoretic security"}},{"authors":[{"name":"Y. Lomnitz"},{"name":"M. Feder"}],"title":{"text":"Universal communication over modulo\u2013 additive channels with an individual noise sequence"}},{"authors":[{"name":"A. Mart´ın"},{"name":"N. Merhav"},{"name":"G. Seroussi"},{"name":"M. J. Weinberger"}],"title":{"text":"Twice\u2013 universal simulation of Markov sources and individual sequences"}},{"authors":[{"name":"J. L. Massey"}],"title":{"text":"An introduction to contemporary cryptology"}},{"authors":[{"name":"N. Merhav"}],"title":{"text":"Universal detection of messages via ﬁnite\u2013state channels"}},{"authors":[{"name":"N. Merhav"}],"title":{"text":"Perfectly secure encryption of individual sequences"}},{"authors":[{"name":"N. Merhav"},{"name":"J. Ziv"}],"title":{"text":"On the Wyner\u2013Ziv problem for individual sequences"}},{"authors":[{"name":"G. Seroussi"}],"title":{"text":"On universal types"}},{"authors":[{"name":"T. Weissman"},{"name":"E. Ordentlich"},{"name":"G. Seroussi"},{"name":"S. Verd´u"},{"name":"M. J. Weinberger"}],"title":{"text":"Universal denoising: known channel"}},{"authors":[{"name":"H. Yamamoto"}],"title":{"text":"Information theory in cryptology"}},{"authors":[{"name":"J. Ziv"}],"title":{"text":"Coding theorems for individual sequences"}},{"authors":[{"name":"J. Ziv"}],"title":{"text":"Perfect secrecy for individual sequences"}},{"authors":[{"name":"J. Ziv"}],"title":{"text":"Distortion\u2013rate theory for individual sequences"}},{"authors":[{"name":"J. Ziv"}],"title":{"text":"Fixed-rate encoding of individual sequences with side infor- mation"}},{"authors":[{"name":"J. Ziv"}],"title":{"text":"Universal decoding for ﬁnite-state channels"}},{"authors":[{"name":"J. Ziv"}],"title":{"text":"Compression, tests for randomness, and estimating the statistical model of an individual sequence"}},{"authors":[{"name":"J. Ziv"},{"name":"A. Lempel"}],"title":{"text":"Compression of individual sequences via variable-rate coding"}},{"authors":[{"name":"J. Ziv"},{"name":"N. Merhav"}],"title":{"text":"A measure of relative entropy between individ- ual sequences with application to universal classiﬁcation"}},{"authors":[{"name":"J. Ziv"},{"name":"N. Merhav"}],"title":{"text":"On context\u2013tree prediction of individual se- quences"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569551905.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T6.2","endtime":"10:30","authors":"Neri Merhav","date":"1341396600000","papertitle":"Perfectly Secure Encryption of Individual Sequences","starttime":"10:10","session":"S9.T6: Synchrony and Perfect Secrecy","room":"Kresge Rehearsal A (033)","paperid":"1569551905"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
