{"id":"1569566591","paper":{"title":{"text":"The Sufﬁciency Principle for Decentralized Data Reduction"},"authors":[{"name":"Ge Xu"},{"name":"Biao Chen"}],"abstr":{"text":"Abstract\u2014 This paper develops the sufﬁciency principle suit- able for data reduction in decentralized inference systems. Both parallel and tandem networks are studied and we focus on the cases where observations at decentralized nodes are conditionally dependent. For a parallel network, through the introduction of a hidden variable that induces conditional independence among the observations, the locally sufﬁcient statistics, deﬁned with respect to the hidden variable, are shown to be globally sufﬁcient for the parameter of inference interest. For a tandem network, the notion of conditional sufﬁciency is introduced and the related theories and tools are developed. Finally, connections between the sufﬁciency principle and some distributed source coding problems are explored."},"body":{"text":"The sufﬁciency principle has played a prominent role in designing data processing methods for statistical inference. A sufﬁcient statistic is a function of the data that contains all the information in the data about the parameter of interest. The primary goal of sufﬁciency-based data reduction is dimension- ality reduction to facilitate subsequent inferences based on the reduced data [1]\u2013[3].\nIf the parameter θ is itself random, the sufﬁciency principle can also be reframed using the data processing inequality [4, Section 2.9]. That is, a function T (X) is a sufﬁcient statistic if and only if the following Markov chain holds:\nFor decentralized inference, data reduction is done locally without access to the global data. Therefore, the contrasting notions of local sufﬁciency and global sufﬁciency [5] need to be treated with care. A sufﬁcient statistic that is deﬁned with respect to local data is referred to as locally sufﬁcient statistic while a sufﬁcient statistic deﬁned with respect to the global data in the network is referred to as a globally\nY T (X)\nsufﬁcient statistic [5]. As such, whether a statistic at a local node is globally sufﬁcient is not determined solely by the statistical characterization of local data but also depends on the joint distribution of the whole data and how data/statistics are passed along within the network.\nFor conditionally independent observations (e.g., X and Y are independent given θ in Figs. 1 and 2), local sufﬁciency implies global sufﬁciency. This result was established in [5]\u2013 [7] for parallel networks (Fig. 1) and it is straightforward to show that the same result holds for tandem networks (Fig. 2). An interesting manifestation of the above result is in decentralized detection. It is well known that for a binary hypothesis testing problem, the likelihood ratio (LR) is a sufﬁcient statistic for the underlying hypothesis. Therefore, it is not surprising that likelihood ratio quantizers are globally optimal for decentralized detection with conditionally indepen- dent observations [8], even with non-ideal, possibly coupling channels between the sensors and the fusion center [9], [10].\nWithout the conditional independence assumption, decen- tralized inference becomes considerably more complex. For the decentralized detection, the optimal solution becomes NP complete when the observations are conditionally dependent [11]. The primary focus of this paper is to develop theories and tools for decentralized data deduction with conditionally dependent observations for both parallel and tandem networks.\nFor parallel networks, we investigate the sufﬁciency prin- ciple under a hierarchical conditional independence (HCI) model, which is a new framework recently proposed to deal\nwith distributed detection with conditionally dependent obser- vations [12]. The main idea is to inject a hidden variable W such that the sensor observations are conditionally independent with respect to this new variable regardless of the dependence structure of the original model. Suitable conditions are identi- ﬁed under this HCI model such that local sufﬁciency implies global sufﬁciency.\nFor tandem networks such as that described in Fig. 2, Y is fully available at the decision node. As such, the novel notion of conditional sufﬁciency is deﬁned to capture the difference in network structure with that of the parallel network. A new set of theories and tools corresponding to conditional sufﬁciency are then developed.\nFinally, the developed notion of sufﬁciency is applied to some classical distributed source coding problems. There, sufﬁciency-based data reduction prior to a source encoder is shown to incur no penalty on the corresponding rate region or the rate distortion function.\nThe rest of the paper is organized as follows. Section II develops the sufﬁciency principle in parallel networks with conditionally dependent observations. Section III deals with tandem networks where the notion of conditional sufﬁciency is introduced and associated theories are developed. In section IV, the connection between the developed sufﬁciency princi- ple and two distributed source coding problems is explored. Section V concludes the paper.\nThis section considers only a parallel network of two sensors as illustrated in Fig. 1. The result extends naturally to the case with arbitrary numbers of sensors. Let data available at node X be X while data available at node Y be Y.\nAssume the parameter θ is random. (T x (X), T y (Y)) are globally sufﬁcient for θ if the Markov chain θ − (T x (X), T y (Y)) − (X, Y) holds.\nIdentifying local statistics that are globally sufﬁcient can be accomplished in theory via the factorization theorem. The process of using the factorization theorem may become cumbersome in a decentralized system or not applicable when the precise joint distribution of the data in the network is not available at local nodes. The following theorem provides cer- tain relation between local sufﬁciency and global sufﬁciency for a class of distributed inference problem.\nLemma 1: Let X, Y ∼ p(x, y|θ) and suppose there exists a random variable W such that\nA statistic T (X, Y) that is sufﬁcient for W is also sufﬁcient for θ.\nProof: The Markov chain (1) implies that θ − W − (X, Y, T (X, Y)) forms a Markov chain for any statistics T (X, Y). That T (X, Y) is sufﬁcient for W implies the Markov chain W−T (X, Y)−(X, Y). It is straightforward to show that these two Markov chains give rise to a long Markov chain\nLemma 1 is not useful in itself as T (X, Y) is a function of the global data which is not available in either of the nodes. Its use is main for establishing the following result.\nTheorem 1: Let X, Y ∼ p(x, y|θ) and suppose there exists a random variable W such that θ − W − (X, Y). Let T (W) be a sufﬁcient statistic for θ, i.e., θ − T (W) − W.\n2) If T (W) induces conditional independence between X and Y, and (T x (X), T y (Y)) are locally sufﬁcient for T (W), then (T x (X), T y (Y)) are globally sufﬁcient for θ .\nProof: To prove 1), from Lemma 1, we only need to show that the Markov chain θ −T (W)−(X, Y) holds. However, the Markov chain T (W)−(θ, W)−(X, Y) together with θ−W− (X, Y) results in the Markov chain (θ, T (W))−W−(X, Y). Combined with the Markov chain θ − T (W) − W, we get θ −T (W)−W −(X, Y) which implies θ −T (W)−(X, Y).\nFor the second one, since conditional independence en- sures that locally sufﬁcient statistics are globally sufﬁcient, (T x (X), T y (Y)) are thus sufﬁcient for T (W). The ﬁrst result then establishes that they are also sufﬁcient for θ.\nRemark 1: It is given in [12] that any general distributed inference model can be represented as a HCI model and vice versa, where the HCI model is constructed by introducing a hidden variable W such that the following Markov chains hold: θ − W − (X, Y) and X − W − Y. Therefore, Theorem 1 indicates that under the HCI model, local sufﬁciency with respect to the hidden variable implies global sufﬁciency.\nFrom the above result, it is clear that whether T x (X) is globally sufﬁcient depends also on T y (Y) and vice versa. This coupling effect makes it rather difﬁcult in studying the global sufﬁciency property. In the following, we consider a somewhat simpliﬁed situation where one is interested in data reduction at one node provided that a locally sufﬁcient statistic from the other node is available at the fusion center. That is, if T y (Y) is known to be a locally sufﬁcient statistic, what should node X transmit such that T x (X) may form a globally sufﬁcient statistic together with T y (Y).\nTheorem 2: Let X, Y be distributed according to p(x, y|θ). Assume T y (Y) is a locally sufﬁcient statistic for θ, then (T x (X), T y (Y)) are globally sufﬁcient for θ if and only if there exist functions g (t 1 |t 2 , θ ) and h(x, y) such that, for all sample points (x, y) and all parameter values θ, the conditional probability p (x|y, θ) satisﬁes\np (x|y, θ) = g(T x (x)|T y (y), θ)h(x, y). \t (2) Proof: Directly from the factorization theorem for\nRemark 2: Given a locally sufﬁcient statistic T y (Y), it is possible that there does not exist a T x (X) forming a globally sufﬁcient statistic together with T y (Y).\nRemark 3: The above result is shown under the assumption that θ is a random variable, similar result can be obtained for\nθ is not random by resorting to factorization theorem instead of data processing inequality.\nwhere Z, U 1 , · · · , U n , V 1 , · · · V n are mutually independent Gaussian random variables such that Z ∼ N(θ, ρ), U i ∼ N (0, 1−ρ), V i ∼ N(0, 1−ρ). Thus, X i , Y i ∼ N(θ, θ, 1, 1, ρ).\nThe parameter of inference interest is θ. X and Y are not conditionally independent given θ.\nLet T (W ) = W = Z. Thus, Z depends on θ through its mean value. Clearly, Z satisﬁes the Markov chains θ − Z − (X, Y) and X − Z − Y as required by the HCI model. Thus, from Theorem 1, the locally sufﬁcient statistic pair for Z, ( i X i , i Y i ), is globally sufﬁcient for θ.\nExample 2: Consider the hypotheses test where the obser- vations X i , i = 1, · · · , k, satisfy the following model\nThe observations are not conditionally independent given H 1 . Let W = S which induces conditional independence among observations under both hypotheses. It is easy to see that T (W ) = |S| is sufﬁcient for H given S. Thus, the Markov chain H − |S| − S − (X 1 , · · · , X k ) holds.\nOn the other hand, given |S|, the observations are con- ditionally independent of each other under the QAM and Rayleigh fading assumptions. For any i, |X i | is a minimal sufﬁcient statistics for |S|. This can be easily veriﬁed by the\nfor two sample points x i and x i . Therefore, by Theorem 1, {|X i |} is globally sufﬁcient for H.\nThe above observation can be used to establish that the optimal detector at each local sensor is an energy detector for the model described in Example 2 [13].\nA tandem network, as illustrated in Fig. 2, is one such that compressed data are transmitted to a node which also has its own observation. The second node will then make a ﬁnal decision using its own data and the input from the ﬁrst node. Knowing that Y is available at the fusion center even without directly observing Y should have an impact on how node X summarizes its own data X. A natural way of extending the\nsufﬁciency principle to this network is as follows: the inference performance should remain the same whether the inference is based on (X, Y) or (T (X), Y). From the data processing inequality, the sufﬁciency of T (X) can thus be characterized using the Markov chain θ − (T (X), Y) − (X, Y). Given that T (X) is a function X, it is straightforward to show that that the Markov chain θ − (T (X), Y) − (X, Y) is equivalent to θ − (T (X), Y) − X. This motivates the following deﬁnition of conditional sufﬁciency.\nDeﬁnition 1: A statistic T (X) is a conditional sufﬁcient statistic for θ, conditioned on Y, if the conditional distribution of the sample X given the value of T (X) and Y does not depend on θ.\nThe deﬁnition allows us to generalize a number of classical results related to sufﬁcient statistics.\nTheorem 3: Let X, Y be distributed according to p(x, y|θ). Let q (T (x), y|θ) be the joint distribution of T (X) and Y, then T (X) is a conditional sufﬁcient statistic for θ, conditioned on Y, if for every (x, y) pair, the ratio p(x,y|θ) q(T (x),y|θ) is constant as a function of θ.\nSimilarly, the Neyman-Fisher factorization theorem can also be generalized to the conditional case.\nTheorem 4: Let X, Y be distributed according to p(x, y|θ). A statistic T (X) is conditionally sufﬁcient for θ, conditioned on Y, if and only if there exist functions g(t, y|θ) and h(x, y) such that,\nThe proof can be constructed similarly to that of the factorization theorem in [2, Theorem 6.2.6]. In fact, this result can be viewed as a special case of Theorem 2 using the fact that Y is naturally a locally sufﬁcient statistic for Y.\nRemark 4: For tandem networks, the deﬁnition of condi- tional sufﬁciency is more general than global sufﬁciency. This is because if there exist a pair of statistics (T x (X), T y (Y)) that are globally sufﬁcient for θ, then T x (X) must be condi- tionally sufﬁcient for θ, conditioned on Y. Therefore, for the inference problem under the HCI model, one can also obtain a conditional sufﬁcient statistic using Theorem 1.\nSimilar to the deﬁnition of minimal sufﬁcient statistic [2], we can deﬁne the notion of minimal conditional sufﬁcient statistic as follows.\nDeﬁnition 2: A conditional sufﬁcient statistic T (X) is a minimal conditional sufﬁcient statistic if it is a function of any other conditional sufﬁcient statistic U (X).\nThe following theorem provides a meaningful way to ﬁnd minimal conditional sufﬁcient statistics.\nTheorem 5: Let X, Y be distributed according to p(x, y|θ). Suppose there exists a function T (x) such that for every two sample points x, ˆx, and y, the ratio f(x,y|θ) f(ˆx,y|θ) is constant as a function of θ if and only if T (x) = T (ˆx). Then T (X) is a minimal conditional sufﬁcient statistic for θ given Y.\nThe proof follows the same line of proof for Theorem 6.2.13 in [2].\nExample 3: Let {X i , Y i }, i = 1, · · · , n be independent and identically distributed (i.i.d) according to p (x, y|θ), where\n2 θ < x < θ + 1, θ < y < x, 0 otherwise.\nIt can be easily shown that no data reduction is possible using the marginal distribution, i.e., no meaningful locally suf- ﬁcient statistics can be found other than the data themselves. Note that X is uniformly distributed on the interval (y, θ +1), therefore, we have\n, y i < x i , (max i {x i } − 1) < θ. Thus, max i {X i } is a conditional sufﬁcient statistic for θ, conditioned on Y. Similarly, we can obtain that min i {Y i } is a conditional sufﬁcient statistic of Y, conditioned on the X sequence. This is consistent with the fact that (max i {X i }, min i {Y i }) is globally sufﬁcient given both X and Y.\nFor the point to point remote rate distortion problem, it was shown that sufﬁcient statistic based data reduction achieves the same rate distortion function as the original data [14]. This section studies the connection between the sufﬁciency principle and distributed source coding problems.\nConsider the lossless source coding problem in Fig. 3. An i.i.d. sequence of source pairs (X n , Y n ) are encoded separately with rates (R 1 , R 2 ) and the descriptions are sent to a decoder where only X n is to be recovered with asymp- totically vanishing probability of error. A rate pair (R 1 , R 2 ) is achievable if there exists a lossless source code with rates (R 1 , R 2 ). The rate region R is deﬁned as the closure of the set of all achievable rate pairs and was shown to be [15], [16],\nAssume T (Y ) is a sufﬁcient statistic for X, i.e., X −T (Y )− Y . Deﬁne\nR = {(R 1 , R 2 ) : R 1 ≥ H(X|U), R 2 ≥ I(T (Y ); U), X − T (Y ) − U},\nwhich is the rate region for encoding (X n , T n (Y n )) where T n (Y n ) is the i.i.d sequence T (Y i ), i = 1, · · · , n. The following theorem shows that encoding reduced data T n (Y n ) achieves the same rate region as encoding the original data.\nProof: It is straightforward to show R ⊇ R . To show R ⊆ R , let (R 1 , R 2 ) ∈ R, then there exists a U such that X − Y − U, R 1 ≥ H(X|U), R 2 ≥ I(Y ; U). Since\n(X, T (Y )) − Y − U and X − T (Y ) − Y , the Markov chain X − T (Y ) − Y − U holds. Therefore, R 1 ≥ H(X|U), R 2 ≥ I (Y ; U) ≥ I(T (Y ); U) by the data processing inequality. Thus, (R 1 , R 2 ) ∈ R .\nA direct consequence of Theorem 6 is that the corner point of the rate region (R 1 = H(X|Y ), R 2 = H(T (Y )) may be strictly smaller than (R 1 = H(X|Y ), R 2 = H(Y ). This observation was ﬁrst reported in [17]. Speciﬁcally, the corner point can be obtained by ﬁnding the smallest admissible R 2 when R 1 = H(X|Y ) and it was shown that [17]\ninf{R 2 : (H(X|Y ), R 2 ) ∈ R} = \t inf X−Y −U,X−U−Y I (Y ; U), = H(Φ X Y ).\nAs it turns out, the quantity Φ X Y is precisely the minimal sufﬁcient statistic of X given Y .\nConsider a model in Fig 4, which is the remote source coding with side information available at both the encoder and decoder. We will show that in this problem, the rate distortion function will not change by encoding a conditional sufﬁcient statistic T (X).\nLet (X, Y, Z) ∼ p(x, y, z) and d(z, ˆz) be a given dis- tortion function. Let (X n , Y n , Z n ) be i.i.d sequences drawn from (X, Y, Z). Upon receiving the sequences (X n , Y n ), the encoder generates a description of the sources with rate R and sends it to the decoder who has the side information Y n and wishes to reproduce Z n with distortion D. The rate distortion function R (D) is the inﬁmum of rate R such that there exist maps f n : X n × Y n → {1, · · · , 2 nR }, g n : Y n × {1, · · · , 2 nR } → ˆ Z n such that\nwhere the minimum is taken over all p (u|x, y) and functions ˆz = f(u, y) such that\nLet T (X) be a conditional sufﬁcient statistic for the remote source Z, conditioned on Y (i.e., Z − (T (X), Y ) − (X, Y )).\nwhere the minimum is taken over all p (u|t, y) and functions ˆz = f(u, y) such that\nR (D) is the rate distortion function when we have (T n (X n ), Y n ) instead of (X n , Y n ) at the encoder, where T n (X n ) is the i.i.d sequence T (X i ), i = 1, · · · , n.\nWe now show R (D) ≥ R (D). For any U that achieves R (D), since T (X) is a function of X, we have the Markov chain (T (X), Y ) − (X, Y ) − U, hence\nGiven that T (X) is a conditional sufﬁcient statistic for Z, we have the following\n⎛ ⎝\nwhere (5) comes from the deﬁnition of conditional sufﬁciency and (6) is true by deﬁning p (t, y, u) = x:T (x)=t p (x, y, u). This shows that for any p (u|x, y) and f(u, y) satisfying (3) there exist p (u|t, y) and f(u, y) such that (4) is satisﬁed. Thus, R (D) ≥ R (D).\nThis paper developed the sufﬁciency principle that guides local data reduction in networked inference with dependent observations for two classes of inference networks: parallel network and tandem network.\nFor the parallel network, a previously proposed hierarchical conditional independence model is used to obtain conditions\nsuch that local sufﬁciency implies global sufﬁciency. A co- operative spectrum sensing example is given to illustrate the usefulness of such an approach. For the tandem network, we introduced the notion of conditional sufﬁciency and developed related theories and tools.\nThe sufﬁciency principle for networked inference has appli- cations beyond that of decentralized inference. In particular, data reduction using suitable notions of sufﬁciency appears to incur no penalty on the rate region for various distributed source coding problem. There are potentially other distributed source coding problems where sufﬁciency based data reduction may also prove to be optimal."},"refs":[{"authors":[{"name":"R. A. Fisher"}],"title":{"text":"On the mathematical foundations of theoretical statistics"}},{"authors":[{"name":"G. Casell"},{"name":"R. L. Berge"}],"title":{"text":"Statistical Inference, Duxbury, Belmont, CA, 1990"}},{"authors":[{"name":"L. Lehman"},{"name":"G. Casell"}],"title":{"text":"E"}},{"authors":[{"name":"M. Cove"},{"name":"A. Thoma"}],"title":{"text":"T"}},{"authors":[{"name":"R. Viswanathan"}],"title":{"text":"A note on distributed estimation and sufﬁciency"}},{"authors":[{"name":"E. B. Hall"},{"name":"A. E. Wessel"},{"name":"G. L. Wise"}],"title":{"text":"Some aspects of fusion in estimation theory"}},{"authors":[{"name":"P. Ishwar"},{"name":"R. Puri"},{"name":"K. Ramchandran"},{"name":"S. S. Pradhan"}],"title":{"text":"On rate- constrained distributed estimation in unreliable sensor networks"}},{"authors":[{"name":"J. N. Tsitsiklis"}],"title":{"text":"Decentralized detection"}},{"authors":[{"name":"B. Chen"},{"name":"K. Willett"}],"title":{"text":"On the optimality of likelihood ratio test for local sensor decisions in the presence of non-ideal channels"}},{"authors":[{"name":"H. Chen"},{"name":"B. Chen"},{"name":"K. Varshney"}],"title":{"text":"Further results on the optimal- ity of likelihood ratio quantizer for distributed detection in non-ideal channels"}},{"authors":[{"name":"N. Tsitsiklis"},{"name":"M. Athans"}],"title":{"text":"On the complexity of decentralized decision making and detection problems"}},{"authors":[{"name":"H. Chen"},{"name":"B. Chen"},{"name":"K. Varshney"}],"title":{"text":"A new framework for distributed detection with conditionally dependent observations"}},{"authors":[{"name":"F. Peng"},{"name":"H. Chen"},{"name":"B. Chen"}],"title":{"text":"On energy detection for cooperative spectrum sensing"}},{"authors":[{"name":"K. Eswaran"},{"name":"M. Gastpar"}],"title":{"text":"Rate loss in the CEO problem"}},{"authors":[{"name":"R. F. Ahlswede"},{"name":"J. K¨oner"}],"title":{"text":"Source coding with side information and a converse for degraded broadcast channels "}},{"authors":[{"name":"A. D. Wyner"}],"title":{"text":"On source coding with side information at the decoder"}},{"authors":[{"name":"S. Kamath"},{"name":"V. Anantharam"}],"title":{"text":"A new dual to the G´acs-K¨orner common information deﬁned via the Gray-Wyner system"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566591.pdf"},"links":[{"id":"1569566381","weight":3},{"id":"1569566485","weight":3},{"id":"1569566725","weight":3},{"id":"1569567049","weight":3},{"id":"1569565067","weight":3},{"id":"1569566875","weight":3},{"id":"1569566683","weight":6},{"id":"1569566697","weight":3},{"id":"1569566597","weight":3},{"id":"1569566943","weight":6},{"id":"1569566571","weight":3},{"id":"1569564481","weight":3},{"id":"1569566415","weight":3},{"id":"1569565613","weight":3},{"id":"1569565355","weight":3},{"id":"1569565931","weight":3},{"id":"1569566373","weight":3},{"id":"1569551535","weight":3},{"id":"1569566207","weight":3},{"id":"1569566671","weight":3},{"id":"1569564233","weight":6},{"id":"1569563411","weight":3},{"id":"1569559541","weight":3},{"id":"1569565123","weight":3},{"id":"1569565859","weight":9},{"id":"1569565809","weight":3},{"id":"1569565455","weight":6},{"id":"1569564989","weight":6},{"id":"1569566523","weight":3},{"id":"1569566895","weight":3},{"id":"1569566239","weight":3},{"id":"1569566679","weight":3},{"id":"1569563981","weight":3},{"id":"1569566311","weight":3},{"id":"1569563307","weight":3},{"id":"1569558681","weight":3},{"id":"1569566511","weight":3},{"id":"1569565841","weight":3},{"id":"1569566531","weight":3},{"id":"1569561143","weight":6},{"id":"1569566845","weight":3},{"id":"1569566423","weight":3},{"id":"1569566437","weight":3},{"id":"1569562285","weight":3},{"id":"1569553537","weight":3},{"id":"1569565915","weight":3},{"id":"1569566885","weight":3},{"id":"1569566231","weight":3},{"id":"1569554881","weight":6},{"id":"1569566909","weight":3},{"id":"1569565087","weight":3},{"id":"1569564857","weight":3},{"id":"1569566809","weight":3},{"id":"1569566257","weight":3},{"id":"1569565033","weight":3},{"id":"1569566357","weight":3},{"id":"1569565055","weight":3},{"id":"1569565633","weight":3},{"id":"1569565219","weight":6},{"id":"1569566037","weight":3},{"id":"1569566593","weight":6},{"id":"1569566043","weight":12},{"id":"1569565029","weight":3},{"id":"1569565357","weight":6},{"id":"1569566191","weight":3},{"id":"1569566695","weight":6},{"id":"1569565909","weight":3},{"id":"1569566673","weight":6},{"id":"1569566297","weight":3},{"id":"1569566481","weight":3},{"id":"1569565961","weight":6},{"id":"1569565463","weight":3},{"id":"1569565439","weight":3},{"id":"1569563395","weight":6},{"id":"1569557275","weight":3},{"id":"1569565263","weight":3},{"id":"1569566129","weight":3},{"id":"1569566261","weight":3},{"id":"1569566711","weight":3},{"id":"1569565661","weight":12},{"id":"1569566887","weight":3},{"id":"1569566691","weight":6},{"id":"1569566651","weight":3},{"id":"1569566823","weight":3},{"id":"1569566595","weight":3},{"id":"1569566137","weight":3},{"id":"1569565375","weight":3},{"id":"1569566755","weight":3},{"id":"1569566813","weight":6},{"id":"1569565293","weight":3},{"id":"1569566771","weight":6},{"id":"1569564437","weight":3},{"id":"1569564861","weight":3},{"id":"1569565457","weight":3},{"id":"1569566487","weight":3},{"id":"1569566619","weight":3},{"id":"1569561185","weight":3},{"id":"1569566817","weight":6},{"id":"1569564923","weight":6},{"id":"1569566933","weight":3},{"id":"1569563919","weight":3},{"id":"1569565537","weight":3},{"id":"1569566457","weight":3},{"id":"1569564961","weight":3},{"id":"1569559251","weight":3},{"id":"1569567013","weight":6},{"id":"1569566807","weight":3},{"id":"1569550425","weight":3},{"id":"1569565165","weight":3},{"id":"1569565635","weight":6},{"id":"1569565113","weight":3},{"id":"1569564257","weight":3},{"id":"1569564141","weight":3},{"id":"1569566973","weight":3},{"id":"1569565031","weight":3},{"id":"1569563007","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S2.T8.2","endtime":"12:10","authors":"Ge Xu, Biao Chen","date":"1341229800000","papertitle":"The sufficiency principle for decentralized data reduction","starttime":"11:50","session":"S2.T8: Distributed Detection and Estimation","room":"Stratton (491)","paperid":"1569566591"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
