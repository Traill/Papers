{"id":"1569566375","paper":{"title":{"text":"Reﬁnement of the Sphere-Packing Bound"},"authors":[{"name":"Y¨ucel Altu˘g"},{"name":"Aaron B. Wagner"}],"abstr":{"text":"Abstract\u2014We provide a reﬁnement of the sphere-packing bound for constant composition codes over discrete memoryless channels that improves the pre-factor in front of the exponential term. The order of our pre-factor is O(N − 1 2 ( 1+ρ ∗ R )), where ρ ∗ R is related to the slope of the sphere-packing exponent and N is the blocklength."},"body":{"text":"Characterizing the interplay between the rate, blocklength, and error probability of optimal codes for a discrete memory- less channel (D.M.C.) is arguably one of the most fundamental problems in information theory. Although the investigation of this interplay dates back to the early days of the ﬁeld [1]\u2013 [9], it remains to be an active research area [10]\u2013[15]. This interplay can be characterized in several regimes. Probably the best-known regime is that of \u201cerror exponents,\u201d in which a rate below the capacity is ﬁxed and the goal is to ﬁnd the best possible rate of decay of the error probability. It has been shown that this error probability decays exponentially in the blocklength for most channels and this exponent is characterized for high rates [3]\u2013[6]. Conversely, one could ﬁx an error probability in (0, 1/2) and characterize the speed with which the rate converges to capacity [2], [13]. This is sometimes called the \u201cnormal approximation.\u201d Between these two regimes is the one in which error probability converges to zero and the rate simultaneously converges to the capacity. Here the goal is to characterize the speed of one convergence as a function of the other [14], [15]. We call these three asymptotics the \u201csmall error probability,\u201d \u201clarge error probability,\u201d and \u201cmedium error probability,\u201d regimes, respectively. It is worth noting that although the ultimate goal\u2014characterizing the interplay between rate, blocklength, and error probability\u2014is independent of the regime, the proof techniques for the three regimes are slightly different and the results for one regime cannot be directly obtained from the results for another.\nIn this paper, we focus on characterizing the error prob- ability in the small error probability regime, especially for moderate blocklengths and rates close to capacity. The existing bounds in this regime are not particularly close because, although the exponential factors in the upper and lower bounds match, the pre-factors in front of the exponent differ signiﬁ- cantly between the bounds. To be speciﬁc, until recently, the\ntightest pre-factor for the upper bound on the error probability was O(1), due to Fano [3] and Gallager [4]. The best pre- factor in the lower bound for constant composition codes was O(N −|X ||Y| ), due to Haroutunian [6], [9, Theorem 2.5.3], where |X | and |Y| are the cardinalities of the input and output alphabets, respectively. (The original sphere-packing bound of Shannon-Gallager-Berlekamp [5, Theorem 6] gave a pre-factor of O(e −\nN ).) Evidently there is a sizable gap between the orders of the pre-factors in the upper and lower bounds. This gap is especially signiﬁcant at rates close to capacity, where the exponent itself is small so the pre-factors play a larger role for ﬁnite blocklengths.\nRecently, the authors have been working to reduce the gap between the pre-factors. The recent paper [16] considers symmetric channels and reﬁnes the sphere-packing lower bound by proving a pre-factor of O(N − 1 2 (1+|E SP (R)|) ), where E SP (R) is the slope of the sphere-packing exponent at point R. The paper [17] considers a broad class of channels, which includes all positive channels with positive dispersion, and reﬁnes the random coding upper bound by proving a pre-factor of O(N − 1 2 ). These results provide signiﬁcant improvements over the existing bounds, and they approximately characterize the order of the pre-factor for rates close to the capacity for a large set of symmetric channels, because E SP (R) tends to zero as R approaches C. It is also worth noting that the optimal pre- factor for the binary symmetric channel is O(N − 1 2 (1+|E SP (R)|) ) for all rates above the critical rate, while for the binary erasure channel it is O(N − 1 2 ), which is shown by Elias [20].\nIn this paper, we generalize [16] to handle asymmetric channels. In particular, we prove a lower bound for constant composition codes with a pre-factor of O(N − 1 2 (1+ρ ∗ R ) ), where ρ ∗ R is related to the slope of the sphere-packing exponent at point R. Although the overall approach is similar to that of [16], the lack of symmetry in the channel makes the argument signiﬁcantly more involved. While some improved ﬁnite-N bounds could be extracted from the proofs in this paper, we defer the task of optimizing these bounds and numerically comparing them to the existing bounds for future work.\nTo place the results of this paper in context, it is instructive to consider an analogy with sums of i.i.d. random variables. The small, medium, and large error probability regimes of channel coding correspond to large deviations, moderate de- viations, and central limit theory, respectively, for i.i.d. sums. Continuing this analogy, the problem considered in this paper corresponds to the \u201cexact asymptotics\u201d problem [18], [19,\nTheorem 3.7.4] in large deviations. The goal of exact asymp- totics is to characterize the pre-factor of the exponentially vanishing term in the large deviations theorem. Bahadur and Rao [18] characterized this pre-factor, including the constant, under some regularity conditions. Their result, in the form stated by Dembo and Zeitouni [19, Theorem 3.7.4], is the following:\nTheorem 1.1: (Bahadur-Rao) Let µ n denote the law of ˆ S n = 1\nZ i , where Z i are i.i.d. real valued random vari- ables with logarithmic moment generating function Λ(λ) = log E[e λZ 1 ]. Consider the set A = [a, ∞), where a = Λ (η) for some positive η ∈ {λ : Λ(λ) < ∞} ◦ . If the law of X 1 is non-lattice, then lim n→∞ J n µ n (A) = 1, where\nFor lattice distributions, the order of the pre-factor remains the same but the constant is different. Thus the correct order of the pre-factor for i.i.d. sums is O(n − 1 2 ), and the channel coding problem will inherit this pre-factor. But we shall see that, when one reduces the error event of a code to an i.i.d. sum, the threshold a must vary slightly with n. This complicates the proof by preventing one from directly applying the Bahadur-Rao result (the random variables are also not i.i.d. but merely independent). More importantly, the slow variation of the threshold changes the order of the pre-factor slightly, to include the slope term mentioned above.\nThe remainder of the paper is devoted to the statement and then the proof of our result. Due to space restrictions, we only sketch the proof for positive channels, and the proofs of all of the lemmas to follow, but Lemma 3.3, will be omitted. Channels with 0\u2019s in the transition matrix can be handled by appropriately modifying the arguments.\nBoldface letters denote vectors, regular letters with sub- scripts denote individual elements of vectors. Furthermore, capital letters represent random variables and lowercase letters denote individual realizations of the corresponding random variable. Throughout the paper, all logarithms are base-e. For a ﬁnite set X , P(X ) denotes the set of all probability measures on X . Similarly, for two ﬁnite sets X and Y, P(Y|X ) denotes the set of all stochastic matrices from X to Y. For a set S, S c (resp. S ◦ ) denotes the complementary set (resp. interior).\nThroughout the paper, let W be a DMC satisfying 1 R ∞ < C. For any P ∈ P(X ), deﬁne\nThe following can be interpreted as the maximum absolute value subgradient of the sphere packing exponent at point R\nGiven any (N, R) code (f, ϕ), let e(f, ϕ) (resp. e m (f, ϕ)) denotes its maximal error probability (resp. error probability of the m-th message).\nTheorem 2.1: Consider any R ∈ (R ∞ , C) and ζ ∈ R + . Then, for any sufﬁciently large N , depending on R, W and ζ and any (N, R) constant composition code (f, ϕ),\nThere are at least three proofs of the sphere-packing bound in the literature: that of Shannon-Gallager-Berlekamp [5], Haroutunian [6] and Blahut [21]. Of these, Blahut\u2019s argument seems to be the most natural starting point for obtaining improved pre-factors, as it allows one to convert the error event of a code into an event involving a sum of i.i.d. random variables, to which one can apply the Bahadur-Rao result. The Shannon-Gallager-Berlekamp argument is similar to Blahut\u2019s in some ways, but it is less amenable to exact asymptotics. The Haroutunian argument is combinatorial and even farther removed from i.i.d. sums.\nBlahut\u2019s argument proceeds as follows. Assume R ∞ < R < C and let (f, ϕ) be an (N, R) code. Let {U m } m∈M denote the decision regions of ϕ corresponding to each message m ∈ M. Let Q ∈ P(Y) be an auxiliary output distribu- tion. Let W (y N |x N ) := N n=1 W (y n |x n ) and Q(y N ) :=\nQ(y n ). Owing to the fact that y N ∈Y N Q(y N ) = 1 and |M| ≥ e N R , there must be a message m ∈ M such that Q{U m } ≤ e −N R . Let x N := f (m) be the codeword for this message. It is clear that e(f, ϕ) ≥ e m (f, ϕ) = W U c m |x N .\nNow consider the hypothesis test over the set Y N in which W (·|x N ) is the null hypothesis (H 0 ) and the i.i.d. output distribution Q is the alternate hypothesis (H 1 ). One feasible test is to accept H 0 on U m and H 1 on U c m , resulting in type-I and type-II error probabilities of W (U c m |x N ) = e m (f, ϕ) and Q{U m }, respectively. Now let α Q,N (R) denote the minimum type-I error probability, optimized over all tests, subject to the constraint that the type-II error probability does not exceed e −N R . Evidently, we must have\nThe error exponent of this test can be expressed via the following deﬁnition. For any V ∈ P(Y|X ), P ∈ P(X ) and Q ∈ P(Y), deﬁne D(V ||Q|P ) := x∈X P (x)D(V (·|x)||Q).\nThen the optimal type-I error exponent can be shown to be (e.g. [21, Section V]) e SP (Q, P, R), where P is the empirical distribution of x N .\nNote that this exponent depends on the output distribution Q, which is to be selected. This distribution can be chosen to depend on P , since it can depend on the code, although allowing such dependence necessitates a restriction to constant composition codes. In the original argument [21, Section V], this freedom is not used, and Q depends on R (and the channel) but not P . Pre-factors aside, it is not clear that this choice yields the standard sphere-packing exponent when (4) is maximized over P . This is asserted to be the case in [21, Theorem 19] and [22, Theorem 10.1.4], but each of these proofs has a nontrivial gap 3 . Moreover, a numerical study indicates that for the Z-channel and for this choice of Q, E SP (R) < max P e SP (Q, P, R), for a broad range of rates. For symmetric channels, Q can indeed be chosen independently of P [16], and so the code need not be constant composition. But in the general case, it appears that some dependence is necessary if one hopes to obtain the sphere-packing exponent.\nOur choice of Q will depend on P and give the sphere- packing exponent. Thus, one of the ancillary contributions of this paper is to give a complete proof that the hypothesis testing reduction described can be used to obtain the sphere- packing exponent. In fact, using the hypothesis testing reduc- tion, we shall prove a lower bound on the error probability of any constant-composition code with composition P that has an exponent of E SP (R, P ); previously, the only proof of this fact used combinatorial techniques.\nIt is worth noting that the Shannon-Gallager-Berlekamp proof also involves the choice of an output distribution. Their choice of output distribution also depends on P , but it is deﬁned differently from ours. Our choice yields the E SP (R, P ) exponent, whereas Shannon-Gallager-Berlekamp only establish an exponent of E SP (R).\nIn order to describe our output distribution, we require the following technical results.\n \nfor all (ρ, Q) ∈ R + × P(Y). Also, for any R ∈ R + , deﬁne P R (X ) := {P ∈ P(X ) : E SP (R, P ) > 0}.\nLemma 3.1: Consider any R ∞ < R < C and P ∈ P R (X ). K R,P (·, ·) has a unique saddle-point, i.e. S(R, P )\u2014the set of saddle-points of K R,P (·, ·)\u2014is a singleton, with the saddle- value E SP (R, P ).\n, \t (5) Q ∗ R,· : P R (X ) → P(Y), s.t. Q ∗ R,P = S(R, P )| P(Y) . (6)\nObserve that owing to Lemma 3.1, both (5) and (6) are well-deﬁned. The distribution Q ∗ R,· in (6) will be our output distribution.\n(i) For any P ∈ P R (X ), both ρ ∗ R,P and Q ∗ R,P are positive. (ii) Both ρ ∗ R,· and Q ∗ R,· are continuous on P R (X ).\nFor any R ∞ < R < C and P ∈ P R (X ), let e SP (R, P, r) := e SP (Q ∗ P,R , P, r) and e SP (R, P ) := e SP (R, P, R).\nwhere (8) follows from the fact that Slater\u2019s condition holds, which can be seen by letting V (·|x) = Q ∗ R,P , (9) follows by solving the convex minimization problem and (10) follows from Lemma 3.1.\nFor any ν, R ∈ R + , deﬁne P R,ν (X ) := {P ∈ P(X ) : E SP (R, P ) ≥ ν}. Fix some R ∈ (R ∞ , C) and some sufﬁ- ciently small ν > 0 that only depends on W and R. Ap- plication of the hypothesis testing reduction of Section III-A to an (N, R) constant composition code (f, ϕ) with common composition 4 P ∈ P R,ν (X ) by using Q ∗ R,P as the auxiliary output distribution yields (recall (3))\nwhere α N (R) := α Q ∗ R,P ,N (R). On account of (11), in order to lower bound the maximal error probability of our code, it sufﬁces to evaluate α N (R).\nFix an arbitrary ∈ R + and let N := 1 2 + log N N and deﬁne R N := R − N . Note that for all sufﬁciently large N ∈ Z + , R ∞ < R N < C. Throughout, we consider such an N ∈ Z + . Also,\n(12) A N and A c N are the decision regions of the test, i.e. the test decides W (·|x N ) if y N ∈ A N and Q ∗ R,P if y N ∈ A c N . Let\nThe analysis of the events A N and A c N would be direct applications of Bahadur-Rao but for two complications 5 . To proceed, then, we must generalize the Bahadur-Rao result. To this end, we require the following technical results.\nDeﬁnition 3.3: Consider any R ∞ < R < C. For any P ∈ P R,ν (X ) and λ ∈ R, Λ i,P (λ) := x∈X P (x)Λ i,P,x (λ),\nW (Y |x) ] for all x ∈ X , for i ∈ {0, 1}. Moreover, for any λ ∈ [0, 1],\nNote that for any x ∈ X and P ∈ P R,ν (X ), Λ i,P,x (·) is inﬁnitely differentiable on R, which is clear from the deﬁnition. In particular, one can check that for all λ ∈ [0, 1]\nlog Q ∗ R,P (Y ) W (Y |x) . Moreover, one can see that for all λ ∈ [0, 1] and P ∈ P R,ν (X ), Λ 1,P,x (λ) = Λ 0,P,x (1 − λ),\nfor any x ∈ X , and hence, Λ 1,P (λ) = Λ 0,P (1 − λ), for any P ∈ P R,ν (X ). Therefore, for any λ ∈ [0, 1], one can compute the derivatives of Λ 1,P,x and Λ 1,P by referring to (15).\nBy exploiting Lemmas 3.1, 3.2 and 3.3, one can prove the following lemma.\nLemma 3.4: For any R ∈ (R ∞ , C) and P ∈ P R,ν (X ), (i) Λ 0,P (λ) > 0, ∀ λ ∈ [0, 1],.\nDeﬁnition 3.4: For any R ∈ (R ∞ , C), P ∈ P R,ν (X ), Λ ∗ 1,P (·) := sup λ ∈ R {(·)λ − Λ i,P (·)}, denotes the Fenchel- Legendre transform of Λ i,P , for i ∈ {0, 1}.\nLemma 3.5: Fix any R ∈ (R ∞ , C), P ∈ P R,ν (X ). Then, for all r ∈ (R ∞ , D(W ||Q ∗ R,P |P )),\n(ii) Λ ∗ 1,P (r − e SP (R, P, r)) = r, (iii) There exists a unique\nΛ 0,P (η(R, P, r)) = e SP (R, P, r) − r, for some 0 < d(ν, W, R) < d(ν, W, R) < 1.\nDeﬁnition 3.5: Consider any R ∈ (R ∞ , C). For any P ∈ P R,ν (X ) and λ ∈ [0, 1], m i,3,P (λ) := E P [m i,3,X (λ)], where\nNote that for all P ∈ P R,ν (X ), m 1,3,P (λ) = m 0,3,P (1−λ), for any λ ∈ [0, 1].\nLemma 3.6: Given any R ∈ (R ∞ , C), Λ 0,· (·), Λ 0,· (·) and m 0,3,· (·) are continuous on P R,ν (X ) × (0, 1].\nFinally, by letting H := [d(ν, W, R), d(ν, W, R)], deﬁne M (ν, W, R) \t := \t max (λ,P )∈H×P R,ν m 0,3,P (λ) Λ\n, V (ν, W, R) := max (λ,P )∈H×P R,ν Λ 0,P (λ), V (ν, W, R) :=\nmin (λ,P )∈H×P R,ν Λ 0,P (λ). Owing to the item (i) of Lemma 3.4 and Lemma 3.6, all of these quantities are well-deﬁned, ﬁnite and positive.\n2πcM (ν, W, R) with 6 c = 30/4. Note that K max ∈ R + . Also, let N ∈ Z + be sufﬁciently large, such that\nBy generalizing the arguments leading to the proof of Theorem 1.1 to handle non-identically distributed random variables in A N and the varying threshold, one can show that\n. Note that K only depends on W, R and ν.\nIf we let N ∈ Z + to be sufﬁciently large, so that KN /2 e > 1, then one can prove the following via the arguments similar to the ones leading to (16) and by employing the item (ii) of Lemma 3.5: β N ≥ KN /2 e e −N R > e −N R . Since our test is a log-likelihood ratio test, by violating the constraint we can only improve the optimal error performance, and hence (16)\nimplies that α N (R) ≥ K √ N exp{−N Λ ∗ 0,P (e SP (R, P, R N ) − R N )}, which, in turn, implies that (cf. (11))\nIn this ﬁnal section, we approximate the exponent in (17) to conclude the proof. To this end, we ﬁrst state the following result.\nLemma 3.7: Consider any R ∈ (R ∞ , C) and P ∈ P R,ν (X ),\nFor the sake of notational convenience, for any given R ∈ (R ∞ , C) and P ∈ P R,ν (X ), we let s ∗ (R, P, r) := − ∂e SP (R,P,r) ∂r \t for all r ∈ (R ∞ , D(W ||Q ∗ R,P |P )). By recalling the item (ii) of Lemma 3.4, the item (iii) of Lemma 3.2 and the item (i) of Lemma 3.7 imply that\nMoreover, one can also show that for any R ∈ (R ∞ , C) and P ∈ P R,ν (X ),\n(19) Using Taylor\u2019s theorem, item (i) of Lemma 3.5, Lemma 3.7, (18), (19) and the fact that e SP (R, P, ·) − (·) is strictly decreas- ing, one can check that\nfor some ¯ R ∈ (R N , R), for all sufﬁciently large N , uniformly over P ∈ P R,ν (X ), such that R N > R ∞ .\nMoreover, by using exactly the same arguments as above, but this time with a ﬁrst order Taylor series, along with Lemma 3.3 and the item (iii) of Lemma 3.5, one can check that (20) implies that\nfor all sufﬁciently large N that only depends on W, R and ν. Deﬁne P ∗ R (X ) := {P ∈ P(X ) : E SP (R, P ) = E SP (R)}. For\n||Q − P || 1 . For any d ∈ R + , P d (X ) := {P ∈ P R,ν (X ) : |P − P ∗ R (X )| ≥ d}. Finally,\nOne could check that P R,ν (X ) is compact, and hence (cf. item (ii) of Lemma 3.2) ρ ∗ R,· is uniformly continuous on this\nset. By exploiting this fact, one could choose some d( ) ∈ R + , such that\n(22) Since we have E SP (R) > max P ∈ P d( ) (X ) E SP (R, P ), it is\npossible to check that for all sufﬁciently large N , uniformly over P d( ) (X ), we have"},"refs":[{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A Mathematical Theory of Communication"}},{"authors":[{"name":"V. Strassen"}],"title":{"text":"Asymptotische Absch¨atzungen in Shannons Informa- tionstheorie"}},{"authors":[{"name":"R. M. Fan"}],"title":{"text":"Transmission of Information, A Statistical Theory of Communications "}},{"authors":[{"name":"R. G. Gallager"}],"title":{"text":"A Simple Derivation of the Coding Theorem and Some Applications"}},{"authors":[{"name":"C. E. Shannon"},{"name":"R. G. Gallager"},{"name":"E. R. Berlekamp"}],"title":{"text":"Lower Bounds to Error Probability for Coding on Discrete Memoryless Channels"}},{"authors":[{"name":"E. A. Haroutunian"}],"title":{"text":"Estimates of the Error Exponents for the Semi- Continuous Memoryless Channel"}},{"authors":[{"name":"R. G. Gallage"}],"title":{"text":"Information Theory and Reliable Communication"}},{"authors":[{"name":"F. Jeline"}],"title":{"text":"Probabilistic Information Theory"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems "}},{"authors":[{"name":"A. Valembois"},{"name":"M. Fossorier"}],"title":{"text":"Sphere-Packing Bounds Revisited for Moderate Block Length"}},{"authors":[{"name":"G. Wiechman"},{"name":"I. Sason"}],"title":{"text":"An Improved Sphere-Packing Bound for Finite-Length Codes Over Symmetric Memoryless Channels"}},{"authors":[{"name":"M. Hayashi"}],"title":{"text":"Information Spectrum Approach to Second\u2013Order Coding Rate in Channel Coding"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"H. V. Poor"},{"name":"S. Verd´u"}],"title":{"text":"Channel Coding Rate in the Finite Blocklength Regime"}},{"authors":[{"name":"Y. Altu˘g"},{"name":"A. B. Wagner"}],"title":{"text":"Moderate Deviation Analysis of Channel Coding: Discrete Memoryless Case"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"S. Verd´u"}],"title":{"text":"Channel Dispersion and Moderate Devi- ations Limits for Memoryless Channels"}},{"authors":[{"name":"Y. Altu˘g"},{"name":"A. B. Wagner"}],"title":{"text":"Reﬁnement of the Sphere Packing Bound for Symmetric Channels"}},{"authors":[{"name":"Y. Altu˘g"},{"name":"A. B. Wagner"}],"title":{"text":"Reﬁnement of the Random Coding Bound"}},{"authors":[{"name":"R. R. Bahadur"},{"name":"R. Ranga Rao"}],"title":{"text":"On Deviations of the Sample Mean"}},{"authors":[{"name":"A. Demb"},{"name":"O. Zeitoun"}],"title":{"text":"Large Deviations Techniques and Applica- tions, 2nd edition "}},{"authors":[{"name":"P. Elias"}],"title":{"text":"Coding For Two Noisy Channels"}},{"authors":[{"name":"R. E. Blahut"}],"title":{"text":"Hypothesis Testing and Information Theory"}},{"authors":[{"name":"R. E. Blahu"}],"title":{"text":"Principles and Practice of Information Theory"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566375.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T8.4","endtime":"12:50","authors":"Yucel Altuğ, Aaron Wagner","date":"1341577800000","papertitle":"Refinement of the Sphere-Packing Bound","starttime":"12:30","session":"S16.T8: Error Exponents","room":"Stratton (491)","paperid":"1569566375"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
