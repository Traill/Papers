{"id":"1569565165","paper":{"title":{"text":"Improved Upper Bounds on the Capacity of Binary Channels with Causal Adversaries"},"authors":[{"name":"Bikash Kumar Dey"},{"name":"Sidharth Jaggi"},{"name":"Michael Langberg"},{"name":"Anand D. Sarwate"}],"abstr":{"text":"Abstract\u2014In this work we consider the communication of information in the presence of a causal adversarial jammer. In the setting under study, a sender wishes to communicate a message to a receiver by transmitting a codeword x = (x 1 , . . . , x n ) bit-by- bit over a communication channel. The adversarial jammer can view the transmitted bits x i one at a time, and can change up to a p-fraction of them. However, the decisions of the jammer must be made in a causal manner. Namely, for each bit x i the jammer\u2019s decision on whether to corrupt it or not must depend only on x j for j ≤ i. This is in contrast to the \u201cclassical\u201d adversarial jammer which may base its decisions on its complete knowledge of x. Binary channels with causal adversarial jammers have seen recent studies in which both lower bounds and upper bounds on their capacity is derived. In this work, we present improved upper bounds on the capacity which hold for both deterministic and stochastic encoding schemes."},"body":{"text":"Alice wishes to transmit a message u to Bob over a binary- input binary-output channel. To do so, she encodes u into a length-n binary vector x and transmits it over the channel. However, the channel is controlled by a malicious adversary Calvin, who may observe the transmissions and attempt to jam communication by ﬂipping up to a fraction p of the transmitted bits. Since he must act in a causal manner, Calvin\u2019s decisions on whether or not to ﬂip bit x i must be a function solely of the bits x 1 , . . . , x i he has observed thus far. This communication scenario models jamming by an adversary who is limited in his jamming capability (perhaps due to limited processing power or limited transmit energy) and is causal. This causality assumption is reasonable for many communication channels, both wired and wireless, where Calvin is not co-located with Alice. Calvin can only corrupt a bit when it is transmitted (and thus the error is based on his view so far). To decode the transmitted message, Bob waits until all n bits have arrived.\nOur main contribution in this work is an impossibility result that helps make progress towards a better understanding of the communication rates achievable against a causal adversary. Speciﬁcally, we describe and analyze a particular jamming strategy of Calvin, and show that it (upper) bounds the rate\nof communication no matter what coding strategy Alice and Bob use. The jamming strategy, corresponding analysis and the resulting bound are all novel.\nMany of the following works deal with more general channels; we mostly restrict our discussion to the binary-input binary-output case.\nCoding theory model: A very strong class of adversarial channels is one where Calvin is omniscient \u2013 he knows Alice\u2019s entire codeword x prior to transmission and can tailor the pattern of up to pn bit-ﬂips to each speciﬁc transmission. This is the worst-case noise model studied in coding theory. For binary channels, characterizing the capacity has been an open problem for several decades. The best known upper bound is due to McEliece et al. [1] as the solution of an LP, and the best known achievable scheme corresponds to codes (GV codes) suggested by Gilbert and Varshamov [2], [3], which achieve a rate of 1 − H(2p). Improving either of these bounds would be a signiﬁcant breakthrough.\nInformation theory model: A much weaker class of adver- sarial channels is one where Calvin generates coin ﬂips in an i.i.d. manner. The original work of Shannon [4] effectively characterized the capacity of such channels. It follows that the capacity in Shannon\u2019s setting is strictly greater than that of the omniscient adversarial model.\nCausal adversarial model: The class of channels considered in this work, i.e., that of causal adversaries, fall in between these two extremes. In one direction this is because a causal adversary is certainly no stronger than an omniscient adver- sary, since he cannot tailor his jamming strategy to take into account Alice\u2019s future transmissions. Indeed, the work of [5] indicates that for 2p < H −1 (1/2) 0.11, rates strictly better than those achievable by GV codes against an omniscient adversary are achievable against a causal adversary. However, since it is still unknown whether GV codes are optimal against omniscient adversaries, it is unknown whether causal adver- saries are indeed strictly weaker than omniscient adversaries. In the other direction, prior work [6] demonstrates deﬁnitively that a causal adversary is strictly \u201cstronger\u201d than random noise \u2013 in particular, for all p such that H(p) < 4p (i.e., p > 0.157), the best rates achievable against a causal adversary are strictly bounded away from the rate of 1 − H(p) achievable against a random BSC(p). The results in [6] hold for deterministic\ncoding schemes, and the possibility of stochastic codes is left open. In stochastic codes, Alice may encode u to one of several possible codewords x ∈ {x(u, r)}, where r is a random source available to Alice but unknown to either Bob or Calvin. Stochastic codes can in some scenarios achieve rates greater than achievable via deterministic codes.\nIn this work, our contribution is two fold. We present strictly tighter upper bounds than those presented in [6], and in addition we show that our upper bounds hold for both deterministic and stochastic encoding.\nArbitrarily Varying Channels: Our model is a variant of the arbitrarily varying channel (AVC) model [7]. AVC models have been extended to include channels with constraints on the adversary (such as pn bit ﬂips) for cases where the adversary has no access to the codeword [8], or has access to the full codeword [9]. However, issues of causality and delay have only been studied in the context of random coding (i.e., in the presence of common randomness), but not for deterministic codes or stochastic encoding.\nDelayed adversaries: A reﬁnement of the causal adversary model, called the delayed adversary, was studied in [10] and [11]. In these models, the jammer\u2019s decision on whether to corrupt x i must depend only on x j for j ≤ i − dn for a delay parameter d ∈ [0, 1]. The case of d = 0 is exactly the standard causal setting, and that of d = 1 corresponds to the \u201coblivious adversary\u201d studied in [10]. The authors [11] showed that for a large class of channels, the capacity for delay d > 0 equals that of AVC model of [12].\nComputational aspects: In the context of encoding/decoding complexity, the works most relevant to ours are [13], [14] which address the computational aspects of coding in settings closely related to ours. In particular, [14] also use a sym- metrization argument to prove a converse for p > 1/4.\nOur improved bounds are given in the following theorem, and are depicted with comparisons to previous bounds in Figure 1. As mentioned above, our bounds improve over previous ones and hold for both deterministic and stochastic codes. Let p ∈ [0, 1/4]. Consider any ¯ p ∈ [0, p]. Let α(p, ¯ p) = 1 − 4(p − ¯ p). In what follows, C(p) is the capacity of the causal channel under study. For precise deﬁnitions and model see Section II.\nIt can be shown that the optimum ¯ p is approximately min{p, (1 − 4p)/8.4445}. Namely, for p > 0.0804, the capac- ity C(p) is bounded away from 1 − H(p) and for p ≤ 0.0804 we have ¯ p = p and our bound equals 1 − H(p).\nTo prove Theorem 1 we must present a strategy for Calvin that does not allow communication at rate higher than C(p) (no matter which encoding/decoding scheme is used by Alice and Bob). Speciﬁcally, the strategy we present will allow Calvin to enforce a constant probability of error bounded\naway from zero whenever Alice and Bob communicate at rate higher than C(p). Roughly speaking, Calvin uses a two-phase babble-and-push strategy. In the ﬁrst phase of (p) channel uses, Calvin \u201cbabbles\u201d by behaving like a BSC(¯ p) for some ¯ p chosen as a function of p. In the second phase of n − (p) channel uses, Calvin randomly selects a codeword from Alice and Bob\u2019s codebook which is consistent with what Bob has received so far. Calvin then \u201cpushes\u201d the remaining part of Alice\u2019s codeword towards his selected codeword.\nOur approach improves on previous results [6] in two ways. Firstly, we propose a stronger attack for Calvin based on increasing the uncertainty of Bob during the \u201cbabble\u201d phase, as opposed to the \u201cwait\u201d strategy of [6]. Secondly, we show that this attack is successful even when Alice and Bob use stochastic encoding. Hence these bounds hold for general codes, rather than prior work in [6], which were shown to hold in a strong sense only for codes in which each message u corresponded to a unique x(u) deterministically chosen by Alice.\nFor any positive integer k, let [k] = {1, 2, . . . , k}. We let U = [2 nR ] denote Alice\u2019s message set, and U denote the message random variable uniformly distributed in U. A deterministic code of rate R and blocklength n is a pair of maps C d = (Φ, Ψ) where Φ : U → X n and Ψ : Y n → U are deterministic maps. The map Φ is called the encoder and the map Ψ is called the decoder.\nA code with stochastic encoding and decoding of rate R and blocklength n is a pair of maps C s = (Φ, Ψ) where Φ : U → X n and Ψ : Y n → U are probabilistic maps. The random map Φ gives a probability distribution ρ(·|u) on X n for every u ∈ U. The mapping Ψ(y) is a random variable taking values from U . The encoding Φ is equivalently represented by ﬁrst picking a random variable R from a set R (with |R| ≤ |X | n )\naccording to a conditional distribution ρ R |U (.|u), and then using a deterministic encoder map Φ : U ×R → X n . Note that our deﬁnition does not preclude there existing pairs (u, r) and (u , r ) such that Φ(u, r) = Φ(u , r ). As we are addressing upper bounds on the capacity C(p) in this work, it is crucial to prove our results in the stochastic setting above.\nA causal adversarial strategy of blocklength n is a sequence of random maps Adv = {f (i) C : i ∈ [n]} where f (i) C : X i → E takes a code C and for each channel use i ∈ [n] uses the past and current inputs (x 1 , x 2 , . . . , x i ) to choose an action e i = f (i) C (x 1 , . . . , x i ) ∈ E at time i, with resulting output y i = x i +e i . In our setting E = {0, 1}. The strategy obeys constraint p if the Hamming weight e = n i=1 e i of e = e 1 , . . . , e n is at most pn almost surely over the randomness in the message, encoder, and strategy. For a given adversarial strategy and an input codeword x, the strategy produces a (possibly random) e and the output is y = x ⊕ e. Let P Adv (y|x) denote the probability of an output y given an input x under the strategy Adv . When the blocklength is understood from the context, let Adv (p) denote all adversarial strategies obeying constraint p.\nThe (average) probability of error for a code with stochastic encoding and decoding is given by\nwhere the probability P (Ψ(y) = u) is over any randomness in the decoder. We can interpret the errors as the error in expec- tation over Alice choosing a message U = u and a codeword x = Φ(u, r) according to the conditional distribution ρ(x|u).\nA rate R is achievable against a causal adversary under average error if for every δ > 0 there exist inﬁnitely many block lengths {n i }, such that for each n i there is an n i block length (stochastic) code of rate at least R and average probability of error at most δ. The supremum of all achievable rates is the capacity. We denote by C(p) the capacity of the channel corresponding to adversaries parametrized by p.\nConsider a code of blocklength n, rate R and error probabil- ity δ. We can, without loss of generality (w.l.o.g.), assume that the encoding probabilities {ρ(x|u) : x ∈ {0, 1} n , u ∈ [2 nR ]} are rational. To see why this is the case, note that for any small η > 0 we can ﬁnd rational numbers { ˜ ρ(x|u)} such that ρ(x|u) − η ≤ ˜ ρ(x|u) ≤ ρ(x|u). Now consider a code with encoding probabilities q(x|u) = ˜ ρ(x|u) for x = 0 and assign the remaining probability to 0. Under the same decoder, this code has error probability at most δ + 2 n+nR η, but since η was arbitrary, the error is at most 2δ.\nNow, for a given stochastic code, let N be the least common multiple of the denominators of ρ(x|u) for all x, u. We can imagine each codeword x of u as N ρ(x|u) copies of the same codeword with conditional probability 1/N each. So we can equivalently associate a random variable R with |R| = N s.t. the conditional distribution ρ R |U (·|u) is uniform, and the encoding map Φ(u, ·) is not necessarily one-one. Since we\nconsider uniform message distribution, henceforth, w.l.o.g., we assume that the joint distribution ρ U ,R (·, ·) is uniform.\nLet p ∈ [0, 1/4] and let ¯ p ≤ p. Let ε > 0. In what follows we prove that the rate of communication over the causal adversarial channel (with parameter p) is bounded by R = C + ε where C = α(p, ¯ p) 1 − H ¯ p α(p, ¯ p) \t and α(p, ¯ p) = 1 − 4(p − ¯ p) as deﬁned in Theorem 1. Namely, for any sufﬁciently large block length n, and any (n-block stochastic) code C s = (Φ, Ψ) shared by Alice and Bob, there exists an adversarial jammer Adv that can impose a constant decoding error. The decoding error we will obtain will depend on ε > 0.\nThe proof for p = ¯ p follows from previous works as described in the Introduction. Thus, we assume that p − ¯ p > 0 and that ε < 2(p − ¯ p). In what follows, we ﬁx a block length n (which will at times be assumed to be sufﬁciently large with respect to 1/ε).\nAdversarial strategy: Our converse bound is based on a particular two-phase adversarial strategy for Calvin that we call \u201cbabble-and-push.\u201d\nLet = (α + ε/2)n and without loss of generality assume ∈ N. For a vector z of length n, let z 1 = (z 1 , z 2 , . . . , z )\n\u2022 (Babble) Calvin chooses a random subset Γ of ¯ pn indices uniformly from all (¯ pn)-subsets of {1, 2, . . . , }. For i ∈ Γ, Calvin ﬂips bit x i ; that is, for i ∈ {1, 2, . . . , }, e i = 1 for i ∈ Γ and e i = 0 for i / ∈ Γ. Let y i = x i + e i . Here x 1 , . . . , x n is the transmitted codeword and y 1 , . . . , y n is the word received by Bob.\n\u2022 (Push) Calvin constructs the set of (u, r) that have encod- ings x(u, r) = Φ(u, r) that are close to y 1 = y 1 , . . . , y . Namely, Calvin constructs the set\nB y 1 = {(u, r) : d H (y 1 , x 1 (u, r)) = ¯ pn}, \t (2) and selects an element (u , r ) ∈ B y 1 uniformly at ran- dom. Calvin then considers the corresponding codeword x = Φ(u , r ). Given the selected x , for i > , if x i = x i , Calvin sets e i equiprobably to 0 or 1 until\ne j = pn or i = n. Note that, under our assumption (w.l.o.g.) of uniform ρ U ,R , the a posteriori distribution of Alice\u2019s choice (u, r) given y 1 is also uniform in B y 1 .\nAnalysis: Let Adv denote the babble-and-push adversarial strategy. We start by proving the following technical lemma that we use in our proof.\nProof: Fix i ≤ m and a set v 1 , v 2 , . . . , v i ∈ V. Let A = {v 1 , . . . , v i } and let W = 1(V ∈ A). We can write\nthe distribution of V as a mixture: Pr [V = v] =\nSince conditioning reduces entropy and the support of V conditioned on W is at most i, we have\nTo prove the upper bound, we now present a series of claims. Let X denote the random variable corresponding to Alice\u2019s input codeword and let Y be the output of the channel. Thus X 1 ∈ {0, 1} is Alice\u2019s input during the \u201cbabble\u201d phase of length and X 2 is her input during the \u201cpush\u201d phase; the randomness comes from the message U and the stochastic encoding. Similarly, Y 1 is the random variable corresponding to the bits received by Bob during the \u201cbabble\u201d phase, and Y 2 the n− bits of the \u201cpush\u201d phase. Let Adv denote the babble-and-push adversarial strategy. Let A 0 = {y 1 : H(U|Y 1 = y 1 ) ≥ nε/4}, where the entropy H(U|Y 1 = y 1 ) is measured over the randomness of the encoder and the randomness of Calvin\u2019s action in the \u201cbabble\u201d phase, and let E 0 = {Y 1 ∈ A 0 }.\nProof: By the data processing inequality, and the choice of Calvin\u2019s strategy, we have\nThus the expected value of H(U|Y 1 = y 1 ) over y 1 is at least nε/2. The maximum value of H(U|Y 1 = y 1 ) is nR, so with probability at least ε/2, we have H(U|Y 1 = y 1 ) ≥ nε/4 (via the Markov inequality).\nNow consider drawing m pairs (U i , R i ) from B y 1 i.i.d. ∼ ρ U ,R|y 1 (which happens to be uniform). Note that the marginal distribution of U i is also i.i.d. ∼ ρ U |y 1 , which is not necessarily uniform. Let\nProof: The proof follows from Claim 3 and Lemma 2 by using λ = nε/4 and the fact that there are at most 2 n messages. The bound then becomes (ε/4 − m/n) m−1 . For ﬁxed m there exists an n sufﬁciently large so that m/n ≤ ε/8.\nLet U and X denote the random choice of Calvin\u2019s message and codeword in the push phase. The events E 2 = {U = U} and E 3 = {d H (X 2 , X 2 ) ≤ 2(p − ¯ p)n − εn/8} must occur with probability bounded away from 0. The ﬁrst event is that Calvin chooses a different message than Alice and the second is that he chooses a codeword that is close enough to Alice\u2019s.\nProof: Conditioned on E 0 , the realization y 1 satisﬁes H(U|Y 1 = y 1 ) ≥ εn/4. We ﬁrst use Claim 4 to lower bound the probability that E 2 holds. First consider randomly sampling a set of pairs S = {(u i , r i ) : i ∈ [m]} uniformly from B y 1 , and let X i be the codeword for (u i , r i ).\nClaim 4 shows that with probability at least (ε/8) m−1 , all the messages in S are distinct and therefore the code- words are distinct as well. In particular, this shows that P Adv (E 2 | E 0 ) ≥ (ε/8). Turning to E 3 , Plotkin\u2019s bound [15] shows that there do not exist binary error-correcting codes of blocklength n − and minimum distance d with more than \t 2d 2d−(n− ) codewords. Setting m = 17/ε, this bound implies that with probability at least (ε/8) m−1 there must exist codewords x, x corresponding to (u, r) and (u , r ) in S, respectively, within a distance d that satisﬁes 17 ε = 2d 2d−(n− ) . Solving for d and using = (1 − 4(p − ¯ p) + ε/2)n shows that d satisﬁes d < ∆ def = 2(p − ¯ p)n − εn/8\nLet γ be the fraction of pairs (u, r) and (u , r ) in B y 1 that satisfy E 2 and E 3 . We would like to lower bound γ. A union bound shows that the probability over the selection of S gives the upper bound P X i ,X j ∈S {d H (X i , X j ) < ∆} ≤ m 2 γ. However, the earlier argument shows that by selecting m = 17/ε pairs in S, we have a lower bound of (ε/8) m−1 . Therefore\nThe next step is to show that Calvin does not \u201crun out\u201d of bit ﬂips during the second \u201cpush\u201d phase of his attack. This follows directly from Sanov\u2019s Theorem [16]. Let\nProof: Let (u, r) denote the message and randomness of Alice, y 1 be the received codeword in the babble phase, and (u , r ) be the message and randomness chosen by Calvin for the push phase. Let ρ(y 1 , u, r, u , r ) be the joint distribution of these variables under Alice\u2019s uniform choice of (u, r) and Calvin\u2019s attack. For each y, let ρ(y|y 1 , u, r, u , r ) be the conditional distribution of y under Calvin\u2019s attack.\nLet F be the set of tuples (y 1 , u, r, u , r ) satisfying events E 0 , E 2 , and E 3 . Claims 3 and 5 show that ρ(F ) ≥ (ε/2) · ε O(1/ε) . For (y 1 , u, r, u , r ) ∈ F , we have that u = u , and that x 2 (u, r) and x 2 (u , r ) are sufﬁciently close.\nLet G be the set of y 2 such that E 4 (bounded in Claim 6) is satisﬁed for x 2 (u, r). Note that by the symmetry of Calvin\u2019s attack in the push phase, for (y 1 , u, r, u , r ) ∈ F , E 4 is also satisﬁed for x 2 (u , r ). Indeed, for y 2 ∈ G, the conditional distribution is symmetric:\nρ(y|y 1 , u, r, u , r ) = ρ(y|y 1 , u , r , u, r). \t (8) Then for (y 1 , u, r, u , r ) ∈ F , by Claim 6,\nNow, returning to the overall error probability, let ρ(y 1 ) be the unconditional probability of Bob receiving y 1 in the babble phase, where the probability is taken over Alice\u2019s uniform choice of (u, r) and Calvin\u2019s random babble e 1 . We rewrite the joint distribution as\n= ρ(y 1 , u , r , u, r). Thus, 2¯ ε ≥\n(P(Ψ(y 1 , y 2 ) = u) + P(Ψ(y 1 , y 2 ) = u )) ≥\nOur analysis implies a reﬁned statement of Theorem 1. Namely, let c be a sufﬁciently large constant. For any block length n, any ε > 0 and any encoding/decoding scheme of Alice and Bob of rate C(p) + c n + ε, Calvin can cause a decoding error of at least ε O(1/ε) .\nIn this work we present a novel upper bound on the rates achievable against a causal adversary, which strictly improves on prior upper bounds known against such adversaries over binary channels. Further, we demonstrate that the upper bound presented herein holds against arbitrary codes, rather than simply against deterministic codes, as is common in the coding theory literature. Since our analysis pertains to adversarial jam- ming rather than random noise, the proof techniques presented may be of independent interest in the more general setting of AVC\u2019s."},"refs":[{"authors":[{"name":"R. J. McEliece"},{"name":"E. R. Rodemich"},{"name":"H. Rumsey"},{"name":"L. R. Welch"}],"title":{"text":"New upper bounds on the rate of a code via the Delsarte-MacWilliams inequalities"}},{"authors":[{"name":"E. N. Gilbert"}],"title":{"text":"A comparison of signalling alphabets"}},{"authors":[{"name":"R. R. Varshamov"}],"title":{"text":"Estimate of the number of signals in error correcting codes"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[{"name":"H. Ishay"},{"name":"M. Langberg"}],"title":{"text":"Beating the Gilbert-Varshamov bound for online channels"}},{"authors":[{"name":"M. Langberg"},{"name":"S. Jaggi"},{"name":"B. Dey"}],"title":{"text":"Binary causal-adversary channels"}},{"authors":[{"name":"D. Blackwell"},{"name":"L. Breiman"},{"name":"A. Thomasian"}],"title":{"text":"The capacities of certain channel classes under random coding"}},{"authors":[{"name":"I. Csisz´ar"},{"name":"P. Narayan"}],"title":{"text":"The capacity of the arbitrarily varying chan- nel revisited : Positivity, constraints"}},{"authors":[{"name":"A. D. Sarwate"},{"name":"M. Gastpar"}],"title":{"text":"Rateless codes for AVC models"}},{"authors":[{"name":"M. Langberg"}],"title":{"text":"Oblivious channels and their capacity"}},{"authors":[{"name":"M. Langberg"},{"name":"S. Jaggi"},{"name":"B. Dey"}],"title":{"text":"Coding against delayed ad- versaries"}},{"authors":[{"name":"I. Csisz´ar"},{"name":"P. Narayan"}],"title":{"text":"Arbitrarily varying channels with constrained inputs and states"}},{"authors":[{"name":"A. Smith"}],"title":{"text":"Scrambling adversarial errors using few random bits, optimal information reconciliation, and better private codes"}},{"authors":[{"name":"V. Guruswami"},{"name":"A. Smith"}],"title":{"text":"Codes for computationally sim- ple channels: Explicit constructions with optimal rate"}},{"authors":[{"name":"A. E. Brouwer"}],"title":{"text":"Bounds on the size of linear codes"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of information theory, 2nd edition"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565165.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S4.T7.1","endtime":"17:00","authors":"Bikash K Dey, Sidharth Jaggi, Michael Langberg, Anand D. Sarwate","date":"1341247200000","papertitle":"Improved Upper Bounds on the Capacity of Binary Channels with Causal Adversaries","starttime":"16:40","session":"S4.T7: Capacity of Finite-Alphabet Channels","room":"Stratton (407)","paperid":"1569565165"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
