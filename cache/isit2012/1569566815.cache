{"id":"1569566815","paper":{"title":{"text":"ON SIMPLE ONE-CLASS CLASSIFICATION METHODS"},"authors":[{"name":"Zineb Noumir"},{"name":"Paul Honeine"},{"name":"C´edric Richard"}],"abstr":{"text":"The one-class classiﬁcation has been successfully ap- plied in many communication, signal processing, and ma- chine learning tasks. This problem, as deﬁned by the one- class SVM approach, consists in identifying a sphere enclos- ing all (or the most) of the data. The classical strategy to solve the problem considers a simultaneous estimation of both the center and the radius of the sphere. In this paper, we study the impact of separating the estimation problem. It turns out that simple one-class classiﬁcation methods can be easily derived, by considering a least-squares formulation. The proposed framework allows us to derive some theoretical results, such as an upper bound on the probability of false detection. The relevance of this work is illustrated on well-known datasets."},"body":{"text":"The one-class classiﬁcation machines has become a very ac- tive research domain in machine learning [1, 2], providing a detection rule based on recent advances in learning theory. In one-class classiﬁcation, the problem consists in covering a single target class of samples, represented by a training set, and separate it from any novel sample not belonging to the same class, i.e., an outlier sample. It has been successfully applied in many novelty detection and classiﬁcation tasks, including communication network performance [3], wireless sensor networks [4], forensic science [5], detection of hand- written digits [6] and objet recognition [7], only to name a few. Moreover, it has been extended naturally to binary and multiclass classiﬁcation tasks, by applying a single one-class classiﬁer to each class and subsequently combining the deci- sion rules [8].\nSince only a single class is identiﬁed, it is essentially a data domain description or a class density estimation prob- lem, while it provides a novelty detection rule. Different methods to solve the one-class problem have been developed, initiated from the so-called one-class support vector machines (SVM) [9, 2]. The one-class classiﬁcation task consists in identifying a sphere of minimum volume that englobes all (or most of) the training data, by estimating jointly its center and\nits radius. These methods exploit many features from con- ventional SVM [10], including a nonlinear extension thanks to the concept of reproducing kernels. They also inherit the robustness to outliers in the training set, by providing a sparse solution of the center. This sparse solution explores a small fraction of the training samples, called support vectors (SVs), and lying outside or on the sphere.\nIn one-class SVM as deﬁned in [9, 2], the resulting convex optimization problem is often solved using a quadratic pro- gramming technique. Several efforts have been made in order to derive one-class classiﬁcation machines with low compu- tational complexity [11]. In the same sense as least-squares SVM is derived from the classical SVM method [12, 13], some attempts have been made to derive from the one-class SVM a least-squares variant, such as in [14]. However, un- like the former, the latter do not have a decision function, thus inappropriate for novelty detection.\nIn this paper, we propose to solve the one-class problem by decoupling the estimation of the center and the radius of the sphere englobing all (or most of) the training samples. In the same spirit as the classical one-class SVM machines, we consider a sparse solution with SVs lying outside or on the sphere. It turns our that the optimal sparse solution can be de- ﬁned using a least-squares optimization problem, thus leading to a low computational complexity problem. This framework allows us to derive some theoretical results. We give an upper bound on the probability of false detection, i.e., probability that a new sample is outside the sphere deﬁned by the sparse solution.\nAs opposed to the jointly optimization of the center and radius by the classical one-class SVM approach, our strategy decouples the estimation problem, thus provides a sub-optimal solution. Consequently, the proposed approach should degrade the performance. In practice, we found that the performance is essentially equivalent to the classical tech- nique, while operating a dramatic speed-up. This is illustrated on experiments from a well-known benchmark for one-class machines [11].\nThe rest of the paper is organized as follows. Section 2 outlines the classical one-class SVM. We describe our ap- proach in Section 3, and derive theoretical results in Section\n4. Section 5 illustrates the relevance of our approach on real datasets. Conclusion and further directions are given in Sec- tion 6.\nThanks to the concept of reproducing kernels [15], a (pos- itive semi-deﬁnite) kernel function κ(·, ·) deﬁnes a nonlin- ear transformation Φ(·) of the input space into some feature space. A sphere deﬁned in the latter corresponds (is pre- imaged [16]) to a nonlinear characteristics in the input space. It turns our that only the inner product is often required, which can be evaluated using a kernel function, Φ(x i ), Φ(x j ) = κ(x i , x j ) for any x i , x j from the input space X .\nThe one-class SVM was initially derived in [2] for the estimation of the support of a distribution with the ν-SVM, and in [9] for novelty detection with the so-called \u201csupport vector data description\u201d. The principle idea is to ﬁnd a sphere, of minimum volume, containing all the training samples. This sphere, described by its center c and its radius r, is obtained by solving the constrained optimization problem\nWhile the above constraint may be too restrictive, one may tolerate a small fraction of the samples to be outside the sphere. This yields robustness, in the sense that it is less sen- sitive to the presence of outliers in the training dataset. For this purpose, let ν be a positive parameter that speciﬁes the tradeoff between the sphere volume and the number of out- liers. Then the problem becomes the estimation of c, r, and a set of non-negative slack variables ζ 1 , ζ 2 , . . . , ζ n :\nBy introducing the Karush-Kuhn-Tucker (KKT) optimality conditions, we get\nIn accordance with the KKT conditions, each sample x i can be classiﬁed into three categories: α i = 0 corresponds to a\nsample lying inside the sphere, samples with 0 < α i < 1 νn lie on the sphere boundary, and samples with α i = 1 νn lie outside the sphere, i.e., are outliers. The samples with non-zero α i are called support vectors (SVs) since they are sufﬁcient to describe the center as deﬁned in expression (4). In practice, only a very small fraction of the data are SV. Let I sv be the set of indices associated to SV, namely\nα i = 0 if i ∈ I sv ; α i = 0 otherwise.\nFinally, the optimal radius is obtained from any SV lying on the boundary, namely any x i with 0 < α i < 1 νn , since in this case Φ(x i ) − c = r. This is equivalent to\nTherefore, the decision rule that any new sample x is not an outlier is given as Φ(x) − c < r, where the distance is computed by using\nBack to basics, the center (or empirical ﬁrst moment) of a set of samples is deﬁned by\n1 n\nand the radius of the sphere englobing all (or most of ) the samples can be easily considered, where the distance is eval- uated using (3) where α i = 1/n for all i = 1, 2, . . . , n and I sv = {1, 2, . . . , n}. While the sphere deﬁned by the above full-model is extremely sensitive to outliers, one may consider a sparse solution by incorporating a small number of relevant samples in the model. This is essentially the spirit of the clas- sical one-class SVM, which estimates jointly the center and the radius, by identifying the SVs. Our approach towards a sparse solution is based on three steps:\n\u2022 Identify the SVs as the farthest samples from the center; \u2022 Estimate accordingly the sparse model parameters.\nThe classical one-class SVM method provides a sparse model for the center, where only samples outside or lying on the sphere are SVs. Inspired by this result, we consider in our approach the distance criterion to identify this subset.\nThe set of SVs can be identiﬁed by considering the dis- tance of each sample to the center, namely\nwhere the number of SVs is ﬁxed in advance. Once the set {x i | i ∈ I} is determined, the radius is given as\nConsider the error of approximating c n with the sparse model c I , c n − c I , which indicates the wellness of such approx- imation using a small subset of the training data. The coefﬁ- cients in (5) are estimated by minimizing this error, with\n1 n\nwhere α is a column vector of the optimal coefﬁcients α k \u2019s for k ∈ I. Taking the derivative of this cost function with re- spect to each α k , namely −2 Φ(x k ), c n − i∈I α i Φ(x i ) , and setting it to zero, we get\n1 n\nwhere K is the kernel matrix, with entries κ(x i , x j ) for i, j ∈ I and κ is a column vector with entries 1 n n i=1 κ(x k , x i ) for k ∈ I. To make this problem well-posed in practice, we include a regularization parameter ν, namely α = (K + νI) −1 κ , where I is the identity matrix of appropriate size. The error of approximating the center with the above solution is\nWhile the box constraint on the coefﬁcients requires advanced optimization techniques, it is easy to satisfy the equality con- straint (see (2)). The constrained optimization problem be- comes\n1 n\nwhere 1 is a column vector of 1\u2019s. By using the Lagrangian multipliers, we obtain\nwhere α is the unconstrained solution, as given in (7). One may also include a regularization term, as above.\nIndependently of the algorithm, one is considering a set of samples in order to estimate the true expectation. Let\nbe the true expectation, where P (x) is the probability dis- tribution generating the samples x 1 , x 2 , . . . , x n . From these samples, one can give an estimate of c ∞ by using the em- pirical ﬁrst moment c n , as deﬁned in expression (4). The accuracy of such approximation is\nBased on the Hoeffding\u2019s inequality, it is shown in [17] (see also [18]) that, with probability at least 1 − δ over the choice of a random set of n samples, we have\nBy the symmetry of the i.i.d assumption, we can bound the probability that a new sample x, generated from the same probability distribution, is beyond the boundary deﬁned by a one-class classiﬁcation method, as given by the following proposition:\nProposition 1. Consider the sphere centered on c I with ra- dius max i=1,...,n Φ(x i ) − c I + 2ǫ 0 + 2 c n − c I . Then, with probability at least 1 − δ over the choice of a random set of n samples, we can bound the probability that a new sample x is outside this sphere, with\nProof. To show this, we consider Φ(x) − c I and apply the triangle inequality twice, we get\nwhere the ﬁrst inequality follows from approximating the full- model center by a subset of samples, while the second in- equality from estimating the expected center by a ﬁnite set of n samples. Equivalently, we have for any x i :\nΦ(x i ) − c I + 2ǫ 0 + 2 c n − c I ≤ P Φ(x) − c ∞ > max i=1,...,n Φ(x i ) − c ∞\nwhere the ﬁrst inequality follows from the above inequalities, and the last inequality is due to the symmetry of the i.i.d as- sumption, considering n + 1 samples drawn from the same distribution.\nAs a special case of this proposition, consider the full- model for the empirical center, namely I = {1, 2, . . . , n}. In this case we get the relation given in [17, Chapter 5]:\nWe can extend this result to the solution deﬁned by con- sidering that the samples deﬁned by indices I are outliers, thus not inside the sphere. The following proposition can be easily proven using the same steps as in the proof of Proposi- tion 1.\nProposition 2. Consider the same setting as in Proposition 1, where the indices in I deﬁne the outliers with |I| the num- ber of outliers. Then, with probability at least 1 − δ over the choice of a random set of n samples, we can bound the prob- ability that a new sample x is outside the sphere excluding outliers, with\nIt is worth noting that in both propositions, the error c n − c I is minimized by our approach, as given by (8).\nSince there are few benchmark datasets for one-class clas- siﬁcation methods, multiclass tasks are often considered. A multiclass classiﬁcation task can be tackled by using one- class machines: each class is deﬁned by a one-class classiﬁer, and subsequently we get the decision rule by combining these classiﬁers. In practice, the model parameters are estimated by considering a subset of the target class ( n train samples), and tested over the remaining samples ( n test ), some from the tar- get class and all the samples from the other classes.\nTo illustrate the relevance of the proposed approach, we have tested the proposed methods on real datasets well-known in the literature of one-class machines [19]: the IRIS dataset with 150 samples in 3 classes and 4 features, and WINE with 178 samples in 3 classes and 13 features. These datasets are available from the UCI machine learning repository. For ex- periments on IRIS data, we used only third and fourth fea- tures, as often investigated.\nThe Gaussian kernel was applied, with κ(x i , x j ) = exp( x i − x j 2 /2σ 2 ). To estimate the classiﬁcation error, a ten-fold cross-validation was used, with parameters opti- mized by a grid search over ν ∈ {2 −5 ; 2 −4 ; · · · ; 2 4 ; 2 5 } and σ ∈ {2 −5 ; 2 −4 ; · · · ; 2 4 ; 2 5 }. In order to provide comparable results, the number of SVs was ﬁxed for all methods, by con- sidering the optimal conﬁguration for the classical one-class SVM. Table (4) gives the results in terms of classiﬁcation er- ror for each of the proposed methods, and compared to the classical one-class SVM. We also included the ratio of com- mon SVs between the latter and each of the proposed meth- ods, as well as the mean values. The computational cost of these machines, using the best conﬁguration, are illustrated in terms of CPU time, as estimated on a 64-bit Matlab run- ning on a Mackbook Pro with a 2.53 GHz Intel Core 2 Duo processor and 4 GB RAM.\nIn this paper, we studied the problem of one-class classiﬁ- cation. By offering three one-class classiﬁcation methods, we have shown that we can achieve a classiﬁcation one-class while minimizing the classiﬁcation error and especially with less computing time. The relevance of our approach is il- lustrated by experiments on well-known datasets. In future works, we study an online strategy for one-class classiﬁca- tion, as well as other sparsiﬁcation rules such as the coherence criterion."},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566815.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T8.3","endtime":"10:50","authors":"Zineb Noumir, Paul Honeine, Cédric Richard","date":"1341484200000","papertitle":"On simple one-class classification methods","starttime":"10:30","session":"S11.T8: Patterns, Estimation, Hypothesis Testing","room":"Stratton (491)","paperid":"1569566815"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
