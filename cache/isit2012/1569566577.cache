{"id":"1569566577","paper":{"title":{"text":"On Reliability Functions for Single-Message Unequal Error Protection"},"authors":[{"name":"Da Wang"},{"name":"Venkat Chandar"},{"name":"Sae-Young Chung"},{"name":"Gregory W. Wornell"}],"abstr":{"text":"Abstract\u2014Single-message unequal error protection (UEP) is a channel coding scheme that protects one special message differently from other (regular) messages. This induces three different types of errors in the system: 1) miss (where we decode the special codeword as a regular codeword), 2) false alarm (where we decode a regular codeword as the special codeword), and 3) decoding error (where we decode a regular codeword to another regular codeword). In this paper, we investigate the fundamental limits of single-message UEP, in the context of discrete memoryless channels (DMCs) without feedback. Similar to Borade et al., we use error exponents as the performance metric, and discuss maximizing the miss error exponent and the false alarm error exponent, respectively. We provide a new converse proof for the miss reliability function, i.e., the optimal miss error exponent as a function of communication rate, and extend the inner and outer bound results for the false alarm reliability function in Borade et al. from rates close to capacity to all rates up to capacity."},"body":{"text":"Classical information theory assumes all messages are equally important and protects them uniformly. However, in certain communication scenarios it may be advantageous to relax this uniformity assumption and to protect certain information more than others, which leads to the framework of unequal error protection (UEP). In this paper we investigate the fundamental limits of single-message unequal error protection, the problem of protecting a special codeword differently from other regular codewords in the codebook.\nThis codeword classiﬁcation leads naturally to three dif- ferent types of errors in the system: 1) miss (where we decode the special codeword as a regular codeword), 2) false alarm (where we decode a regular codeword as the special codeword), and 3) decoding error (where we decode a regular codeword to another regular codeword). We denote the probabilities of these three events by P m , P f and P d respectively. In most applications, we are interested in the regime that P m , P f and P d can be made arbitrarily small by increasing the codeword block length n. Given this reliability condition, we can deﬁne the error exponents e m , e f and e d for these three probabilities, which tell us asymptotically how fast these error probabilities decay as the codeword block length increases. Speciﬁcally, we investigate the maximal attainable\ne m without constraining e f and e d , and similarly, the maximal attainable e f without constraining e m and e d . These two values correspond to the strongest protection for the special codeword and that for regular codewords respectively, and are denoted by E m and E f .\nSome earlier information-theoretic analysis of single- message UEP is motivated by communication with delay requirements, such as distributed control or streaming media applications. In this setting, going beyond block coding, i.e., allowing non-blocking encoding schemes, improves perfor- mance. For example, one of the earliest work [1] uses a single- message UEP code in a variable-delay communication setting. In [2], the authors consider delay in streaming communication with noisy feedback, and use the special message in UEP as an NACK signal. This achieves an error exponent that is much higher than the traditional channel coding error exponent. In a related work [3], error probability of the special message is interpreted as \u201cthe best-case probability of (undetected) block error across messages\u201d and the miss exponent (called hallucination exponent therein) \u201cgives an upper bound on the maximum probability of bit error in a non-blocking streaming context where noiseless feedback is available and the desti- nation is (occasionally) allowed to declare erasures.\u201d There the authors characterize the miss exponent E m for the binary symmetric channel (BSC) at all rates up to capacity.\nAnother example application of UEP is frame synchroniza- tion. In [4], a slotted asynchronous channel model is proposed, and the channel output induced by noise can be viewed as that induced by a special codeword with repeated symbols. This corresponds to a UEP problem with a design constraint on the special message.\nFinally, [5] provides a detailed introduction to the infor- mation theoretic perspectives of UEP and investigates the fundamental limits of UEP on the discrete memoryless channel (DMC). The authors characterize the miss exponent (red-alert exponent therein) at capacity and provide lower and upper bounds for the false alarm error exponent at capacity. In addition, the miss error exponent for DMC at all rates up to capacity is mentioned after [5, Lemma 1], and a proof is given in [6, Theorem 34]. This result also follows from [7, Lemma 1,Lemma 5]. Recently, [8] characterizes the miss error\nexponent for the AWGN channel with both peak and average power constraints at all rates up to capacity.\nIn this paper, we further investigate the problem of single- message unequal error protection for the general DMC in the absence of feedback. We provide a new converse proof for the miss error exponent at all rates up to capacity. Furthermore, we provide lower and upper bounds for the false alarm error exponent at all rates up to capacity, and both bounds match the results at capacity in [5]. All results are obtained via a few generalizations of standard results in the method of types [9].\nThis paper uses lower case letters (e.g., x) to denote a particular value of the corresponding random variable, which is denoted in capital letters (e.g., X). A vector is denoted by its length as a superscript (e.g., x n ). Calligraphic fonts (e.g., X ) represent a set and P (X ) denotes all probability distributions on the alphabet X . Z + and Z + denote the set of non-negative integers and the set of positive integers respectively. We deﬁne\n≤ e nF ⇔ F ≥ lim n→∞ 1 n log a n , and we deﬁne . ≥ and . = similarly.\nWe deﬁne the index set of a certain element a in a sequence x n by\nand we may denote this simply by I a when the sequence is clear from context. The subsequence of x n formed from index set I a is denoted by x I a .\n[P ] δ {P ∈ P (X ) : Support(P ) ⊂ Support(P ) and P − P ∞ < δ},\nFor distributions P (·) ∈ P (X ), Q(·) ∈ P (Y) and condi- tional distributions W ( · | ·) : X → Y, V ( · | ·) : X → Y, deﬁne:\n[P × W ] (x, y) W ( y | x) P (x), P W (y)\nwhere D (V W |P ) is the conditional information divergence between V (·|·) and W (·|·) under P (·), and D (Q W |P ) is the conditional information divergence between Q(·) and W (·|·) under P (·).\nOur proofs make use of the method of types [9] and follow the notations therein. Speciﬁcally, the type of a sequence x n with length n is denoted by ˆ P x n , where the type is the empirical distribution of this sequence, i.e., ˆ P x n (a) = N (a|x n )/n, ∀a ∈ X , where N (a|x n ) is the number of occurrences of symbol a in sequence x n . A type class T n P is the set of sequences that have type P ; a typical set T n [P ]\nis the set of sequences that have type P ∈ [P ] δ ; and a typical shell T n [W ]\nA constant composition code is a code whose codewords all have the same type, i.e., lie in the same type class.\nFinally, we extend the notion of typical shell and deﬁne the tightly typical shell with respect to a sequence x n as\n(1) where I a = I a (x n ).\nWe follow the standard deﬁnition of the discrete memoryless channel (DMC) W : X → Y, which has input alphabet X = {1, 2, . . . , |X |} and output alphabet Y = {1, 2, . . . , |Y|}. The conditional distribution of output letter Y when the channel input letter X equals x ∈ X is denoted by W Y |X (·|x):\nWhen the input and output alphabets are clear from context, W is used instead of W Y |X . Also, for simplicity, we denote W x W ( · | x).\nIn this paper, we only consider the case that the support of W ( · | x) is Y for all x ∈ X , i.e., for any x ∈ X and any y ∈ Y, W ( y | x) > 0, because the analysis for other cases is either simple or can be reduced to this case.\nFor a DMC, a length n block code with input alphabet X , output alphabet Y and some ﬁnite message set M f n = {0, 1, 2, . . . , |M f n |} is composed of a pair of mappings, encoder mapping f n : M f n → X n and decoder mapping g n : Y n → M f n . Given a message m, the encoder maps it to a sequence x n (m) ∈ X n and transmits this sequence through the channel, where we call x n (m) the codeword for message m and the entire set of codewords {x n (m), m ∈ M f n } a codebook . The receiver receives a sequence y n ∈ Y n , where W n ( y n | x n (m)) \t n i=1 W ( y i | x i (m)). We denote (f n , g n ) by C (n) .\nIn the context of unequal error protection, we use message 0 to denote the special message and refer to the collec- tion of regular codewords as the regular codebook. We use A n ∪ m=0 g −1 (m) to denote the decoding region for regular codewords and use B n \t g −1 n (m = 0) to denote decoding region for the special codeword. If y n ∈ A n , we consider\nthe channel input to be a regular codeword x n (m), m ∈ {1, 2, · · · , |M f n |}, otherwise we consider the channel input to be the special message m = 0.\nA code ˆ C (n) = ( ˆ f n , ˆ g n ) is called a subcode of C (n) = (f n , g n ) if M ˆ f\nand m = 0, there is some m ∈ M f n and m = 0 such that ˆ f n (m) = f n (m ).\nThe performance of a codebook over a channel W can be characterized by three types of error: 1) miss, where we decode a special codeword as a regular codeword; 2) false alarm , where we decode a regular codeword as a special codeword; 3) decoding error, where we decode a regular codeword incorrectly into another regular codeword. Formally, we deﬁne\nlog |M f n | /n. For a sequence of codebooks Q = {C (n) , n ∈ Z + }, we deﬁne its rate as R Q = lim inf n→∞ R(C (n) ). When the codebook sequence is clear from context, we denote\nIn this paper, unless otherwise mentioned, all rates are between 0 and the channel capacity C(W ) max P X ∈P(X ) I (P X , W ).\nDeﬁnition 1 (Reliable codebook sequence). A codebook se- quence Q = {C (n) , n ∈ Z + } is called reliable if\nTo investigate how the above error probabilities decay with the codeword block length n, we deﬁne their error exponents. Deﬁnition 2 (Miss, false alarm, and decoding error exponents). Given a reliable codebook sequence Q = {C (n) , n ∈ Z + } with rate R and a DMC (X , Y, W ), deﬁne its miss error exponent as\nSimilarly, we deﬁne its false alarm error exponent e f and decoding error exponent e d in terms of P (n) f and P (n) d .\nA triplet of numbers (e m (Q), e f (Q), e d (Q)) is called achievable if they can be achieved simultaneously, and the set of achievable triples is denoted by E(Q, R). In addition, we deﬁne E(R) ∪ Q:R Q ≥R E(Q, R).\nThe most general problem\u2014characterization of the achiev- able error exponent region E (R)\u2014is open and this paper focuses on the false alarm and miss errors by putting no constraints on e d . In particular, this paper investigates the reliability functions for false alarm and miss errors:\nDeﬁnition 3 (Reliability functions). For a DMC (X , Y, W ), given a rate R, we deﬁne the miss reliability function and the false alarm reliability function as\nThis section presents a characterization of the miss reliabil- ity function in Theorem 1. While this result is not new, we present it for completeness, and provide a new, conceptually simpler, converse proof based on a few generalizations of the method of types.\nTheorem 1 (Miss reliability function, Theorem 34 in [6]). A DMC (X , Y, W ) has miss reliability function\nSpecializing Theorem 1 to R = C, we have the miss error exponent at capacity.\nCorollary 2 (Miss reliability function at capacity). Let P ∗ Y be the (unique) capacity-achieving output distribution, then\nThe converse argument to Theorem 1 is implied by a converse proof for UEP with feedback in [6]. Below we present the sketch of an alternative converse proof, based on a few generalizations of the method of types.\nIn the method of types, it is known that for every reliable channel code, there exists a constant composition subcode with essentially the same rate [9, Collrollary 6.3 and Exercise 6.19]. Lemma 3 below shows that a similar argument holds when we replace constant composition code by a code whose subsequences are constant composition.\nand P d < ε for a given channel W , then for any given s n with type P S and I b \t I b (s n ), there exists a collection of types {P b , b ∈ X } and a subcode ( ˆ f n , ˆ g n ) with all regular codewords in the set\nLemma 3 enables us to focus the converse argument on the (more structured) subcode ˆ C that satisﬁes\n(x n (m)) (see (1)). It can be shown that for any ε > 0, when n sufﬁciently large,\nNow we generalize [9, Lemma 2.14] to the subcode in Lemma 3 and obtain the following lemma:\nLemma 4. Given a sequence s n with type P S and I b I b (s n ), if a set B ⊂ Y n satisﬁes\nThe proof of Lemma 4 is omitted due to space constraint. Noting (7) and (8), Lemma 4 indicates\nP m ˆ C = W n ( A n | s n ) ≥ W n ( ∪ m F m | s n ) .\nNote that using a subcode reduces P m , hence any lower bound to P m ˆ C is a lower bound to P m (C). Finally, we let P X|S=b P b and maximize over P X,S to complete the converse argument.\nIn this section we investigate the false alarm reliability function of a DMC (X , Y, W ). We start with the case of a special codeword consisting of a repeated symbol, then extend the results to an unconstrained special codeword. Unlike the case of miss detection, the exact characterization of the false alarm reliability function is open, but we are able to provide inner and outer bounds.\nTheorem 5 (Bounds for the false alarm reliability function). The false alarm reliability function of an DMC (X , Y, W ) satisﬁes\n(10) E f (R) \t max\nSpecializing Theorem 5 to R = C, we obtain bounds for the false alarm error exponent at capacity.\nCorollary 6 (Bounds for the false alarm reliability function at capacity). Let the set of capacity-achieving input distributions be Π = {P X : I (P X , W ) = C}, then\nRemark 4. Corollary 6 agrees with [5, Theorem 10], which speciﬁes the false alarm error exponent at capacity. Therefore, Theorem 5 generalizes [5, Theorem 10] from at capacity to all rates up to capacity.\nThis section assumes the special codeword to be n and investigates the corresponding performance, which is summa- rized in Theorem 7.\nTheorem 7 (False alarm reliability function for special code- word with repeated symbol ). Given a special codeword n where ∈ X , the false alarm reliability function of an DMC (X , Y, W ) satisﬁes\nE rep f (R) ≤ E rep f (R) ≤ E rep f (R), where\nThe detailed proofs are omitted due to space limits and a sketch of the proof is provided in [4], where this corresponds to ﬁnding the miss error exponent in asynchronous communi- cation.\nFor the case of unconstrained special codeword, the inner and outer bounds of the false alarm reliability function are given in Theorem 5. Omitting the detailed proof due to space limits, here we provide proof sketches of the outer bound argument and the achievability scheme for the inner bound derivation, which is constructed via the achievability scheme for the repeated-symbol special message.\n1) Achievability: To achieve (10), we choose a special codeword s n with type P S and let I b = I b (s n ), and accord- ingly a set of distributions {P b ∈ P n b (X ) , b ∈ X } such that\nThen for each b ∈ X , we follow Theorem 7 to construct a codebook C b = (f b , g b ) that is constant composition with type P b and rate I (P b , W ). Using these codebooks as building blocks, we design the encoding and decoding schemes below.\nDenote each message m by a |X |-tuple (i 1 , i 2 , . . . , i |X | ), where i b ∈ {0, 1, 2, . . . , |C b |} , b ∈ X . Then we deﬁne the following encoding and decoding functions for C (n) = (f, g):\nx n with x I b = f b (i b ) when i b > 0 ∀ b ∈ X s n \t when i b = 0 ∀ b ∈ X\ng (y n ) = ˆi 1 , ˆi 2 , . . . ,ˆi |X | with ˆi b = g b (y I b ) ∀ b ∈ X . This leads to the following error events\nE m = g b (y I b ) = 0, ∃ b ∈ X | i 1 = i 2 = i |X | = 0 , E f = g b (y I b ) = 0, ∀ b ∈ X | i 1 i 2 · · · i |X | = 0 ,\nTherefore, we can choose P S and {P b } to maximize this expression. Finally, we construct a joint distribution P X,S where P X|S=b P b and obtain (10).\n2) Outer bound argument: Following Lemma 3 in Sec- tion III, for any given s n with type P S and I b I b (s n ), we only need to show the E f outer bound for a subcode ˆ C that has essentially the same rate and x I b ∈ T n b P\nfor b ∈ X . Given an arbitrary codeword x n and the special codeword s n , we deﬁne J a,b {i : x i = a, s i = b} , and consider the following set\nWe can show that for each y n ∈ G(x n , s n ), W n ( y n | x n ) are not too small:\nwhere P X,S is the joint type of (x n , s n ). Furthermore, we show that |B ∩ G(x n , s n )| is not too small either. First,\nW n ( B ∩ G(x n , s n )| s n ) ≥ ε/2 because W n ( B| s n ) ≥ 1 − ε and W n ( G(x n , s n )| s n ) =\nwhen n sufﬁciently large. Then applying [9, Lemma 2.14], |B ∩ G(x n , s n )|\n \n \nThe authors would like to thank the anonymous reviewers for helpful comments."},"refs":[{"authors":[{"name":"B. D. Kudryashov"}],"title":{"text":"On message transmission over a discrete channel with noiseless feedback"}},{"authors":[{"name":"S. Draper"},{"name":"A. Sahai"}],"title":{"text":"Beating Burnashev in delay with noisy feed- back"}},{"authors":[{"name":"A. Sahai"},{"name":"S. C. Draper"}],"title":{"text":"The \u201challucination\u201d bound for the BSC"}},{"authors":[{"name":"D. Wang"},{"name":"V. Chandar"},{"name":"S. Chung"},{"name":"G. W. Wornell"}],"title":{"text":"Error exponents for asynchronous communication"}},{"authors":[{"name":"S. Borade"},{"name":"B. Nakibo˘glu"},{"name":"L. Zheng"}],"title":{"text":"Unequal error protection: An Information-Theoretic perspective"}},{"authors":[{"name":"S. P. Borade"}],"title":{"text":"When all information is not created equal"}},{"authors":[{"name":"S. Gorantla"},{"name":"B. Nakibo˘glu"},{"name":"T. Coleman"},{"name":"L. Zheng"}],"title":{"text":"Bit-wise unequal error protection for variable length blockcodes with feedback"}},{"authors":[{"name":"B. Nazer"},{"name":"Y. Shkel"},{"name":"S. C. Draper"}],"title":{"text":"The AWGN red alert problem"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems , 2nd ed"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566577.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T8.1","endtime":"11:50","authors":"Da Wang, Venkat Chandar, Sae-Young Chung, Gregory Wornell","date":"1341574200000","papertitle":"On Reliability Functions for Single-Message Unequal Error Protection","starttime":"11:30","session":"S16.T8: Error Exponents","room":"Stratton (491)","paperid":"1569566577"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
