{"id":"1569566293","paper":{"title":{"text":"Constructing Polar Codes for Non-Binary Alphabets and MACs"},"authors":[{"name":"Ido Tal"},{"name":"Artyom Sharov"},{"name":"Alexander Vardy"}],"abstr":{"text":"Abstract\u2014Consider a channel with an input alphabet that is ﬁnite but not necessarily binary. A method for approximating such a channel having a large output alphabet size by a degraded version of it having a smaller output alphabet size is presented and analyzed. The approximation method is used to construct polar codes for both single-user and multiple-access channels with prime input alphabet sizes."},"body":{"text":"Polar codes [1] have recently been invented by Arıkan. In his seminal paper, Arıkan introduced polar codes in the context of a binary-input, memoryless, output-symmetric channel over which information is to be sent. The deﬁnition of polar codes was soon generalized to a channel with prime input alphabet size in [2]. A further generalization to a polar coding scheme for a multiple-access channel (MAC) with prime input alphabet size is presented in [3] and [4].\nThe merits of the polar coding schemes presented in [1], [2], [3], and [4] are as follows. Firstly, the schemes are both explicit and symmetric-capacity achieving (sum-rate symmetric-capacity achieving in the MAC setting). Secondly, they have corresponding encoding and decoding algorithms that are efﬁcient. To date, no other family of codes attains all of these properties. However, a major shortcoming of the above schemes is that, although the constructions are explicit, the corresponding papers do not suggest an efﬁcient method of carrying them out. In fact, in a naive implementation, the construction complexity is exponential in n, the codeword length. Recently, an efﬁcient construction algorithm for the single-user binary-input channel was presented in [5] and analyzed in [6]. Our aim in this paper is to generalize the construction algorithm presented in [5] to single-user channels and MACs with input alphabets having size p, where p is a prime.\nThe main building block that our construction algorithm will make use of, is a method to approximate a MAC by another MAC. The utility of the approximation is that the new MAC will have a reduced output alphabet size. As was the case in [5], our method will have a tunable ﬁdelity parameter, allowing for an arbitrarily close approximation (in a sense which will shortly be deﬁned). Also, in contrast to the naive implementation which requires running time exponential in n, our construction algorithms will run in time linear in n. Let the underlying MAC have an input alphabet of size p and t users\n(t = 1 if we are in fact considering a single-user channel). We would like to mention up-front that the running time of our building-block method and thus of our whole algorithm grows very fast in q = p t . Thus, our algorithm can only be argued to be practical for small values of q.\nThe structure of our paper is as follows. In Section II we setup the basic concepts and notation that will be used later on. Section III introduces the sum-rate of a MAC, and explains why we choose it as the ﬁgure of merit of our approximation. Section IV contains our approximation algorithm and Section V outlines how it is used as a building block in constructing a polar code. In Section VI we introduce a more reﬁned analysis of our approximating algorithm.\nSince a single-user channel is a special case of a MAC, we ﬁnd it more general to consider MACs. Let W : X t → Y designate a generic t-user MAC, where X = {0, 1, . . . , p − 1} is the input alphabet, p is prime, and Y is the ﬁnite output alphabet. Denote a vector of user inputs by u ∈ X t , where u = (u (i) ) t i=1 . Our MAC is speciﬁed through the probability function W , where W (y|u) is the probability of observing the output y given that the user input was u.\nFor input vectors u 0 = (u (i) 0 ) t i=1 and u 1 = (u (i) 1 ) t i=1 , denote by u 0 ⊕ p u 1 the component-wise sum modulo p of u 0 and u 1 . That is,\nLet the underlying MAC through which information is to be transmitted be denoted by W : X t → Y. Denote the length of the codewords to be used by n = 2 m . Given an index 0 ≤ i < n, we recursively deﬁne W (m) i , termed the ith MAC to have evolved from m transforms, as follows. The base of the recursion is the underlying channel:\nAs explained in [3] and [4], polar coding is achieved through the evolved MACs. Thus, they are the objects we will be interested in.\nWe deﬁne the notion of a degraded MAC in an analogous way to that of a degraded single-user channel. Speciﬁcally, let two t-user MACs W : X t → Y and Q : X t → Y be given. We say that Q is degraded with respect to W and denote this as Q W if there exists a function P : Y → Y such that the following holds. First, P must be a probability function in the following sense: for all y ∈ Y and y ∈ Y we must have that P (y |y) ≥ 0. More so, we must have for all y ∈ Y that\nSecondly, the concatenation of P to W must result in Q. That is, for all y ∈ Y and u ∈ X t we require that\nThe following lemma is essentially a restatement of [7, Lemma 4.7].\nLemma 1: Let two t-user MACs W : X t → Y and Q : X t → Y be such that Q W . Denote by Q − and Q + the result of applying the Arıkan transforms to Q and let W − and W + be the result of applying the same transforms on W . Then,\nLet a t-user MAC W be given. Next, let U = (U (i) ) t i=1 be a random variable uniformly distributed over X t . Let Y be the random variable one gets as the output of W when the input is U. The sum-rate of W is deﬁned as the mutual information\nWe ﬁrst mention the following simpliﬁcation. As shown in [3, Appendix A], there is a conceptually simple coding scheme in which the sum of the rates of all users 1 can be made to approach R(W ). In short: code for user i assuming that the previous i−1 users are known to the receiver and the next t−i users are treated as noise. In this scheme, the coding problem essentially reduces to coding for a single-user channel, and we can effectively consider only the case t = 1 (which is solved in [5] for p = 2). Note that in this scheme, we do not code using the evolved MACs. However, the problem of coding directly via the evolved MACs seems to also have merit, and so we chose to consider the more general case of t ≥ 1.\nIn the next section, we show a method by which to ap- proximate one MAC by another MAC with a smaller output alphabet size. The ﬁgure of merit we chose to measure our approximation by is the sum-rate. Namely, we give bounds on how much the sum-rate might decrease due to the ap- proximation. At this point, we would like to explain why we chose the sum-rate as the ﬁgure of merit. In fact, there are two complementary explanations. First, as was shown in [3] and [4], although the mean symmetric-capacity region is generally reduced after each polarization step, the sum-rate is not. Namely, for every MAC W we have that\nThis, together with the fact that [3] and [4] show that the sum-rate can be approached by polar coding make R(W ) a natural ﬁgure of merit. Secondly, the following lemma shows that a given ﬁdelity of R(Q) with respect to R(W ) essentially implies the same ﬁdelity with respect to the other mutual informations associated with the MACs Q W . Note that these mutual informations do indeed play a role when constructing a polar coding scheme for a MAC [3][4].\nLemma 2: Let W : X t → Y and Q : X t → Y be a pair of t-user MACs such that Q W and\nwhere ε ≥ 0. Let U be uniformly distributed over X t . Denote by Y and Y the random variables one gets as the output of W and Q, respectively, when the input is U. Let the sets A, B, and C form a partition of the user index set {1, 2, . . . , t}. Denote U A = (U (i) ) i∈A and U B = (U (i) ) i∈B . Then,\nBy deﬁnition, since Q is degraded with respect to W , there exists a corresponding intermediate channel P : Y → Y . Thus, by the data processing inequality applied to Y and Y\nthrough P , we get that each term on the RHS of (9) is an upper bound on the corresponding term in (10). Since\n[I(U B ; Y ) − I(U B ; Y )]+[I(U A ; U B , Y ) − I(U A ; U B , Y )] + [I(U C ; U A , U B , Y ) − I(U C ; U A , U B , Y )] , (11)\nand each parenthesized difference term of the RHS is positive, we deduce that\nWe now start the exposition of our MAC approximation algorithm. As before, consider a t-user MAC W : X t → Y, where X = {0, 1, . . . , p − 1}. Let the random variables U and Y be as before, and deﬁne the function ϕ W = ϕ : X t × Y → [0, 1] as follows: for u ∈ X t and y ∈ Y,\nϕ(u|y) = P(U = u|Y = y) = P(U = u, Y = y) P(Y = y)\nNote that in our deﬁnition of ϕ, we make the implicit assump- tion that the output alphabet Y has been purged of all letters y for which the denominator in (12) is zero, since these outputs will never occur. Next, for y ∈ Y let us abuse notation and denote ϕ W (y) = ϕ(y) as shorthand for\nIn a nutshell, our algorithm will merge output letters y 1 , y 2 ∈ Y if they both fall into the same \u201cbin\u201d. We now show how to place output letters into bins.\nIt is easy to prove that the function η(x) is ∩-concave on 0 ≤ x ≤ 1 and attains its maximum when x is equal to\nα = 1 e\nLet the positive real µ be a given ﬁdelity parameter and deﬁne\n  \nj \t if x < α and j−1 µ ≤ η(x) < j µ , µ \t if x = α ,\n(14) The next lemma is a simple consequence of the deﬁnition of b(x).\nLemma 3: Let 0 ≤ x ≤ 1 and 0 ≤ x ≤ 1 be such that b(x) = b(x ). Then,\nWe say that two output letters are in the same bin if for all u ∈ X t we have that b(ϕ(u|y 1 )) = b(ϕ(u|y 2 )). The degrading process consists of the following procedure.\n\u2022 We count the number of non-empty bins, and then con- struct an output alphabet Y , the size of which is the number of non-empty bins.\n\u2022 To each non-empty bin we associate a distinct letter in Y . For a y ∈ Y , denote by B(y ) all the y ∈ Y in the bin associated with y .\n\u2022 The degraded MAC is obtained from the original MAC through an intermediate channel P : Y → Y . The channel maps (with probability 1) each letter y ∈ Y to the letter y ∈ Y associated with the bin y is in. That is, each member of B(y ) is mapped to y .\nDenote the MAC one gets by the above degrading procedure applied to a MAC W by Q. By deﬁnition, Q is a t-user MAC which is degraded with respect to W . Let Y be the random variable one gets as the output of Q when the input is U. Clearly, for all y ∈ Y and u ∈ U,\nLemma 4: Let µ be speciﬁed and y ∈ Y be given. Then, for all y ∈ B(y ) and for all u ∈ X t ,\nThat is, loosely speaking, if y were to be binned, it would occupy the same bin as (any) y ∈ B(y ).\nProof: The main point to notice is that for a ﬁxed 1 ≤ j ≤ 2µ, the set of x for which b(x) = j is contiguous (as is exempliﬁed in Figure 1). To see this, recall the deﬁnition of b(x) in (14) and note that η(x) is strictly increasing for 0 ≤ x < α and strictly decreasing for α < x ≤ 1.\nTo ﬁnish the proof, it sufﬁces to show that ϕ Q (u|y ) is a convex combination of ϕ W (u|y), y ∈ B(y ), since this implies that ϕ Q (u|y ) is contained in the contiguous set all the ϕ W (u|y) are members of. Indeed, it can be easily seen that\nϕ W (y) ϕ Q (y )\nTheorem 5: Let W be a t-user MAC and let Q be the MAC one gets by applying the degrading operation described above with ﬁdelity criterion µ. Then,\nProof: By (13) and (16), we can write the difference R(W ) − R(Q) as\nConsidering the innermost sum, we have by Lemma 4 that b(ϕ W (u|y)) = b(ϕ Q (u|y )) .\nThus, by Lemma 3 we conclude that each innermost term has absolute value at most 1/µ. Plugging this in yields\nRecall that the point of running the degrading approximation was to reduce the size of the output alphabet. The following lemma gives an upper bound on its size.\nLemma 6: Let W : X t → Y be a t-user MAC and let Q : X → Y be the MAC one gets by applying the degrading operation described above with ﬁdelity criterion µ. Recall that q = p t . Then,\nProof: Recall that two letters y 1 , y 2 ∈ Y are in the same bin if and only if b(ϕ(u|y 1 )) and b(ϕ(u|y 2 )) are equal for all u ∈ X t . The proof follows by recalling that the number of values u can take is q and the number of values b(·) can take is 2µ.\nIn this section we outline a construction method for polar codes. Due to space limitations, we only show and analyze the algorithm used to approximate W (m) i . That is, we show here the analog of [5, Algorithm A]. In order to actually construct polar codes, one needs an analog of [5, Algorithm C]. However, getting from one algorithm to the other, with [3], [4], and [5] as references, should be rather straightforward.\nFor a given channel index 0 ≤ i < n, denote by i = b 1 , b 2 , . . . , b m 2 the binary representation of i, where b 1 is\nthe most signiﬁcant bit. Also, let degrading_merge(W, µ) be the result of applying the approximation method outlined in Section IV to a MAC W using the ﬁdelity parameter µ.\nAlgorithm A: A high level description of the degrading procedure\ninput : An underlying MAC W, a ﬁdelity parameter µ, an index i = b 1 , b 2 , . . . , b m 2 .\noutput: A MAC that is degraded with respect to W (m) i . Q ← degrading_merge(W, µ); 1 for j = 1, 2, . . . , m do 2\nif b j = 0 then 3 W ← Q Q 4\nelse 5 W ← Q Q 6\nQ ← degrading_merge(W, µ); 7 return Q; 8\nNote that since we trim the output alphabet size after each iteration, it does not grow out of control. Speciﬁcally, it is easy to show that the output alphabet size of each MAC encountered during the run of Algorithm A, except for possibly W, is at most q · (2µ) 2q . Thus, if p, t, and µ are regarded as constants, and i ranges from 0 to n−1, the running time of the algorithm (assuming calculations are shared) is a constant times n = 2 m , the codeword length.\nThe following theorem gives an upper bound on the average loss in sum-rate due to running Algorithm A. It is a direct consequence of Lemma 1 and Theorem 5\nTheorem 7: Let an underlying t-user MAC W : X t → Y be given, where X = {0, 1, . . . , p − 1} and p is prime. Denote by Q (m) i the channel returned by running Algorithm A with parameters i and µ. Then,\n1 n\nRecall that the bound derived in Theorem 5 came about by bounding the difference in the innermost sum in (17) by 1/µ. We now show how to rewrite the difference R(W ) − R(Q) in a slightly different manner, which will lead to a tighter bound.\nNext, recalling the deﬁnition of ϕ Q (u|y ) given in (15), we have that\nϕ W (y) ϕ Q (y )\nϕ W (y) ϕ Q (y )\nϕ W (y) ϕ Q (y )\nRecall that by the binning operation, we have that b(ϕ W (u|y)) has the same value for all y ∈ B(y ). This observation implies that the following is well-deﬁned. Pick some y ∈ B(y ) and denote by I y the interval that is the pre-image of b(ϕ W (u|y)) with respect to the binning function b:\nLemma 8: Let W , u, and y be given. Denote by a and b the inﬁmum and supremum of I y , respectively. Then, the difference in (18) is at most\nProof: Obtaining the expression for θ max is an easy application of calculus. We now focus on the ﬁrst part of the theorem, and begin by setting up some notation. Let,\nand that for each δ y there is a corresponding 0 ≤ θ y ≤ 1 such that\nwhere the ﬁrst step follows from Jensen\u2019s inequality. Recall that here we have set θ to a speciﬁc value, whereas in (19) θ is optimized. Thus, the last displayed equation is a lower bound on the expression given in (19).\nWe would like to thank Eren S¸as¸o˘glu for very productive discussions."},"refs":[{"authors":[{"name":"E. Arıkan"}],"title":{"text":"Channel polarization: A method for constructing capacity- achieving codes for symmetric binary-input memoryless channels"}},{"authors":[{"name":"E. S¸as¸o˘glu"},{"name":"E. Telatar"},{"name":"E. Arıkan"}],"title":{"text":"Polarization for arbitrary discrete memoryless channels"}},{"authors":[{"name":"E. S¸as¸o˘glu"},{"name":"E. Telatar"},{"name":"E. Yeh"}],"title":{"text":"Polar codes for the two-user multiple- access channel"}},{"authors":[{"name":"E. Abbe"},{"name":"E. Telatar"}],"title":{"text":"Polar codes for the m-user MAC and matroids"}},{"authors":[{"name":"I. Tal"},{"name":"A. Vardy"}],"title":{"text":"How to construct polar codes"}},{"authors":[{"name":"R. Pedarsani"},{"name":"S. H. Hassani"},{"name":"I. Tal"},{"name":"E. Telatar"}],"title":{"text":"On the construction of polar codes"}},{"authors":[{"name":"S. B. Korada"}],"title":{"text":"Polar codes for channel and source coding"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566293.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T5.1","endtime":"11:50","authors":"Ido Tal, Artyom Sharov, Alexander  Vardy","date":"1341487800000","papertitle":"Constructing Polar Codes for Non-Binary Alphabets and MACs","starttime":"11:30","session":"S12.T5: Polar Codes over Non-Binary Alphabets","room":"Kresge Little Theatre (035)","paperid":"1569566293"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
