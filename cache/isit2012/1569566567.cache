{"id":"1569566567","paper":{"title":{"text":"Rateless Feedback Codes"},"authors":[{"name":"Jesper H. Sørensen ∗\u2020"},{"name":"Toshiaki Koike-Akino \u2020"},{"name":"Philip Orlik \u2020"}],"abstr":{"text":"Abstract\u2014This paper proposes a concept called rateless feed- back coding. We redesign the existing LT and Raptor codes, by introducing new degree distributions for the case when a few feedback opportunities are available. We show that incorporating feedback to LT codes can signiﬁcantly decrease both the coding overhead and the encoding/decoding complexity. Moreover, we show that, at the price of a slight increase in the coding overhead, linear complexity is achieved with Raptor feedback coding."},"body":{"text":"Achieving reliable communications over erasure channels has been an active research topic for many years, especially when packet-based communications emerged with the rise of the Internet. When a strong feedback channel is available, the capacity of the erasure channels is easily achieved with well- known automatic repeat-request (ARQ) schemes. The receiver simply feeds back an acknowledgment on each individual symbol, and any unacknowledged symbols are retransmitted. The fact that ARQ makes extensive use of feedback channels makes it difﬁcult to employ this scheme in many practical systems. A rateless coding can achieve reliable communica- tions on erasure channels with no frequent feedback. The so- called LT codes [1] and Raptor codes [2] are such examples. These codes generate a potentially inﬁnite amount of encoded symbols from k information symbols, with an encoding and decoding complexity order of O(k log k). A successful de- coding is possible when (1 + )k encoded symbols have been received, where is a coding overhead.\nNow the question is; assuming we have a communication system, where the receiver has m feedback opportunities (0 < m < k), what scheme should be applied? While the existing schemes are designed for the extreme cases, m = k and m = 0, the more practical assumptions pose an interesting problem. An approach called doped fountain coding is proposed in [3], where the receiver feeds back information on undecoded symbols. This makes the transmitter able to transmit input symbols, which accelerate the decoding process. In [4], another approach called real-time oblivious erasure correcting is proposed. This approach utilizes feedback telling how many of the k input symbols have been decoded. With this information the transmitter chooses a ﬁxed degree for future encoded symbols, which maximizes the probability of decoding new symbols. The same type of feedback is applied in [5], but with the purpose of minimizing the redundancy.\nWe propose in this paper redesigned versions of LT and Raptor codes, called LT feedback codes and Raptor feedback codes, where any amount of feedback opportunities are taken into account. We focus on more informative feedback than simple acknowledgments to tell the transmitter which informa- tion symbols have been recovered, not just how many. This\ninformation is used to modify the degree distribution in the encoder. The goal is to decrease the coding overhead especially for short k, in which the conventional rateless codes are inefﬁcient. We will show that the proposed feedback scheme can decrease both the coding overhead and the complexity.\nAn overview of standard LT codes is ﬁrst described here. Assume we wish to transmit data, which is divided into k input symbols. From the input symbols, a potentially inﬁnite amount of encoded symbols, called output symbols, are generated. Output symbols are XOR combinations of input symbols. The number of input symbols used in the XOR is referred to as the degree of the output symbol, and all input symbols contained in an output symbol are called neighbors of the output symbol. The output symbols follow a certain degree distribution Ω. The encoding process can be broken down into three steps: 1) Randomly choose a degree d by sampling Ω(d). 2) Choose uniformly at random d of the k input symbols. 3) Perform XOR of the d chosen input symbols. The resulting symbol is the output symbol. This process can be iterated as many times as needed, which results in a rateless code.\nA widely used decoder for LT codes is a belief propagation (BP) decoder whose complexity is low [2]. First, all degree- 1 output symbols are identiﬁed, which makes it possible to recover their corresponding neighboring input symbols. These are moved to a storage referred to as the ripple. Symbols in the ripple are processed one by one, which means they are XOR\u2019ed with all output symbols, who have them as neighbors. Once a symbol has been processed, it is removed from the ripple and considered decoded. The processing of symbols in the ripple will potentially reduce the buffered symbols to degree one, in which case the neighboring input symbol is recovered and moved to the ripple. This is called a symbol release. Such an iterative decoding process can be explained in two steps: 1) Identify all degree-1 symbols and add the corresponding input symbols to the ripple. 2) Process a symbol from the ripple, remove it afterwards and go to step 1. Decoding succeeds when all input symbols are recovered. If at any point, the ripple size equals zero, decoding has failed.\nThe ripple size is an important parameter in the design of LT codes. This design contains two steps; 1) ﬁnd a suitable ripple evolution to aim for, and 2) ﬁnd a degree distribution which achieves that ripple evolution. This section describes such a design for the case where feedback opportunities exist. We ﬁrst focus on the case of a single feedback located when\nf 1 k symbols have been decoded. It will be later scaled to any amount of feedback opportunities. A. LT Feedback Codes\nThe feedback informs the transmitter which input symbols have been decoded. With the information, the encoder excludes all decoded symbols from future encoding. An important im- plication of this is that the processing of the ﬁrst f 1 k symbols has no inﬂuence on the release of the symbols received after the feedback because no symbol encoded after the feedback has those ﬁrst f 1 k processed symbols as neighbors. In order to fully understand the beneﬁt from this, we look at a proposition presented in [1]. It expresses the probability, q(d, L, k), that a symbol of degree d is released when L input symbols remain unprocessed, i.e. in the (k − L)-th decoding step:\n− j) \t , \t (1) for d = 2, . . . , k, and L = k − d + 1, . . . , 1,\nIt holds for the traditional LT code without feedback. With feedback, we can divide encoded symbols into two groups; one for symbols received prior to the feedback and one for symbols received after. Symbols received before the feedback follow the release probabilities in (1). Symbols received after the feedback are based on the reduced set of input symbols of size (1−f 1 )k . Moreover, their releases are independent of the processing of the ﬁrst f 1 k input symbols. We can therefore ﬁnd their release probabilities as q(d, L, (1 − f 1 )k) , for L > f 1 k .\nFig. 1 shows a plot of q(d, L, k) (before feedback) and q(d, L, (1 − f 1 )k) (after feedback), with parameters k = 100 and f 1 = 0.5 . It illustrates how releases of symbols received after the feedback are conﬁned to the end of the decoding pro- cess. This is useful when designing degree distributions, since it gives more freedom to distribute the releases throughout the decoding process. For example, see the release distributions of low degree symbols after the feedback; they are released with high probability immediately after the feedback. This means that we have the opportunity to give the ripple a boost at an intermediate point in the decoding process. This should be exploited in the design of the LT feedback code.\nIn [2], it was argued that the ripple size should be kept larger than c √ L , for some positive constant c. The justiﬁcation is based on viewing the ripple evolution as a simple random\nwalk, i.e. every time a symbol is processed, the ripple size is either increased or decreased by one with equal probabilities. In such a random walk, the expected distance from the origin after z steps is √z. Hence, if L steps remain in the decoding process, the ripple size should be of the order of √ L in order to avoid decoding failure. The choice of ripple evolution for the LT feedback code is based on the same reasoning. However, due to the feedback, the ripple evolution can be viewed as two random walks. The ﬁrst random walk represents decoding until the feedback, i.e. when (1 − f 1 )k remain undecoded. Hence, we argue that until this point, the ripple size should be equal to c 1 L − (1 − f 1 )k , for a positive constant c 1 . The second random walk represents decoding after the feedback, where all remaining symbols must be recovered. Hence, the ripple size should be c 2 √ L , for a positive constant c 2 . Fig. 2 illustrates the proposed ripple evolution for c 1 = c 2 = 1 . The proposed ripple evolution, R(L), is summarized as below:\nThe next step in the design is to ﬁnd a degree distribution which achieves the proposed ripple evolution (2). Two degree distributions must be found, for the case of a single feedback, one for before the feedback and the other for after. First, the vector R(L) is mapped into another vector, Q(L), which denotes the expected number of releases in the (k − L)-th decoding step. We derive this mapping assuming all releases in a single step are unique, i.e. the same input symbol will not be recovered twice within a single step. This is a valid assumption, since the number of releases in a single step is low compared to L. The mapping can be expressed as\n    \n            \nwhere k 1 = (1 − f 1 )k . The achieved Q(L) can be expressed as a function of the applied degree distribution, Ω(d), through (1). This is done for the general case without feedback as\nnΩ(d)q(d, L, k), \t (5) where n denotes the number of received symbols in decoding.\nFor the case with feedback, we have contributions from two different degree distributions, Ω 1 (d) and Ω 2 (d) , when L < (1 − f 1 )k . Correspondingly, n 1 and n 2 symbols have been received prior to decoding. When Ω 2 (d) is applied, the encoder only includes the remaining (1 − f 1 )k input symbols. Therefore, we have\n                \n(6) Equations (4) and (6) yield (7) shown at the top of the\nnext page. Here, n 1 and n 2 are the free parameters. Hence, a solution tells us how many symbols we must collect of the different degrees before and after the feedback in order to achieve the desired ripple evolution. Normalizing the solution vectors with n 1 and n 2 provides the degree distributions. Since the matrix in (7) becomes singular for high k, we propose to use the least-squares nonnegative solution for (7) to achieve close to the desired ripple evolution. Table I exempliﬁes the solution for k = 128, f 1 = 0.75 and c 1 = c 2 = 1 . The actual expected ripple evolution achieved with these degree distributions is easily found through the use of (7) and (3). Fig. 3 demonstrates that the least-squares solution closely approaches the proposed evolution.\nThe design procedure explained above is easily extended to multiple feedback opportunities. Consider m feedback oppor- tunities whose locations are at f i , for i = 1, 2, . . . , m. The\ndecoding can be viewed as m + 1 random walks, for which the proposed ripple evolution is written as\n    \nwhere k i = (1 − f i )k , with suitable constants c i for i = 1, 2, . . . , m + 1 .\nAn example of two feedback opportunities for k = 128, m = 2 , f 1 = 0.25 , f 2 = 0.75 and c 1 = c 2 = c 3 = 1 is shown in Fig. 3. It also compares what is achieved with the least-squares solution to a generalized version of (7):\n        \n... ... 0 0 0 ... H i,j H i,i 0 0 ... ... ... ...\n        \n       \n       \n... ... ...\n(9) where, with notations k 0 = k and k m+1 = 0 ,\n  ,\n  .\nAn important performance metric is the average degree, ¯ d . When no feedback is considered, it is calculated as\nd Ω(d) . For the proposed LT feedback code, it is calcu- lated using the following equations:\nn i . \t (10) Fig. 4 shows a comparison of LT feedback codes with in- creasing number of feedback opportunities and the Robust Soliton distribution (RSD). The RSD proposed in [1] is the de facto standard for traditional LT codes. The legend shows the number of feedbacks in parenthesis. The LT feedback code with zero feedbacks, refers to a code using a degree distribution which achieves R(L) = √ L .\nFig. 4 shows that LT feedback codes decrease the average degree, and thereby computational complexity. It should be noted that as the number of feedbacks increases, ¯ d as a function of k approaches a constant, which suggests linear complexity. Superlinear complexity is one of the drawbacks of traditional LT codes and we can now conclude that feedback is a way to solve it. However, at small m the average degree still increases with k in the order of log(k). It is therefore relevant to consider the concept of Raptor coding, which is another approach to ensuring linear complexity [2].\nRaptor coding is a concatenation of an LT code and a high- rate block code, e.g. low-density parity-check (LDPC) code. During encoding, the block encoder is ﬁrst applied on the k input symbols. These block-encoded symbols are fed to an LT encoder, which creates a rateless code. Successful decoding is possible whenever the LT decoder has recovered enough symbols for the block decoder to succeed. It was shown in [2] that there exist degree distributions of the LT code to ensure successful decoding after receiving (1 + )k symbols and has a constant average degree for increasing k. We will now show how our design approach of LT feedback coding can be extended to a Raptor feedback code.\nFor a given desired ¯ d in (10), a constraint is easily added to the equation in (7), as shown in the second row from the bottom in (11), which is shown at the top of the next page. Since we can ﬁnd the least-squares solutions to this system of equations, a weight, w 1 , has been multiplied in this row, in order to make it more strict. Moreover, in order to avoid the case that the ripple lacks robustness around the feedback point, a constraint is added to ensure that the correct amount of releases is achieved prior to the feedback. This is also a strict constraint and is therefore also multiplied by w. It has been added as the last row in (11).\nThe least-squares solution for k = 128, f 1 = 0.75 , c 1 = c 2 = 1 and ¯ d = 2.7 is shown in Table II. The\ncorresponding ripple evolution is seen in Fig. 5 compared to the evolution proposed for the LT feedback code. There is still a slight lack of robustness near the feedback point. Q(L) was found assuming that a ripple size equal to the desired is achieved. This is valid even when a slight deviation exists, as in Fig. 3. However, in this case the deviation has an impact and it can be compensated by transmitting the feedback slightly earlier. There is also a lack of robustness near the end of decoding. However, as in traditional Raptor codes, this is dealt with by an outer block code.\nThe performance metrics we are concerned with are average coding overhead, i.e. (1 + )k, and the average degree which determines the complexity. First, the performance of LT feed- back codes is evaluated in the low range of k. This is due to the fact that traditional LT codes are inefﬁcient for low k.\nThe ﬁrst simulation serves the purpose of optimizing the feedback point for a single feedback opportunity. Fig. 6 shows the results over averages of 5000 runs for c 1 = c 2 = 1 . The best performance was achieved with f 1 = 23 32 for k = 32, 64 and 96. For higher k values, the best performance was achieved with f 1 = 24 32 . A similar optimization has been performed for two and three feedback opportunities. For two feedbacks, best performing values were f 1 = 18 32 and f 2 = 28 32 for k = 32, 64, 96 and 128. For k = 160, f 1 = 18 32 and f 2 = 29 32 performed best. For the case of three feedbacks, f 1 = 13 32 , f 2 = 24 32 , and f 3 = 30 32 were either optimal or near optimal for all short k.\nAn average coding overhead is shown in Fig. 7. LT feedback codes with increasing number of feedbacks are compared to a traditional LT code with no feedback using the RSD. We can observe a signiﬁcant gain of the feedback in average coding overhead. Even the LT feedback code with zero feedbacks provides a slight decrease in coding overhead due to the inef- ﬁciency of RSD. It is also evident that there is a diminishing return from adding feedback to the system.\nIn Fig. 8, Raptor feedback codes are compared to LT feedback codes at higher k values, where complexity becomes relevant to consider. For zero, one and two feedback oppor- tunities in the Raptor feedback codes, ¯ d of 5, 3 and 2.5 have been chosen respectively. For all cases, a random LDPC code with variable nodes of degree 3 and a rate of 0.9688 has been applied as an outer code. Feedback points for all schemes have been optimized as described earlier. The results show that, except for the case of no feedback, the Raptor feedback code performs slightly worse than the LT feedback code. Hence, linear complexity comes at the price of an overhead loss, as is the case for traditional rateless codes.\nFinally, Fig. 9 shows the average overhead and complexity of LT feedback codes for k = 128 as a function of the number of feedback opportunities. For this simulation, the feedback locations have not been optimized but are uniformly distributed over the decoding process. This ﬁgure shows that any amount of feedback opportunities can be utilized for decreasing overhead and complexity all the way towards the extreme case of ARQ, where ¯ d = 1 and = 0 . Note that there is a room of improving the performance by optimizing the feedback points.\nWe have introduced LT feedback codes and Raptor feed- back codes, those of which differ from traditional rateless codes through their ability to take any amount of feedback opportunities into account. The key component is the design of well-performing degree distributions for feedback-aided transmissions. We have shown signiﬁcant improvements in coding overhead and complexity. Moreover, it was shown that LT feedback codes approach linear complexity for increasing number of feedback opportunities, which eliminates the major drawback of traditional LT codes. For a small number of feedback opportunities, complexity is still superlinear. For this case, Raptor feedback codes have been introduced in order to achieve linear complexity."},"refs":[{"authors":[{"name":"M. Luby"}],"title":{"text":"LT codes"}},{"authors":[{"name":"A. Shokrollahi"}],"title":{"text":"Raptor codes"}},{"authors":[{"name":"S. Kokalj-Filipovi´c"},{"name":"P. Spasojevi´c"},{"name":"E. Soljanin"},{"name":"R. Yates"}],"title":{"text":"ARQ with doped fountain decoding"}},{"authors":[{"name":"A. Beimel"},{"name":"S. Dolev"},{"name":"N. Singer"}],"title":{"text":"RT oblivious erasure correcting"}},{"authors":[{"name":"A. Hagedorn"},{"name":"S. Agarwal"},{"name":"D. Starobinski"},{"name":"A. Trachtenberg"}],"title":{"text":"Rateless coding with feedback"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566567.pdf"},"links":[{"id":"1569566485","weight":6},{"id":"1569566875","weight":12},{"id":"1569559617","weight":6},{"id":"1569565091","weight":12},{"id":"1569565607","weight":6},{"id":"1569564805","weight":6},{"id":"1569565609","weight":6},{"id":"1569566157","weight":6},{"id":"1569566015","weight":6},{"id":"1569566679","weight":12},{"id":"1569566617","weight":31},{"id":"1569555999","weight":6},{"id":"1569566661","weight":6},{"id":"1569566003","weight":25},{"id":"1569565185","weight":6},{"id":"1569565469","weight":12},{"id":"1569566297","weight":6},{"id":"1569565961","weight":6},{"id":"1569566779","weight":12},{"id":"1569565093","weight":12},{"id":"1569565919","weight":6},{"id":"1569566927","weight":12},{"id":"1569565319","weight":6},{"id":"1569565353","weight":6},{"id":"1569564291","weight":6},{"id":"1569566547","weight":6},{"id":"1569566529","weight":6},{"id":"1569566771","weight":12},{"id":"1569565457","weight":6},{"id":"1569564923","weight":6},{"id":"1569566601","weight":6},{"id":"1569566147","weight":6},{"id":"1569566973","weight":6}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T5.4","endtime":"12:50","authors":"Jesper H Sørensen, Toshiaki Koike-Akino, Philip Orlik","date":"1341405000000","papertitle":"Rateless Feedback Codes","starttime":"12:30","session":"S10.T5: Rateless Codes","room":"Kresge Little Theatre (035)","paperid":"1569566567"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
