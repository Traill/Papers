{"id":"1569559805","paper":{"title":{"text":"Linear-Programming Decoding of Tanner Codes with Local-Optimality Certiﬁcates"},"authors":[{"name":"Nissim Halabi"},{"name":"Guy Even"}],"abstr":{"text":"Abstract\u2014Given a channel observation y and a codeword x, we are interested in a one-sided error test that answers the questions: is x optimal with respect to y? is it unique? A positive answer for such a test is called a certiﬁcate for the optimality of a codeword.\nWe present new certiﬁcates that are based on combinatorial characterization for local-optimality of a codeword in irregular Tanner codes. The certiﬁcate is based on weighted normalized trees in computation trees of the Tanner graph. These trees may have any ﬁnite height h (even greater than the girth of the Tanner graph). In addition, the degrees of local-code nodes are not restricted to two (i.e., skinny trees). We prove that local- optimality in this new characterization implies ML-optimality and LP-optimality, and show that a certiﬁcate can be computed efﬁciently.\nWe apply the new local-optimality characterization to regular Tanner codes, and prove lower bounds on the noise thresholds of LP-decoding in MBIOS channels. When the noise is below these lower bounds, the probability that LP-decoding fails decays doubly exponentially in the girth of the Tanner graph."},"body":{"text":"In this paper we deal with the problem of analyzing the probability of successful decoding using linear-programming (LP) decoding for Tanner codes. Tanner [1] introduced graph representations of linear codes. In the standard setting, con- straint nodes compute the parity function. In the generalized setting, constraint nodes use a local error-correcting code. One may view a constraint node with a linear local code as a coalescing of multiple parity check nodes. Therefore, a code may have a sparser and smaller representation when represented as a Tanner code.\nWiberg [2] studied representations of codes using factor graphs. He used these representations to analyze message passing decoding algorithms. The analysis uses minimal com- binatorial structures (now also known as skinny trees) to characterize decoding errors when using message-passing de- coding algorithms.\nKoetter and Vontobel [3] analyzed LP-decoding of regular low-density parity-check (LDPC) codes. Their analysis is based on decomposing each codeword (and pseudocodeword) to a ﬁnite set of skinny trees with uniform vertex weights. Arora et al. [4] extended the work in [3] by introducing nonuniform weights to the vertices in the skinny trees, and deﬁning local-optimality. For a BSC, Arora et al. proved that local optimality implies both maximum-likelihood (ML) optimality and LP-optimality. They used analysis techniques similar to those used in density evolution analysis to improve bounds on the probability of a decoding error. This work\nwas further extended in [5] to MBIOS channels. The analyses in [3], [4], [5] are limited to skinny trees, the height of which is bounded by a half of the girth of the Tanner graph.\nVontobel [6] extended the decomposition of a codeword (and pseudocodeword) to skinny trees in graph covers. This enabled Vontobel to mitigate the limitation on the height by the girth. The decomposition is obtained by a random walk, and applies also to irregular Tanner graphs.\nContribution: We present a new combinatorial characteri- zation for local-optimality of a codeword in irregular Tanner codes with respect to any MBIOS channel. This characteri- zation is a generalization of [4], [6], [5] and is based on a conical combination of subtrees in the computation trees. The main novelty is that the subtrees may have any ﬁnite height h (even greater than the girth of the Tanner graph). In addition, the degrees of local-code nodes are not restricted to two. We prove that local-optimality in this new characterization implies ML-optimality and LP-optimality. Given a codeword and the channel output, we also show how to efﬁciently recognize if the codeword is locally optimal.\nFor a ﬁxed height, trees in our new characterization contain more vertices than a skinny-tree because the internal degrees are bigger. Hence, over an MBIOS channel, the probability of a locally-optimal certiﬁcate with dense deviations (local-code node degrees bigger than two) is greater than the probability of a locally-optimal certiﬁcate based on skinny trees (i.e., local- code nodes have degree two).\nWe extend the probabilistic analysis of the min-sum process by Arora et al. [4] to a sum-min-sum process on regular trees. For regular Tanner codes, we prove bounds on the word error probability of LP-decoding under MBIOS channels. These bounds are inverse doubly-exponential in the girth of the Tanner graph. We also prove bounds on the threshold of regular Tanner codes whose Tanner graphs have logarithmic girth. This means that if the noise in the channel is below that threshold, then the decoding error diminishes exponentially as a function of the block length.\nThe full details and proofs in this paper are contained in [7, Sections 3-6;8].\nGraph Terminology: Let G = (V, E) denote an undirected graph. Let N G (v) denote the set of neighbors of node v ∈ V , and let deg G (v) |N G (v)| denote the degree of node v in graph G. A path p = (v, . . . , u) in G is a sequence of vertices\nsuch that there exists an edge between every two consecutive nodes in the sequence p. A path p is backtrackless if every two consecutive edges along p do not close a cycle. Let |p| denote the number of edges in p. Let girth(G) denote the length of the shortest cycle in G.\nThe subgraph of G induced by S ⊆ V consists of S and all edges in E, both endpoints of which are contained in S. Let G S denote the subgraph of G induced by S.\nTanner-codes and Tanner graph representation: Let G = (V ∪J , E) denote an edge-labeled bipartite-graph, where V = {v 1 , . . . , v N } is a set of N vertices called variable nodes, and J = {C 1 , . . . , C J } is a set of J vertices called local-code nodes. We associate with each local-code node C j a linear code C j of length deg G (C j ). Let C J \t C j : 1 j J\ndenote the set of local-codes, one for each local code node. We say that v i participates in C j if (v i , C j ) is an edge in E.\nWe view a word x = (x 1 , . . . , x N ) ∈ {0, 1} N as an assignment to variable nodes in V where x i is assigned to v i . Let V j denote the set N G (C j ) ordered according to labels of edges incident to C j . Denote by x V j ∈ {0, 1} deg(C j ) the projection of the word x = (x 1 , . . . , x N ) onto entries associated with V j .\nThe Tanner code C(G, C J ) based on the labeled Tanner graph G is the set of vectors x ∈ {0, 1} N such that x V j is a codeword in C j for every j ∈ {1, . . . , J}. Let C j denote the extension of the local code C j from length deg(C j ) to length N deﬁned by C j {x ∈ {0, 1} N | x V j ∈ C j }. The Tanner code is simply the intersection of the extensions of the local codes, i.e., C(G, C J ) = j∈{1,...,J} C j .\nLet d j denote the minimum distance of the local code C j . The minimum local distance d ∗ of a Tanner code C(G, C J ) is deﬁned by d ∗ min j d j . We assume that d ∗ ≥ 2.\nIf the bipartite graph is (d L , d R )-regular, i.e., the vertices in V have degree d L and the vertices in J have degree d R , then the graph deﬁnes a (d L , d R )-regular Tanner code. If the Tanner graph is sparse, i.e., |E| = O(N ), then it deﬁnes a low- density Tanner code. Tanner codes with single parity check local codes that are based on sparse Tanner graphs are called low-density parity-check (LDPC) codes.\nLP decoding of Tanner codes over memoryless channels: Let c i ∈ {0, 1} denote the ith transmitted binary symbol (channel input), and let y i ∈ R denote the ith received symbol (channel output). A memoryless binary-input output- symmetric (MBIOS) channel is deﬁned by a conditional probability density function f (y i |c i = a) for a ∈ {0, 1}, that satisﬁes f (y i |0) = f (−y i |1). In MBIOS channels, the log-likelihood ratio (LLR) vector λ ∈ R N is deﬁned by λ i (y i ) ln f (y i |c i =0) f (y i |c i =1) for every input bit i. For a code C, Maximum-Likelihood (ML) decoding is equivalent to\nFeldman et al. [8], [9] introduced a linear programming relaxation for the problem of ML decoding of Tanner codes\nwith single parity check codes acting as local codes. This deﬁnition is based on a fundamental polytope that corresponds to the Tanner graph G. We consider an extension of this deﬁnition to the case in which the local codes are arbitrary linear-codes as follows. The generalized fundamental polytope P P(G, C J ) of a Tanner code C = C(G, C J ) is deﬁned by P \t C j ∈C J conv(C j ).\nGiven an LLR vector λ for a received word y, LP-decoding is deﬁned by the following linear program:\nThe difference between ML-decoding and LP-decoding is that the fundamental polytope P(G, C J ) may strictly contain the convex hull of C. Vertices of P(G, C J ) that are not codewords of C must have fractional components and are called pseudocodewords.\nIn this section we present combinatorial certiﬁcates for codewords of Tanner codes that apply both to ML-decoding and LP-decoding. A certiﬁcate is a proof that a given code- word is the unique solution of maximum-likelihood decoding and linear-programming decoding. The certiﬁcate is based on combinatorial weighted structures in the Tanner graph, referred to as local conﬁgurations. These local conﬁgurations generalize the minimal conﬁgurations (skinny trees) presented by Vontobel [6] as extension to Arora et al. [4]. We note that for Tanner codes, the support of each weighted local conﬁguration is not necessarily a local valid conﬁguration.\nNotation: Let y ∈ R N denote the word received from the channel. Let λ = λ(y) denote the LLR vector for y. Let x ∈ C(G) be a candidate for ˆ x ML (y) and ˆ x LP (y).\nDeﬁnition 1 (Path-Preﬁx Tree). Consider a graph G = (V, E) and a node r ∈ V . Let ˆ V denote the set of all backtrackless paths in G with length at most h that start at node r, and let ˆ E \t (p 1 , p 2 ) ∈ ˆ V × ˆ V p 1 is a preﬁx of p 2 , |p 1 | + 1 = |p 2 | . We identify the empty path in ˆ V with (r). Denote by T h r (G) ( ˆ V , ˆ E) the path-preﬁx tree of G rooted at node r with height h.\nPath preﬁx trees of G that are rooted in variable nodes are often called computation trees. We consider also path preﬁx trees of subgraphs that may be either rooted at a variable node or at a local-code node.\nWe use the following notation. Because vertices in T h r (G) are paths in G, we denote vertices in path-preﬁx trees by p and q. Vertices in G are denoted by u, v, r. For a path p ∈ ˆ V , let t(p) denote the last vertex (target) of path p. Denote by Preﬁx + (p) the set of proper preﬁxes of the path p, i.e.,\nare called variable paths, and paths in ˆ J are called local-code paths.\nThe following deﬁnitions expand the combinatorial notion of minimal valid deviations [2] and weighted minimal local- deviations (skinny trees) [4], [6] to the case of Tanner codes. Deﬁnition 2 (d-tree). Consider a Tanner graph G = (V ∪ J , E). Denote by T 2h r (G) = ( ˆ V ∪ ˆ J , ˆ E) the path-preﬁx tree of G rooted at node r ∈ V. A subtree T ⊆ T 2h r (G) is a d- tree if: (i) T is rooted at (r), (ii) for every local-code path p ∈ T ∩ ˆ J , deg T (p) = d, and (iii) for every variable path p ∈ T ∩ ˆ V, deg T (p) = deg T 2h r (p).\nLet T [r, 2h, d](G) denote the set of all d-trees rooted at r that are subtrees of T 2h r (G).\nIn the following deﬁnition we use \u201clevel\u201d weights w = (w 1 , . . . , w h ) that are assigned to variable paths in a subtree of a path-preﬁx tree of height 2h.\nDeﬁnition 3 (w-weighted subtree). Let T = ( ˆ V∪ ˆ J , ˆ E) denote a subtree of T 2h r (G), and let w = (w 1 , . . . , w h ) ∈ R h + \\ {0 h } denote a non-negative weight vector. Let w T : ˆ V → R denote a weight function based on weight vector w for variable paths p ∈ ˆ V deﬁned as follows. If p is an empty variable path, then w T (p) = 0. Otherwise,\nFor any w-weighted subtree w T of T 2h r (G), let π G,T ,w : V → R denote a function whose values correspond to the projection of w T to the Tanner graph G. That is, for every variable node v in G,\nFor a Tanner code C(G), let B (w) d ⊆ [0, 1] N denote the set of all projections of w-weighted d-trees to G. That is,\nFor two vectors x ∈ {0, 1} N and f ∈ [0, 1] N , let x ⊕ f ∈ [0, 1] N denote the relative point deﬁned by (x ⊕ f ) i |x i − f i | [8].\nFrom this point forward, let G = (V∪J , E) denote a Tanner graph, and let C(G) ⊂ {0, 1} N denote a Tanner code based on G with minimum local distance d ∗ . Let w = (w 1 , . . . , w h ) ∈ R h + \\ {0 h } denote a non-negative weight vector for some positive integer h ∈ N + and let 2 d d ∗ .\nThe following deﬁnition is an extension of local- optimality [4], [6] to Tanner codes on memoryless channels.\nDeﬁnition 4 (local-optimality). A codeword x ∈ C(G) is (h, w, d)-locally optimal with respect to λ ∈ R N if for all vectors β ∈ B (w) d ,\nBased on random walks on the Tanner graph, the result of Vontobel [6] implies that (h, w, 2)-local optimality is sufﬁcient both for ML-optimality and LP-optimality. The random walks are deﬁned in terms derived from the generalized fundamental polytope. We extend the results of Vontobel [6] to \u201cthicker\u201d sub-trees by using combinatorial arguments on graphs and the properties of graph cover decoding [10]. Speciﬁcally, we prove that (h, w, d)-local optimality, for any 2 d d ∗ , implies both ML- and LP-optimality (Theorems 5 and 10). Given the decomposition of Lemma 11 proved in Section V, the following theorem can be obtained by modiﬁcation of the proof of [4, Theorem 2] or [5, Theorem 6].\nTheorem 5 (local-optimality is sufﬁcient for ML). Let λ ∈ R N denote the LLR vector received from the channel. If x is an (h, w, d)-locally optimal codeword w.r.t. λ and some 2 d d ∗ , then x is also the unique maximum-likelihood codeword w.r.t. λ.\nIn order to prove a sufﬁcient condition for LP optimality, we consider graph cover decoding introduced by Vontobel and Koetter [10]. We note that the characterization of graph cover decoding and its connection to LP decoding can be extended to the case of Tanner codes in the generalized setting.\nWe use the terms and notation of Vontobel and Koetter [10] in the statements of Proposition 6 and Lemma 9. Speciﬁcally, let ˜ G denote an M -cover of G. Let ˜ x = x ↑M ∈ C( ˜ G) and ˜ λ = λ ↑M ∈ R N ·M denote the M -lifts of x and λ, respectively. Proposition 6 (local-optimality of all-zero codeword is pre- served by M -lifts). 0 N is an (h, w, d)-locally optimal code- word for λ ∈ R N if and only if 0 N ·M is an (h, w, d)-locally optimal codeword w.r.t. ˜ λ.\nFor a word x ∈ {0, 1} N , let (−1) x ∈ {±1} N denote a vector whose ith component equals (−1) x i . For two vectors y, z ∈ R N , let \u201c∗\u201d denote coordinatewise multiplication, i.e., y ∗ z (y 1 · z 1 , . . . , y N · z N ).\n(−1) x ∗ λ, β = λ, x ⊕ β − λ, x . \t (6) The following proposition states that the mapping (x, λ) →\nProposition 8 (symmetry of local-optimality). For every x ∈ C, x is (h, w, d)-locally optimal for λ if and only if 0 N is (h, w, d)-locally optimal for (−1) x ∗ λ.\nThe following lemma states that local-optimality is pre- served by lifting to an M -cover.\nLemma 9. x is (h, w, d)-locally optimal w.r.t. λ if and only if ˜ x is (h, w, d)-locally optimal w.r.t. ˜ λ.\nThe following theorem is obtained as a corollary of Theo- rem 5 and Lemma 9. The proof is based on arguments utilizing properties of graph cover decoding. Those arguments are used\nfor a reduction from ML-optimality to LP-optimality similar to the reduction presented in the proof of [5, Theorem 8].\nTheorem 10 (local optimality is sufﬁcient for LP optimality). If x is an (h, w, d)-locally optimal codeword w.r.t. λ, then x is also the unique optimal LP solution given λ.\nIn this section we address the problem of how to verify whether a codeword x is (h, w, d)-locally optimal with respect to λ. By Proposition 8, this is equivalent to verifying whether 0 N is (h, w, d)-locally optimal with respect to (−1) x ∗ λ.\nThe veriﬁcation algorithm is listed as Algorithm 1. It applies dynamic programming to ﬁnd, for every variable node v, a d-tree T v , rooted at v, that minimizes the cost\n(−1) x ∗ λ, π G,T v ,w . The algorithm returns false if and only if it ﬁnds a deviation with nonpositive cost.\nThe algorithm is presented as a message passing algorithm. In every step, a node propagates to its parent the minimum cost of the d-subtree that hangs from it based on the minimum values received from its children. The time and message complexity of Algorithm 1 is O(|E| · h). The following notation is used in Line 8 of the algorithm. For a set S of real values, let min [i] {S} denote the ith smallest member in S.\nAlgorithm 1 VERIFY - LO (x, λ, h, w, d) - An iterative veriﬁca- tion algorithm. Let G = (V ∪ J , E) denote a Tanner graph. Given an LLR vector λ ∈ R |V| , a codeword x ∈ C(G), level weights w ∈ R h + , and a degree d ∈ N + , outputs \u201ctrue\u201d if x is (h, w, d)-locally optimal w.r.t. λ; otherwise, outputs \u201cfalse\u201d.\n9: end for 10: end for\nIn this section we prove Lemma 11, the key structural lemma in the proof of Theorem 5. This Lemma states that every codeword of a Tanner code is a ﬁnite sum of projections of weighted trees in the computation trees of G.\nThroughout this section, let C(G) denote a Tanner code with minimum local distance d ∗ , let x denote a nonzero codeword, let h ∈ N + , and let w ∈ R h + \\ {0 h }.\nLemma 11. Consider a codeword x = 0 N . Then, for every 2 d d ∗ , there exists a distribution ρ over d-trees of G of height 2h such that for every weight vector w ∈ R h + \\ {0 h }, it holds that\nThe proof of Lemma 11 is outlined in this section and is based on Lemmas 12\u201313 and Corollary 14. Lemma 12 states that every codeword x ∈ C(G) can be decomposed into a set of weighted path-preﬁx trees. The number of trees in the decomposition equals x 1 . Lemma 13 states that every weighted path-preﬁx tree is a convex combination of weighted d-trees. This lemma implies that the projection of a weighted path-preﬁx tree equals to the expectation of projections of weighted d-trees (Corollary 14).\nFor a codeword x ∈ C(G), let V x {v ∈ V | x v = 1}. Let G x denote the subgraph of G induced by V x ∪ N (V x ).\nLemma 12. For every codeword x = 0 N , for every weight vector w ∈ R h + , and for every variable node v ∈ V,\nProof sketch: If x v = 0, then π G,T 2h r (G x ),w (v) = 0. It remains to show that equality holds for variable nodes v ∈ V x . Consider a uniform weight vector η = 1 h . Then, for every 1 i 2h, the accumulated contribution of all the path-preﬁx trees {T (η) r (G x ) : x r = 1} to path nodes p with length i such that t(p) = v equals 1/h. Therefore, for every weight vector w ∈ R h + and every 1 \t h, the accumulated contribution of all the w-weighted path-preﬁx trees to path nodes p of length 2 such that t(p) = v equals w / w 1 . The lemma follows by projecting the path-preﬁx trees {T (w) r (G x ) : x r = 1} to the base graph G.\nLemma 13. Consider a subgraph G x of a Tanner graph G, where x ∈ C(G)\\{0 N }. Then, for every variable node r ∈ G x , every positive integer h, every 2 d d ∗ , and every weight vector w ∈ R h + , it holds that\nProof sketch: The lemma follows by showing that the uniform distribution over w-weighted d-trees has the property that the expectation of trees over the distribution equals\nThe following corollary follows Lemma 13 and linearity of expectation.\nCorollary 14. For every positive integer h, every 2 d d ∗ , and every weight vector w ∈ R h + , it holds that\nBefore proving Lemma 11, we state a proposition from probability theory.\nProposition 15. Let {ρ r } K i=1 denote K probability distribu- tions. Let ρ 1 K K r=1 ρ r . Then, K r=1 E ρ r [x] = K · E ρ [x].\nProof of Lemma 11. By Lemma 12 and Corollary 14 we have for every v ∈ V x\nLet ρ denote the distribution deﬁned by ρ \t 1 x 1 · r∈V x ρ r . By Proposition 15 and (7), x v = x 1 · E ρ π G,T ,w , and the lemma follows.\nDue to lack of space, we only state the main result of this section for a concrete case and the BSC (Theorem 16). Concrete bounds are given for a (2, 16)-regular Tanner code with code rate at least 0.375 when using [16, 11, 4]-extended Hamming codes as local codes. We refer the reader to [7, Section 8] for further details and the case of general regular Tanner codes over BSC and MBIOS channels.\nTheorem 16. Let G denote a (2, 16)-regular bipartite graph with girth g, and let C(G) denote a Tanner code based on G with minimum local distance d ∗ = 4. Let x ∈ C(G) be a codeword. Suppose that y ∈ {0, 1} N is obtained from x by ﬂipping every bit independently with probability p. Then,\n1) [ﬁnite length bound] Let d = d 0 and p p 0 . For the values of d 0 and p 0 in Table I it holds that x is the unique optimal solution to the LP decoder with a probability of at least\nPr ˆ x LP (y) = x 1 − N · α (d−1) 1 4 g for some constant α < 1.\n2) [asymptotic bound] Let d = d 0 and g = Ω(log N ) sufﬁciently large. For the values of d 0 and p 0 in Table I it holds that x is the unique optimal solution to the LP decoder with a probability of at least 1 − exp(−N δ ) for some constant 0 < δ < 1, provided that p p 0 (d 0 ).\nA new combinatorial characterization for local optimality of a codeword in an irregular Tanner code is presented. This characterization provides an ML-certiﬁcate and an LP- certiﬁcate for a given codeword. Moreover, the certiﬁcate can be efﬁciently computed by a dynamic programming algorithm. The main novelty in this characterization is that it is based\non linear combination of subtrees in the computation trees. These trees may have any degree d in the local-code nodes, for 2 d d ∗ . This increased degree enables each subtree to be larger than a skinny tree. The bounds on the probability that a local-optimality certiﬁcate exists for regular Tanner codes show that the larger a subtree is in the decomposition, the higher the probability that a certiﬁcate exists.\nThe error correction capability of expander codes depends on the expansion, thus a fairly large degree and huge block- lengths are required to achieve good error correction. Feldman and Stein [11] analyzed LP decoding of (2, d R )-regular ex- pander codes. In the case of code rate of 0.375, they proved that LP-decoding can recover any pattern of at most 0.0008N bit ﬂips (with d R 16). The best results for iterative decoding of expander codes, reported by Skachek and Roth [12], implies correction of at most 0.0016N bit ﬂips. These analyses yield overly pessimistic predictions for the average-case (i.e., the BSC). Theorem 16(2) deals with average case analysis and implies that LP-decoding can correct up to 0.044N bit ﬂips with high probability. Furthermore, previous iterative decoding algorithms for expander Tanner codes deal only with bit- ﬂipping channels. Our analysis for LP-decoding applies to any MBIOS channel, in particular, it can be applied to the BI-AWGN channel.\nThe lower bounds on the noise threshold proved for Tanner codes do not improve the best previous bounds for regular LDPC codes. An open question is whether using deviations denser than skinny trees for Tanner codes can beat the best previous bounds for regular LDPC codes [4], [5]. In particular, for a concrete family of Tanner codes with rate 1 2 , it would be interesting to prove lower bounds on the threshold of LP- decoding that are larger than p ∗ = 0.05 in the case of BSC, and σ ∗ = 0.0735 in the case of BI-AWGN channel."},"refs":[{"authors":[{"name":"R. M. Tanner"}],"title":{"text":"A recursive approach to low-complexity codes"}},{"authors":[{"name":"N. Wiberg"}],"title":{"text":"Codes and decoding on general graphs"}},{"authors":[{"name":"R. Koetter"},{"name":"P. O. Vontobel"}],"title":{"text":"On the block error probability of LP decoding of LDPC codes"}},{"authors":[{"name":"S. Arora"},{"name":"C. Daskalakis"},{"name":"D. Steurer"}],"title":{"text":"Message passing algorithms and improved LP decoding"}},{"authors":[{"name":"N. Halabi"},{"name":"G. Even"}],"title":{"text":"LP decoding of regular LDPC codes in memoryless channels"}},{"authors":[{"name":"P. Vontobel"}],"title":{"text":"A factor-graph-based random walk, and its relevance for LP decoding analysis and Bethe entropy characterization"}},{"authors":[{"name":"G. Even"},{"name":"N. Halabi"}],"title":{"text":"On decoding irregular Tanner codes with local- optimality guarantees"}},{"authors":[{"name":"J. Feldman"}],"title":{"text":"Decoding error-correcting codes via linear programming"}},{"authors":[{"name":"J. Feldman"},{"name":"M. J. Wainwright"},{"name":"D. R. Karger"}],"title":{"text":"Using linear program- ming to decode binary linear codes"}},{"authors":[{"name":"P. O. Vontobe"},{"name":"R. Koette"}],"title":{"text":"Graph-cover decoding and ﬁnite-length analysis of message-passing iterative decoding of LDPC codes, CoRR, http://www"}},{"authors":[{"name":"J. Feldman"},{"name":"C. Stein"}],"title":{"text":"LP decoding achieves capacity"}},{"authors":[{"name":"V. Skachek"},{"name":"R. M. Roth"}],"title":{"text":"Generalized minimum distance iterative decoding of expander codes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569559805.pdf"},"links":[{"id":"1569565867","weight":10},{"id":"1569564605","weight":5},{"id":"1569566683","weight":5},{"id":"1569566855","weight":5},{"id":"1569566761","weight":5},{"id":"1569564469","weight":15},{"id":"1569558325","weight":5},{"id":"1569565837","weight":5},{"id":"1569560427","weight":5},{"id":"1569560613","weight":5},{"id":"1569565809","weight":5},{"id":"1569556091","weight":5},{"id":"1569564387","weight":5},{"id":"1569566795","weight":5},{"id":"1569561679","weight":5},{"id":"1569566787","weight":5},{"id":"1569566895","weight":5},{"id":"1569564613","weight":5},{"id":"1569565321","weight":5},{"id":"1569566193","weight":5},{"id":"1569566311","weight":5},{"id":"1569565535","weight":5},{"id":"1569565257","weight":5},{"id":"1569566139","weight":5},{"id":"1569566445","weight":10},{"id":"1569565559","weight":5},{"id":"1569566809","weight":10},{"id":"1569565817","weight":5},{"id":"1569557083","weight":10},{"id":"1569566003","weight":10},{"id":"1569565185","weight":5},{"id":"1569564973","weight":5},{"id":"1569566695","weight":5},{"id":"1569566297","weight":5},{"id":"1569565493","weight":10},{"id":"1569559199","weight":5},{"id":"1569565665","weight":15},{"id":"1569566779","weight":5},{"id":"1569565241","weight":5},{"id":"1569565661","weight":5},{"id":"1569566887","weight":10},{"id":"1569566917","weight":5},{"id":"1569566595","weight":5},{"id":"1569566237","weight":5},{"id":"1569566529","weight":5},{"id":"1569565237","weight":10},{"id":"1569566819","weight":5},{"id":"1569565541","weight":10},{"id":"1569563975","weight":5},{"id":"1569564787","weight":10},{"id":"1569564923","weight":15},{"id":"1569565039","weight":5},{"id":"1569566601","weight":5},{"id":"1569557851","weight":5},{"id":"1569565035","weight":5},{"id":"1569564961","weight":5},{"id":"1569567013","weight":5},{"id":"1569565853","weight":5},{"id":"1569565731","weight":5},{"id":"1569566797","weight":10},{"id":"1569565707","weight":5},{"id":"1569566973","weight":5},{"id":"1569565139","weight":10}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S15.T5.1","endtime":"10:10","authors":"Nissim Halabi, Guy  Even","date":"1341568200000","papertitle":"Linear-Programming Decoding of Tanner Codes with Local-Optimality Certificates","starttime":"09:50","session":"S15.T5: Decoding Algorithms for Codes on Graphs","room":"Kresge Little Theatre (035)","paperid":"1569559805"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
