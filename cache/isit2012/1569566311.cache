{"id":"1569566311","paper":{"title":{"text":"Nonparametric Decentralized Detection Based on Weighted Count Kernel"},"authors":[{"name":"Jiayao Hu"},{"name":"Yingbin Liang"},{"name":"Eric P. Xing"}],"abstr":{"text":"Abstract\u2014The nonparametric decentralized detection problem is investigated, in which the joint distribution of the environmen- tal event and the sensors\u2019 observations are not known and only a set of training samples are available. The system features rate constraints, i.e., integer bit constraints on sensors\u2019 transmissions, different qualities of observations, additional observations to the fusion center, and multi-level tree-structured network. Our study adopts the kernel-based nonparametric approach proposed by Nguyen, Wainwright, and Jordan with the following gen- eralization. A weighted count kernel is introduced so that the corresponding reproducing kernel Hilbert space (RKHS) (over which the fusion center\u2019s decision rule is optimized) allows the fusion center\u2019s decision rule to count information from sensors and its own observations differently. In order to ﬁnd the optimal decision rules, our optimization is solved by alternatively and recursively conducting three optimization steps: ﬁnding the optimal weight parameters in the weighted count kernel for selecting the best associated RKHS, ﬁnding the best optimal decision rule for the fusion center over the identiﬁed RKHS, and ﬁnding the local decision rules for sensors. Generalization to multilevel tree-structured networks is also discussed. Finally numerical results are provided to demonstrate the performance based on the proposed weighted count kernel."},"body":{"text":"As a classical decision-making problem, decentralized de- tection has been extensively studied in the literature, e.g., [1]\u2013[3] and references therein. Most of previous work on this topic used parametric approaches, which assumed the joint distribution of the environmental event and the sensors\u2019 observations is known in advance.\nNonparametric (de)centralized detection was studied previ- ously in, e.g., [4], [5], which employed detectors that perform well for certain statistical environments. A learning-based nonparametric linear regression problem was studied in [6], [7]. More recently, a kernel-based classiﬁcation approach was proposed in [8], which is more generally applicable with mathematical guarantee on the performance. The basic idea is to introduce a kernel function that determines a reproducing kernel Hilbert space (RKHS), over which the decision rule of the fusion center is searched to optimize a given risk function. It has been shown by numerical examples in [8] that the kernel-based approach yields better performances than other approaches based on estimating joint distributions. Further- more, compared to parametric approaches, such a kernel-based nonparametric approach is also applicable for the case with\ncorrelated observations, in which the correlation is implicitly embedded in training data and their inﬂuence on the decision rules is automatically incorporated by optimizing empirical risk functions determined by the training data. In our previous work [9], we generalized the kernel-based approach in [8] to tree-structured sensor networks, and proposed a distributed protocol which achieves an efﬁcient implementation for ﬁnd- ing the optimal decision rules over a tree structure.\nIn this paper, we study more realistic sensor networks, which generalize the models studied in [8], [9] to include several new features: (1) sensors\u2019 observations can have dif- ferent qualities and hence different alphabet sizes due to their different locations in capturing the environmental event; (2) the fusion center can receive observations of the environmental event directly; (3) sensors\u2019 transmissions to the fusion center are subject to certain rate bit constraints and hence sensors\u2019 quantization levels can be different; and (4) sensors may be networked in a multilevel tree structure toward the fusion center. Our goal is to characterize the impact of these practical features on the decision rules of the fusion center and sensors in the nonparametric decentralized detection.\nOur study adopts the nonparametric kernel-based approach proposed by Nguyen, Wainwright, and Jordan in [8] with the following generalization. We introduce a weighted count kernel so that the corresponding Hilbert space, i.e., the RKHS, (over which the fusion center\u2019s decision rule is optimized) allows the fusion center\u2019s decision rule to count information from sensors and its own observations differently based on the quality of these information sources. In this way, by introducing the weight parameters in the weighted count kernel into the risk minimization framework, the best RKHS associated with the weighted count kernel is selected jointly with the decision rules for the fusion center and sensors. Thus, the impact of the network features including the quality of sensors\u2019 observations, fusion center\u2019s direct observations, and rate constraints on sensors\u2019 transmissions are naturally incor- porated into the fusion center\u2019s decision rules via selecting the RKHS that these decision rules lie in.\nWe solve the risk minimization for ﬁnding the decision rules by recursively and alternatively conducting three optimization steps: ﬁnding the optimal weight parameters for selecting the best RKHS associated with the weighted count kernel,\nﬁnding the optimal decision rule for the fusion center over the identiﬁed RKHS, and ﬁnding the local decision (i.e., quantization) rules for sensors. For each step, risk functions are typically convex, but not differentiable everywhere. By adopting the approach in [8] based on conjugate dual argu- ments, we analytically characterize the optimal decision rule for the fusion center. We also characterize some components in subdifferentials for optimizing weight parameters and decision rules for sensors efﬁciently. We further discuss the generaliza- tion to multilevel tree-structured networks, in which the impact of the network structure on the decision rules is also captured by the selection of the optimal weighted count kernel.\nWe also derive an upper bound on the true risk function based on the approximate empirical risk function, whose asymptotic behavior suggests that additional optimization over RKHSs associated with the weighted count kernel does not require more training samples for the approximate empirical risk function to be close to the true risk function from the above. We ﬁnally provide numerical results to demonstrate the impact of the rate constraints of sensors\u2019 transmissions to the fusion center and direct observations of the fusion center on the detection error probability based on our weighted count kernel approach.\nThe rest of the paper is organized as follows. In section II, we provide the necessary background on learning by kernels. In section III, we describe our system model and problem formulation. In section IV, we provide our main results in ﬁnding the optimal decision rules. In section V, we discuss about the generalization of our study to multilevel tree-structured sensor networks. In section VI, we provide the simulation results, and ﬁnally in section VII, we conclude our paper with a few remarks.\nIn this section, we introduce the basic concepts and deﬁni- tions on learning by kernels, which is the basic technique that we apply in this paper.\nDeﬁnition 1. A function k on X 2 → R is called a kernel if for all positive integer m and all x 1 , · · · , x m ∈ X , the m×m matrix K with elements K ij = k(x i , x j ) for i, j = 1, . . . , m is positive semideﬁnite.\nDeﬁnition 2. A Hilbert space H containing functions f : X → R is called a reproducing kernel Hilbert space (RKHS) if there exists a kernel k : X × X → R with the following properties:\nin particular, k(x, ·), k(x , ·) = k(x, x ), and \u2022 k spans H.\nGiven a kernel k, we deﬁne a feature mapping Φ : x ∈ X → k(·, x), which maps an element x ∈ X to a function. We then deﬁne a vector space containing\nwhere m is any positive integer, α i ∈ R, and x 1 , · · · , x m ∈ X are arbitrary. For this vector space, we deﬁne an inner product between f and another function g( ·) = m j=1 β j k(·, x j ) as\nIt can be shown that after completing this vector space, we obtain a RKHS associated with the kernel k.\nWe study the decentralized detection problem over a sensor network (see Fig. 1), in which sensors receive observations about an environmental event, quantize their observations based on their own local decision rules (i.e., quantization rules), and then forward their quantized information to a fusion center, which will make the decision about the state of the environmental event. We use Y to denote the environmental event, which can take binary values +1 and −1. We assume there are S sensors in the network. We use X s ∈ X to denote the observation received by sensor s for s = 1, . . . , S, and use Z s to denote the quantized value of sensor s. The sensor\u2019s observation X s can have different alphabet sizes, which may possibly due to nonuniform noise corruption of signals received by these sensors.\nThe decision rule of a sensor can be characterized by a probability distribution Q s Z s |X s (z s |x s ) mapping from its input variable X s to an output variable Z s , thus allowing a random decision rule. In particular, we assume that there is a bit constraint R s (which is assumed to be an integer) on each sensor\u2019s transmission to the fusion center, and hence each Z s has an alphabet size 2 R s . Consequently, Z s may also have different alphabet size due to different rate constraints. We also assume that the fusion center receives not only quantized information from all sensors but also observations directly from the environment denoted by X 0 , and hence the fusion center\u2019s decision rule can be written as a function γ(Z 1 , . . . , Z S , X 0 ).\nIn our problem, we assume that the joint probability distri- bution of the event and the observations for all sensors and the fusion center, i.e., P (Y, X 0 , . . . , X S ), is unknown. Instead, a set of training data are available, i.e., (x 0 i , . . . , x S i , y i ) for i = 1, . . . , N. We adopt the framework of empirical risk min- imization for decentralized detection in [8] to ﬁnd the jointly optimal decision rule γ for the fusion center and decision rules\nQ s for all sensors that minimize a given risk function φ( ·) which is properly chosen as the system performance measure.\nWe consider decision rule for the fusion center that lies in the RKHS H determined by a kernel function k(·, ·) : (Z × X )×(Z ×X ) → R. We note that the domain of the kernel has one more space compared to that in [8] to take into account the observations of the fusion center. As such, we can express the fusion center\u2019s decision rule as:\nOur problem is then formulated as the following optimiza- tion problem:\nwhere x i = (x 1 i , . . . , x S i ) and Q is the set which includes all possible conditional probabilities for every sensor. In particu- lar, Q(z |x i ) can be decomposed as Q(z|x i ) = s i=1 Q(z s |x s ), because sensors follow independent decision rules.\nSince it is computationally complex to solve the above optimization problem in general. Similarly to [8], by applying Jensen\u2019s inequality, we obtain a lower bound for (1) as a relaxation.\nwhere Φ (x i , x 0 i ) = z Φ(z, x 0 i )Q(z|x i ) ∈ H. It can be shown that the approximate empirical risk function approaches to the true risk function from the above as the number of training data becomes large as in Section IV-D.\nIn this section, we introduce a weighted count kernel, which thus deﬁnes a Hilbert space of the RKHS that enables to count contributions from sensors differently based on quality of observations, transmission constraints, additional observations of the fusion center, and the impact network structure for designing decision rules of the fusion center and sensors. We then optimize the risk function over the weight parameters in the weighted count kernel in order to select the optimal Hilbert space that the decision rules lie in jointly with the decision rules for the fusion center and sensors.\nWe introduce the following weighted count kernel, which can be shown to satisfy the deﬁnition of kernel given in Deﬁnition 1\ns , represents the contribution of sensor s in the decision rule of the fusion center. In particular, β 0 represents the contribution of the direct observations of the fusion center. Thus, the Hilbert space H β over which the decision rule of the fusion center is chosen is spanned by the weighted count kernel k β (·, ·).\nRemark 1. Although our study focuses on the weighted count kernel, the idea of introducing weights to kernels for counting information differently may be applicable for more general types of kernels.\nBy introducing the optimization of the weight parameters into the risk minimization problem, our optimization problem now becomes:\nWe note that without loss of generality, we set one di- mension of β s to be ﬁxed as a reference value. Due to possibly large number of training samples and sensors, di- mensions of the parameters to be optimized can be very large. Hence, optimizing over all parameters simultaneously is very complex. Furthermore, the risk function φ( ·) such as the hinge loss function is not differentiable everywhere, which adds more complexity to the problem. Thus, we adopt the coordinate gradient algorithm to recursively and alternatively optimize over these three types of parameters, i.e., β, w and Q. This approach is justiﬁed because the objective function is convex over any type of parameter given two other types of parameters.\nWe optimize the objective function over the weight parame- ters β s , s = 0, 1, . . . , S one after another with w and all Q(·|·)s ﬁxed. Since risk functions are typically nondifferentiable ev- erywhere (e.g. the hinge loss function we use for our numerical simulation), we characterize an analytical expression for an element in the subdifferential of the objective function with respect to each component β s in order for implementing the gradient algorithm, which can be complex otherwise.\nProposition 1. Consider β 0 , which corresponds to the contri- bution of direct observations of the fusion center. An element in the subdifferential of the objective function with respect to the weight parameter β 0 with w, all Qs and other β s , s = 0 being ﬁxed is given by\nConsider β s for s = 1, . . . , S, which corresponds to the contribution of sensor s. An element in the subdifferential of the objective function with respect to the weight parameter β s with w, all Qs and all other β s , s = s being ﬁxed is given by\nThe above proposition can be proved by conjugate dual arguments, which is omitted here due to the space limitations.\nGiven the optimal RKHS associated with k β , we now ﬁnd the optimal decision rule for the fusion center by optimizing G(β; w; Q) with respect to w. Due to the arguments similar to the proof of the kernel representer theorem [10], the optimal w ∈ H β can be expressed as w = N i=1 α i y i Φ β (x i , x 0 i ). Following the arguments in [8], the coefﬁcients α i for i = 1, . . . , N in w should solve the following maximization prob- lem.\n⎨ ⎩\nThe above problem is a quadratic optimization problem and can hence be solved easily by alternatively updating the value of each α i . We note that the difference here from [8] is that the inner product Φ β (x i , x 0 i ), Φ β (x j , x 0 j ) H β is taken over the Hilbert space RKHS which allows the fusion center\u2019s decision rule to count contributions from sensors and its own observations differently based on the quality of sensors\u2019 observations, the quality of the fusion center\u2019s observations, and the rate constraints on sensors\u2019 transmissions.\nIn this subsection, we ﬁnd the optimal decentralized deci- sion rules for sensors. Similar to the optimization over β, the major step here is to ﬁnd an element in the subdifferential of the objective function G(β; w; Q) with respect to each Q s (·|·). Proposition 2. Consider sensor s for s = 1, . . . , S. An element in the subdifferential of the objective function with respect to the local decision rule of sensor s with w, β and all other Q( ·|·)s being ﬁxed is given by\nThe proof here is based on conjugate dual arguments and is omitted due to the space limitations.\nIn this section, we derive bounds on the true risk func- tion Eφ(Y γ(Z, X 0 )) based on the approximate empirical risk function inf f∈F 1 N N i=1 φ(y i w, Φ β (x i , x 0 i ) H β ), where γ(Z, X 0 ) = w, Φ β (Z, X 0 ) H β , w ∈ H β , Q ∈ Q, β ∈ R S+1 ,\nFollowing the arguments similar to [8], we show that with a probability at least 1 − 2δ the true φ-risk Eφ(Y γ(Z, X 0 ))\nφ(y i w, Φ β (x i , x 0 i ) H β ) − 2lR N (F) − ln( 2 N ) 2N ≤ inf f∈F Eφ(Y γ(Z, X 0 )) ≤\n1 N\nwhere φ is Lipschitz with constant l, F 0 includes only de- terministic rules for sensors, and R N (F) is the Rademacher complexity of the function class F given in [8]. It is clear that the bounds on the Rademacher complexity characterize how close the approximate empirical risk function that the designed decision rules achieve is to the true risk function. In the following, we provide an bound on the Rademacher complexity for our problem.\nProposition 3. Let the alphabet sizes of the observations be bounded by C x and the alphabet sizes of the quantized variables by all sensors be bounded by C z , i.e. the rate constraints from the sensors to the fusion center be bounded by log C z . An upper bound on the Rademacher complexity is given by\nwhere k ((z, x 0 ), (z , x 0 )) = S s=1 I[z s = z s ] + I[x 0 = x 0 ], Φ (x i , x 0 i ) = z k ((z, x 0 i ), ·)Q(z|x i ), B is the upper bound on w H β , and D is the upper bound on β s .\nWe note that the above bound approaches zero as the num- ber N of training samples approaches inﬁnity, thus providing a tightest upper bound. It can be seen from the above that the number of samples needed for R N (F 0 ) to approach zero is the same as in [8] suggesting that optimization over weight parameters in the weighted count kernel does not require more number of samples for the approximate empirical risk function to be close to the true risk function.\nIn this section, we brieﬂy describe how to generalize our study to a tree-structured sensor network. In particular, we now consider a sensor network with sensors conﬁgured in a tree structure with the fusion center being the root of the tree. All sensors and the fusion center can receive observations of an environmental event. Sequentially from the leave sensors, each sensor node quantizes its observation and its received information from all its children in the tree and then forwards the quantized information to its parent. Finally, the root, which is the fusion center, makes a decision about the event state based on its own observations and the information received from all its children sensors. There are rate constraints (i.e.,\ninteger bit constraints) for all transmission links in the network so that the quantized alphabet size of each sensor is determined by the rate constraint on this sensor\u2019s transmission to its parent.\nThe optimization problem for ﬁnding the decision rules for the fusion center and sensors can be formulated and solved in the same fashion as for the one-level sensor network studied in Section IV. The difference lies in that now the optimization over the weights in the weighted count kernel also takes into account the impact of the network structure on the fusion center\u2019s decision rule. More details will be included in the journal version of this work, which is now in preparation.\nWe ﬁrst study the impact of the fusion center\u2019s observations on the performance of the system. In particular, we compare the decision error probabilities for two cases with the fusion center having or not having observations, respectively. For both cases, we assume that the sensors have independent observations. We generate 300 data samples for training, and 10000 data samples for testing. For each sample, we generate y i with uniform probability on +1 and −1. We then generate the noise variable n s for each sensor and the fusion center independently, where P (n s = 0) = 0.6, P (n s = +1) = 0.2 and P (n s = −1) = 0.2. Each observation equals x s = y + n s for s = 0, 1, . . . , S. In Table I, we compare the error proba- bilities for cases 1 and 2 with the fusion center respectively receiving or not receiving observations. It can be seen that case 2 always has smaller error probabilities than case 1 due to the additional observations at the fusion center. Such improvement gets smaller as the number of sensors increases, because the observations by the fusion center should have less effect on the performance of the system. It can also be seen that the weight β 0 of the fusion center\u2019s direct observations is bigger than the weights of sensors which are set to be one, suggesting that the fusion center count on direct observations more than quantized information.\nWe now study the impact of the rate constraints on the performance of system. We study a four-sensor network. Training samples are generated in the same fashion as the ﬁrst experiment. We study the following ﬁve cases. For the ﬁrst case, all sensors\u2019 transmission rates to the fusion center are limited to 1 bit. For the second case, one sensor\u2019s transmission rate increases to 2 bits, and all other sensors\u2019 rates are still 1 bit. For each of the remaining three cases, we allow one more sensor\u2019s transmission rate to increase to 2 bits.\nIn Table II, we provide the optimal weights corresponding to all sensors and the resulting detection error probability for the ﬁve cases. It is clear that the detection error decreases as more sensors are allowed to transmit at 2 bits. Furthermore, for each case, the weight parameters corresponding to the sensors that\ncan transmit at 2 bits to the fusion center are higher than those that can transmit only at 1 bit. This is reasonable because the fusion center counts more on the less quantized information transmitted from sensors.\nIn this paper, we have proposed a weighted count kernel, and introduced the associated weight parameters into the risk minimization formulation for ﬁnding decision rules for fusion center and sensors. Consequently, these decision rules can take into account the quality of sensors\u2019 observations, the quality of the fusion center\u2019s observations, and the rate constraints on sensors\u2019 transmissions. We have also exploited the properties of the optimization problem for simplifying the optimization algorithm. We have further discussed the generalization to multilevel tree-structured networks based on our previous work [9]. Moreover, we have demonstrated the performance of the weighted count kernel via numerical results.\nThe work of J. Hu and Y. Liang was supported by the National Science Foundation CAREER Award under Grant CCF-10-26565."},"refs":[{"authors":[{"name":"J. N. Tsitsiklis"}],"title":{"text":"Decentralized detection"}},{"authors":[{"name":"R. S. Blum"},{"name":"S. A. Kassam"},{"name":"H. V. Poor"}],"title":{"text":"Distributed detection with multiple sensors: Part II-advanced topics"}},{"authors":[{"name":"J. F. Chamberland"},{"name":"V. V. Veeravalli"}],"title":{"text":"Decentralized detection in sensor networks"}},{"authors":[{"name":"M. M. AI-Ibrahim"},{"name":"P. K. Varshney"}],"title":{"text":"Nonparametric sequential detec- tion based on multisensor data"}},{"authors":[{"name":"A. Nasipuri"},{"name":"S. Tantaratana"}],"title":{"text":"Nonparametric distributed detection using wilconxin statistics"}},{"authors":[{"name":"J. B. Predd"},{"name":"S. R. Kulkarni"},{"name":"H. V. Poor"}],"title":{"text":"Distributed learning in wireless sensor networks: Application issues and the problem of distributed inference"}},{"authors":[],"title":{"text":"Consistency in models for distributed learning under commu- nication constraints"}},{"authors":[{"name":"X. Nguyen"},{"name":"M. J. Wainwright"},{"name":"M. I. Jordan"}],"title":{"text":"Nonparametric decentralized detection using kernel methods"}},{"authors":[{"name":"J. Hu"},{"name":"Y. Liang"},{"name":"E. P. Xing"}],"title":{"text":"Nonparametric decision making based on tree-structured information aggregation"}},{"authors":[{"name":"B. Scholkop"},{"name":"A. Smol"}],"title":{"text":"Learning with Kernels"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566311.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S2.T8.3","endtime":"12:30","authors":"Jiayao Hu, Yingbin Liang, Eric P Xing","date":"1341231000000","papertitle":"Nonparametric Decentralized Detection Based on Weighted Count Kernel","starttime":"12:10","session":"S2.T8: Distributed Detection and Estimation","room":"Stratton (491)","paperid":"1569566311"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
