{"id":"1569561085","paper":{"title":{"text":"Information Divergence is more χ 2 -distributed than the χ 2 -statistics"},"authors":[{"name":"Peter Harremo¨es"},{"name":"G´abor Tusn´ady"}],"abstr":{"text":"Abstract\u2014For testing goodness of ﬁt it is very popular to use either the χ 2 -statistic or G 2 -statistics (information divergence). Asymptotically both are χ 2 -distributed so an obvious question is which of the two statistics that has a distribution that is closest to the χ 2 -distribution. Surprisingly, when there is only one degree of freedom it seems like the distribution of information divergence is much better approximated by a χ 2 -distribution than the χ 2 -statistic. For random variables we introduce a new transformation that transform several important distributions into new random variables that are almost Gaussian. For the binomial distributions and the Poisson distributions we formulate a general conjecture about how close their transform are to the Gaussian. The conjecture is proved for Poisson distributions."},"body":{"text":"We consider the problem of testing goodness-of-ﬁt in a discrete setting. Here we shall follow the classic ap- proach to this problem as developed by Pearson, Neyman and Fisher. The question is whether a sample with ob- servation counts (X 1 , X 2 , . . . , X k ) has been generated by the distribution Q = (q 1 , q 2 , . . . , q k ) . For sample size n the counts (X 1 , X 2 , . . . , X k ) is assumed to have a multi- nomial distribution. We introduce the empirical distribution\n, X 2 n , . . . , X k n where n denotes the sample size n = X 1 + X 2 + · · · + X k . Often one uses one of the Csisz´ar [1] f -divergences\nD f ˆ P , Q is considered to be small or large depends on the signiﬁcance level [2]. The most important cases are obtained for the convex functions f (t) = n(t − 1) 2 and f (t) = 2nt ln t leading to the Pearson χ 2 -statistic\nwhich is a scaled version of information divergence that we will denote D without subscript. In this paper we shall focus on the case where there are only two bins because this allow us to formulate in a qualitattive manner in terms of what we will call the intersection conjecture. With only two bins the multinomial distribution of counts can be described by a binomial distribution. We will also consider the limiting case where the binomial distribution is replaced by a Poisson distribution. This corresponds in a sense to having only one bin.\nNotation 1: Please note that we follow the notation from [3] by denoting the likelihood ratio statistic by G 2 rather than G as done in many textbooks and articles. Our G 2 should not be confused with Getis\u2013Ord\u2019s G statistic [4].\nOne way of choosing between various statistics is by computing their asymptotic efﬁciency. In 1985 it was proved that the G 2 -statistic is more efﬁcient in the Bahadur sense than the χ 2 -statistic, and this result has been extended in a number of papers [5]\u2013[9]. The asymptotic Bahadur efﬁciency of G 2 implies that a much smaller sample size is needed when using G 2 than when using χ 2 if a ﬁxed power should be achieved at a very small signiﬁcance level for some alternative. Since this type of result only holds asymptotically for large sample sizes it may be difﬁcult to use for a speciﬁc ﬁnite sample size. Therefore we will turn our attention to another important property for the choice of statistic.\nFor the practical use of a statistic it is important to calculate or estimate the distribution of the statistic. This can be done by exact calculation, by approximations, or by simulations. Exact calculations may be both time consuming and difﬁcult. Simulation often requires statistical insight and programming skills. Therefore most statistical tests use approximations to calculate the distribution of the statistic. For a ﬁxed number of bins the distribution of the χ 2 -statistic becomes closer and closer to the χ 2 -distributions as the sample size tends to inﬁnity. For a large sample size the empirical distribution will with high probability be close to the generating distribution and the Csisz´ar f -divergence D f can be approximated by a scaled version of the χ 2 -statistic\nTherefore the distribution of any f -divergence may be ap- proximated by a scaled χ 2 -distribution, i.e. a Γ-distribution. From this argument one might get the impression that the distribution of the χ 2 -statistic is closer to the χ 2 -distribution. Figure 1 and Figure 2 show that this is far from the the case. Both ﬁgures are Q-Q plots where for each p ∈ [0, 1] a point is plottet with the p quantile the square of a standard Gaussian as ﬁrst coordinate and the p quantile of the distribution of the statistic as the second coordinate. Figure 1 shows that the G 2 -statistic is almost as χ 2 -distributed as it can be when one takes into account that the likelihood ratio statistic has a discrete distribution. Each step is intersected very close to its midpoint. Figure 2 shows that the distribution of the χ 2 -statistic deviates systematically from the χ 2 -distribution for small signiﬁcance levels. For larger signiﬁcance levels both statistics will give approximately the same results which is related to the fact that the two statistics have the same asymptotic Pitman efﬁciency. The two plots show that at least in some cases the distribution of the G 2 -statistic is much closer to a χ 2 -distribution than Pearson statistic is. The next question is whether there are situations where the likelihood ratio statistic is not approximately χ 2 -distributed. For binomial distributions that are very skewed the intersection property of\nFigure 1 is not satisﬁed when the G 2 -statistic is plotted against the χ 2 -distribution so in the rest of this paper a different type of plots will be used. For getting a better approximation another strategy is Bartlett\u2019s adjustment, see [10].\nT. Dunning [12] has made a summary of what the typical recommendations are about whether one should use the χ 2 - statistic or the G 2 -Statistic. The short version is that the statistic is approximately χ 2 -distributed when each bin con- tains at least 5 observations or the calculated variance for each bin is at least 5, and if any bin contains more than twice the expected number observations then the G 2 -statistic is preferable to the χ 2 -statistic. Our main idea is to change the statistic into a signed version as it was introduced by Barndorff-Nielsen as a signed likelihood ratio [13]. We call the operation G-transform and change our orientation from hypothesis testing to normal approximation of distributions of sums of independent variables. Our main observation is that the G-transform covers probabilities in the whole domain including large deviations.\nNotation 2: In the rest of this paper we will let τ denote the circle constant 2π and let φ denote the standard Gaussian density\nWe let Φ denote the distribution function of the standard Gaussian\nHere we shall introduce a transformation that is useful for our understanding of the ﬁne structure of the distribution of the likelihood ratio statistics. Consider a 1-dimensional exponential family P β where\ndP β dP 0\nLet P µ denote the element in the exponential family with mean value µ. Let µ 0 denote the mean value of P 0 . Then\nTo verify that D (P µ P 0 ) is χ 2 -distributed it is sufﬁcient to verify that the square root is a centered Gaussian. This motivates the next deﬁnition:\nDeﬁnition 3: Let X be a random variable with distribution P 0 . Then the G-transform G (X) of X is the random variable given by\n0 ; (2D (P x P 0 )) 1/2 , for x ≥ µ 0 .\nUsing G (x) instead of D (P x P 0 ) as statistic is essentially the difference between using a one-sided test instead of a two- sided test. With this deﬁnition one easily sees that the G- transform of a Gaussian is a standard Gaussian. In [14] it was veriﬁed that if a random variable X satisﬁes a Cram´er condition the then with a minor correction G n 1 n n i=1 X i\nis Gaussian within a factor of the order 1 + O 1 n . In this paper we are interested in sharp bounds rather than asymptotic results.\nNow we can make quantile plots of the Gaussian distribution against the distribution of the G-transform of various random variables. On Figure 3-7 the G-transform of some binomial and Poisson distributions are compared with the standard Gaussian via their Q-Q plot. In these plots the red lines correspond to the bounds P (X ≤ x) ≤ exp (−D (P x P 0 )) for x ≤ µ 0 and P (X ≥ x) ≤ exp (−D (P x P 0 )) for x ≥ µ 0 .\nConjecture 4 (The intersection property): Let M denote a binomial distributed or Poisson distributed random variable and let G (M ) denote the G-transform of M. The quantile transform between G (M ) and a standard Gaussian Z is a step function and the identity function intersects each step, i.e.\nAnother way of reformulating the intersection property is that in the stochastic ordering X should be less than a random variable with point probabilities Φ (G (m)) − Φ (G (m − 1)) and greater than a random variable with point probabilities Φ (G (m + 1)) − Φ (G (m)) , where G (−1) is deﬁned as −∞ and G (n + 1) is deﬁned to be ∞ for a binomial distribution number parameter n. The conjecture is so well supported by numerical calculations that we would not hesitate to recom- mend it for estimation of tail probabilities for the binomial distributions in goodness of ﬁt tests instead of using the usual χ 2 -approximation of the χ 2 -statistic.\nAs we see both skewed binomial distributions and Poisson distributions have different step sizes for positive and negative values. Although the quantile transform between G (M ) and a standard Gaussian has the intersection property interference between the step sizes may have the effect that the quantile transform between the G 2 -statistic and the χ 2 -distribution does not necessarily have the intersection property. We believe that the G-transform is always closer to a standard Gaussian than the original. We have no idea, which distributions have the intersection property.\nHitherto we have discussed inequalities for discrete distri- butions but there is an interesting link to inequalities for con- tinuous distributions associated with waiting times. Assume that M is Poisson distributed with mean t and T is Gamma distributed with shape parameter m and scale parameter 1, i.e. the distribution of the waiting time until m observations from an Poisson process with intensity 1. Then\nIf G P is the G-transform for P o (t) and G Γ is the G- transform for Γ (m, 1) then G P (m) = −G Γ (t) . This shows that if the G-transforms of the Gamma distributions are close to a Gaussian then so are the G-transforms of the Poisson distributions. Figure 7 shows Q-Q plots of the G-transform of some Gamma distributions.\nWe see that the ﬁt with a straight line of slope 1 is extremely good. The point (0,0) is not on the line reﬂecting the fact that the means and the medians of the Gamma distributions do not coincide. In the next section we shall see that the quantile transform between a Gaussian and the G-transform of Gamma distributions is always below the identity.\nIn this section we shall formulate some conditions that are stronger than the intersection property. The proof of the following lemma is an easy exercise so we omit the proof.\nLemma 5: Let f 1 and f 2 be the densities of the random variables X 1 and X 2 with respect to some measure µ on the real numbers. If\nis an increasing then X 1 is less than X 2 in the usual stochastic ordering.\nTheorem 6: The G-transform of a Gamma distributed ran- dom variable is less than a standard Gaussian in the stochastic ordering.\nProof: Let T be a Γ (m, 1) distributed random variable. The distribution in the exponential family based Γ (m, 1) with mean t is Γ m, t m . The G-transform is\nwhere ± means that we will use + when t is greater than the mean k and use − when t is less than m. For the Gamma distribution we have\n(2 (u − 1 − ln u)) 1/2 is increasing. We have\nNow we have to prove that (u) = u − 2 ln u − 1 u is positive for u > 1 and negative for u < 1. Obviously (1) = 0 so it is sufﬁcient to prove that (u) ≥ 0, but\nNext we shall formulate an even stronger conjecture and see that it actually implies that binomial distributions and Poisson distributions have the intersection property.\nConjecture 7 (The increasing property): If M is a binomi- ally or Poisson distributed random variable with G-transform G (M ) then\nΦ (G (m)) − Φ (G (m − 1)) is decreasing.\nThe conjecture is supported by numerous numerical com- putations. If it holds the intersection property follows by Lemma 5. The increasing property implies log-concavity of the distribution but for instance the geometric distribution is log-concave but does not satisfy the intersection property. We have some indications that the conjecture also holds for any distribution of a sum of independent Bernoulli random variables.\nTheorem 8: The intersection property is satisﬁed for any Poisson random variable.\nfollows from Theorem 6 combined with Equation 4. The inequality\ncan be proved case by case for m ≤ 5. For m > 5 it is proved using the intersection property.\nTheorem (8) gives bounds on the tail probabilities for Poisson distributions that are far better than what can be found in the literature (see for instance [15]). At the same time the theorem gives bounds on the median that are compatible with the bounds in the literature [16], [17].\nMany goodness-of -ﬁt tests involve parameter estimation (that is, the model is a parametric family of distributions, not a single distribution). In such cases, the G 2 -statistic may converge slower to a χ 2 -distribution than the χ 2 -statistic [18]. How such results are related to the presents results is now\nclear yet. Since we only discuss the cases with one or two bins our results can be reformulated in terms of conﬁcence intervals. We hope to cover conﬁdence intervals in a future paper.\nIn the present paper the focus has been on the two bin case. We do not know if something equivalent of the intersection property can be formulated for more than two bins. For results on more than two bins it may be better to try to generalize the results on asymptotics presented in [14].\nThe authors want to thank Unnikrishnan Jayakrishnan for useful discussions. We also want to thank Jen˝o Reiczigel and L´ıdia Rejt˝o for helping with some numerical computations at an early stage of the developing the ideas presented in this paper and L´aszl´o Gy¨orﬁ who we worked in parallel on other aspects of the intersection conjecture. Finally we would like to thank Sune Jakobsen for comments to this manuscript."},"refs":[{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"Eine informationstheoretische Ungleichung und ihre Anwen- dung auf den Beweis der ergodizit¨at von Markoffschen Ketten"}},{"authors":[{"name":"E. Lehma"},{"name":"G. Castell"}],"title":{"text":"Testing Statistical Hypotheses"}},{"authors":[{"name":"B. S. Everit"}],"title":{"text":"The Cambridge Dictionary of Statistics"}},{"authors":[{"name":"T. Zhang"}],"title":{"text":"Limiting distribution of the G statistics"}},{"authors":[{"name":"M. P. Quine"},{"name":"J. Robinson"}],"title":{"text":"Efﬁciencies of chi-square and likelihood ratio goodness-of-ﬁt tests."}},{"authors":[{"name":"J. Beirlant"},{"name":"L. Devroye"},{"name":"L. Gy¨orﬁ"},{"name":"I. Vajda"}],"title":{"text":"Large deviations of divergence measures on partitions"}},{"authors":[{"name":"L. Gy¨orﬁ"},{"name":"G. Morvai"},{"name":"I. Vajda"}],"title":{"text":"Information-theoretic methods in testing the goodness-of-ﬁt"}},{"authors":[{"name":"P. Harremo¨es"},{"name":"I. Vajda"}],"title":{"text":"On the Bahadur-efﬁcient testing of unifor- mity by means of the entropy"}},{"authors":[{"name":"P. Harremo¨es"},{"name":"I. Vajda"}],"title":{"text":"Efﬁciency of entropy testing"}},{"authors":[{"name":"O. E. Barndorff-Nielsen"},{"name":"P. Hall"}],"title":{"text":"On the level-error after Bartlett adjustment of the likelihood-ratio statistic"}},{"authors":[{"name":"R. R. Soka"},{"name":"R. J. Rohl"}],"title":{"text":"Biometry: the principles and practice of statistics in biological research "}},{"authors":[{"name":"T. Dunning"}],"title":{"text":"Accurate methods for the statistics of surprise and coinci- dence"}},{"authors":[{"name":"O. E. Barndorff-Nielse"},{"name":"D. R. Co"}],"title":{"text":"Inference and asymptotics"}},{"authors":[{"name":"L. Gy¨orﬁ"},{"name":"P. Harremo¨es"},{"name":"G. Tusn´ady"}],"title":{"text":"Gaussian approximation of large deviation probabilities."}},{"authors":[{"name":"P. W. Glynn"}],"title":{"text":"Upper bounds on Poisson tail probabilities"}},{"authors":[{"name":"J. Chen"},{"name":"H. Rubin"}],"title":{"text":"Bounds for the difference between median and mean of Gamma and Poisson distributions"}},{"authors":[{"name":"K. Hamza"}],"title":{"text":"The smallest uniform upper bound on the distance between the mean and the median of the binomial and Poisson distributions"}},{"authors":[{"name":"W. Perkins"},{"name":"M. Tygert"},{"name":"R. Ward"}],"title":{"text":"χ 2 and classical exact tests often wildly misreport signiﬁcance; the remedy lies in computers."}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569561085.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T8.4","endtime":"16:00","authors":"Peter Harremoës, Gábor Tusnády","date":"1341243600000","papertitle":"Information Divergence is more chi square distributed than the chi squared statistics","starttime":"15:40","session":"S3.T8: Directed Information, Common Information, and Divergence","room":"Stratton (491)","paperid":"1569561085"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
