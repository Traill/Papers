{"id":"1569566799","paper":{"title":{"text":"Recovery of Sparse 1-D Signals from the Magnitudes of their Fourier Transform"},"authors":[{"name":"Kishore Jaganathan"},{"name":"Samet Oymak"},{"name":"Babak Hassibi"}],"abstr":{"text":"Abstract\u2014The problem of signal recovery from the autocorre- lation, or equivalently, the magnitudes of the Fourier transform, is of paramount importance in various ﬁelds of engineering. In this work, for one-dimensional signals, we give conditions, which when satisﬁed, allow unique recovery from the autocorrelation with very high probability. In particular, for sparse signals, we develop two non-iterative recovery algorithms. One of them is based on combinatorial analysis, which we prove can recover signals upto sparsity o(n 1/3 ) with very high probability, and the other is developed using a convex optimization based framework, which numerical simulations suggest can recover signals upto sparsity o(n 1/2 ) with very high probability.\n1 Index Terms\u2014Autocorrelation, Phase Retrieval, Convex Op- timization, Sparse Spectral Factorization"},"body":{"text":"Signal extraction from the autocorrelation, or equivalently, from the magnitude of the Fourier Transform is known as phase retrieval. This problem fundamentally arises in many practical systems such as X-ray crystallography [1], astronom- ical imaging [2], channel estimation, speech recognition [3] etc, and has attracted a considerable amount of attention from researchers over the last few decades [4]. Various algorithms have been proposed to retrieve phase information [5], [6] and a comprehensive survey of them can be found in [7], [8].\nFor one-dimensional signals, since the mapping from signals to autocorrelation is not one-to-one, unique recovery is not possible in general. For any given Fourier transform magni- tude, every possible phase corresponds to a different signal. Hence, additional prior information on the signal is required to limit the number of valid phase combinations. [9] uses multiple structured illuminations, in which several patterns using different masks are collected to guarantee uniqueness.\nWe assume that the signal is sparse, i.e., the number of non-zero entries in the signal is much less compared to the length of the signal. This constraint greatly limits the number of possible phase combinations, and research has been done recently to exploit this feature [11], [12]. In many applications of phase retrieval, the signals encountered are naturally sparse. For example, astronomical imaging deals with the locations of the stars in the sky, electron microscopy deals with the density of electrons, and so on.\nIn this work, we prove that signals can be recovered from their autocorrelation with arbitrarily high probability under certain conditions. We prove this using dimension counting, based on the ideas used in [10], [13] for multidimensional signals. We also propose two non-iterative recovery algorithms to extract sparse signals from their autocorrelation. Note that the phase recovery problem is inherently non-convex, and relaxations similar to the ones used in [12], [14] are used to develop a convex-optimization based framework.\nThe paper is organized as follows. In Section 2, we discuss some properties of autocorrelation and spectral factorization which we use for signal extraction. In Section 3, we prove that signals can be recovered from their autocorrelation with very high probability under certain conditions. Non-iterative recovery algorithms are proposed for extraction of the signal from their autocorrelation in Section 4. Section 5 presents the simulation results and concludes the paper.\nLet x = (x 0 , x 1 , ....x n−1 ) be a real-valued signal of length n. Its autocorrelation, denoted by a = (a 0 , a 1 , ....a n−1 ), is deﬁned as\nwhere ˜ x is the time-reversed version of x. Note that cyclic indexing scheme is used in this deﬁnition. Rewriting (1) in the z-domain, we get\nwhere A(z) and X(z) are the z-transforms of a and x respectively. Since x is real valued, X(z) is a polynomial in z with real coefﬁcients and hence its zeros occur in conjugate pairs. Also, since A(z) = A(z −1 ), if z 0 is a zero of A(z), then z −1 0 is also a zero. Hence, the zeros of A(z) appear in quadruples of the form (z 0 , z 0 , z −1 0 , z − 0 ).\nThe extraction of x from a, or equivalently X(z) from A(z), is known as spectral factorization and deals with the distribu- tion of these quadruples between X(z) and X(z −1 ). For every quadruple (z 0 , z 0 , z −1 0 , z − 0 ), we can either assign (z 0 , z 0 ) to X(z) and (z −1 0 ,z − 0 ) to X(z −1 ), or assign (z −1 0 ,z − 0 ) to X(z) and (z 0 , z 0 ) to X(z −1 ). The total number of different valid factorizations hence is exponential in the number of such quadruples.\nLemma II.1. If two distinct ﬁnite-length real-valued signals f 1 and f 2 have the same autocorrelation, then there exists ﬁnite- length real-valued signals g and h such that\nLet F 1 (z), F 2 (z), G(z) and H(z) be the z-transforms of the signals f 1 , f 2 , g and h respectively. Since f 1 and f 2 have the same autocorrelation, (2) gives us\nwhere A(z) is the z-transform of the autocorrelation of f 1 and f 2 . For every quadruple (z 0 ,z 0 ,z −1 0 ,z − 0 ) which are zeros of A(z), (z 0 , z 0 ) has to be assigned to F 1 (z) or F 1 (z −1 ), and F 2 (z) or F 2 (z −1 ). Let P 1 (z), P 2 (z) and P 3 (z) be the poly- nomials constructed from such conjugate pairs of zeros which are assigned to (F 1 (z), F 2 (z)) and (F 1 (z), F 2 (z −1 )) and (F 1 (z −1 ), F 2 (z)) respectively. Note that P 2 (z) = P 3 (z −1 ). We have\nIn this section, we establish the fact that within the class of signals with non-uniform support (deﬁned later), there is a one-to-one mapping between signals and their autocorrelation almost surely.\nLemma III.1. If f : A → B is a map from A to B, where A is a manifold of dimension d a and B is a manifold of dimension d b , then the image of f is measure zero in B if d a < d b .\nNote that any signal of length n can be represented as a vector in R n . Let f be a ﬁnite-length real-valued signal of length n. Let I represent its support, deﬁned as the set of locations where the f can have non-zero entries. We say that a signal f has uniform support if the indices of the elements belonging to the support are periodic, i.e., in an arithmetic progression. The size of the set I denotes the sparsity of f . Let F k denote the set of signals with sparsity k. Observe that F k is a manifold of dimension k.\nLemma III.2. Suppose g and h are ﬁnite-length real-valued signals with support set I g and I h of sparsity k g and k h respectively. If F gh denotes the set of signals g ∗ h, and I gh its support. Then\n(ii) I gh has sparsity k gh ≥ k g + k h − 1, with equality iff g and h have uniform support.\n(iii) If f = g ∗ h, where I, the support of f , is a subset of I gh with sparsity k. The set of such f is a manifold of dimension k g + k h − 1 − γ, where γ = k gh − k.\nProof: We refer the readers to [10] for the proof of (i) and (iii). (ii) directly follows from the properties of convolution.\nLemma III.3. Suppose f = g ∗ h, with f having non-uniform support where as g and h have uniform support, also has the additional property that f has non-uniform support. Then, the set of such signals is a manifold of dimension strictly lesser than k g + k h − 1 − γ.\nProof: The idea of the proof is similar to [10], based on dimension counting. We saw in Lemma III.2 that the set of signals f which can be represented as g ∗ h with sparsity k can be written as a manifold of dimension k g + k h − 1 − γ. The new set of constraints introduced by terms in f being 0 result in a further reduction in dimension. Hence the set of such signals belong to a manifold of dimension strictly lesser than k g + k h − 1 − γ.\nTheorem III.1 (Main Theorem). Signals can be uniquely recovered from their autocorrelation, or equivalently, from the magnitudes of their Fourier Transforms almost surely iff they have non-uniform support.\nProof: Let F k be the set of all signals f with non-uniform support of sparsity k which have another signal f with non- uniform support and same autocorrelation. Note that F k is the set of signals of sparsity k which cannot be recovered uniquely from their autocorrelation. Lemma II.1 showed the existence of signals g and h such that\nFrom Lemma III.2, we note that the dimension of F k is less than or equal to k g + k h − 1 − γ\nCase I: k gh > k g + k h − 1: This is the case if g and h do not have uniform support. In this case, the dimension of F k is strictly less than k. Hence from Lemma III.1, we see that F k is a set of measure zero in F k and signals with non- uniform support can be recovered from their autocorrelation almost surely.\nCase II: k gh = k g + k h − 1: In this case, g and h have uniform support. If f and f have non-uniform support, from Lemma III.3, we see that the dimension of F k is strictly lesser than k = k g +k h −1−γ, and hence can be uniquely recovered from their autocorrelation almost surely.\nSuppose f or f have uniform support, there will be no additional reduction in dimension. This case is equivalent to recovering a one-dimensional signal uniquely with no additional constraints, which is almost surely not possible.\nIn this section, we develop two non-iterative recovery algorithms for the extraction of sparse signals from their autocorrelation.\nAlgorithm 1 is based on combinatorial analysis. We propose a method to recover the support of the signal from the support of the autocorrelation, and prove that recovery is possible with very high probability if the sparsity of the signal is o(n 1/3 ). Using this support knowledge, we show that signals can be recovered from the autocorrelation with very high probability.\nSuppose x is a signal of length n such that each element in x belongs to the support with a probability s n , where s = n α , α ≤ 1, independent of each other. Let a denote its autocorrelation, k denote its sparsity and D = {d 1 , d 2 , .....d k } be the set of indices of the elements belonging to the support. Also, let d ij be deﬁned as |d i − d j | for (i, j) = {1, 2, ....k}. If A is the set of indices of elements belonging to the support of the autocorrelation, then A = { i,j d ij }. Note that d i,i+1 is a geometric random variable with parameter s n . Without loss of generality, let us assume d k−1,k ≥ d 12 , otherwise we could just ﬂip the signal and consider the ﬂipped signal. Deﬁne A 1 = {d ij − d 12 |d ij ∈ A} and A 2 = {d ij − d k−1,k |d ij ∈ A}.\nThe algorithm for signal recovery is described below. In what follows, we give a sequence of lemmas to justify various steps of the algorithm.\n\u2022 Extract d 12 and d k−1,k from A (Lemma IV.4). Calculate the sets A 1 and A 2 .\n\u2022 Perform (A ∩ A 1 ) ∩ (d 2,k−1 − (A ∩ A 2 )) and identify the support u of x (Lemma IV.5)\n\u2022 Construct the graph G (Lemma IV.6) using u, identify an odd cycle and a path connecting all the vertices and extract x.\nLemma IV.1. The sparsity k of the signal satisﬁes (1 − )s ≤ k ≤ (1 + )s with very high probability for any > 0, n > n( ).\nLemma IV.2. For three independent random variables X 1 , X 2 and X 3 where X 1 and X 2 are geometric random variables with parameter s n , P (X 1 − pX 2 = qX 3 ) ≤ s n if s = n α , α < 1 for n > n( ), where p and q are integers.\nLemma IV.3. P (d k−1,k −d 12 ∈ A) ≤ (1+ ) s 3 n for any > 0, n > n( ).\nProof: Using union bound, we obtain P (d k−1,k − d 12 ∈ A) ≤\nNote that the d ij \u2019s for i = 1, j = k are independent of d 12 and d k−1,k . Hence Lemma IV.2 can be applied and each term in the ﬁrst summation can be upper bounded by s n . Since d k−1,k < d ik and d 12 > 0, all the terms in the second summation are zero. The terms in the third summation can be equivalently written as P (d k−1,k − 2d 12 = d 2j ), and Lemma IV.2 can be used to upper bound every term by s n . Since d 1k is the largest sum, we need not consider it in the summation. Hence, we get\nP (d k−1,k −d 12 ∈ A) ≤ k 2 s n\n≤ (1+ ) 2 s 3 n\nLemma IV.4. d 12 and d k−1,k can be recovered from the autocorrelation with very high probability if s = o(n 1/3 ).\nProof: The ﬁrst and second highest terms in A are d 1k and d 2k respectively since d 12 ≤ d k−1,k . Note that d 1k − d 2k = d 12 , hence d 12 can be recovered from the autocorrelation. The only terms that can be higher than d 1,k−1 in A are {d 3k , d 4k , .....d k−1,k }. Note that d 2k − d ik = d 2i , which belongs to A for all i = {3, .....k−1}. So if d 2k −d 1,k−1 doesn\u2019t belong to A, we can recover d 1,k−1 by considering the highest term which when subtracted from d 2k produces a value which doesn\u2019t belong to A. The probability of failure can hence be written as P (d k−1,k − d 12 ∈ A) which goes to zero if s = o(n 1 3 ), as seen in Lemma IV.3. Hence both d 12 and d k−1,k can be recovered with very high probability if s = o(n 1/3 ).\nWith the knowledge of d 12 and d k−1,k , we can construct the sets A 1 and A 2 . Consider the intersection of A and A 1 . All entries of the form d 2i for i = {3, 4, ...k} will survive trivially for any signal. Similarly, all entries of the form d i,k−1 for i = {1....k − 2} will survive the intersection of A and A 2 for any signal. If we subtract the survivors of the intersection of A and A 2 from d 2,k−1 , we get d 2i for i = {3, 4, ...k − 1}. Hence the elements d 2i for i = {3, 4, ...k − 1} will survive (A ∩ A 1 ) ∩ (d 2,k−1 − (A ∩ A 2 )).\nLemma IV.5. No other d ij will survive (A ∩ A 1 ) ∩ (d 2,k−1 − (A ∩ A 2 )) and hence the support can be recovered with very high probability if s = o(n 1/3 )\nProof: Suppose you choose d ij such that i and j are picked at random. The probability that d ij is a particular value can be upper bounded by 1 n . For a non-trivial d ij in A to survive A A 1 , d ij + d 12 has to be in A. Similarly, d 2k − d ij and d 2,k−1 − d ij has to be in A for it to survive d 2k − A A 2 . Using union bounds, we see that the probability of survival of some other d ij goes to 0 when s = o(n 1/3 ). Note that we have information about d k−1,k upto s = o(n 1/3 ).\nIf no other elements survive, from d 2i for i = {3, 4, ...k−1}, we can extract d i,i+1 for i = {3, 4, ...k − 2} and since we\nalready know d 12 and d k−1,k , we have the support of the signal.\nSuppose we have the support of the signal, D = {d 1 , d 2 , .....d k } being the indices of the elements belonging to the support. Deﬁne a pair (d i , d j ) as a good pair if they are the only pair separated by |d i − d j |. Note that for such a pair,\nLemma IV.6. Consider a graph G with k vertices, each vertex representing an element of the support. Draw a weighted edge between every good pair, the weight being the value of the corresponding autocorrelation. If the graph G has an odd cycle and is connected, then the signal can be extracted from the autocorrelation upto a global sign.\nConsider an odd cycle with 2r − 1 vertices i 1 , i 2 , ...i 2r−1 . The term (x i 1 i 2 x i 3 i 4 ....x i 2r−1 i 1 )/(x i 2 i 3 ...x i 2r−2 i 2r−1 ) gives x 2 i 1 , from which x i 1 can be extracted upto a sign, and from it the other terms in the odd cycle can be extracted using the weight corresponding to the edges. Since the graph is connected, all the other terms can be calculated.\nLemma IV.7. The graph G has an odd cycle and is connected with very high probability for s = o(n 1/3 ).\nPick any three vertices randomly. Choose any path of length k − 3 from one of those vertices to cover all the remaining vertices randomly. If all the edges exists between the three vertices and the chosen k−3 length path exists, we are through. If any of the k edges doesn\u2019t exist, it implies that the distance between that pair of vertices occurs more than once. Since there are less than k 2 pairs, the probability of a pair of vertices not having an edge can be union bounded by k 2 n . Since there are k edges to be considered, the probability of failure can be upper bounded by k 3 n . Hence if s = o(n 1/3 ), any chosen triangle and path exists with very high probability.\nAlgorithm 2 is developed using a convex optimization based framework. Semideﬁnite relaxation is used to convert the non- convex constraints into a set of convex constraints. We break the problem into two stages. First, the support of the signal is recovered from the autocorrelation and then we solve for the signal in the support.\n1) Support Recovery: We have to extract u from the autocorrelation of the signal. We will assume that the support of the signal is a subset of the support of the autocorrelation. This is the same as assuming there is no cancellation of support in the autocorrelation, which is a very weak requirement and holds with probability one if the coefﬁcients of the signal are chosen randomly from a non-degenerate distribution. With this assumption, a i = 0 implies that no two elements in the support are separated by a distance i, and if a i is non-zero, there is atleast one pair of elements in the support separated by a distance i, i.e.,\nwhere u is the binary support vector. This is clearly non- convex as the constraints are non-convex and u is binary. Deﬁne S = uu T , which is allowed to be positive semideﬁnite, as it is the smallest convex set containing all rank one matrices. The entries of S are allowed to be in [0, 1], which is the best convex relaxation for binary variables. The trace of S is given by i u 2 i = i u i = k, the sparsity of the signal. Also, note that i S ij = i u i u j = u j i u i = ku j = ku 2 j = kS jj and similarly j S ij = kS ii . Since ﬂipped version of the support also satisﬁes all the constraints, a random matrix V is used to bias the cost. The support estimation problem becomes\nNote that we assume apriori knowledge of the sparsity of the signal, i.e., the number of non-zero locations of the signal is known.\n2) Signal Recovery: Note that the autocorrelation con- straints are non-convex. As we did in the support extraction, we use the semideﬁnite relaxation X = xx T . We append n zeros to the signal so that cyclic indexing scheme can be ap- plied, hence a m = 2n order DFT matrix is required. Suppose M k is the m × m matrix deﬁned by M k = f k f T k , where f k is the k th column of the DFT matrix for k = {0, 1, ....m − 1}. The autocorrelation constraints can be written in the Fourier domain as\nwhere Y = {|y 0 | 2 , |y 1 | 2 , ......|y m−1 | 2 } is the vector containing the squared magnitude of the Fourier transform of x. We can solve for the signal using L1-minimization [15], [16], [17].\nsubject to Y k = trace(M k X), k = 0, ......m − 1 (17) X ij = 0 if S ij = 0 0 ≤ i, j ≤ m − 1 X 0\nFigure 1 shows the success rate of signal recovery using Algorithm 1 as a function of the sparsity of the signal. We see that signals with s = o(n 1/3 ) are recovered successfully with very high probability. While the algorithm is computationally very cheap, it is not robust to noise due to error propagation.\nFigure 2 demonstrates the performance of Algorithm 2 as a function of the sparsity of the signal. Numerical simulations strongly suggest that signals with sparsity upto s = o(n 1/2 ) can be recovered using this algorithm. It is also very robust to\nnoise and hence more practical. We hope to provide theoretical guarantees in a future publication.\nLemma VI.1. For a pair of geometric random variables X 1 and X 2 with parameter s n each, P (X 1 − pX 2 = c) ≤ s n if s = n α , α < 1 for n > n( ), where p and c are integers."},"refs":[{"authors":[{"name":"R. P. Millan"},{"name":"J. Opt"}],"title":{"text":"Phase retrieval in crystallography and optics,\u201d  Soc"}},{"authors":[{"name":"C. Dainty"},{"name":"R. Fienup"}],"title":{"text":"Phase Retrieval and Image Re- construction for Astronomy"}},{"authors":[{"name":"L. Rabiner"},{"name":"H. Juang"}],"title":{"text":"Fundamentals of Speech Recogni- tion"}},{"authors":[{"name":"A. Walthe"}],"title":{"text":"The question of phase retrieval in optics,\u201d Opt"}},{"authors":[{"name":"R. W. Gerchberg"}],"title":{"text":"A practical algorithm for the determination of the phase from image and diffraction plane pictures"}},{"authors":[],"title":{"text":"Phase retrieval from modulus data"}},{"authors":[{"name":"J. R. Fienup"}],"title":{"text":"Phase retrieval algorithms: a comparison"}},{"authors":[{"name":"H. Sayed"}],"title":{"text":"A survey of spectral factorization methods"}},{"authors":[{"name":"E. J. Candes"},{"name":"Y. Eldar"},{"name":"T. Strohmer"}],"title":{"text":"Phase retrieval via matrix completion"}},{"authors":[],"title":{"text":"Absolute Uniqueness of Phase Retrieval with Random Illumination"}},{"authors":[{"name":"M. Lu"}],"title":{"text":"Sparse spectral factorization: Unicity and reconstruction algorithms"}},{"authors":[{"name":"K. Jaganathan"},{"name":"S. Oymak"},{"name":"B. Hassibi"}],"title":{"text":"Phase Retrieval for Sparse Signals using Rank Minimization"}},{"authors":[{"name":"M. Hayes"},{"name":"J. McClellan"}],"title":{"text":"Reducible Polynomials in more than One Variable"}},{"authors":[{"name":"E. J. Cande"},{"name":"T. Strohme"},{"name":"V. Voroninski"}],"title":{"text":"PhaseLift: exact and stable signal recovery from magnitude measurements via convex programming"}},{"authors":[{"name":"E. J. Candes"}],"title":{"text":"Decoding by linear programming"}},{"authors":[{"name":"E. J. Candes"}],"title":{"text":"Exact matrix completion via convex optimization"}},{"authors":[{"name":"E. J. Candes"}],"title":{"text":"The power of convex relaxation: Near- optimal matrix completion"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566799.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S8.T9.3","endtime":"17:40","authors":"Kishore Jaganathan, Samet Oymak, Babak Hassibi","date":"1341336000000","papertitle":"Recovery of Sparse 1-D Signals from the Magnitudes of their Fourier Transform","starttime":"17:20","session":"S8.T9: Sampling and Signaling","room":"Stratton West Lounge (201)","paperid":"1569566799"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
