{"id":"1569565635","paper":{"title":{"text":"Channel Simulation via Interactive Communications"},"authors":[{"name":"Mohammad Hossein Yassaee"},{"name":"Amin Gohari"},{"name":"Mohammad Reza Aref"}],"abstr":{"text":"Abstract\u2014In this paper, we study the problem of channel simulation via interactive communication, known as the coor- dination capacity, in a two-terminal network. We assume that two terminals observe i.i.d. copies of two random variables and would like to generate i.i.d. copies of two other random variables jointly distributed with the observed random variables. The terminals are provided with two-way communication links, and shared common randomness, all at limited rates. Two special cases of this problem are the interactive function computation studied by Ma and Ishwar, and the tradeoff curve between one-way communication and shared randomness studied by Cuff. The latter work had inspired Gohari and Anantharam to study the general problem of channel simulation via interactive communication stated above. However only inner and outer bounds for the special case of no shared randomness were obtained in their work. In this paper we settle this problem by providing an exact computable characterization of the multi- round problem. To show this we employ the technique of \u201coutput statistics of random binning\u201d that has been recently developed by the authors."},"body":{"text":"The minimum amount of interaction needed to create de- pendent random variables is an operational way to quantify the correlation among random variables. Wyner considered the problem of remote reconstruction of two dependent random variables by two terminals which are provided with shared randomness at a limited rate [1]. He used this approach to measure the intrinsic common randomness between two random variables. An alternative characterization of Wyner\u2019s common information as an extreme point of a channel simu- lation problem was provided in [2]. In this setup, a terminal who observes i.i.d. copies of X sends a message at rate R 1 to a remote random number generator (decoder) that produces i.i.d. copies of another random variable Y that is jointly distributed with X. The total variation distance between the achieved joint distribution and the i.i.d. distribution induced by passing X through a DMC channel p(y|x) should be negligible. In other words, the generated distribution and the i.i.d. distribution should be statistically indistinguishable. Shared common randomness exists between the two parties at a limited rate R 0 . Cuff found the tradeoff between R 0 and R 1 showing that when R 0 = 0 the minimum admissible rate for R 1 is the Wyner\u2019s common information; and when R 0 = ∞, the minimum admissible rate for R 1 is the mutual information between X and Y (this special case was already shown in [?]).\nThis setup was generalized in [3] by assuming that two terminals have access to i.i.d. copies of X 1 and X 2 respectively\nand would like to generate i.i.d. copies of Y 1 and Y 2 . Instead of a one-way communication, now the terminals are provided with a two-way communication at rates R 12 and R 21 (see Fig. 1). They can use up these two resources in r rounds of interactive communications as they wish (i.e. we only impose the constraint that i odd H(C i ) is less than or equal to nR 12 where H(C i ) is the entropy of the message sent from terminal 1 to terminal 2 at round i; a similar statement holds for R 21 ). Inner and outer bounds on R 12 and R 21 were derived in the special case of no shared common randomness [3]. In this paper we completely solve this problem under both the strong and empirical coordination models. Strong coordination demands a total variation converging to zero. On the other hand, empirical coordination only demands closeness of the empirical distribution of the generated random variables and the i.i.d. ones [4] (See Section II for a detailed description of these two models). Our result relates to the literature of coordinating distributed controllers to carry out some joint action (see e.g. [4]) since the generated random variables can be thought of as coordinated actions. Also, our result relates to the problem of ﬁnding the communication cost of simulating non-local correlations in quantum information theory. See [10].\nThis paper is organized as follows: in the next subsection we describe the main proof technique at an intuitive level. In Section II we deﬁne the problem and in Section III we state the main results followed by proofs in Sections IV and V.\nNotation: In this paper, we use X S to denote (X j : j ∈ S). We use p U A to denote the uniform distribution over the set A and p(x n ) to denote the i.i.d. pmf n i=1 p(x i ), unless otherwise stated. The total variation between two pmf\u2019s p and q on the same alphabet X , is denoted by p(x) − q(x) 1 .\nWhen a pmf itself is random, we use capital letter, e.g. P X . In the intuitive discussion of the proofs we use P X ≈ Q X when E P X − Q X 1 is small.\nOur proof is based on the technique of \u201coutput statistics of random binning\u201d (OSRB) that has been recently developed in [9]. To explain the technique we begin by describing the resolvability lemma used by Cuff [2, Lemma 6.1], and originally proved by Wyner. We report this lemma in a slightly different form that suits our purpose. Although we do not use this lemma in this work, since it is very central to the achievability proof of [2], we illustrate how this lemma can be proved using the OSRB approach.\nTo discuss the resolvability lemma of [2, Lemma 6.1], let us ﬁx some p(x, y). Roughly speaking the lemma states that one can ﬁnd 2 nR sequences in X n , namely x n (1), x n (2), · · · , x n (2 nR ), such that if we choose one of these sequences at random and pass it through the DMC channel p(y|x) we get an output sequence that is almost i.i.d. according to p(y), as long as R > I(X; Y ). We can restate this lemma by letting M to be a random variable whose alphabet is M = [1 : 2 nI(X;Y ) ], and assuming that X n (M ) is transmitted over the DMC channel q(y|x). To prove this lemma in the traditional way one would construct a random codebook parametrized by a random variable B. Every choice of B = b corresponds to a particular codebook (particular set of sequences in the X n space). The probability distribution imposed on the Y n space depends on the value of B, which is itself random. Therefore we use the capital letter P Y n to denote the random p.m.f. induced on Y n , by the random codebook. To show the above lemma one would need to show that the expected value of the total variation distance between the probability measure y n → P (y n ) and the i.i.d. distribution is small. Therefore there exists B = b where the total variation distance is small. Indeed this is the way that Cuff proves this lemma in [6, Lemma 19].\nTo illustrate the proof of this lemma using the OSRB approach, one would need to start from n i.i.d. copies of X n and Y n from the given p(x, y). Random variables B and M are identiﬁed as random binnings of X n at rates 2 n ˜ R and 2 nR respectively. Note the conceptual change is in starting from the i.i.d. distribution and then deﬁning B as a function of X n . It is proved that if ˜ R < H(X|Y ), B is almost independent of Y n . Therefore, for almost any choice of B = b, the distribution of Y n conditioned on B = b is almost i.i.d. On the other hand, if ˜ R + R > H(X), X n will be a function of (M, B) with high probability by the Slepian-Wolf. We are interpreting B and M as two messages coming from two encoders both observing X n . These imply that one can ﬁnd B = b such that the conditional law y n → p(y n |B = b) is close to the i.i.d. distribution, and at the same time X n is almost a function of M conditioned on B = b. All the approximations in this intuitive argument can be made accurate.\nThe crucial departure from the traditional argument was our treatment of random variable B. As discussed in [9], the\nrandomness in generating a random codebook is generally con- ceived of a common randomness shared among the terminals in a problem. However, we are changing the order by ﬁrst generating i.i.d. distributions and then treating B as a random binning on this product i.i.d. space.\nTwo terminals observe i.i.d. copies of sources X 1 , X 2 (taking values in ﬁnite sets X 1 and X 2 and having a joint pmf q(x 1 , x 2 )) respectively. A random variable ω which is independent of X n [1:2] = X n 1 X n 2 and is uniformly distributed over [1 : 2 nR 0 ] represents the common randomness provided to the terminals. Given an arbitrary r ∈ N, an (n, R 0 , R 12 , R 21 ) channel simulation code for simulating a channel with r interactive rounds of communications , consists of\n\u2022 a set of r randomized encodings speciﬁed with the conditional pmf\u2019s ˜ p enc 1 (c i |c [1:i−1] x n 1 ω) for odd numbers i ∈ [1 : r] and ˜ p enc 2 (c i |c [1:i−1] x n 2 ω) for even numbers i ∈ [1 : r], where C i denotes the communication of the i-th round,\nsuch that 1 n\nDeﬁnition 1: Given a channel with transition probability q(y [1:2] |x [1:2] ), a rate tuple (R 0 , R 12 , R 21 ) is said to be achiev- able if there exists a sequence of (n, R 0 , R 12 , R 21 ) channel simulation codes, such that the total variation between the probability ˜ p(y n [1:2] , x n [1:2] ) induced by the code and the i.i.d. repetitions of the desired pmf q(y [1:2] |x [1:2] )q(x [1:2] ) vanishes as n goes to inﬁnity, that is\nDeﬁnition 2: The simulation rate region is the closure of all the achievable rate tuples (R 0 , R 12 , R 21 ).\nRemark 1: In the special case r = 1, Y 1 = X 2 = ∅, our problem reduces to the one considered by Cuff in [2].\nRemark 2: Observe that if Y 1 = f 1 (X [1:2] ) and Y 2 = f 2 (X [1:2] ) are deterministic functions, the total variation con- straint of eq. (1) reduces to\nThus our problem reduces to the problem of interactive function computation considered in [5].\nDeﬁnition 3 (Empirical coordination [4]): Assume that in- stead of simulating the channel q(y [1:2] |x [1:2] ), the demand is to ﬁnd encoders and decoders such that the output sequences Y n [1:2] are jointly typical with the inputs X n [1:2] , with high probability. In this case, condition (1) should be replaced by the following condition:\nY n [1:2] is the empirical distribution of the pair (X n [1:2] , Y n [1:2] ) induced by the chosen code.\nRemark 3: It can be shown that if a sequence of codes satisﬁes the channel simulation condition (1), then it also satisﬁes the empirical coordination constraint (2). On the other hand it was shown in [4, Theorem 2] that the empirical rate region does not depend on the amount of common random- ness , that is, if (R 0 , R 12 , R 21 ) is achievable for empirical coordination, then (0, R 12 , R 21 ) is also achievable. These two facts imply that the achievability of a pair of (R 12 , R 21 ) for empirical coordination can be proved indirectly through the achievability proof for channel simulation in the presence of an unlimited common randomness. In [4], it was conjectured that this relation is two-sided, i.e. the rate regions for empirical coordination and channel simulation with unlimited common randomness are equal.\nTheorem 1: The simulation rate region is the set S(r) of all non-negative rate tuples (R 0 , R 12 , R 21 ), for which there exists p(f 1 , · · · , f r , x [1:2] , y [1:2] ) ∈ T (r) such that\nR 12 ≥ I(X 1 ; F [1:r] |X 2 ), R 21 ≥ I(X 2 ; F [1:r] |X 1 ),\nR 0 + R 12 ≥ I(X 1 ; F [1:r] |X 2 ) + I(F 1 ; Y [1:2] |X [1:2] ), R 0 + R 12 + R 21 ≥ I(X 1 ; F [1:r] |X 2 ) + I(X 2 ; F [1:r] |X 1 )\nF i −F [1:i−1] X 2 − X 1 , if i is even, Y 1 − F [1:r] X 1 − X 2 Y 2 ,\nUse of Caratheodory Theorem makes the region com- putable.\nRemark 4: Note that the above region is not symmetric since we have a ﬁnite r rounds of communication and the ﬁrst party starts the communication. The region will become symmetric for inﬁnite rounds of communication (i.e. r → ∞). See [10] for a discussion on the form of the region where we highlight the role of the ﬁrst round of communication.\nAssume that the desired channel is deterministic, that is, Y 1 = f 1 (X [1:2] ) and Y 2 = f 2 (X [1:2] ). Setting R 0 = 0 in Theorem 1 gives the following full characterization of the rate region of reliable interactive computation,\nR(r) = {∃F [1:r] :R 12 ≥ I(X 1 ; F [1:r] |X 2 ) R 21 ≥ I(X 2 ; F [1:r] |X 1 )\nTheorem 2 (Empirical coordination): The empirical coor- dination rate region is the set of all non-negative rate pairs (R 12 , R 21 ), for which there exists p(f 1 , · · · , f r , x [1:2] , y [1:2] ) ∈ T (r) (deﬁned in Theorem 1) such that\nThe achievability comes from setting R 0 = ∞ in Theorem 1. See [10] for the converse.\nRemark 5: Interactive empirical coordination is related to the problem of interactive lossy source coding solved by Kaspi in [8]. The above theorem in conjunction with [6, Theorem 9] provides an alternative proof for that result.\nLet (X [1:T ] , Y ) be a DMCS distributed according to a joint pmf p X [1:T ] ,Y on ﬁnite sets. A distributed random binning consists of a set of random mappings B i : X n i → [1 : 2 nR i ], i ∈ [1 : T ], in which B i maps each sequence of X n i uniformly and independently to [1 : 2 nR i ]. We denote the random vari- able B t (X n t ) by B t . A random distributed binning induces the following random pmf on the set X n [1:T ] ×Y n × T t=1 [1 : 2 nR t ],\nTheorem 3 ([9]): If for each S ⊆ [1 : T ], the following constraint holds\nIn this section we use a combination of the Slepian-Wolf theorem and Theorem 3 to illustrate the proof of Theorem 1; see [10] for a rigorous proof.\nThe proof is divided into three parts. In the ﬁrst part we introduce two protocols each of which induces a pmf on a certain set of r.v.\u2019s. The ﬁrst protocol has the desired i.i.d. property on (X n [1:2] , Y n [1:2] ) but leads to no concrete coding algorithm. However the second protocol is suitable for construction of a code, with one exception: the second protocol is assisted with an extra common randomness that does not really exist in the model. In the second part we ﬁnd constraints on R 0 , R 12 , R 21 implying that these two induced distributions are almost identical. In the third part of the proof, we eliminate the extra common randomness given to the second protocol without disturbing the pmf induced on the desired random variables (X n [1:2] , Y n [1:2] ) signiﬁcantly. This makes the second protocol useful for code construction.\nPart (1) of the proof: Take an arbitrary p(f [1:r] x [1:2] y [1:2] ) ∈ T (r). We deﬁne two protocols each of which induces a joint distribution on random variables that are deﬁned during the protocol.\nProtocol A. Let (F n [1:r] X n [1:2] Y n [1:2] ) be i.i.d. and distributed according to p(f [1:r] x [1:2] y [1:2] ). Since p(f [1:r] x [1:2] y [1:2] ) ∈ T (r), it factors as\n\u2022 To each sequence f n 1 , assign uniformly and independently three bin indices b 1 ∈ [1 : 2 n ˜ R 1 ], k 1 ∈ [1 : 2 nR 1 ] and ω ∈ [1 : 2 nR 0 ],\n\u2022 For 2 ≤ i ≤ r, to each sequence (f n 1 , · · · , f n i ), assign uniformly and independently two bin indices b i ∈ [1 : 2 n ˜ R i ] and k i ∈ [1 : 2 nR i ].\nThe random 1 pmf induced by the random binning, denoted by P , can be expressed as follows:\nwhere (i) 2 := i mod 2 and ω 1 = ω, and ω i is a constant variable for i ≥ 2.\nProtocol B. In this protocol we assume that the terminals have access to the shared randomness B [1:r] where B [1:r] are mutually independent r.v.\u2019s and uniformly distributed on\n[1 : 2 n ˜ R t ]. R.v. ω is also used for the common random- ness. Random variable K i is used for the communication at round i. Then, the protocol proceeds as follows,\n\u2022 In the ﬁrst round, knowing (b 1 , ω, x n 1 ), terminal 1 gen- erates a sequence f n 1 according to P (f n 1 |b 1 , ω, x n 1 ) of protocol A, and sends the bin index k 1 (f n 1 ) to the terminal 2. At the end of the ﬁrst round, terminal 2 having (b 1 , ω, k 1 , x n 2 ), uses the Slepian-Wolf decoder to obtain an estimate of f n 1 . The decoding is reliable if,\n\u2022 Assume that the estimation of f n 1 at the end of the ﬁrst round is correct. In the second round, knowing (b 2 , x n 2 , f n 1 ), terminal 2 generates a sequence f n 2 accord- ing to P (f n 2 |b 2 , x n 2 ) of protocol A and sends the bin index k 2 (f n 1 , f n 2 ) to the terminal 1. At the end of the second round, terminal 1 having (b 2 , k 2 , x n 1 , f n 1 ), uses the Slepian-Wolf decoder to estimate f n 2 . This procedure is repeated interactively for i ∈ [3 : r]. Thus, at the end of the round r, both terminals obtain an accurate\nestimate of f n [1:r] . The Slepian-Wolf decoding for the rounds i ∈ [2 : r] is reliable, if\nTo simplify the discussion we assume that the Slepian-Wolf decoders succeed with probability exactly one (and not close to one discussed in the rigorous proof in [10]).\nThe random pmf induced by the protocol, denoted by ˆ P , factors as\nPart (2) of the proof: Sufﬁcient conditions that make the induced pmfs approximately the same : To ﬁnd the constraints that imply that the pmf ˆ P is close to the pmf P in total variation distance, we start with P and make it close to ˆ P in a few steps. The ﬁrst step is to note that (b i , ω i ) can be interpreted as a bin index of f n [1:i] . Then substituting T = 1, X 1 = F [1:i] and Y = X (i) 2 F [1:i−1] in Theorem 3 yields that if\nR 0 + ˜ R 1 < H(F 1 |X 1 ), ˜ R i < H(F i |X (i)\nUsing a simple lemma about total variation distance that is provided in [6, Lemma 16], we can deduce the same approximation as above over the marginals\nIn particular, the marginal pmf of (X n [1:2] , Y n [1:2] ) of the RHS of this expression is equal to p(x n [1:2] , y n [1:2] ) which is the desired pmf.\nPart (3) of the proof: In the protocol we assumed that the terminals have access to a shared randomness B [1:r] which is not present in the model. To get rid of the shared randomness B [1:r] , we would like to condition on a particular instance of B [1:r] = b [1:r] . In this case, the induced pmf ˆ P (x n [1:2] , y n [1:2] ) changes to the conditional pmf ˆ P (x n [1:2] , y n [1:2] |b [1:r] ). But if B [1:r] is independent of (X n [1:2] , Y n [1:2] ), then the conditional pmf ˆ P (x n [1:2] , y n [1:2] |b [1:r] ) is also close to the desired distri- bution. To obtain the independence, we again use Theorem 3\nwhere we substitute T = r, X i = F [1:i] and Y = X [1:2] Y [1:2] to get the following sufﬁcient condition:\nWe have found all the necessary constraints on the size of the bins for the protocol to work. Finally, eliminating ( ˜ R 1 , · · · , ˜ R r ) from the inequalities (9)-(13) gives rise to the constraints given in the statement of the problem. This completes the proof of the achievability of Theorem 1.\nLet (R 0 , R 12 , R 21 ) be an achievable rate tuple for r rounds of communications. Then, for any < 1 2 , there exists a sim- ulation code of length n such that the total variation between the induced pmf ˜ p(y n [1:2] , x n [1:2] ) and the n i.i.d. repetitions of the desired pmf q(x, y) is less than .\nThe following lemmas which are consequences of a gener- alized version of Lemma 2.7 of [7], will be useful throughout the proof of the converse.\nLemma 1: For any discrete random variables W n and Z whose joint pmf satisﬁes\nfor some ˆ p q (w|z), we have \t n q=1 I(W q ; W q−1 |Z) ≤ 4n log |W| 2 .\nLemma 2: Take an arbitrary i.i.d. sequence X n distributed according to p(x) and a conditional pmf p(y n |x n ) which is not necessarily i.i.d. If there exists a conditional pmf ˆ p(y|x) such that\n< < 1 4\nthen I(X [∼q] ; Y q |X q ) ≤ 4 log |Y| 2 , where [∼ q] = [1 : n]\\{q}. B. Proof of converse\nTake a random variable Q uniform on [1 : n] and independent of all other random variables. Deﬁne F i = ωC i X n 1,Q+1 X Q−1 2 Q for 1 ≤ i ≤ r and X i = X iQ , Y i = Y iQ for i = 1, 2. In the ﬁrst step of the proof, we show the Markov chain conditions given in the deﬁnition of T (r) are satisﬁed by this choice of auxiliary r.v.\u2019s (see [10] for a rigorous proof). Also, we observe that\nwhere (14) follows from the Markov chain C i −ωC [1:i−1] X n 2 − X n 1 for even number i. It can be shown that the above term can be bounded from below by nI(F [1:r] ; X 1 |X 2 ). The inequality R 21 ≥ I(F [1:r] ; X 2 |X 1 ) can be proved similarly. Next consider,\nwhere (15) follows from the same lines as used in the bounding of R 12 . Now, consider\nwhere g( ) := 4 log( |X [1:2] ||Y [1:2] | 2 \t ), (a) is a result of Lemma 1, (b) follows from the Lemma 2 and the last inequality is a result of [6, Lemma 21]. The proof for other inequalities is similar and is omitted here. The last step is to let converge to zero. See [10] for details."},"refs":[{"authors":[{"name":"A. Wyner"}],"title":{"text":"The Common Information of Two Dependent Random Variables"}},{"authors":[{"name":"P. Cuff"}],"title":{"text":"Communication requirements for generating correlated random variables"}},{"authors":[{"name":"A. Gohari"},{"name":"V. Anantharam"}],"title":{"text":"Generating dependent random variables over networks"}},{"authors":[{"name":"P. Cuff"},{"name":"H. Permuter"},{"name":"T. M. Cover"}],"title":{"text":"Coordination capacity"}},{"authors":[{"name":"N. Ma"},{"name":"P. Ishwar"}],"title":{"text":"Some results on distributed source coding for interactive function computation"}},{"authors":[],"title":{"text":"Communication in networks for coordinating behavior"}},{"authors":[{"name":"I. Csiszar"},{"name":"J. Korner"}],"title":{"text":"Information theory: coding theorems for discrete memoryless systems"}},{"authors":[{"name":"A. H. Kaspi"}],"title":{"text":"Two-way source coding with a ﬁdelity criterion"}},{"authors":[{"name":"M. H. Yassaee"},{"name":"M. R. Aref"},{"name":"A. Gohari"}],"title":{"text":"Achievability proof via output statistics of random binning"}},{"authors":[{"name":"M. H. Yassaee"},{"name":"A. Gohari"},{"name":"M. R. Aref"}],"title":{"text":"Channel simulation via interactive communications"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565635.pdf"},"links":[{"id":"1569566381","weight":12},{"id":"1569566725","weight":6},{"id":"1569565867","weight":6},{"id":"1569566875","weight":6},{"id":"1569566943","weight":12},{"id":"1569566591","weight":12},{"id":"1569566765","weight":6},{"id":"1569564227","weight":6},{"id":"1569563411","weight":6},{"id":"1569565291","weight":6},{"id":"1569556713","weight":25},{"id":"1569565859","weight":6},{"id":"1569566579","weight":6},{"id":"1569565455","weight":6},{"id":"1569566709","weight":6},{"id":"1569564189","weight":6},{"id":"1569563981","weight":18},{"id":"1569566753","weight":6},{"id":"1569565213","weight":6},{"id":"1569561795","weight":6},{"id":"1569566423","weight":6},{"id":"1569553909","weight":12},{"id":"1569566939","weight":6},{"id":"1569553537","weight":12},{"id":"1569553519","weight":12},{"id":"1569566885","weight":6},{"id":"1569565655","weight":50},{"id":"1569558985","weight":6},{"id":"1569566809","weight":6},{"id":"1569566257","weight":6},{"id":"1569565033","weight":6},{"id":"1569566447","weight":6},{"id":"1569565887","weight":6},{"id":"1569566721","weight":6},{"id":"1569565633","weight":6},{"id":"1569566037","weight":18},{"id":"1569565029","weight":68},{"id":"1569565357","weight":6},{"id":"1569566505","weight":6},{"id":"1569566695","weight":6},{"id":"1569566673","weight":12},{"id":"1569565311","weight":6},{"id":"1569566297","weight":6},{"id":"1569560997","weight":12},{"id":"1569566481","weight":6},{"id":"1569565439","weight":6},{"id":"1569563395","weight":6},{"id":"1569565415","weight":12},{"id":"1569565397","weight":6},{"id":"1569566873","weight":6},{"id":"1569565435","weight":6},{"id":"1569566129","weight":6},{"id":"1569565385","weight":6},{"id":"1569565919","weight":6},{"id":"1569566711","weight":6},{"id":"1569565511","weight":12},{"id":"1569561221","weight":6},{"id":"1569566237","weight":6},{"id":"1569566283","weight":6},{"id":"1569556759","weight":6},{"id":"1569561185","weight":6},{"id":"1569558779","weight":6},{"id":"1569566299","weight":6},{"id":"1569564769","weight":6},{"id":"1569557851","weight":25},{"id":"1569565537","weight":18},{"id":"1569564961","weight":6},{"id":"1569559251","weight":6},{"id":"1569550425","weight":6},{"id":"1569564931","weight":12},{"id":"1569551751","weight":6},{"id":"1569564419","weight":12}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S17.T4.2","endtime":"15:40","authors":"Mohammad Hossein Yassaee, Amin Aminzadeh Gohari, Mohammad Reza Aref","date":"1341588000000","papertitle":"Channel Simulation via Interactive Communications","starttime":"15:20","session":"S17.T4: Communication Models","room":"Stratton 20 Chimneys (306)","paperid":"1569565635"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
