{"id":"1569553519","paper":{"title":{"text":"Information Masking and Ampliﬁcation: The Source Coding Setting"},"authors":[{"name":"Thomas A. Courtade"}],"abstr":{"text":"Abstract\u2014The complementary problems of masking and am- plifying channel state information in the Gel\u2019fand-Pinsker chan- nel have recently been solved by Merhav and Shamai, and Kim et al., respectively. In this paper, we study a related source coding problem. Speciﬁcally, we consider the two-encoder source coding setting where one source is to be ampliﬁed, while the other source is to be masked. In general, there is a tension between these two objectives which is characterized by the ampliﬁcation-masking tradeoff. In this paper, we give a single-letter description of this tradeoff.\nWe apply this result, together with a recent theorem by Courtade and Weissman on multiterminal source coding, to solve a fundamental entropy characterization problem."},"body":{"text":"The well known source coding with side information prob- lem has an achievable rate region given by\nas originally shown by Ahlswede and K¨orner [1], and inde- pendently by Wyner [2]. In this setting, the side information encoder merely serves as a helper with the sole purpose of aiding in the recovery of X n at the decoder. However, for given rates (R x , R y ), there may be many different coding schemes which permit recovery of X n at the decoder. In some cases, it may be desirable to select a coding scheme that reveals very little information about the side information Y n to the decoder. We refer to this objective as masking the side information.\nTo motivate this setting, consider the following example. Suppose X is an attribute of an online customer that an advertiser would like to speciﬁcally target (e.g., gender), and Y is other detailed information about the same customer (e.g., credit history). Companies A and B separately have databases X n and Y n corresponding to n different customers (the databases could be indexed by IP address, for example). The advertiser pays Companies A and B to learn as much about the database X n as possible. Now, suppose governing laws prohibit the database Y n from being revealed too extensively. In this case, the material given to the advertiser must be chosen so that at most a prescribed amount of information is revealed about Y n .\nIn general, a masking constraint on Y n may render near- lossless reconstruction of X n impossible. This motivates the study the ampliﬁcation-masking tradeoff. That is, the tradeoff\nbetween amplifying (or revealing) information about X n while simultaneously masking the side information Y n .\nSimilar problems have been previously studied in the literature on secrecy and privacy. For example, Sankar et al. determine the utility-privacy tradeoff for the case of a single encoder in [3]. In their setting, the random variable X is a vector with a given set of coordinates that should be masked and another set that should be revealed (up to a prescribed distortion). In this context, our study of the ampliﬁcation- masking tradeoff is a distributed version of [3], in which utility is measured by the information revealed about the database X n . The problem we consider is distinct from those typically studied in the information-theoretic secrecy literature, in that the masking (i.e., equivocation) constraint corresponds to the intended decoder, rather than an eavesdropper.\nWe remark that the present paper is inspired in part by the recent, complementary works [4] and [5] which respectively study ampliﬁcation and masking of channel state information. We borrow our terminology from those works. We also note that ampliﬁcation of channel state information subject to masking constraints was investigated in [6].\nThis paper is organized as follows. Section II formally deﬁnes the problems considered and delivers our main results. The corresponding proofs are given in Section III. Final remarks and directions for future work are left to Section IV.\nThroughout this paper we adopt notational conventions that are standard in the literature. Speciﬁcally, random variables are denoted by capital letters (e.g., X) and their corresponding alphabets are denoted by corresponding calligraphic letters (e.g., X ). We abbreviate a sequence (X 1 , . . . , X n ) of n random variables by X n , and we let δ( ) represent a quantity satisfying lim →0 δ( ) = 0.\nFor a joint distribution p(x, y) on ﬁnite alphabets X × Y, consider the source coding setting where separate Encoders 1 and 2 have access to the sequences X n and Y n , respec- tively. We make the standard assumption that the sequences (X n , Y n ) are drawn i.i.d. according to p(x, y) (i.e., X n , Y n ∼\nThe ﬁrst of the following three subsections characterizes the ampliﬁcation-masking tradeoff. This result is applied to solve a fundamental entropy characterization in the second subsection. The ﬁnal subsection comments on the connection\nbetween information ampliﬁcation and list decoding. Proofs of the main results are postponed until Section III.\nFormally, a (2 nR x , 2 nR y , n) code is deﬁned by its encoding functions\nA rate-ampliﬁcation-masking tuple (R x , R y , ∆ A , ∆ M ) is achievable if, for any > 0, there exists a (2 nR x , 2 nR y , n) code satisfying the ampliﬁcation criterion:\nThus, we see that the ampliﬁcation-masking problem is an entropy characterization problem similar to that considered in [7, Chapter 15].\nDeﬁnition 1: The achievable ampliﬁcation-masking region R AM is the closure of the set of all achievable rate- ampliﬁcation-masking tuples (R x , R y , ∆ A , ∆ M ).\nTheorem 1: R AM consists of the rate-ampliﬁcation- masking tuples (R x , R y , ∆ A , ∆ M ) satisfying\nR x ≥ ∆ A − I(X; U) R y ≥ I(Y ; U)\n∆ M ≥ max {I(Y ; U, X) + ∆ A − H(X), I(Y ; U)} ∆ A ≤ H(X).\n   \n  \nfor some joint distribution p(x, y, u) = p(x, y)p(u |y), where |U| ≤ |Y| + 1.\nObserve that R AM characterizes the entire tradeoff between amplifying X n and masking Y n . We remark that maximum ampliﬁcation ∆ A = H(X) does not necessarily imply that X n can be recovered near-losslessly at the encoder. However, if an application demands near lossless reproduction of the sequence X n , Theorem 1 can be strengthened to include this case. To this end, deﬁne a rate-masking triple (R x , R y , ∆ M ) to be achievable if, for any > 0, there exists a (2 nR x , 2 nR y , n) code satisfying the masking criterion (2), and a decoding function\nDeﬁnition 2: The achievable rate-masking region R M is the closure of the set of all achievable rate-masking triples (R x , R y , ∆ M ).\nCorollary 1: R M consists of the rate-masking triples (R x , R y , ∆ M ) satisfying\nR x ≥ H(X|U) R y ≥ I(Y ; U)\nfor some joint distribution p(x, y, u) = p(x, y)p(u |y), where |U| ≤ |Y| + 1.\nAs we previously noted, the ampliﬁcation-masking trade- off solves a multi-letter entropy characterization problem by reducing it to single-letter form. The reader is directed to [7] for an introduction to entropy characterization problems. Here, we apply our results to yield a fundamental characterization of the information revealed about X n and Y n , respectively, by arbitrary encoding functions f x and f y (of rates R x , R y ).\nDeﬁnition 3: Deﬁne the region R (R x , R y ) as follows. The pair (∆ X , ∆ Y ) ∈ R (R x , R y ) if and only if, for any > 0, there exists a (2 nR x , 2 nRy , n) code satisfying\nUltimately we obtain a single-letter description of R (R x , R y ). However, in order to do so, we require some notation. To this end, let:\nSymmetrically, let R M A be the region where X n is subject to masking ∆ X and Y n is subject to ampliﬁcation ∆ Y . Let\nFinally, let R AA (R x , R y ) consist of all pairs (∆ X , ∆ Y ) satisfying\nR x ≥ I(U x ; X |U y , Q) R y ≥ I(U y ; Y |U x , Q)\nR x + R y ≥ I(U x , U y ; X, Y |Q) ∆ X ≤ I(X; U x , U y |Q) ∆ Y ≤ I(Y ; U x , U y |Q)\nTheorem 2: The region R (R x , R y ) has a single-letter characterization given by\nMoreover, restriction of the encoding functions to vector- quantization and/or random binning is sufﬁcient to achieve any point in R (R x , R y ).\nThe second statement of Theorem 2 is notable since it states that relatively simple encoding functions (i.e., vector quantization and/or binning) can asymptotically reveal the same amount of information about X n and Y n , respectively, as encoding functions that are only restricted in rate. In\ncontrast, this is not true for the setting of three or more sources, as the modulo-sum problem studied by K¨orner and Marton [8] provides a counterexample where the Berger-Tung achievability scheme [9] is not optimal. Thus, obtaining a characterization like Theorem 2 for three or more sources represents a formidable challenge.\nWe remark that the points in R (R x , R y ) with ∆ X = H(X) and/or ∆ Y = H(Y ) also capture the more stringent constraint(s) of near-lossless reproduction of X n and/or Y n , respectively. This is a consequence of Corollary 1.\nTo give a concrete example of R (R x , R y ), consider the following joint distribution:\nBy performing a brute-force search over the auxiliary random variables deﬁning R (R x , R y ) for the distribution P X,Y , we have obtained numerical approximations of R (·, ·) for several different pairs of (R x , R y ). The results are given in Figure 1. C. Connection to List Decoding\nWe brieﬂy comment on the connection between an ampli- ﬁcation constraint and list decoding. As discussed in detail in [4], the ampliﬁcation criterion (1) is essentially equivalent to the requirement for a list decoder\nThus maximizing the ampliﬁcation of X n subject to given rate and masking constraints can be thought of as characterizing the best list decoder in that setting.\nProof of Theorem 1: \t Converse Part: Suppose (R x , R y , ∆ A , ∆ M ) is achievable. For convenience, deﬁne F x = f x (X n ), F y = f y (Y n ), and U i = (F y , Y i−1 ).\nFirst, note that ∆ A ≤ H(X) is trivially satisﬁed. Next, the constraint on R x is given by:\nEquality (5) follows since X i ↔ F y , Y i−1 ↔ X i−1 form a Markov chain, and inequality (6) follows since ampliﬁcation ∆ A is achievable.\nThe second lower bound on ∆ M requires slightly more work, and can be derived as follows:\nObserving that the Markov condition U i ↔ Y i ↔ X i is satisﬁed for each i, a standard timesharing argument proves the existence of a random variable U such that U ↔ Y ↔ X forms a Markov chain and (3) is satisﬁed.\nDirect Part: Fix p(u |y) and suppose (R x , R y , ∆ A , ∆ M ) satisfy (3) with strict inequality. Next, ﬁx > 0 sufﬁciently small so that it is less than the minimum slack in said inequalities, and set ˜ R = I(Y ; U ) + . Our achievability scheme uses a standard random coding argument which we sketch below.\nCodebook generation. Randomly and independently, bin the typical x n \u2019s uniformly into 2 n(∆ A −I(X;U )+ ) bins. Let b(x n ) be the index of the bin which contains x n . For l ∈ {1, . . . , 2 n ˜ R }, randomly and independently generate u n (l), each according to n i=1 p U (u i ).\nEncoding. Encoder 1, upon observing the sequence X n , sends the corresponding bin index b(X n ) to the decoder. If X n is not typical, an error is declared. Encoder 2, upon observing the sequence Y n , ﬁnds an L ∈ {1, . . . , 2 n ˜ R } such that (Y n , U n (L)) are jointly -typical, and sends the unique index L to the decoder. If more than one such L exists, ties are broken arbitrarily. If no such L exists, then an error is declared.\nThis coding scheme clearly satisﬁes the given rates. Further, each encoder errs with arbitrarily small probability as n → ∞. Hence, we only need to check that the ampliﬁcation and masking constraints are satisﬁed. To this end, let C be the random codebook. We ﬁrst check that the ampliﬁcation and masking constraints are separately satisﬁed when averaged over random codebooks C.\nTo see that the (averaged) ampliﬁcation constraint is satis- ﬁed, consider the following:\nwhere (8) follows since X n is independent of C and, aver- aged over codebooks, there are at most 2 n(H(X)−∆ A + δ( )) sequences x n in bin b(X n ) which are typical with U n (L), where L ∈ {1, . . . , 2 n ˜ R }. The details are given in [10].\nWe now turn our attention to the masking criterion. First note the following inequality:\nI(Y n ; F x , F y |C) = I(Y n ; L |C) + I(Y n ; b(X n ) |L, C) ≤ I(Y n ; L |C) + H(b(X n ) |C) − H(b(X n ) |Y n , C)\n= I(Y n ; L |C) + I(X n ; Y n ) − H(X n ) + H(b(X n ) |C) − H(b(X n ) |Y n , C) + H(X n |Y n )\n≤ I(Y n ; L |C) + I(X n ; Y n ) − H(X n ) + H(b(X n ) |C) − I(b(X n ); X n |Y n , C) + H(X n |Y n )\nTwo of the terms in (9) can be bounded as follows: First, since L ∈ {1, . . . , 2 n ˜ R }, we have\nSecond, there are 2 n(∆ A −I(X;U )+ ) bins at Encoder 1 by construction, and hence H(b(X n ) |C) ≤ n(∆ A −I(X; U)+ ). Therefore, substituting into (9) and simplifying, we have:\nWe now consider three separate cases. First, assume ∆ A ≤ I(U ; X). Then,\nI(Y ; X, U ) + ∆ A − H(X) ≤ I(Y ; X, U) − H(X|U) = I(Y ; U ) − H(X|Y ),\nI(Y n ; F x , F y |C) ≤ nI(Y ; U) − I(X n ; b(X n ) |Y n , C) + n2 ≤ nI(Y ; U) + n2 .\nNext, suppose that ∆ A ≥ I(X; U) + H(X|Y ). In this case, there are greater than 2 n(H(X|Y )+ ) bins in which the X n sequences are distributed. Hence, knowing Y n and b(X n ) is sufﬁcient to determine X n with high probability (i.e., we have a Slepian-Wolf binning at Encoder 1). Therefore, H(X n |Y n , b(X n ), C) ≤ n , and (10) becomes\nFinally, suppose ∆ A = I(X; U ) + θH(X |Y ) for some θ ∈ [0, 1]. In this case, we can timeshare between a code C 1 designed for ampliﬁcation ∆ A = I(X; U ) with probability θ, and a code C 2 designed for ampliﬁcation ∆ A = I(X; U ) + H(X |Y ) with probability 1 − θ to obtain a code C with the same average rates and averaged ampliﬁcation\nThen, applying the inequalities obtained in the previous two cases, we obtain: I(Y n ; F x , F y |C)\n≤ θnI(Y ; U) + (1 − θ)n(I(Y ; X, U) + ∆ A − H(X)) + 3n = nI(Y ; U ) + 3n .\nCombining these three cases proves that 1\n≤ max{I(Y ; U, X) + ∆ A − H(X), I(Y ; U)} + 3 ≤ ∆ M + 3 .\nTo show that there exists a code which satisﬁes the am- pliﬁcation and masking constraints simultaneously, we con- struct a super-code ¯ C of blocklength Nn by concatenating N randomly, independently chosen codes of length n (each constructed as described above). By the weak law of large numbers and independence of the concatenated coded blocks,\nfor N and n sufﬁciently large. Thus, there must exist one super-code which simultaneously satisﬁes both desired con- straints. This completes the proof that (R x , R y , ∆ A , ∆ M ) is\nachievable. Finally, we invoke the Support Lemma [7] to see that |Y| − 1 letters are sufﬁcient to preserve p(y). Plus, we require two more letters to preserve the values of H(X |U) and I(Y ; U |X).\nProof of Theorem 2: First, we remark that the strength- ened version of [11, Theorem 6] states that R AA (R x , R y ) is the closure of pairs (∆ X , ∆ Y ) such that there exists a (2 nR x , 2 nR y , n) code satisfying\nSuppose (∆ X , ∆ Y ) ∈ R (R x , R y ). By deﬁnition of R (R x , R y ), Theorem 1, and the above statement, (∆ X , ∆ Y ) also lies in each of the sets R AM (R x , R y ), R M A (R x , R y ), and R AA (R x , R y ). Since each of these sets are closed by deﬁnition, we must have\nSince each point in the sets R AM (R x , R y ), R M A (R x , R y ), and R AA (R x , R y ) is achievable by vector quantization and/or random binning, the second statement of the Theorem is proved.\nTo show the reverse inclusion, ﬁx > 0 and suppose (∆ X , ∆ Y ) ∈ R AM (R x , R y ) ∩R M A (R x , R y ) ∩R AA (R x , R y ).\nThis implies the existence of (2 n AM R x , 2 n AM R y , n AM ), (2 n M A R x , 2 n M A R y , n M A ), and (2 n AA R x , 2 n AA R y , n AA ) codes satisfying:\nAlso, by taking f M M x , f M M y \t to be constants, we trivially have a (2 n M M R x , 2 n M M R y , n M M ) code satisfying\nIt is readily veriﬁed that, by an appropriate timesharing be- tween these four codes, there exists a (2 nR x , 2 nR y , n) code\nIn this paper, we considered a setting where two sepa- rate encoders have access to correlated sources. We gave a complete characterization of the tradeoff between amplifying information about one source while simultaneously masking another. By combining this result with recent results by Courtade and Weissman [11], we precisely characterized the amount of information that can be revealed about X n and Y n by any encoding functions satisfying given rates. There are three notable points here: (i) this multi-letter entropy characterization problem admits a single-letter solution, (ii) restriction of encoding functions to vector quantization and/or random binning is sufﬁcient to achieve any point the region, and (iii) this simple characterization does not extend to three or more sources/encoders.\nFinally, we remark that in the state ampliﬁcation and masking problems considered in [4] and [5], the authors obtain explicit characterizations of the achievable regions when the channel state and noise are independent Gaussian random variables. Presumably, this could also be accomplished in our setting using known results on Gaussian multiterminal source coding, however, a compete investigation into this matter is beyond the scope of this paper\nThe author gratefully acknowledges the conversations with Tsachy Weissman and comments by an anonymous reviewer which contributed to this paper."},"refs":[{"authors":[{"name":"R. Ahlswede"},{"name":"J. Korner"}],"title":{"text":"Source coding with side information and a converse for degraded broadcast channels"}},{"authors":[{"name":"A. Wyner"}],"title":{"text":"On source coding with side information at the decoder"}},{"authors":[{"name":"L. Sankar"},{"name":"S. R. Rajagopalan"},{"name":"H. V. Poor"}],"title":{"text":"A theory of privacy and utility in databases"}},{"authors":[{"name":"Y.-H. Kim"},{"name":"A. Sutivong"},{"name":"T. Cover"}],"title":{"text":"State ampliﬁcation"}},{"authors":[{"name":"N. Merhav"},{"name":"S. Shamai"}],"title":{"text":"Information rates subject to state masking"}},{"authors":[{"name":"O. O. Koyluoglu"},{"name":"R. Soundararajan"},{"name":"S. Vishwanath"}],"title":{"text":"State ampliﬁ- cation subject to masking constraints"}},{"authors":[{"name":"I. Csisza"},{"name":"J. Korne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems "}},{"authors":[{"name":"J. Korner"},{"name":"K. Marton"}],"title":{"text":"How to encode the modulo-two sum of binary sources (corresp.)"}},{"authors":[{"name":"T. Berge"},{"name":"G. Longo (Ed"}],"title":{"text":"Multiterminal Source Coding"}},{"authors":[{"name":"T. A. Courtade"}],"title":{"text":"Information masking and ampliﬁcation: The source coding setting"}},{"authors":[{"name":"T. A. Courtade"},{"name":"T. Weissman"}],"title":{"text":"Multiterminal source coding under logarithmic loss"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569553519.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S2.T1.4","endtime":"12:50","authors":"Thomas Courtade","date":"1341232200000","papertitle":"Information Masking and Amplification: The Source Coding Setting","starttime":"12:30","session":"S2.T1: Network Source Coding with Side Information","room":"Kresge Rehearsal B (030)","paperid":"1569553519"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
