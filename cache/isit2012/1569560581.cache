{"id":"1569560581","paper":{"title":{"text":"On Linear Index Coding for Random Graphs"},"authors":[{"name":"Ishay Haviv"},{"name":"Michael Langberg"}],"abstr":{"text":"Abstract\u2014In the index coding problem, the goal is to transmit an n character word over a ﬁeld F to n receivers (one character per receiver), where the receivers have side information repre- sented by a graph G. The objective is to minimize the length of a codeword broadcasted to all receivers which allows each receiver to learn its character. For linear index coding, the minimum possible length is known to be equal to the minrank parameter.\nIn this paper we initiate the study of the typical minimum length of a linear index code for the random graph G ( n, p ) over a ﬁeld F . First, we prove that for every constant size ﬁeld F and a constant p, the minimum length of a linear index code for G ( n, p ) over F is almost surely Ω (\nn ) . Second, we introduce and study two special models of index coding and study their typical minimum length: Locally decodable index codes in which the receivers are required to query at most q characters from the encoded message (such codes naturally correspond to efﬁcient decoding); and low density index codes in which every character of the broadcasted word affects at most q characters in the encoded message (such codes naturally correspond to efﬁcient encoding procedures). We present enhanced results for these special models."},"body":{"text":"In the index coding problem, a sender wishes to broadcast an n character word x ∈ F n (for a ﬁnite ﬁeld F ) to n receivers R 1 , . . . , R n in a way that enables every R i to retrieve the ith character x i . Every receiver has some side information on x. The side information is represented by a directed graph G on the vertex set [ n ] = { 1, 2, . . . , n } in which a vertex i is connected to a vertex j if and only if the receiver R i knows x j . Given a side information graph G, the goal is to ﬁnd a coding scheme of minimum length, by which every receiver R i is able to retrieve x i given the encoded message and the side information that it has on x according to G. The settings are naturally extended to undirected graphs in which an edge { i, j } means that R i knows x j and R j knows x i .\nFor example, assume that every receiver R i knows x j for every j ∈ [ n ] \\ { i } . The corresponding side information graph is the complete graph on the vertex set [ n ] . In this case, broadcasting the sum ∑ i ∈ [n] x i over F enables every receiver R i to retrieve x i , and hence the minimum message length required here is 1.\nThe study of index coding was initiated by Birk and Kol in [6] and further developed by Bar-Yossef, Birk, Jayram and Kol in [5]. This research is motivated by applications, such as video on demand and wireless networking, in which a network transmits information to clients, and during the transmission\nevery client misses some of the information. At this step, the clients have side information on the transmitted information, and the network is interested in minimizing the broadcast length in a way that enables the clients to decode their target (see, e.g., [22]). The study of index coding is also motivated by the more general problem of network coding, introduced by Ahlswede et al. [1]. El Rouayheb et al. showed in [10] that network coding instances can be efﬁciently reduced to index coding instances. Hence, understanding the computational complexity of solving the index coding problem (exactly or approximately) has implications on that of network coding (see [14], [15]).\nFor a graph G and a ﬁeld F we denote by β 1 ( G ) the minimum length of an index code for G over F . This graph parameter is well-known to be related to several classical graph parameters. Indeed, for an undirected graph G, β 1 ( G ) is bounded from below by α ( G ) , the maximum size of an independent set in G, as follows from the fact that an independent set in G corresponds to a set of receivers with no mutual information. On the other hand, β 1 ( G ) is bounded from above by χ ( G ) , the clique cover number of G, as follows from broadcasting the sum over F of the characters corresponding to the vertices in every clique in an optimal clique cover. 1\nIn this paper we focus on linear index coding schemes, i.e., coding schemes in which the encoding function is linear. Bar- Yossef et al. [5] proved that the minimum length of a linear index code for a graph G over F equals the minimum rank over F of a matrix that has nonzero values from F in the diagonal and zeros in the entries that correspond to non-edges (and arbitrary values from F in the other entries). This graph parameter is called minrank and is denoted by minrk F ( G ) . Clearly, minrk F ( G ) bounds β 1 ( G ) from above for every F . Interestingly, it was proven in [5] that this upper bound is tight and is equal to β 1 ( G ) for several graph families for the binary ﬁeld F 2 . This includes directed acyclic graphs, perfect graphs, odd holes (undirected odd-length cycles of length at least 5) and odd anti-holes (complements of odd holes). These results raised the question whether the minrank parameter characterizes the minimum length of general index codes. This question was answered in the negative by Lubetzky and Stav [17], who showed that for any ε > 0 and a sufﬁciently large n there is an n vertex graph G with β 1 ( G ) n ε and\nminrk F 2 ( G ) n 1−ε (see [3] for additional counterexamples). We note that the proof in [17] uses a property of the minrank (see also [12]), saying that for every ﬁeld F and an n vertex undirected graph G,\nThe ﬁrst to deﬁne the minrank parameter was Haemers [11], [12], who related it to what is known as the Shannon capac- ity of graphs introduced in [21]. Haemers showed that for every ﬁeld F and an undirected graph G, α ( G ) \t c ( G )\nminrk F ( G ) , where c ( G ) stands for the Shannon capacity of G. He also showed that there are graphs for which the minrank upper bound on the Shannon capacity is tighter than the one given by the well-known Lov´asz ϑ-function introduced in [16]. We note that calculating the minrank of a given input graph is known to be computationally hard [14], [20], as opposed to the efﬁciently computable Lov´asz ϑ-function.\nThe following theorem summarizes some of the bounds mentioned above.\nTheorem 1.[ [5], [11], [12]] For every ﬁeld F and an undirected graph G, α ( G ) β 1 ( G ) minrk F ( G ) χ ( G ) .\nAll the inequalities in the above statement are known to be strict for certain graphs. This makes the task of understanding β 1 ( G ) challenging. A fundamental parameter to study in this context is the typical value of β 1 ( G ) for random graphs G. This question was raised by Lubetzky and Stav in [17] for the well-known random graph G ( n, 1 2 ) , where G ( n, p ) denotes the random undirected graph with n vertices in which every edge is chosen to exist independently with probability p. In this paper we focus on linear index codes and study the following question:\nEquivalently, we are asking for the typical minrank over F of the random graph G ( n, p ) .\nLet us start with some bounds yielded by Theorem 1. Both the independence number and the clique cover number of G ( n, p ) are well understood (see [9] for the former and [8], [18] for the latter). For a constant edge probability p, we obtain that almost surely (i.e., with probability that tends to 1 as n tends to inﬁnity),\nminrk F ( G ( n, p )) O ( n log n ) . The gap between these lower and upper bounds is exponential, and, surprisingly, no better bounds are known to hold almost surely for G ( n, p ) . Yet, it is plausible to expect the minrank of G ( n, p ) to be much higher than the Ω ( log n ) lower bound, since the bound in (1) implies that the expected minrank of G ( n, p ) is Ω (\n(and hence for any p 1 2 as well). To see this, notice that if G is distributed according to G ( n, 1 2 ) then so is its complement, and hence the probability that minrk F ( G )\nlower bound on the expectation above would imply an ω (\nlower bound which holds almost surely, as follows from the large deviation inequality for vertex exposure martingales (see, e.g., [4], Chapter 7). Understanding the true value of minrk F ( G ( n, p )) and, more speciﬁcally, the question whether one can show an\nIn the current paper we study the typical minimum length of a linear index code for the random graph G ( n, p ) over a ﬁeld F . We start by showing that an Ω (\nlower bound holds with probability that (exponentially) tends to 1 as n tends to inﬁnity (and not only in expectation). In addition, the bound holds for every constant size ﬁeld F and a constant edge probability p. 2 Theorem 2. For every constant size ﬁeld F and a constant p ∈ ( 0, 1 ) , almost surely minrk F ( G ( n, p )) = Ω ( √ n ) .\nObserve that Theorem 2 implies that the random graph G ( n, 1 2 ) almost surely has an exponential gap between its independence number and its minrank over any constant size ﬁeld. In [2], Alon conjectured that the Shannon capacity of G ( n, 1 2 ) satisﬁes c ( G ( n, 1 2 )) = O ( log n ) almost surely. This, if true, would imply an exponential gap between the Shannon capacity and the minrank upper bound of Haemers [12] on it for a typical graph G ( n, 1 2 ) .\nIn the attempt to understand where the minrank of G ( n, p ) exactly lies in the range from\nlog n we introduce and study two natural special models of index coding.\nLocally decodable index coding. In our ﬁrst model we study index codes in which the decoders are allowed to query a limited number of characters from the encoded message. More precisely, these are index codes in which the sender maps x ∈ F n to an encoded message, and each of the receivers should be able to recover x i using at most q queries to the encoded message and the information that the receiver has on x according to the side information graph. Locally decodable index codes naturally correspond to efﬁcient decoding, and typically, as q grows smaller the minimum length of a locally decodable index code for a given graph increases. The fol- lowing theorem says that every linear locally decodable index code for G ( n, p ) over F with q signiﬁcantly smaller than\nTheorem 3. For every constant size ﬁeld F and a constant p ∈ ( 0, 1 ) , if there exists a linear index code of length for G ( n, p ) over F , such that every decoding function queries at most q = Ω ( n 1 3 ) characters from the encoded message, then almost surely = Ω ( n q ) .\nLow density index coding. The second model we study consists of linear index codes in which every character of the word x (that the sender wishes to broadcast) affects a limited number, say q, of characters in the encoded message. Such codes are generated by generator matrices in which every row has at most q nonzero entries, thus we call them low density generator matrix index codes (or, in short, low density index codes). Complementary to locally decodable index codes, low density index codes correspond to efﬁcient encoding procedures. Again, as q grows smaller the minimum length of a low density index code for a given graph may increase.\nLow density (generator matrix) codes are usually not so useful in coding theory. The reason is that such codes have minimum distance at most q, whereas, in most applications, one desires codes of large minimum distance. However, for our purposes such codes turn to play a major role. More specif- ically, our next theorem says that improving the\nbound on the length of low density index codes for G ( n, p ) will imply such an improvement on the length of linear index codes for G ( n, p ) in general. This is quite surprising since low density index codes intuitively seem signiﬁcantly weaker than general linear index codes. We state this result here informally, and the formal statement can be found in Section V.\nTheorem 4.[informal] Assume that every linear index code for G ( n, p ) over F , with at most q = ω ( 1 ) nonzero entries in a row of its generator matrix, has length ω (\nn ) with high probability. Then, almost surely, minrk F ( G ( n, p )) = ω (\n. Theorem 4 motivates studying lower bounds on the length\nof low density index codes for G ( n, p ) . Observe that the minimum length of a low density index code with q = 1 for a graph G equals the clique cover number χ ( G ) . This implies a tight lower bound of Ω ( n log n ) for q = 1. We are also able to prove ω (\nlower bounds for low density index codes for q = 2 and q = 3, as stated below. However, the analysis for larger q (especially q = ω ( 1 ) ) remains open.\nTheorem 5. For every constant size ﬁeld F and sufﬁciently small constants ε , p > 0, every q-low density index code for G ( n, p ) over F almost surely has length at least n 1−ε for q = 2 and n 2 3 −ε for q = 3.\nG ( n, p ) versus G ( n, p ) . Although the results above are stated for the undirected random graph G ( n, p ) , it turns out that the probability analysis is simpler if one were to consider the directed random graph G ( n, p ) . This is due to the independence between the (outgoing) neighborhoods of different vertices. 3 We thus start by reducing the study of the random undirected graph to that of the random directed graph. Intuitively, this reduction is possible as the directed graph G ( n, p ) essentially contains a copy of the undirected graph G ( n, p 2 ) and is contained in the undirected graph G ( n, 1 − ( 1 − p ) 2 ) (due to space limitations, details of this reduction are omitted and appear in the full version of this work [13]).\nIn Section II we provide some background preliminaries needed throughout the paper. In Section III we prove the\nn ) lower bound given in Theorem 2. In Section IV we prove our result on locally decodable index codes, and in Section V we prove our results on low density index codes. The ﬁnal Section VI discusses some concluding remarks and open questions. Due to space limitations, our assertions appear without detailed proofs. Complete proofs can be found in the full version of the paper available online [13].\nIn the index coding problem a sender wishes to broadcast a word x ∈ F n (for a ﬁnite ﬁeld F ) to n receivers R 1 , . . . , R n . Every receiver R i knows some ﬁxed subset of the characters of x and is interested solely in the character x i . An -index code for this setting is a length code over F , which enables R i to recover x i for every x ∈ F n and i ∈ [ n ] .\nThe index coding problem can be stated as a graph param- eter. For a directed graph G and a vertex v let N + G ( v ) denote the set of out-neighbors of v in G, and for x ∈ F n and S ⊆ [ n ] let x | S denote the restriction of x to the coordinates of S. The setting of the deﬁnition of an index code is characterized by the directed side information graph G on the vertex set [ n ] where ( i, j ) is an edge if and only if the receiver R i knows x j . An -index code for G over F is a function E : F n → F\nand functions D 1 , . . . , D n , so that for all i ∈ [ n ] and x ∈ F n , D i ( E ( x ) , x | N +\n) = x i . The deﬁnition of an index code is naturally extended to undirected graphs by replacing every undirected edge by two oppositely directed edges. We say that the index code is linear if the encoding function E is linear (and thus is represented by an n × matrix). Bar-Yossef et al. [5] showed that the minimum length of a linear index code for G over F equals minrk F ( G ) , a graph parameter deﬁned as follows.\nDeﬁnition 6. Let A = ( a i j ) be an n by n matrix over some ﬁeld F . We say that A represents an n vertex graph G over F if a ii = 0 for all i, and a i j = 0 whenever i = j and ( i, j ) is not an edge in G. The minrank of a graph G over F is deﬁned as\nWe need the following simple claim, in which we use B n ( r ) to denote the set of vectors in F n of Hamming weight (i.e., number of nonzero entries) at most r. The claim can be proven in a greedy manner, details appear in our full version [13].\nClaim 7. For every ﬁeld F , n, , r ∈ N , and a basis E ∈ F n× , the number of indices of coordinates that are nonzero in at least one vector in span ( E ) ∩ B n ( r ) is at most r · . Here, span ( E ) is the column span of E.\nLet G ( n, p ) , resp. G ( n, p ) , denote the random undirected, resp. directed, graph with n vertices in which every edge is chosen to exist independently with probability p. We say that G ( n, p ) , resp. G ( n, p ) , satisﬁes a graph property almost surely if the probability that G ( n, p ) , resp. G ( n, p ) , satisﬁes this property tends to 1 as n tends to inﬁnity.\nThroughout the paper we ignore ﬂoors and ceilings when- ever appropriate as this does not affect the asymptotic nature of our results.\nalmost surely. As noted, it sufﬁces to prove the lower bound for the directed random graph G ( n, p ) . We start with some intuition. Fix a linear -index code generated by E ∈ F n× for certain = O (\n. Our goal is to show that the probability that E is an index code for G ( n, p ) is exponentially small, so that applying the union bound over all the codes E will give us the result. It is not hard to see that E is an index code for a graph G if and only if for each vertex i there exists a vector in the column span span ( E ) = span ( e 1 , . . . , e ) that is nonzero in the ith entry and is zero in all the entries that correspond to non-neighbors of i (see, e.g., [5, Claim 10]). This motivates the following deﬁnition which will be useful throughout the paper.\nDeﬁnition 8. For a (directed) graph G on the vertex set [ n ] , a vector v ∈ F n satisﬁes a vertex i ∈ [ n ] if v i = 0 and v j = 0 for every j ∈ [ n ] \\ { i } such that i is not connected to j in G.\nIt now follows that any vector in span ( E ) of Hamming weight r, whose ith entry is nonzero, satisﬁes a vertex i in G ( n, p ) with probability p r−1 . Using this, we show that the probability that at least n 2 vertices are satisﬁed by vectors of \u201chigh\u201d Hamming weight is small (Lemma 9). On the other hand, by Claim 7 we show that at most n 2 vertices can be satisﬁed by vectors of \u201clow\u201d Hamming weight (Lemma 10). Thus, with high probability there exists a vertex in the graph which is not satisﬁed by any vector in span ( E ) , and hence with such probability, E is not an index code for the graph.\nThe following lemma bounds from above the probability that the graph G ( n, p ) has an index code for which many vertices are satisﬁed by vectors of \u201chigh\u201d Hamming weight.\nLemma 9. For every ﬁeld F and n, r, s ∈ N , the probability that there exist a linear -index code E ∈ F n× for G ( n, p ) over F and s vertices, each of which is satisﬁed by a vector in\nProof: Fix a linear -index code E for G ( n, p ) over F and a set S ⊆ [ n ] of s vertices. The probability that a vertex i is satisﬁed by a ﬁxed vector y ∈ span ( E ) \\ B n ( r ) is at most p r . To see this, notice that every vertex (except i) which corresponds to a nonzero entry of y must be a neighbor of i, and this happens independently with probability p. Taking the union bound over all the vectors in span ( E ) \\ B n ( r ) , we get that the probability that a vertex is satisﬁed by a vector in span ( E ) \\ B n ( r ) is at most |F| · p r . Hence, by the independence of the edges in G ( n, p ) , the probability that every vertex in S is satisﬁed by a vector in span ( E ) \\ B n ( r ) is at most |F| · p r\n. Now, apply the union bound over all the matrices E and sets S to get the desired bound.\nNow we turn to deal with vertices which are satisﬁed by vectors of \u201clow\u201d Hamming weight and to bound from above their number.\nLemma 10. For every ﬁeld F , a graph G, and a linear -index code for G over F , at most n 2 vertices in G are satisﬁed by vectors of Hamming weight at most n 2 .\nProof: Let E ∈ F n× be a generator matrix of a linear -index code for G over F . By Claim 7, the number of\nindices of coordinates that are nonzero in at least one vector in span ( E ) ∩ B n ( n 2 ) is at most n 2 . Recall that a vector which satisﬁes a vertex i must have the ith entry nonzero. Hence, the number of vertices that can be satisﬁed by vectors in span ( E ) of Hamming weight at most n 2 is at most n 2 .\nlower bound follows from combining Lem- mas 9 and 10.\n \nIn this section we study locally decodable index codes deﬁned as follows.\nDeﬁnition 12. A ( q, ) -locally decodable index code is an - index code in which the query complexity of the decoding is at most q. This means that for every i the decoding function D i of the ith receiver queries at most q characters from the encoded message.\nRemark 13. For every graph G, the minimum for which there is a ( 1, ) -locally decodable index code for G over F is the clique cover number χ ( G ) of G.\nThe following theorem shows a lower bound on the length of a linear locally decodable index code for G ( n, p ) over F . Although more involved, its proof follows the nature of the proof given for Theorem 11 and can be found in [13].\nTheorem 14. For every constant size ﬁeld F and a constant p ∈ ( 0, 1 ) , there exist constants c 1 , c 2 > 0 such that if\nc 1 · n 2 / 3 (log n) 1 / 3 and q c 2 · n ·log then almost surely there is no linear ( q, ) -locally decodable index code for G ( n, p ) over F .\nIn this section we study low density generator matrix index codes (or, in short, low density index codes). To obtain our lower bounds (in Section V-B) we use proof techniques that differ signiﬁcantly from those previously presented. A formal deﬁnition of low density index codes follows.\nDeﬁnition 15. A ( q, ) -low density index code is a linear - index code in which every character of the broadcasted word affects at most q characters in the encoded message. Equiva- lently, it is a linear -index code whose generator matrix has at most q nonzero entries in a row.\nlower bound on the minimum length of a linear index code for G ( n, p ) over a ﬁeld F , it sufﬁces to prove such a lower bound on the length of a low density code for G ( n, p ) over F for some q = ω ( 1 ) . Full proof can be found in [13].\nTheorem 16. For every ﬁeld F and p ∈ ( 0, 1 ) , if the probability that G ( n, p ) has a ( q, ) -low density index code over F is 2 −ω(n) for some q = ω ( 1 ) and = ω (\nn ) , then the minimum length of a linear index code for G ( n, p ) over F is almost surely ω (\nThe following theorem says that every index code for G ( n, p ) whose generator matrix has at most 3 nonzero entries in a row has length ω (\nn ) . Roughly speaking, our proof is based on the following overview. Consider a ﬁxed linear index code for G ( n, p ) with generator matrix E ∈ F n× , and assume that every row of E consists of at most 3 nonzero entries. To utilize the low density assumption under study we tie the notion of satisfaction from Deﬁnition 8 to the rows of E instead of the column space (as in the previous proof technique). More speciﬁcally, we show that a vertex i ∈ [ n ] is satisﬁed by some vector in span ( E ) if and only if the ith row of E cannot be written as a linear combination of the rows corresponding to non-neighbors of i. Using the fact that the rows of E are sparse, we analyze the probability that a vertex i is satisﬁed by some vector in span ( E ) . Our proof employs a result of Naor and Verstra¨ete [19] addressing the maximum possible size of a set of sparse vectors with no small linearly dependent subsets. The proof can be found in [13].\nTheorem 17. For every ﬁeld F and a sufﬁciently small ε > 0 there exists a p = p (|F| , ε ) > 0 such that for any p ∈ ( 0, p ) the following holds almost surely.\n1) If there is a ( 2, ) -low density index code for G ( n, p ) over F then \t n 1−ε .\n2) If there is a ( 3, ) -low density index code for G ( n, p ) over F then \t n 2 3 −ε .\nIn this paper we initiated the study of index coding for the random graph G ( n, p ) over a ﬁeld F and introduced two new models of index coding \u2013 locally decodable index coding and low density index coding. We proved several lower bounds on the length of linear index codes for G ( n, p ) (Theorems 11, 14, 17) and showed that in order to improve the\nThe main task left for further work is to obtain tighter bounds on the minimum length of index codes for the random graph G ( n, p ) over a ﬁeld F . More speciﬁcally, it is not known if there exists an index code for G ( n, p ) (linear or not) shorter than the one achieved by the clique cover. It is interesting if our lower bounds can be extended to general (non-linear) index codes. It would be nice to understand better how the limit on\nthe number of queries affects the length of locally decodable index codes for G ( n, p ) . We hope that the new notion of low density index codes and Theorem 16 will be found useful in understanding the minrank of G ( n, p ) over F .\nAnother challenging research direction is to study the vector capacity of the random graph G ( n, p ) (see [3], [7], [17]). Here, the sender wishes to broadcast a word x of n blocks, each of t bits, to n receivers. The ith receiver is interested in the ith block and has side information consisting of a subset of the other blocks according to G ( n, p ) . Denoting by β t the minimum number of bits that has to be transmitted, we are interested in lim t→ ∞ β t t . This limit represents the average communication cost per bit in each block (for long blocks), and it will be very interesting to compare it to β 1 of a typical random graph."},"refs":[{"authors":[{"name":"R. Ahlswed"},{"name":"N. Ca"},{"name":"S.-Y. R. L"},{"name":"R. W. Yeung"}],"title":{"text":"Network information ﬂow"}},{"authors":[{"name":"N. Alon"}],"title":{"text":"The shannon capacity of a union"}},{"authors":[{"name":"N. Alo"},{"name":"E. Lubetzk"},{"name":"U. Sta"},{"name":"A. Weinstei"},{"name":"A. Hassidim"}],"title":{"text":"Broad- casting with side information"}},{"authors":[{"name":"N. Alo"},{"name":"J. H. Spencer"}],"title":{"text":"The probabilistic method"}},{"authors":[{"name":"Z. Bar-Yosse"},{"name":"Y. Bir"},{"name":"T. S. Jayra"},{"name":"T. Kol"}],"title":{"text":"Index coding with side information"}},{"authors":[{"name":"Y. Bir"},{"name":"T. Kol"}],"title":{"text":"Coding on demand by an informed source (ISCOD) for efﬁcient broadcast of different supplemental data to caching clients"}},{"authors":[{"name":"A. Blasia"},{"name":"R. D. Kleinber"},{"name":"E. Lubetzky"}],"title":{"text":"Index coding via linear programming"}},{"authors":[{"name":"B. Bollob´as"}],"title":{"text":"The chromatic number of random graphs"}},{"authors":[{"name":"B. Bollob´a"},{"name":"P. Erd˝os"}],"title":{"text":"Cliques in random graphs"}},{"authors":[{"name":"S. El Rouayhe"},{"name":"A. Sprintso"},{"name":"C. Georghiades"}],"title":{"text":"On the relation between the index coding and the network coding problems"}},{"authors":[{"name":"W. Haemers"}],"title":{"text":"On some problems of Lov´asz concerning the Shannon capacity of a graph"}},{"authors":[{"name":"W. Haemers"}],"title":{"text":"An upper bound for the Shannon capacity of a graph"}},{"authors":[{"name":"I. Havi"},{"name":"M. Langberg"}],"title":{"text":"On Linear Index Coding for Random Graphs"}},{"authors":[{"name":"M. Langber"},{"name":"A. Sprintson"}],"title":{"text":"On the hardness of approximating the network coding capacity"}},{"authors":[{"name":"A. R. Lehma"},{"name":"E. Lehman"}],"title":{"text":"Complexity classiﬁcation of network information ﬂow problems"}},{"authors":[{"name":"L. Lov´asz"}],"title":{"text":"On the Shannon capacity of a graph"}},{"authors":[{"name":"E. Lubetzk"},{"name":"U. Stav"}],"title":{"text":"Nonlinear index coding outperforming the linear optimum"}},{"authors":[{"name":"T. Luczak"}],"title":{"text":"The chromatic number of random graphs"}},{"authors":[{"name":"A. Nao"},{"name":"J. Verstra¨ete"}],"title":{"text":"Parity check matrices and product repre- sentations of squares"}},{"authors":[{"name":"R. Peeters"}],"title":{"text":"Orthogonal representations over ﬁnite ﬁelds and the chro- matic number of graphs"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"The zero error capacity of a noisy channel"}},{"authors":[{"name":"R. W. Yeun"},{"name":"Z. Zhang"}],"title":{"text":"Distributed source coding for satellite communications"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569560581.pdf"},"links":[{"id":"1569565663","weight":2},{"id":"1569565867","weight":2},{"id":"1569559617","weight":2},{"id":"1569566981","weight":4},{"id":"1569566683","weight":2},{"id":"1569559259","weight":2},{"id":"1569565551","weight":2},{"id":"1569552245","weight":4},{"id":"1569564805","weight":4},{"id":"1569567005","weight":2},{"id":"1569566469","weight":2},{"id":"1569565355","weight":2},{"id":"1569564469","weight":2},{"id":"1569566871","weight":2},{"id":"1569565171","weight":2},{"id":"1569565837","weight":7},{"id":"1569565317","weight":2},{"id":"1569566941","weight":2},{"id":"1569556713","weight":2},{"id":"1569566467","weight":2},{"id":"1569566843","weight":2},{"id":"1569558483","weight":7},{"id":"1569564387","weight":23},{"id":"1569565455","weight":2},{"id":"1569566795","weight":21},{"id":"1569566015","weight":7},{"id":"1569565953","weight":2},{"id":"1569566895","weight":2},{"id":"1569565321","weight":2},{"id":"1569566095","weight":2},{"id":"1569566193","weight":7},{"id":"1569566167","weight":2},{"id":"1569566679","weight":2},{"id":"1569566575","weight":16},{"id":"1569563981","weight":2},{"id":"1569559565","weight":2},{"id":"1569566063","weight":4},{"id":"1569565213","weight":2},{"id":"1569565841","weight":4},{"id":"1569566423","weight":2},{"id":"1569566437","weight":2},{"id":"1569558901","weight":2},{"id":"1569565915","weight":2},{"id":"1569566139","weight":21},{"id":"1569564209","weight":2},{"id":"1569554881","weight":2},{"id":"1569566445","weight":2},{"id":"1569566209","weight":2},{"id":"1569565559","weight":2},{"id":"1569566909","weight":2},{"id":"1569566913","weight":2},{"id":"1569566809","weight":2},{"id":"1569565055","weight":2},{"id":"1569555879","weight":4},{"id":"1569565219","weight":2},{"id":"1569566003","weight":4},{"id":"1569565185","weight":2},{"id":"1569556671","weight":2},{"id":"1569566553","weight":2},{"id":"1569566603","weight":2},{"id":"1569566695","weight":2},{"id":"1569566051","weight":2},{"id":"1569566297","weight":7},{"id":"1569566317","weight":2},{"id":"1569560503","weight":2},{"id":"1569565439","weight":4},{"id":"1569562551","weight":2},{"id":"1569563395","weight":2},{"id":"1569566901","weight":2},{"id":"1569551347","weight":2},{"id":"1569565571","weight":2},{"id":"1569564411","weight":2},{"id":"1569565665","weight":2},{"id":"1569566983","weight":2},{"id":"1569566873","weight":2},{"id":"1569565435","weight":2},{"id":"1569566129","weight":2},{"id":"1569566711","weight":2},{"id":"1569566253","weight":2},{"id":"1569565349","weight":2},{"id":"1569566137","weight":2},{"id":"1569566813","weight":7},{"id":"1569563975","weight":28},{"id":"1569566487","weight":2},{"id":"1569565271","weight":2},{"id":"1569561185","weight":2},{"id":"1569566397","weight":2},{"id":"1569558779","weight":2},{"id":"1569566817","weight":2},{"id":"1569564923","weight":7},{"id":"1569565805","weight":2},{"id":"1569563919","weight":2},{"id":"1569557851","weight":2},{"id":"1569565389","weight":2},{"id":"1569564961","weight":2},{"id":"1569559251","weight":2},{"id":"1569561861","weight":7},{"id":"1569564253","weight":2},{"id":"1569565853","weight":4},{"id":"1569564505","weight":4},{"id":"1569565165","weight":7},{"id":"1569565113","weight":4},{"id":"1569566375","weight":2},{"id":"1569564257","weight":2},{"id":"1569564141","weight":2},{"id":"1569566973","weight":2},{"id":"1569564509","weight":2},{"id":"1569565579","weight":4},{"id":"1569566067","weight":2},{"id":"1569566443","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T1.1","endtime":"15:00","authors":"Ishay Haviv, Michael Langberg","date":"1341499200000","papertitle":"On Linear Index Coding for Random Graphs","starttime":"14:40","session":"S13.T1: Index Coding","room":"Kresge Rehearsal B (030)","paperid":"1569560581"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
