{"id":"1569565347","paper":{"title":{"text":"Linear Information Coupling Problems"},"authors":[{"name":"Shao-Lun Huang"},{"name":"Lizhong Zheng"}],"abstr":{"text":"Abstract\u2014Many network information theory problems face the similar difﬁculty of single letterization. We argue that this is due to the lack of a geometric structure on the space of probability distribution. In this paper, we develop such a structure by assuming that the distributions of interest are close to each other. Under this assumption, the K-L divergence is reduced to the squared Euclidean metric in an Euclidean space. Moreover, we construct the notion of coordinate and inner product, which will facilitate solving communication problems. We will also present the application of this approach to the point-to-point channel and the general broadcast channel, which demonstrates how our technique simpliﬁes information theory problems."},"body":{"text":"Since Shannon introduced the notion of capacity sixty years ago, ﬁnding the capacity of channels and networks are core problems in information theory. The analyzation of capacity answers the problem that how many bits can be transmitted through a communication network, and also provides many insights in engineer problems [1]. However, for general prob- lems, there is no systematic way to obtain optimal single-letter solutions. By cleverly picking auxiliary random variables, we sometimes can prove the constructed single-letter solutions are optimal for some problems. But, when this fails, we cannot tell whether it is because we have not tried hard enough or the problem itself does not have an optimal single-letter solution.\nThe difﬁculty of obtaining optimal single letter solutions comes from the fact that, most of the information theoretical quantities, such as entropy, mutual information, and error exponents, are all special cases of the Kullback-Leibler (K- L) divergence. The K-L divergence is a measure of distance between two probability distributions. However, in multi- terminal communication problems, there are multiple input and output distributions, and we usually need to deal with problems in high dimensional probability spaces. In these cases, describing problems only with the distance measure is cumbersome, and solving these information theory problems turns out to be extremely hard even with numerical aids. Therefore, we need more geometric structures to describe the problems, such as inner products. This is however difﬁcult, as the K-L divergence between two distributions, D(P Q), is not symmetric between P and Q, and the K-L divergence is in general not a valid metric. Thus, the space of probability distributions is not a linear vector space but a manifold [2], when the K-L divergence behaves as the distance measure.\nIn this paper, we present an approach [3] to simplify the problems with the assumption that the distributions of interest are close to each other. With this assumption, the manifold formed by distributions can be approximated by a tangent plane, which is an Euclidean space. Moreover, the K-L diver- gence will behave as the squared Euclidean metric between distributions in this Euclidean space. Therefore, we obtain the notion of coordinate, inner product, and orthogonality in a linear metric space, to describe information theory problems. Moreover, we will demonstrate in the rest sections that, the linear structure constructed from out local approximation will transfer information theory problems to linear algebra problems, which can be solved easily. In particular, we show that a systematic approach can be used to solve the single- letterization problems. We apply this to the general broadcast channel, and obtain new insights of the optimality of the existing solutions [4]-[7]\nThe rest of this paper is organized as follows. We introduce the notion of local approximation in section II, and show that the K-L divergence can be approximated as the squared Euclidean metric. In section III and IV, we present the application of our local approximation to the point-to-point channel and the general broadcast channel, respectively. We will illustrate how the information theory problems become simple linear algebra problems when applying our technique.\nThe key step of our approach is to use a local approximation of the Kullback-Leibler (K-L) divergence. Let P and Q be two distributions over the same alphabet X . We assume that Q(x) = P (x) + J(x), for some small value , then the K-L divergence can be written, with second order Taylor expansion, as\nWe denote x J 2 (x)/P (x) as J 2 P , which is the weighted norm square of the perturbation vector J. It is easy to verify here that replacing the weight in this norm by Q only results in\na o( 2 ) difference. That is, up to the ﬁrst order approximation, the weights in the norm simply indicate the neighborhood of distributions where the divergence is computed. As a consequence, D(P Q) and D(Q P ) are considered as equal up to the ﬁrst order approximation.\nFor convenience, we deﬁne the weighted perturbation vector as\nthe diagonal matrix with entries \t P (x) −1 , x ∈ X . This allows us to write J 2 P = L 2 , where the last norm is simply the Euclidean norm.\nWith this deﬁnition of the norm on the perturbations of distributions, we can generalize to deﬁne the corresponding notion of inner products. Let Q i (x) = P (x)+ ·J i (x), ∀x, i = 1, 2, we can deﬁne\ni = 1, 2. From this, notions of orthogonal perturbations and projections can be similarly deﬁned. The point here is that we can view a neighborhood of distributions as a linear metric space, and deﬁne notions of orthonormal basis and coordinates on it.\nWe start by using this local geometric structure to study the point-to-point channels to demonstrate the new insights we can obtain from this approach, even on a well-understood problem. It is well-known that the capacity problem is\nfor some discrete random variable U , such that U → X n → Y n forms a Markov chain.\nNow, to apply the local approximations, instead of solving (2). we study a slightly different problem\nWe call the problem (3) as the linear information coupling problem . The only difference between (2) and (3) lies in the constraint 1 n I(U ; X n ) ≤ 1 2 2 on (3). That is, instead of trying to ﬁnd how many bits in total that we can send through the given channel, we ask the question of how efﬁciently we can send a thin layer of information through this channel. One advantage of (3) is that it allows easy single letterization as we will demonstrate in the following. In fact, the step of single-letterization, namely, form (2) to (1), is the difﬁcult step of most network problems. For these problems, the approach\nwe used for the point-to-point problem can not be applied. What we will show in the rest of this section is that there is an alternative approach to do the well-known steps [1] to go from (2) to (1), and this new approach based on the geometric structures can be applied to more general problems. For simplicity, in this paper, we assume that the marginal distribution P X n is given, and is an i.i.d. distribution over the n letters 1 , so that we can focus on ﬁnding U and the conditional distribution P X n |U optimizing (3).\nFirst, we solve the single-letter version, namely n = 1, of this problem. Observing that we can write the constraint as\nThis implies that for each value of u, the conditional dis- tribution P X|U =u is a local perturbation from P X , that is, P X|U =u = P X + · J u .\nu , for each value of u, we observe that\nwhere the channel applied to an input distribution is simply viewed as the channel matrix W , of dimension |Y| × |X |, multiplying the input distribution as a vector. At this point, we have reduced both the spaces of input and output distributions as linear spaces, and the channel acts as a linear transform between these two spaces. The information coupling problem can be rewritten as, ignoring the o( 2 ) terms:\nThis problem of linear algebra is simple. We need to ﬁnd the joint distribution U → X → Y by specifying the P U and the perturbations J u for each value of u, such that the marginal constraint on P X is met, and also these perturbations are the most visible at the Y end, in the sense that multiplied by the channel matrix, W J u \u2019s have large norms. This can be readily solved by setting the weighted perturbation vectors L u \u2019s to be along the input (right) singular vectors of the matrix B √\nchoice of P U has no effect in the optimization, and might be taken as binary uniform for simplicity. This is illustrated in Figure 1(a).\nWe call the matrix B the divergence transition matrix (DTM). It maps divergence in the space of input distribu- tions to that of the output distributions. The singular value decomposition (SVD) structure of this linear map has a critical role of our analysis. It can be shown that the largest singular value of B is 1, corresponding to an input singular vector √\nP X , x ∈ X T , which is orthogonal to the simplex of prob- ability distributions. This is not a valid choice for perturbation vectors. However, all vectors orthogonal to this vector, or equivalently, all linear combinations of other singular vectors are valid choices of the perturbation vectors L u . Thus, the optimum of the above problem is achieved by setting L u to be along the singular vector with the second largest singular value.\nThis can be visualized as in Figure 1(b), the orthonormal bases for the input and output spaces, respectively, according to the right and left singular vectors of B. The key point here is that while I(U ; X) measures how many bits of information is modulated in X, depending on how they are modulated, in terms of which direction the corresponding perturbation vector is, these bits have different levels of visibility at the Y end. This is a quantitative way to show why viewing a channel as a bit-pipe carrying uniform bits is a bad idea.\nMoreover, recalling that the data processing inequality tells that, from the Markov chain U → X → Y , the mutual informations have the relation I(U ; X) ≥ I(U; Y ). Let us assume that the second largest singular value of B is σ ≤ 1, then the above derivations imply that σ 2 ·I(U; X) ≥ I(U; Y ). Thus, we actually come up with a stronger result than the\ndata processing inequality, and the equality can be achieved by setting the perturbation vector to be along the right singular vector of B, with the second largest singular value.\nThe most important feature of the linear information cou- pling problem is that the single-letterization (3) is simple. To illustrate the idea, we consider a 2-letter version of the point- to-point channel:\nLet P X , P Y , W , and B be the input and output distributions, channel matrix, and the DTM, respectively for the single letter version of the problem. Then, the 2-letter problem has P (2) X = P X ⊗ P X , P (2) Y = P Y ⊗ P Y , and W (2) = W ⊗ W , where ⊗\ndenotes the Kronecker product. As a result, the new DTM is B (2) = B ⊗ B. We have the following lemma on the singular values and vectors of B (2) .\nLemma 1. Let v i and v j denote two singular vectors of B with singular values µ i and µ j . Then v i ⊗ v j is a singular vector of B (2) and its singular value is µ i µ j .\nNow, recall that the largest singular value of B is µ 0 = 1, with the singular vector v 0 =\nT , which corre- sponds to the direction orthogonal to the distribution simplex. This implies that the largest singular value of B (2) is also 1, again corresponds to the direction that is orthogonal to all valid choices of the perturbation vectors.\nThe second largest singular value of B (2) is a tie between µ 0 µ 1 and µ 1 µ 0 , with singular vectors v 0 ⊗v 1 and v 1 ⊗v 0 . The optimal solution of (4) is thus to set the perturbation vectors to be along these two vectors. This can be written as\nT . This means that the optimal conditional distribution P X 2 |U =u for any u has the product form, up to the ﬁrst order approximation. With a simple time-sharing argument, it is easy to see that we can indeed set = , that is, pick this conditional distribution to be i.i.d. over the two symbols, to achieve the optimum.\nThe simplicity of this proof of the optimality of the single letter solutions is astonishing. All we have used is the fact that the singular vector of B (2) corresponding to the second largest singular value has a special form. A distribution in the neighborhood of P X ⊗P X is a product distribution if and only if it can be written as a perturbation from P X ⊗ P X , along the subspace spanned by vectors v 0 ⊗ v i and v j ⊗ v 0 , in the form of v 0 ⊗ v + v ⊗ v 0 , for some v and v . Thus, all we need to do is to ﬁnd the eigen-structure of the B-matrix, and verify if the optimal solutions have this form. This procedure is used in more general problems.\nOne way to explain why the local approximation is useful is as follows. In general, tradeoff between multiple K-L\ndivergence (mutual information) is a non-convex problem. Thus, ﬁnding global optimum for such problems is in gen- eral intrinsically intractable and extremely hard. In contrast, with our local approximation, the K-L divergence becomes a quadratic function. Now, the tradeoff between quadratic functions remains quadratic. Effectively, our approach focus on verifying the local optimality of the quadratic solutions, which is a natural thing to do, since the overall problem is not convex.\nLet us now apply our local approximation approach to the general broadcast channel. A general broadcast channel with input X ∈ X , and outputs Y 1 ∈ Y 1 , Y 2 ∈ Y 2 , is speciﬁed by the memoryless channel matrices W 1 and W 2 . These channel matrices specify the conditional distributions of the output signals at two users, 1 and 2 as W i (y i |x) = P Y i |X (y i |x), for i = 1, 2. Let M 1 , M 2 , and M 0 be the two private messages and the common message, with rate R 1 , and R 2 , and R 0 , respectively. The multi-letter capacity region can be written as \t \n \nR 0 ≤ 1 n min {I(U; Y n 1 ), I(U ; Y n 2 ) }, R 1 ≤ 1 n I(V 1 ; Y n 1 ), R 2 ≤ 1 n I(V 2 ; Y n 2 ),\nfor some mutually independent random variables U , V 1 , and V 2 , such that (U, V 1 , V 2 ) → X n → (Y n 1 , Y n 2 ) forms a Markov chains.\nThe linear information coupling problems of the private messages, given that the common message is decoded, are essentially the same as the point-to-point channel case. Thus, we only need to focus on the linear information coupling problem of the common message:\nThe core problem we want to address here is that whether or not the single-letter solutions are optimal for (5). To do this, suppose that P X n |U =u = P (n) X + · J u . Deﬁne the DTMs B i \t P Y i −1 W i √ P X , for i = 1, 2, and the scaled\nwhere B (n) i is the n th Kronecker product of the single-letter DTM B i , for i = 1, 2.\nDifferent from the point-to-point problem, we need to choose the perturbation vectors L u to have large images si- multaneously through two different linear systems. In general, the tradeoff between two SVD structures can be rather messy problems. However, in this problem, for both i = 1, 2, B (n) i\nhave the special structure of being the Kronecker product of the single letter DTMs. Furthermore, both B 1 and B 2 have the largest singular value of 1, corresponding to the same singular\n∈ X T , although the rest of their SVD structures are not speciﬁed. The following theory characterizes the optimality of single-letter and ﬁnite-letter solutions for the general cases.\nTheorem 1. Suppose that B i are DTM\u2019s for some DMC and input/output distributions, for i = 1, 2, . . . , k, then the linear information coupling problem\nhas optimal single letter solutions for the case with 2 receivers. In general, when there are k > 2 receivers, single letter solutions can not be optimal, when the cardinality |U| is bounded by some function of |X |. However, there still exists k-letter solutions that are optimal.\nWhile we will not present the full proof of this re- sult in this paper, it worth pointing out how conceptually straightforward it is. We can write the right singular vec- tors of the two DTMs, B 1 and B 2 , as φ 0 , φ 1 , . . . , φ n−1 and ϕ 0 , ϕ 1 , . . . , ϕ n−1 . The only structure we have is that φ 0 = ϕ 0 = \t P X (x), x ∈ X T , both correspond to the largest singular value of 1. For other vectors, the relation between the two bases can be written as a unitary matrix Ψ, with φ i = j Ψ ij ϕ j . Now, we can deﬁne an orthonormal basis for the space of multi-letter distributions on X n . For example, with 2-letter distributions, we can use φ i ⊗ φ j , i, j ∈ {0, 1, ..., n − 1} and (i, j) = (0, 0). Note that any L u can be written as L u = i,j=(0,0) α ij φ i ⊗ φ j . If a perturbation vector L u has any non-zero component along φ i ⊗ φ j , with i, j = 0, we can always move this component to either φ i ⊗φ 0 or φ 0 ⊗φ j to have, say, φ i ⊗φ 0 = j Ψ ij ϕ j ⊗ϕ 0 . This results in larger norms of the output vectors through both channels. As a result, the optimizer of (7) can only have components on the vectors φ 0 ⊗ φ j and φ i ⊗ φ 0 . This means that the resulting conditional distribution must be product distributions, i.e. P X n |U =u = P X 1 |U =u · P X 2 |U =u . . .. This simple observa- tion greatly simpliﬁes the multi-letter optimization problem: instead of searching for general joint distributions, now we have the further constraint of conditional independence. This directly gives rise to the proof of the optimality of i.i.d. distributions and hence single letter solutions for the 2 user case, which is the ﬁrst deﬁnitive answer on the general broadcast channels.\nThe more interesting case is when there are more than 2 receivers. In such cases, i.i.d. distributions simply do not have enough degrees of freedom to be optimal in the tradeoff of more than 2 linear systems. Instead, one has to design multi-letter product distributions to achieve the optimal. The following example, constructed with the geometric method, illustrate the key ideas.\nWe consider a 3-user broadcast channel. Let the input al- phabet X be ternary, so that the perturbation vectors have 2 dimensions and can be easily visualized. Suppose that\nthe three DTMs are rotations of 0, 2π/3, 4π/3, respec- tively, followed (left multiplied) by the projection to the horizontal axis. This corresponds to the ternary input chan- nels as shown in Figure 2(a). Now if we use single-letter inputs, it can be seen that for any L u with L u 2 = 1, min B 1 L u 2 , B 2 L u 2 , B 3 L u 2 ≤ 1/4. The problem here is that no matter what direction L u 0 takes, the three output norms are unequal, and the minimum one always limits the performance. Now, if we use 3-letter input, and denote φ θ = [cos θ, sin θ] T , then we can take\nfor any value of θ, as shown in Figure 2(b). Intuitively, this input equalizes the three channels, and gives for all i = 1, 2, 3,\nB (3) i L (3) u 2 = 1/2, which doubles the information coupling rate. Translating this solution to the coding language, it means that we take turns to feed the common information to each individual user. Note that the solution is not a standard time- sharing input, and hence the performance is strictly out of the convex hull of i.i.d. solutions. One can interpret this input as a repetition of the common message over three time-slots, where the information is modulated along equally rotated vectors. For this reason, we call this example the \u201cwindmill\u201d channel. Additionally, it is easy to see that the construction of the windmill channel can be generalized to the cases of k > 3 receivers, where k-letter solutions is necessary.\nNote that in this example, we let U be a binary random variable, and in this case, while there are optimal 3-letter solutions, the optimal single-letters do not exist. However, one can in fact take U to be non-binary. For example, let U = {0, 1, 2, 3, 4, 5} with P U (u) = 1/6 for all u, and let\n, then we can still achieve the information coupling rate 1/2. Thus, there actually exits an op- timal single-letter solution with cardinality |U| = 6. However, when there are k receivers, it requires cardinality |U| = 2k for obtaining optimal single-letter solutions. Essentially, this example shows that ﬁnding a single perturbation vector with a large image at the outputs of all 3 channels is difﬁcult. The tension between these 3 linear systems requires more degrees of freedom in choosing the perturbations, or in other words, the way that common information is modulated. Such more degrees of freedom can be provided either by using multi-letter solutions or have larger cardinality bounds. This effect is not captured by the conventional single-letterization approach.\nTheorem 1 reduces most of the difﬁculty of solving the multi-letter optimization problem (5). The remaining is to ﬁnd the optimal scaled perturbation L u for the single-letter version of (5), if the number of receivers k = 2, or the k-letter version, if k > 2. These are ﬁnite dimensional convex optimization problems [8], which can be readily solved.\nWe can see that all these information theory problems are solved with essentially the same procedure, and all we need in solving these problems is simple linear algebra. This again, demonstrates the simplicity and uniformity of our approach in dealing with information theory problems.\nIn this paper, we present the local approximation approach, and show that with this approach, we can handle the issue of single-letterization in information theory by just solving simple linear algebra problems. Moreover, we demonstrate that our approach can be applied to different communication problems with the same procedure, which is a very attractive property. Finally, we provide the geometric insight of the optimal ﬁnite-letter solutions in sending the common message to k > 2 receivers, which also explains why optimal single- letter solutions fail to be existed in these cases."},"refs":[{"authors":[{"name":"T. Cove"},{"name":"J. Thoma"}],"title":{"text":"Elementary of Information Theory, , Wiley Interscience, 1991"}},{"authors":[],"title":{"text":"Shun-ichi Amari and Hiroshi Nagaoka, Methods of Information Geome- try, , Oxford University Press, 2000"}},{"authors":[{"name":"S. Borade"},{"name":"L. Zheng"}],"title":{"text":"Euclidean Information Theory"}},{"authors":[{"name":"T. Cover"}],"title":{"text":"An Achievable Rate Region for the Broadcast Channel"}},{"authors":[{"name":"E. van der Meulen"}],"title":{"text":"Random Coding Theorems for the General Discrete Memoryless Broadcast Channel"}},{"authors":[{"name":"B. E. Hajek"},{"name":"M. B. Pursley"}],"title":{"text":"Evaluation of an Achievable Rate Region for the Broadcast Channel"}},{"authors":[{"name":"K. Marton"}],"title":{"text":"A Coding Theorem for the Discrete Memoryless Broadcast Channel"}},{"authors":[{"name":"S. Boy"},{"name":"L. Vandenbergh"}],"title":{"text":"Convex Optimization,, Cambridge Uni- versity Press, 2004 "}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565347.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S6.T7.1","endtime":"11:50","authors":"Shao-Lun Huang, Lizhong Zheng","date":"1341315000000","papertitle":"Linear Information Coupling Problems","starttime":"11:30","session":"S6.T7: Tools for Bounding Capacity","room":"Stratton (407)","paperid":"1569565347"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
