{"id":"1569566639","paper":{"title":{"text":"Relaxed Gaussian Belief Propagation"},"authors":[{"name":"Yousef El-Kurdi"},{"name":"Dennis Giannacopoulos"},{"name":"Warren J. Gross"}],"abstr":{"text":"Abstract\u2014The Gaussian Belief Propagation (GaBP) algorithm executed on Gaussian Markov Random Fields can take a large number of iterations to converge if the inverse covariance matrix of the underlying Gaussian distribution is ill-conditioned and weakly diagonally dominant. Such matrices can arise from many practical problem domains. In this study, we propose a relaxed GaBP algorithm that results in a signiﬁcant reduction in the number of GaBP iterations (of up to 12.7 times). We also propose a second relaxed GaBP algorithm that avoids the need of determining the relaxation factor a priori which can also achieve comparable reductions in iterations by only setting two basic heuristic measures. We show that the new algorithms can be implemented without any signiﬁcant increase, over the original GaBP, in both the computational complexity and the memory requirements. We also present detailed experimental results of the new algorithms and demonstrate their effectiveness in achieving signiﬁcant reductions in the iteration count."},"body":{"text":"Gaussian belief propagation is a message passing algorithm on Gaussian Markov Random Fields (GMRF) that efﬁciently computes the marginal at each node by sharing intermediate results. If the graph is a tree, then GaBP is guaranteed to converge to exact marginals. However, if the graph contains cycles, then GaBP takes an iterative form, also referred to as Loopy Belief Propagation (LBP), which was proposed by [1] as an approximation. GaBP can be shown to be a speciﬁc instance of a more generic message-passing algorithm referred to as the sum-product algorithm that operates on factor graphs [2]. The solution developed in this paper is motivated by the fact that GaBP algorithms exhibit distributed computations which potentially make them candidates for implementation on emerging parallel architectures. These ar- chitectures can achieve good speedups due to parallelism for problem domains requiring solutions for a large number of unknowns [3], such as the solution to Laplace\u2019s equation using the Finite Element Method (FEM) [3].\nGaBP was found to exhibit fast convergence for problems where the inverse covariance matrix of the underlying multi- variate Gaussian distribution, also referred to as the precision matrix, is strictly diagonally dominant [4], [5]. However, if the precision matrix is large, sparse and ill-conditioned, then GaBP may require a large number of iterations. Such graphs can arise from many practical problem domains. In this work, we present a relaxed form of GaBP that reduces the number of iterations, resulting in signiﬁcant computational reduction for ill-conditioned large linear systems. In addition, to circumvent the need of determining the relaxation factor for the relaxed\nGaBP a priori, we propose a second algorithm that incremen- tally determines a suitable relaxation factor based on iterative error improvements that also results in signiﬁcant reductions in GaBP iterations. We demonstrate the advantages of our algorithms using empirical results of large, ill-conditioned, and weakly diagonally dominant inverse covariance matrices.\nGaBP was recently introduced in [5] and [6] as an it- erative method to solve linear systems of equations which algebraically are formulated as Ax = b, where x is the vector of unknowns and A is a sparse and positive deﬁnite matrix representing the Gaussian graphical model. In this case, A is also referred to as the precision matrix or, alternatively, the inverse covariance matrix of the multi-variate Gaussian distribution. The solution to the linear system can be directly computed by x ∗ = A −1 b. However, when the linear system is large and sparse, iterative methods, e.g. the Preconditioned Conjugate Gradient (PCG), are traditionally used to to solve such systems, since they exhibit lower computational complex- ity and memory requirement than directly computing A −1 . We believe that GaBP provides an attractive alternative to PCG, since the locality and parallelism inherent in such message scheduling algorithms provide a potential for efﬁcient accel- eration on emerging parallel architectures such as multicore CPUs and GPUs [3].\nConsidering the matrix A symmetric and positive deﬁnite, where the nodal variables denoted by [x i ], the linear system Ax = b can be viewed as an undirected graph model, also referred to as Gaussian Markov Random Field (GMRF), where each non-zero coefﬁcient (A ij = 0) represents an undirected edge between random variable node x i and random variable node x j . By the Hammersley-Clifford theorem [7], the graph\u2019s distribution p(x) can be factored into the nodal functions φ i (x i ) exp(− 1 2 A ii x 2 i + b i x i ) and edge functions ψ i,j (x i , x j ) \t exp(− 1 2 x i A ij x j ). Therefore, the distribution p(x) is a multivariate Gaussian probability with inverse covari- ance matrix A and nodal variable means µ = A −1 b. Hence the solution to the linear system Ax = b can be found by employing belief propagation inference algorithm to compute the marginal means of the variables x i in the multivariate Gaussian distribution p(x) [5].\nIn Belief Propagation (BP), each node n i computes a message m ij towards node n j on a particular pairwise edge (i → j) using all messages received from nodes in the neighborhood N (i) of node n i excluding the message received\nfrom n j . The general BP message update rules for GMRF distributions are obtained as follows:\nMessage updates from each node can be performed either sequentially or concurrently subject to a speciﬁc schedule. Since the underlying distribution is Gaussian, and each of the nodal Φ i and edge Ψ ij potentials takes the form of a parameterized Gaussian as N (α, β) ∝ exp( −1 2 αx 2 + βx). By substituting the corresponding nodal and edge functions into (1) and (2), the belief update rules can be derived as propagating two edge parameters α ij and β ij [5] obtained by:\nα i\\j = α i − α ji \t (5) β i\\j = β i − β ji \t (6)\nFor large sparse systems, the overall GaBP computational complexity per iteration is O (cN ), where N is the number of unknowns and c is a constant (c N ) determined by the sparsity of the underlying problem, e.i. the average number of links per node. The marginal means can then be computed by:\nGaBP was shown in [8] to converge for a particular class of matrices referred to as walk-summable models. The walk- summability condition states that the spectral radius of the normalized off-diagonals of A in the absolute sense should be less than one, ρ(|I − D − 1 2 AD − 1 2 |) < 1, where D is the diagonal elements of A. Diagonally dominant matrices, later deﬁned in (13), are a subset of walk-summable models.\nIn practice, the relative error norm (e r ) can be used as a computationally efﬁcient measure to test the convergence of GaBP. This error can be computed each iteration, or each number of iterations, as an indicator for nodal solution (x i ) convergence. The relative error norm is computed as follows:\nThe relative error will be an important measure in our develop- ment of faster convergent relaxed GaBP algorithms. Another important measure for convergence is the normalized residual Frobenius norm, which is computed as follows:\nb F \t (11) where x est is the solution estimate of the iterative solver in iteration t. The residual R F provides an upper bound for the relative error; however in practice, the relative error is used in GaBP to test convergence since it is less costly to compute. The residual is only used as the convergence termination criteria in our experiments when we compare different algorithms.\nλ min (A) \t (12) where λ max (A) and λ min (A) are the largest and smallest eigenvalues of A. In numerical linear algebra, the condition number measures the sensitivity of the solution of the linear system to small perturbations in A. It may also be used to bound the convergence rate of iterative solvers of linear systems. Well-conditioned matrices have k (A) ≈ 1 while ill- conditioned matrices can have a much larger condition number.\nThe matrix A is weakly diagonally dominant if |a ii | ≥\nwith strict inequality (>) for at least one i. If the inequality is replaced by (>) for all i, then the matrix A is referred to as strictly diagonally dominant. The work in [4] provides a rough upper bound on the number of iterations required by GaBP to reach a given convergence tolerance ; however, it is only applicable for strictly diagonally dominant matrices. In this study we will consider ill-conditioned matrices that are not strictly diagonally dominant, which require considerably larger number of GaBP iterations. Though, ﬁnding a theoretical upper bound for the convergence rate of GaBP for such matrices is still an open research question.\nA common convergence acceleration method referred to as the Aitken-Steffensen\u2019s iteration was used in [5] to speedup GaBP convergence. An improved estimate of a message is computed at each third iteration as:\nm (t+2) − 2m (t+1) + m (t) \t (14) where m (t) represents a message estimate at iteration t. However, when this acceleration is used in GaBP with ill- conditioned matrices, it yields unstable results even with double-precision implementations.\nIn the following sections, we will present an iterative acceleration approach for GaBP using over-relaxation on the aggregates (β i ) of nodal messages that signiﬁcantly reduces the GaBP iterations. We will proceed with our discussion whereby all the information available from the underlying\nproblem is the matrix A. We will consider matrices that are large, sparse, ill-conditioned, and weakly diagonally dominant. With such matrices, the original GaBP algorithm requires a very large number of iterations to converge, whereby our relaxed GaBP algorithm is shown to achieve up to 12.7 times reduction in iterations. We also present a second re- laxed GaBP algorithm that can iteratively update the over- relaxation parameter to circumvent the need to determine the over-relaxation parameter a priori, which can also achieve comparable reductions in iterations.\nAs can be seen from (5), (6) and (9), the marginal mean of node i, which is also the solution estimate of x (t) i at iteration t, can be obtained using the two sums α (t) i and β (t) i\nof messages received on all connected edges N (i). It can also be noted from these equations that applying any relaxation on the β messages does not affect the α messages convergence properties. Relaxation on β ij can be applied as follows:\nwhere γ is referred to as the relaxation factor. The new relaxed message ˆ β (t) ij can be communicated instead of β (t) ij . It is clear that this relaxation does not require additional memory. However, it was observed that for GaBP based on pairwise graphical models, greater reductions in iterations can be obtained if we apply relaxation to the nodal sum β i messages as opposed to each individual edge message β ij . Instead, the β i messages can be relaxed as follows:\nRelaxing β i messages require additional memory of order O (N ) to store the previous iteration β (t−1) i \t messages; how- ever, for every matrix we tested from the class of weakly diagonally dominant matrices the α messages converged much faster (within 10 to 20 iterations as demonstrated by our empirical results). If we decide to use this property, we can eliminate the additional memory requirement due to relaxing β i messages as follows:\nwhere α ∗ i is the ﬁxed point reached after t o iterations. The x (t−1) i \t values are here reused since they are preserved each iteration in order to also test for relative error convergence as shown in (10). As will be shown later, the relative error will also be used in the second relaxation algorithm as an iterative improvement measure. The relaxation factor γ is obtained from the limit γ ∈ [0, 2]. if γ is in [0, 1], the method is referred to as under-relaxation or damping and is used to make divergent algorithms convergent. If γ is in [1, 2], the method\nis referred to as over-relaxation and is used to accelerate convergence which is the objective of this work.\nTo investigate the impact of β i relaxation on the original BP update rules of β ij messages, we substitute (16) into (6) and then into (4) to obtain the following:\nIt can be seen from the above modiﬁed BP rule for β ij messages that if γ is chosen such that the relaxed GaBP converges to a ﬁxed point, the additional term ∆β (t+t o ) ji\napproaches zero and the relaxed-GaBP ﬁxed point vector [x i ] equals the unique ﬁxed point solution of the original GaBP for Gaussian models. The detailed relaxed GaBP algorithm is shown in Fig. 1. We will refer to this algorithm as R-GaBP.\nα ij = 0 \t β ij = 0 γ ∈ [1, 2] \t x (0) i = 0\n9: \t ˆ β i = β i 10: \t end if\nThe condition for variance convergence in step-6 of Al- gorithm Fig. 1 is not inherently required and is due to implementation purposes. As explained earlier by waiting for\nin order to relaxe β (t) i which saves implementation memory of up to O (N ).\nBy using an over-relaxation factor γ ∈ [1, 2], our empirical results indicate that the optimal γ opt , which yields lowest iteration count for a given relative error tolerance , depends strongly on the elements of the matrix A, which makes γ opt dependent on the underlying problem. Hence, ﬁnding a suitable γ may prove difﬁcult especially since using the wrong value will cause the algorithm to fail to converge. In the following section, we propose a heuristic algorithm that iteratively and incrementally ﬁnds an approximation to γ that in general produces a sufﬁciently fast convergence by iteratively reducing the relative error.\nBased on the empirical observation that a threshold γ opt exists in the interval [1, 2], which is also found to be the only maximum in the same interval for given initial conditions. Any further increase on γ opt will cause a substantial increase in e (t) r . Intuitively then, we can gradually increase γ starting from an initial value by adding an increment ∆γ each d number of iterations as long as the e r is improving. However, if e r is found not to improve, we can likewise decrement γ. Fig. 2 shows the details of an algorithm that can be used to ﬁnd a rough estimate of the over-relaxation γ which results in a higher relative error decrease rate.\ne best = 1.0 \t γ (0) = 1.0 d = 10 \t ∆γ = 0.1\n9: \t if γ (t) < 1.0 then 10: \t γ (t) = 1.0 11: \t end if\nThe algorithm in Fig. 2 relies on two basic settings ∆γ and d. The parameter ∆γ is a ﬁxed increment or decrement size which nominally can be set to 0.1, while d is the iteration interval length on which e r can be tested in order to adjust γ. The error sampling interval d should be chosen wide enough so that the ﬂuctuations in e r , resulting from the prior γ change, can diminish and a nominal value for e r can be tested. In all of the cases we analyzed for ill-conditioned weak diagonally dominant matrices, a good value for d was found to be around\n10 to 20 iterations. We will refer to this algorithm as DR- GaBP.\nIf ∆γ is chosen sufﬁciently small with a sufﬁciently wide error sampling iteration interval d, the DR-GaBP algorithm should converge. Unlike the Aitken-Steffensen or similar ac- celeration methods, this algorithm is computationally more stable specially for ill-conditioned matrices. Another key ad- vantage of the DR-GaBP algorithm is that it does not require a signiﬁcant increase in both computation and memory over the original GaBP algorithm.\nOur test matrices are obtained from the classical L-shaped conductor problem in the ﬁeld of Electromagnetics. As shown in Fig. 3, the potential in the space between the two square conductors carrying different voltages is solved using the Laplace equation, 2 u = 0. However in practice, Laplace\u2019s equation is solved numerically by dividing the interconductor space with triangular elements. These discretization problems typically require the solution of linear systems of equations that is large, sparse, ill-conditioned, and weakly diagonally dominant. In this section, we demonstrate the effectiveness of our developed R-GaBP and DR-GaBP algorithms by solving Ax = b arising from the Laplace equation discretization. We also use a selected set of other generated matrices in order to illustrate our empirical results. We used asynchronous message scheduling for our algorithms. The relative error e r is recorded at each iteration. All algorithms were terminated when the residual reached R F < 10 −9 .\nThe plots in Fig. 4 demonstrate the iteration reduction due to our R-GaBP algorithm. The size of the matrix A is N = 2700 unknowns, with number of non-zeros NNZ = 17572. The original GaBP algorithm required 2449 iterations while R-GaBP algorithm required as low as 389 iterations for γ = 1.538 resulting in a reduction factor of 6.2. It can be observed that the overall relative error decreases consistently as γ increase to a certain value in the interval [1, 2]. It is expected that for this problem the best γ opt can empirically be obtained as γ opt ≈ 1.538, any further increase on γ will cause e r to increase, and consequently causing the algorithm to fail to converge.\nThe plots in Fig. 5 shows the results of our DR-GaBP algorithm in obtaining a considerably lower GaBP iteration count without prior knowledge of γ opt . The parameter ∆γ is varied from a ﬁne value of 10 −2 to a coarser value of 2×10 −1 . An error sampling interval of d = 10 was used for all plots. It is worth noting here that the best iteration reduction was found for ∆γ = 2 × 10 −1 resulting in 337 iterations. This is lower than the previously reported 389 iterations by R- GaBP assuming prior knowledge of γ opt . This reduction may be attributed to the dynamics of the algorithm in alternating between two values of γ which are (1.3 and 1.6).\nWe illustrate in Table I a selected set of test matrices. One matrix was obtained from the Matrix Market website repos- itory [9]. The other two matrices were generated randomly using Matlab and set to be sparse and weakly diagonally dom- inant. The generated matrices all have negative off-diagonals which results in ill-conditioned matrices. For all algorithms we used d = 10. The variances for all matrices converged in less than 10 iterations. The density % of the matrix is measured by NNZ/N 2 × 100.\nTable I demonstrates iteration reductions produced by the R-GaBP algorithm in all cases. The second matrix shows reduction factor of 12.7 for γ = 1.8. Table II shows the results of DR-GaBP which also produced signiﬁcant iteration\nreductions versus the original GaBP. The performance of DR- GaBP algorithm can depend on the choice of ∆γ, however the algorithm converged in all cases tested. It was observed that a nominal value of ∆γ = 0.1 resulted in best reductions on average.\nWe have presented a new relaxed GaBP algorithm (R-GaBP) to accelerate GaBP convergence for ill-conditioned weakly diagonally dominant inverse covariance matrices. We have demonstrated empirical reductions of up to 12.7 times. We have also introduced and demonstrated the effectiveness of a variant of the relaxed GaBP algorithm (DR-GaBP) that avoids the complexity of setting a prior over-relaxation parameter by iteratively varying the relaxation factor in a way to achieve a faster convergence rate. In addition, the new algorithms do not require any increase of the computational complexity or the memory requirements of the original GaBP algorithm hence facilitating efﬁcient implementations on emerging parallel architectures."},"refs":[{"authors":[{"name":"J. Pear"}],"title":{"text":"Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference "}},{"authors":[{"name":"F. R. Kschischang"},{"name":"B. J. Frey"},{"name":"H. A. Loeliger"}],"title":{"text":"Factor graphs and the sum-product algorithm"}},{"authors":[{"name":"Y. El-Kurdi"},{"name":"W. J. Gross"},{"name":"D. Giannacopoulos"}],"title":{"text":"Efﬁcient implemen- tation of Gaussian belief propagation solver for large sparse diagonally dominant linear systems"}},{"authors":[{"name":"Y. Weiss"},{"name":"W. T. Freeman"}],"title":{"text":"Correctness of belief propagation in Gaussian graphical models of arbitrary topology"}},{"authors":[{"name":"O. Shental"},{"name":"P. Siegel"},{"name":"J. Wolf"},{"name":"D. Bickson"},{"name":"D. Dolev"}],"title":{"text":"Gaussian belief propagation solver for systems of linear equations"}},{"authors":[{"name":"E. Sudderth"},{"name":"M. Wainwright"},{"name":"A. Willsky"}],"title":{"text":"Embedded trees: Estima- tion of Gaussian processes on graphs with cycles"}},{"authors":[{"name":"J. Hammersley"},{"name":"P. Clifford"}],"title":{"text":"Markov Fields on Finite Graphs and Lattices"}},{"authors":[{"name":"J. K. Johnson"},{"name":"D. M. Malioutov"},{"name":"A. S. Willsky"}],"title":{"text":"Walk-sum interpreta- tion and analysis of Gaussian belief propagation"}},{"authors":[],"title":{"text":"Matrix Market: Harwell Boeing Collection"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566639.pdf"},"links":[{"id":"1569565867","weight":4},{"id":"1569564669","weight":4},{"id":"1569566697","weight":4},{"id":"1569566761","weight":4},{"id":"1569566943","weight":4},{"id":"1569552245","weight":4},{"id":"1569566415","weight":4},{"id":"1569565355","weight":4},{"id":"1569566999","weight":4},{"id":"1569566895","weight":4},{"id":"1569565785","weight":4},{"id":"1569566679","weight":4},{"id":"1569565735","weight":4},{"id":"1569559111","weight":4},{"id":"1569565559","weight":4},{"id":"1569564857","weight":4},{"id":"1569566913","weight":4},{"id":"1569566809","weight":8},{"id":"1569565633","weight":4},{"id":"1569565219","weight":4},{"id":"1569566505","weight":4},{"id":"1569566853","weight":8},{"id":"1569567235","weight":12},{"id":"1569566901","weight":4},{"id":"1569559199","weight":8},{"id":"1569565665","weight":24},{"id":"1569566873","weight":4},{"id":"1569565765","weight":8},{"id":"1569565093","weight":4},{"id":"1569566917","weight":8},{"id":"1569566595","weight":8},{"id":"1569566137","weight":4},{"id":"1569566819","weight":12},{"id":"1569564437","weight":8},{"id":"1569564861","weight":4},{"id":"1569564787","weight":8},{"id":"1569564923","weight":4},{"id":"1569566171","weight":28},{"id":"1569566601","weight":4},{"id":"1569565805","weight":4},{"id":"1569557851","weight":4},{"id":"1569567691","weight":4},{"id":"1569561397","weight":4},{"id":"1569566797","weight":4},{"id":"1569565031","weight":4},{"id":"1569558697","weight":8},{"id":"1569565139","weight":4},{"id":"1569564807","weight":8},{"id":"1569566727","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T7.3","endtime":"10:50","authors":"Yousef El-Kurdi, Dennis Giannacopoulos, Warren Gross","date":"1341484200000","papertitle":"Relaxed Gaussian Belief Propagation","starttime":"10:30","session":"S11.T7: Message Passing Algorithms","room":"Stratton (407)","paperid":"1569566639"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
