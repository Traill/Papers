{"id":"1569564919","paper":{"title":{"text":"On the Uncertainty of Information Retrieval in Associative Memories"},"authors":[{"name":"Eitan Yaakobi ∗\u2020"},{"name":"Jehoshua Bruck ∗"}],"abstr":{"text":"Abstract\u2014We (people) are memory machines. Our decision processes, emotions and interactions with the world around us are based on and driven by associations to our memories. This natural association paradigm will become critical in future mem- ory systems, namely, the key question will not be \u201cHow do I store more information?\u201d but rather, \u201cDo I have the relevant informa- tion? How do I retrieve it?\u201d\nThe focus of this paper is to make a ﬁrst step in this direc- tion. We deﬁne and solve a very basic problem in associative retrieval. Given a word W, the words in the memory that are t-associated with W are the words in the ball of radius t around W. In general, given a set of words, say W, X and Y, the words that are t-associated with { W, X, Y } are those in the memory that are within distance t from all the three words. Our main goal is to study the maximum size of the t-associated set as a function of the number of input words and the minimum dis- tance of the words in memory - we call this value the uncertainty of an associative memory. We derive the uncertainty of the as- sociative memory that consists of all the binary vectors with an arbitrary number of input words. In addition, we study the re- trieval problem, namely, how do we get the t-associated set given the inputs? We note that this paradigm is a generalization of the sequences reconstruction problem that was proposed by Leven- shtein (2001). In this model, a word is transmitted over multiple channels. A decoder receives all the channel outputs and decodes the transmitted word. Levenshtein computed the minimum num- ber of channels that guarantee a successful decoder - this value happens to be the uncertainty of an associative memory with two input words."},"body":{"text":"One of the interpretations of the term association, espe- cially in the context of psychology, is the connection between two or more concepts. Throughout our life, we remember and store an enormous amount of information. However, while we are not aware of the method this information is stored, amaz- ingly, it can be accessed and retrieved with relative ease. The way we think and process information is mainly performed by associations. Our memory content retrieval process is done by stimulating it with an external or internal inputs. That is, the knowledge of some information gives rise to related informa- tion that was stored earlier. Mathematically speaking, assume the memory is a set of words M = { m 1 , . . . , m N } . Then, given an arbitrary word x as an input to the memory M, its output is another word or a set of words from the memory M that are related or close to the input word x. Here, the term \u201cclose\u201d can be interpreted as using any distance metric between words, for example the Hamming distance. A word is associated with another word or words which they again can be associated with more words and so on, resulting in a sequence of associations.\nFrom the information theory perspective, we say that the words associated with an input word x are those in distance at most t (a prescribed value) from x. This set comprises a\nball of radius t, see Fig. 1(a). We generalize this paradigm and consider a set of words that are presented as an input to the memory. For example, for two input words x, y, their set of associated words are the ones that their distance from both x and y is at most t, see Fig. 1(b). Clearly, this set of associa- tions is strictly smaller then the ball of radius t. For more than two input words, the set of associated words is getting even smaller, and in general, the larger the set of input words is, the smaller the set of associated words is, see Fig. 1(c). For exam- ple, assume the input word is \u201ctall\u201d, then many words can be associated with it, such as \u201ctree\u201d, \u201cmountain\u201d, \u201ctower\u201d, \u201clad- der\u201d, etc. But if the input words are \u201ctall\u201d and \u201cfruit\u201d, then out of these four associated words, only the word \u201ctree\u201d will be associated with \u201ctall\u201d and \u201cfruit\u201d.\nAssume that the memory is the set of all binary vectors, M = { 0, 1 } n . For any input word x, it is immediate to see that if its set of associated words are the ones of distance at most t, then there are ∑ t i=0 ( n i ) such words. However, for two input words, x, y, the problem of ﬁnding the set of associ- ated words of distance at most t from both x and y becomes more complex. In fact, this problem was proposed and solved by Levenshtein in [10], [11]. The motivation came from a completely different scenario in the context of the sequences reconstruction problem . In this model, a codeword x is trans- mitted through multiple channels. Then, a decoder receives all channel outputs and generates an estimation on the transmit- ted word, while it is guaranteed that all channel outputs are different from each other, see Fig. 2. If x belongs to a code C with minimum distance d and in every channel there can be at most t > d−1 2 errors, then Levenshtein studied the mini- mum number of channels N that guarantees the existence of\na successful decoder. This number has to be greater than N = max\nwhere B t ( x ) is the ball of radius t surrounding x. To see that, notice that if the intersection of the radius- t balls of x 1 and x 2 contains N words and the channel outputs are these N words, then a decoder cannot determine what the transmitted word is. However, if the number of channel outputs is greater than the maximum size of the intersection of two balls, then there is only one codeword of distance at most t from all received channel outputs.\nThe motivation to the model studied by Levenshtein came from ﬁelds such as chemistry and biology, where the redun- dancy in the codewords is not sufﬁcient to construct a success- ful decoder. Thus, the only way to combat errors is by repeat- edly transmitting the same codeword. Recently, this model was shown to be also relevant in storage technologies [3], [16]. Due to the high capacity and density of today\u2019s and future\u2019s storage medium, it is no longer possible to read individual memory elements, but, rather, only a multiple of them at once. Hence, every memory element is read multiple times, which is trans- lated into multiple estimations of the same information stored in the memory.\nFinding the maximum intersection problem in (1) was stud- ied in [11] with respect to the Hamming distance and other metric distances. In [7]\u2013[9], it was analyzed over permuta- tions, and in [13], [14] for error graphs. In [12], the equivalent problem for insertions and deletions was studied and recon- struction algorithms for this model were given in [2], [5], [15]. The case of deletions only was studied in the context of trace reconstruction in [4].\nReturning to our original problem, the set of associated words with x and y is B t ( x ) ∩ B t ( y ) and the maximum in- tersection is the value N in (1). The generalized problem of ﬁnding the maximum size of associated words of m 2 input words with mutual distance d is expressed as\nThe main goal in this paper it to analyze the value of N t ( m, d ) with respect to the Hamming distance for differ- ent values of t, m, d. In particular, we show that if A ( D ) is the size of a maximal anitcode of diameter D, that is, the largest set of words with maximum distance D, then N t ( A ( D ) , 1 ) = A ( 2t − D ) .\nThe rest of the paper is organized as follows. In Section II, we deﬁne the concept of associative memories and describe the connection to the sequences reconstruction problem. In\nSection III, we solve the problem stated in (2) for the case d = 1. Extensions for arbitrary d are given in Section IV. In Section V, we give efﬁcient decoders to the reconstruction problem studied by Levenshtein. Finally, Section VI concludes the paper.\nIn this work, the words are binary vectors of length n. The Hamming distance between two words x and y is denoted by d H ( x, y ) and the Hamming weight of a word x is denoted by w H ( x ) . For a word x ∈ { 0, 1 } n , B n t ( x ) is its surrounding ball of radius t, B n t ( x ) = { y ∈ { 0, 1 } n : d H ( x, y ) t } . The size of B n t ( x ) , comprising of length- n words, is b t,n = ∑ t i=0 ( n i ) . If the length of the words is clear from the context, we use the notation B t ( x ) . For 1 i n, e i is the unit vector where only its i-th bit is one, and 0 is the all-zero vector. Two words x, y ∈ { 0, 1 } n are called t-associated if d H ( x, y ) t. Deﬁnition. The t-associated set of the words x 1 , . . . , x m is de- noted by S t ({ x 1 , . . . , x m }) and is deﬁned to be the set of all words y that are t-associated with x 1 , . . . , x m ,\nNote that for a single word x, we have S t { x } = B t ( x ) . Given an associative memory M , we deﬁne the maximum size of a t-associated set of any m words from the memory.\nDeﬁnition. Let M be an associative memory and m, t be two positive integers. The uncertainty of the associative memory M for m and t, denoted by N t ( m, M) , is the maximum size of a t-associated set of m different input words from M . That is,\nIn case the associative memory M is a code with mini- mum distance d, we will use the notation N t ( m, d ) instead of N t ( m, M) . Then, the value in Equation (3) becomes\nWe now give the deﬁnitions that establish the connec- tion with the channel model by Levenshtein [11]. Assume a codeword x is transmitted over N channels. The channel outputs, denoted by y 1 , . . . , y N , are all different from each other (Fig. 2). A list decoder D L receives the N channel out- puts and returns a list of at most L words x 1 , . . . , x , where\nL . We call it an L -decoder D L . The L -decoder D L is said to be successful if and only if the transmitted word x belongs to the decoded output list, i.e.,\nThe next Lemma shows the connection between the value of N t ( m, d ) and the decoding success of an L -decoder.\nLemma 1. Assume the transmitted word x belongs to a code C of minimum distance d. Then, there exists a successful L - decoder with N channels if and only if\nProof: Assume to the contrary that the number of chan- nels is N t (L + 1, d ) and let x 1 , . . . , x L+1 be L + 1 words\nsuch that the set S t ({ x 1 , . . . , x L+1 }) contains N t (L + 1, d ) words. If one of these L + 1 words is the transmitted one and the received channel outputs are the N t (L + 1, d ) words in S t ({ x 1 , . . . , x L+1 }) , then any of the L + 1 words x 1 , . . . , x L+1 could be the transmitted one and thus can be- long to the decoder\u2019s output list. Hence, the transmitted word may not belong to the output list.\nOn the other hand, if there are N t (L + 1, d ) + 1 channels, then for any transmitted word x, there are at most L words in C , all of distance at least d from each other, such that the N t (L + 1, d ) + 1 channel outputs are located in the intersec- tion of their radius- t balls.\nFor L = 1, the value N t ( 2, d ) was studied by Leven- shtein [11] and was shown to be\nLet us ﬁrst remind how this value was calculated. Assume d H ( x, y ) = d and the goal is to ﬁnd the cardinality of the set\nFor any word z ∈ S t ({ x, y }) , let S 0,0 , S 0,1 , S 1,0 , S 1,1 be the following four sets:\nS 0,0 = { i : y i = z i = x i } , \t S 0,1 = { i : y i = x i , z i = x i } , S 1,0 = { i : y i = x i , z i = x i } , S 1,1 = { i : y i = z i = x i } .\nNote that | S 0,0 | + | S 0,1 | = n − d and | S 1,0 | + | S 1,1 | = d. Since d H ( z, x ) t and d H ( z, y ) t we get that\nDenote | S 0,1 | = i and | S 1,1 | = k so we get i + k t, i + d − k t,\nk . where ( a b ) = 0 if b < 0 or b > a.\nIf we substitute the order of i, k in the last term, we get 0 k min { d, t } , 0 i t − max { k, d − k } , and\nd k\ni . (6) This last representation of N t ( 2, d ) will be helpful in showing the following property. Due to the lack of space, we omit the proof of the next lemma as well as the following one, which extends the solution of N t ( 2, d ) for m = 3.\nLemma 2. Let t, d be two positive integers such that d is even, then \t N\nLemma 3. Let t, d be such that t > d−1 2 , and n large enough. The value of N t ( 3, d ) is given by\nwhere i 1 , i 2 , i 3 , i 4 satisfy the following constraints: 1) 0 i 1 t − d 2 ,\nIn this section, we analyze the value of N t ( m, d ) for d = 1. A ﬁrst observation on the value of N t ( m, 1 ) is stated in the next lemma.\nLemma 4. For m, t 1, if N t ( m, 1 ) \t and N t ( m + 1, 1 ) < , then N t ( , 1 ) = m.\nProof: Since N t ( m, 1 ) \t , there exist m different words x 1 , . . . , x m such that\nand assume y 1 , . . . , y are words which belong to this in- tersection. Therefore, d H ( x i , y j ) t for all 1 i m and 1 j , and thus\n{ x 1 , . . . , x m } ⊆ S t ({ y 1 , . . . , y }) = B t ( y 1 ) ∩ · · · ∩ B t ( y ) , and hence N t ( , 1 ) m.\nAssume to the contrary that N t ( , 1 ) \t m + 1 and let z 1 , . . . , z be words such that\nAs in the ﬁrst part, we get that N t ( m + 1, 1 ) \t , which is a contradiction. Hence, N t ( , 1 ) = m.\nIn general, for a given set of words x 1 , . . . , x m , the closer the words are, the larger the size of the set S t ({ x 1 , . . . , x m }) is. In case d = 1, we look for a set of words that are all close to each other, or equivalently - the maximum distance between all pairs of words is minimized.\nAn anticode of diameter D is a set A ⊆ { 0, 1 } n of words such that the maximum distance between every two words in A is at most D. That is, for all x, y ∈ A, d H ( x, y ) D. For D 1, A ( D ) is the size of the largest anticode of diameter D. It was shown in [6] that the value of A ( D ) is given by\nOur next goal is to show that for all D 1, N t ( A ( D ) , 1 ) = A ( 2t − D ) .\nThat is, the t-associated set of a maximum anticode of diam- eter D is a maximum anticode of diameter 2t − D. Lemma 5. For all 0 D 2t n,\nProof: Assume that D is even. We take the A ( D ) words in B D\n( 0 ) and consider the set S t B D\n( 0 ) and hence N t ( A ( D ) , 1 ) A ( 2t − D ) for even D.\nIn case that D is odd, let i = ( D − 1 )/ 2. Let us start with a maximal anticode of diameter 2i + 1. Let X be the set\nX = B i ( 0 ) ∪ B i ( e 1 )={ aw : a ∈ { 0, 1 } , w ∈ B n−1 i ( 0 )} , and let\nThe equivalent upper bound is proved in the next two lem- mas.\nLemma 6. For all 0 D 2t and n ( t − D 2 )( 2 D+1 + 1 ) , where D is even,\nProof: Let X = { x 1 , . . . , x A(D)+1 } be a set of A ( D ) + 1 words. Since the largest anticode with diameter D has size A ( D ) , there exist two words, say x 1 , x 2 , where d H ( x 1 , x 2 )\nD + 1. Hence, the size of S t ( X ) is no greater than the size of S t ({ x 1 , x 2 }) , which, according to (5), is at most\nk . Note that\nAn equivalent property can be shown for D odd. We skip its details due to its long proof and the lack of space.\nLemma 7. For all 0 D 2t, where D is odd, and n large enough,\nWe summarize this result in the following corollary. Corollary 8. For all 0 D 2t and n large enough,\nWe note that the result shown by Levenshtein for d = 1 is a special case of Corollary 8 for D = 1.\nOur goal in this section is to use the results found in Sec- tion III in order to derive bounds on N t ( m, d ) for arbitrary d. First, we state a useful Theorem from [1].\nTheorem 9. [1] Let C be a code in the Hamming graph Γ with distances from D = { d 1 , . . . , d s } ⊆ { 1, . . . , n } . Further let L D ( B ) be a maximal code in B ⊆ Γ with distances from D . Then, one has \t |C|\nFor all d 1, we denote by ρ d to be the maximal rate of a code C d of minimum distance d and length n, that is,\nLemma 10. Let B be a set and let L ( B ) be a maximal code in B with minimum distance d, then\nProof: We take the code C in Theorem 9 to be a code C d of minimum distance d and maximal rate ρ d . Then, we get\nNow, we can derive a connection with N t ( m, d ) . Lemma 11. For all m, t, d and n large enough,\nProof: Assume that X is a set of m words such that S t ( X ) has size A. According to Lemma 10, let L ( X ) be a code in X of minimum distance d and size ρ d m . Then, S t ( X ) ⊆ S t ( L ( X )) and thus N t ( ρ d m , d ) N t ( m, 1 ) .\nProof: Since N t ( m, d ) \t N t ( 2, d ) and N t ( 2, d ) = Θ ( n t− d 2 ) , then N t ( m, d ) is at most O ( n t− d 2 ) .\nTo show the other direction of this equality, we show an example of a set X such that the cardinality of S t ( X ) is O ( n t− d 2 ) . Let e j i be the vector which its -th bit is one if and only if ∈ { i, . . . , j } . For 1 \t i \t m, let i 0 = ( i − 1 ) d 2 + 1 and i 1 = i d 2 , and x i = e i 1 i 0 . Then, for all i = j, d H ( x i , x j ) = 2 d 2 \t 2d. For any vector y of weight at most t − d 2 , such that its ﬁrst m d 2 bits are zero, y ∈ S t ( X ) . Since there are ∑ t− d 2 =0 ( n−m d 2 ) such vectors we get that for n large enough, N t ( m, d ) is at least O ( n t− d 2 ) . Together we conclude that N t ( m, d ) = Θ ( n t− d 2 ) .\nThe main goal in [11] was to ﬁnd the necessary and sufﬁ- cient number of channels in order to have a successful decoder for a code with minimum distance d. This number was studied for different error models in [7]\u2013[14], however the only de- coder constructions, which we are aware of, were given in [2], [5], [15] for channels with insertion and deletions, and in [4] for deletions only. In this section, we show how to construct decoders for substitution errors, where the decoder has to out- put the transmitted word (and not a list of words).\nThe case d = 1 was solved in [11] where the majority algo- rithm on each bit successfully decodes the transmitted word. According to Lemma 2, this algorithm works for d = 2 as well since the number of channels has to be the same. How- ever, if d is greater than two, then the majority algorithm on each bit no longer works. In general, according to Lemma 2, if d is even then the number of channels for a code with min- imum distance d or d − 1 is the same. Hence, we only need to solve here the case of odd minimum distance.\nFor the rest of this section, we assume that the transmit- ted word belongs to a code C with odd minimum distance d, there are at most t > d−1 2 errors in every channel, and the number of channels is N = N t ( 2, d ) + 1. The N channel out- puts are denoted by y 1 , . . . , y N . Furthermore, the code C has a decoder D C , which can successfully correct d−1 2 errors.\nA ﬁrst observation in constructing a decoder is that we can always detect whether the output word is the transmitted one. This can simply be done by checking if the maximum distance from all channel outputs is at most t.\nLemma 13. For any c ∈ C , c = c if and only if max\nProof: If c = c then every channel suffers at most t errors and thus max 1 i N { d H ( c, y i )} t. In case c = c, let us assume to the contrary that max 1 i N { d H ( c, y i )} t. Then the set S t ({ c, c }) contains at least N = N t ( 2, d ) + 1 words in contradiction to the deﬁnition of N t ( 2, d ) .\nA naive algorithm can choose any of the channel outputs and add all error vectors of weight at most t − d−1 2 . For one of these error vectors we will get a word with at most d−1 2\nerrors which can be decoded by the decoder of the code C . We will show how to modify and improve the complexity of this algorithm. Assume for example that t = d−1 2 + 1. Then, there are two channel outputs, say y 1 and y 2 , that are different in at least one bit location. If we ﬂip this bit in both y 1 and y 2 , then in exactly one of them the number of errors reduces by one and thus is at most d−1 2 , which can be decoded by D C . We show how to generalize this idea for arbitrary t. We let ρ = t − d−1 2 . First, we prove the following Lemma.\nLemma 14. There exist two channel outputs y i , y j such that d H ( y i , y j ) 2ρ − 1.\nProof: Assume to the contrary that there do no exist such words. Then, the words y 1 , y 2 , . . . , y N form an anticode of diameter 2ρ − 2. According to [6], the maximum size of such an anticode is b ρ −1,n = ∑ ρ −1 i=0 ( n i ) , while according to (5) the value of N satisﬁes\nAlgorithm 15. The input to the decoder are the N words y 1 , . . . , y N and it returns an estimation c on c.\nStep 1. Find two words y i , y j such d H ( y i , y j ) 2ρ − 1, and let i 1 , i 2 , . . . , i 2ρ−1 be 2ρ − 1 different indices that the two vectors are different from each other.\nProof: The success of Step 1 is guaranteed according to Lemma 14. For every index i j , 1 j 2ρ − 1, exactly one of the channel outputs y 1 or y 2 has an error. Therefore, either y 1 or y 2 has at least ρ errors on these indices. Without loss of generality assume it is y 1 and let I ⊆ { i 1 , . . . , i 2ρ−1 } be a subset of its error locations, where | I | = ρ. In Step 2 we exhaustively search over all error vectors e of weight ρ on these 2ρ − 1 indices. For every error vector e let I e = { i : e i = 1 } . Therefore, there exists an error vector e 1 such that its the set of indices with value one is covered by the set I,\ni.e. I e 1 ⊆ I. Hence, d H ( c, y 1 + e 1 ) d−1 2 , so the decoder in Step 2.b succeeds. Hence the algorithm succeeds and c = c.\nThe complexity of Algorithm 15 is signiﬁcantly better than the naive approach. However, the larger the value of ρ is, the larger the algorithm\u2019s complexity is. We report on another al- gorithm with better complexity, O ( nN ) , for the case d = 3.\nThis paper proposed a model of an associative memory: Two words are associated if their Hamming distance is no greater than some prescribed value t. Our main goal was to study the maximum size of the associative memory output as a function of the number of input words and their minimum distance. We observed that this problem is a generalization of the sequences reconstruction problem that was proposed by Levenshtein. Finally, we presented a decoding algorithm for the sequences reconstruction problem.\nThe authors thank Tuvi Etzion for helpful discussions on anticodes. This research was supported in part by the ISEF Foundation, the Lester Deutsch Fellowship, and the NSF Ex- peditions in Computing Program under grant CCF-0832824."},"refs":[{"authors":[{"name":"R. Ahlswede"},{"name":"K. Aydinian"},{"name":"H. Khachatrian"}],"title":{"text":"On perfect codes and related concepts"}},{"authors":[{"name":"T. Batu"},{"name":"S. Kannan"},{"name":"S. Khanna"},{"name":"A. McGregor"}],"title":{"text":"Reconstructing strings from random traces"}},{"authors":[{"name":"Y. Cassuto"},{"name":"M. Blaum"}],"title":{"text":"Codes for symbol-pair read channels"}},{"authors":[{"name":"T. Holenstein"},{"name":"M. Mitzenmacher"},{"name":"R. Panigrahy"},{"name":"U. Wieder"}],"title":{"text":"Trace re- construction with constant deletion probability and related results"}},{"authors":[{"name":"S. Kannan"},{"name":"A. McGregor"}],"title":{"text":"More on reconstructing strings from ran- dom traces: insertions and deletions"}},{"authors":[{"name":"J. Kleitman"}],"title":{"text":"On a combinatorial conjecture of Erd˝os"}},{"authors":[{"name":"E. Konstantinova"}],"title":{"text":"On reconstruction of signed permutations distorted by reversal errors"}},{"authors":[{"name":"E. Konstantinova"}],"title":{"text":"Reconstruction of permutations distorted by single reversal errors"}},{"authors":[{"name":"E. Konstantinova"},{"name":"I. Levenshtein"},{"name":"J. Siemons"}],"title":{"text":"Reconstruc- tion of permutations distorted by single transposition errors"}},{"authors":[{"name":"I. Levenshtein"}],"title":{"text":"Reconstructing objects from a minimal number of dis- torted patterns"}},{"authors":[{"name":"I. Levenshtein"}],"title":{"text":"Efﬁcient reconstruction of sequences"}},{"authors":[{"name":"I. Levenshtein"}],"title":{"text":"Efﬁcient reconstruction of sequences from their subse- quences or supersequences"}},{"authors":[{"name":"I. Levenshtein"},{"name":"E. Konstantinova"},{"name":"E. Konstantinov"},{"name":"S. Molodtsov"}],"title":{"text":"Reconstruction of a graph from 2-vicinities of its vertices"}},{"authors":[{"name":"I. Levenshtein"},{"name":"J. Siemons"}],"title":{"text":"Error graphs and the reconstruction of elements in groups"}},{"authors":[{"name":"K. Viswanathan"},{"name":"R. Swaminathan"}],"title":{"text":"Improved string reconstruction over insertion-deletion channels"}},{"authors":[{"name":"E. Yaakobi"},{"name":"J. Bruck"},{"name":"H. Siegel"}],"title":{"text":"Decoding of cyclic codes over symbol-pair read channels"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564919.pdf"},"links":[{"id":"1569565867","weight":3},{"id":"1569566875","weight":7},{"id":"1569566597","weight":3},{"id":"1569565091","weight":3},{"id":"1569566571","weight":3},{"id":"1569564481","weight":3},{"id":"1569566415","weight":3},{"id":"1569565547","weight":3},{"id":"1569558325","weight":3},{"id":"1569565837","weight":7},{"id":"1569565317","weight":11},{"id":"1569564249","weight":11},{"id":"1569565809","weight":3},{"id":"1569561679","weight":3},{"id":"1569566787","weight":3},{"id":"1569566015","weight":7},{"id":"1569566895","weight":3},{"id":"1569564647","weight":3},{"id":"1569566679","weight":3},{"id":"1569566905","weight":3},{"id":"1569555999","weight":3},{"id":"1569565841","weight":3},{"id":"1569561143","weight":3},{"id":"1569561795","weight":3},{"id":"1569566423","weight":11},{"id":"1569558901","weight":11},{"id":"1569553519","weight":3},{"id":"1569566885","weight":3},{"id":"1569564209","weight":3},{"id":"1569566425","weight":3},{"id":"1569554881","weight":3},{"id":"1569566445","weight":3},{"id":"1569564857","weight":7},{"id":"1569566629","weight":3},{"id":"1569565847","weight":11},{"id":"1569565279","weight":3},{"id":"1569566003","weight":3},{"id":"1569566037","weight":3},{"id":"1569565393","weight":3},{"id":"1569566695","weight":11},{"id":"1569561123","weight":34},{"id":"1569566673","weight":3},{"id":"1569563395","weight":3},{"id":"1569566831","weight":3},{"id":"1569566779","weight":3},{"id":"1569565765","weight":3},{"id":"1569565925","weight":7},{"id":"1569557275","weight":3},{"id":"1569565263","weight":7},{"id":"1569566261","weight":3},{"id":"1569565093","weight":3},{"id":"1569565385","weight":3},{"id":"1569566927","weight":3},{"id":"1569565319","weight":3},{"id":"1569566267","weight":3},{"id":"1569564595","weight":11},{"id":"1569565353","weight":3},{"id":"1569564291","weight":3},{"id":"1569566823","weight":3},{"id":"1569564247","weight":3},{"id":"1569564437","weight":3},{"id":"1569563975","weight":3},{"id":"1569564861","weight":7},{"id":"1569565457","weight":3},{"id":"1569564787","weight":3},{"id":"1569566075","weight":3},{"id":"1569566397","weight":3},{"id":"1569567483","weight":3},{"id":"1569564923","weight":3},{"id":"1569561713","weight":3},{"id":"1569566933","weight":3},{"id":"1569567013","weight":3},{"id":"1569565853","weight":3},{"id":"1569563725","weight":3},{"id":"1569566413","weight":3},{"id":"1569566555","weight":3},{"id":"1569565373","weight":3},{"id":"1569565139","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S1.T6.2","endtime":"10:30","authors":"Eitan Yaakobi, Jehoshua Bruck","date":"1341223800000","papertitle":"On the Uncertainty of Information Retrieval in Associative Memories","starttime":"10:10","session":"S1.T6: Combinatorial Problems in Coding","room":"Kresge Rehearsal A (033)","paperid":"1569564919"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
