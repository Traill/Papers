{"id":"1569564195","paper":{"title":{"text":"Best Rate 1/2 Convolutional Codes for Turbo Equalization with Severe ISI"},"authors":[{"name":"John B. Anderson"},{"name":"Mehdi Zeinali"}],"abstr":{"text":"Abstract\u2014We give a procedure to ﬁnd good convolutional codes for use with iterative decoding (turbo equalization) of the coded AWGN intersymbol interference (ISI) channel. The method is based on the input\u2013output bit error rate characteristic of the convolutional BCJR module in the iterative decoder, in combination with the code minimum distance. Both are essential to ﬁnd best codes. Best feedforward and recursive systematic codes are listed for memories 2\u20134, and some individual good codes at memory 5. Codes are tested over a root raised-cosine faster than Nyquist channel with severe ISI. Turbo BER performance reaches 0.8\u20131.6 dB from capacity with a 4\u201316 state codes. Feedforward codes perform better than recursive systematic."},"body":{"text":"Turbo equalization is the name given to iterative decoding of error-correcting codewords that have passed through a linear intersymbol interference (ISI) channel. Most often in research studies, the code is a simple rate 1/2 convolutional code. Figure 1 shows a transmitter\u2013receiver model based on a discrete- time AWGN channel. Binary convolutional codeword symbols pass through an interleaver Π and are converted to a ±\nstream u = u 0 , u 1 , . . . which convolves with a length-m unit- energy ISI response v = v 0 , v 1 , . . . , v m . To this discrete-time sequence the channel adds independent Gaussian variates with mean zero and variance N 0 /2. ISI channels are common in communication, and interleaving is a standard technique to break up long ISI error events before they reach the error- correcting decoder. What turbo equalization adds is iterative detection in which the decoding/demodulation is performed by two cooperating BCJR algorithms, one for the ISI (the ISI- BCJR) and one for the code (the CC-BCJR), that feed soft log likelihood ratio (LLR) information to each other. After a time the detection converges to a good estimate of the transmitted data, if the signal-to-noise ratio (SNR) E s /N 0 is above a threshold. Ref. [2] is an excellent survey of recent research in turbo equalization.\nThis paper studies how to ﬁnd good convolutional codes. The iterative decoder places special constraints on the code, so that the search cannot simply look for the largest Hamming minimum distance d H . As shown in the ﬁgure, the LLR input to each BCJR (the \u201cintrinsic information\u201d) is subtracted from the BCJR\u2019s output LLR before this information is passed to the other BCJR. This acts to prevent false convergence and is an effective technique in most applications. However, the LLR subtraction markedly reduces the CC-BCJR performance\ny a\nas a decoder. Furthermore, both the convergence threshold and the the required number of iterations depend to some degree on the code. We will develop an analysis that takes into account intrinsic subtraction and apply it to ﬁnd good rate 1/2 convolutional codes at memories 2\u20135. Both feedforward and recursive systematic (RS) codes are found; unlike the case with traditional turbo decoding, feedforward are better.\nIn the earlier turbo equalization literature the RS code (46,72) has sometimes been studied, e.g., [1], [3], [5]. [We use left-justiﬁed octal generator notation (g 1 , g 2 ) = (10011, 11101) → (46, 72), where in the recursive case the ﬁrst entry is the feedback polynomial; in right-justiﬁed notation, (46,72) is (23,35)]. Little justiﬁcation is given for this code; it is perhaps used because of its success in tra- ditional turbo decoding. The most often used code is the simple memory-2 (7,5) feedforward code, e.g., [4], [6], [8]. It provides good error performance and a relatively early \u201csoft\u201d convergence threshold. A few other codes have been studied [7], but these often show a late, sharp convergence threshold.\nWhat constitutes a good convolutional code depends only weakly on the ISI response v. The severity of an ISI can be measured by its state space size (here ≈ exp 2 [number of components of v with signiﬁcant energy]) and by its square Euclidean minimum distance d min ; some details are in Section II. An interesting special case of large state size ISIs are those with z-plane zeros on or near the unit circle. Most ISIs studied\nin the literature are \u201ceasy\u201d, meaning they have a combination of large d min , small state space, and zeros relatively far from the circle. We intentionally choose a rather hard class called faster than Nyquist (FTN) ISIs. In these, the spectrum of v is narrowband and has in theory an entire zero spectral region, which implies an inﬁnitely long v; in reality v is ﬁnite but very long.\nA long v that stems from spectral zeros means that some sort of near-optimal but reduced ISI-BCJR needs to be em- ployed in order to keep computation and memory reasonable. Receiver performance can be lower if the ISI-BCJR is improp- erly conﬁgured. Reduced ISI-BCJR algorithms are a major subject and are beyond the scope of this paper. The type in this paper is based on a white noise model and is described [4], [6], [8]; another type uses precoding[2].\nWe have two motivations for choosing our ISI class: We want to test the convolutional codes with narrowband severe ISI, and second, coded FTN is an interesting special case, both in theory and in practice. FTN signals are ordinary linear modulation signals of the form s(t) = u n h(t − nτT ), τ < 1, where the pulse h(t) is orthogonal to T shifts (i.e., \u201cNyquist\u201d) but not to τ T shifts. With uncorrelated symbols {u n }, the average power spectral density (PSD) of s(t) is proportional to H(f ), the Fourier transform of h(t), but s(t) carries more symbols per second than orthogonal transmission, by a factor 1/τ . For a given PSD shape, coded signals can be constructed by means of the usual Nyquist pulses, but in terms of bits/Hz-s carried, both the theoretical Shannon capacity and the performance of practical schemes are markedly worse than FTN constructions [9]. Uncoded FTN signals can double the bits/Hz-s carried by OFDM transmission, in a given PSD and at a given error rate. These facts motivate our interest in coded FTN.\nThere are a number of ways a discrete-time v channel model can be derived from an FTN signal; these are reviewed in [6], [8]. The FTN model for the code searches in this paper is\nThe model has d 2 min = .58 and derives from the pulse h(τ t), where τ = .35 and h(t) is the standard root raised- cosine (rRC) pulse with 30% excess bandwidth. 1 With rate 1/2 coding, the coded FTN signals carry (1/2)/τ T (1/2T ) = 1/τ = 2.86 bits/Hz-s, where bandwidth is taken as the 3dB positive Hz bandwidth (on this scale, coded QPSK with pulse h(t) achieves 1 bit/Hz-s). With the simple codes in this paper, decoders can perform within a dB of the Shannon capacity that corresponds to the signal PSD shape [8], [9].\nIn the next section we will review some basic properties of the iterative decoder in Fig. 1, and then in Section III show how these lead to a heuristic for choosing good convolutional codes. The outcome of a search for best short codes is given\nin Section IV and some test results with best codes are given in Section V.\nAs shown in Fig. 1, each decoder BCJR algorithm produces LLR values for the convolutional codeword symbols, from which are subtracted the LLRs that were the input to that BCJR. The outcome is apriori information for the other BCJR. The CC-BCJR observes only this input, while the ISI-BCJR also observes the channel outputs y. What circulates are LLR values for the codeword symbols, not the data symbols. A separate CC-BCJR calculation decides the data, presumably after the last iteration. This calculation is not subject to LLR subtraction and consequently the data bit error rate BER dat is generally much lower than the CC-BCJR module\u2019s output LLR would imply; that is, intrinsic information subtraction damages BER dat .\nThe decoder progress can be followed by tracking the codeword symbol BER, the LLR statistics, or their mutual information in the form of an EXIT chart. Lee et al. [5] make a convincing case that symbol BER gives the most direct and informative description. One plots the input\u2013output BER relationship as two curves, or \u201ccharacteristics\u201d, one for the CC-BCJR and one for the ISI-BCJR. Both are plotted as log BER i versus log BER o from the point of view of the CC- BCJR module, in a picture like Fig. 2 (from the ISI-BCJR point of view, input and output are reversed). The top lines are CC-BCJR characteristics and the bottom line is the ISI-BCJR characteristic. As the code block length grows, a law of large numbers applies to each module, and the two lines determine the behavior of the decoder. We will give properties of these lines and argue for what consitutes a good convolutional code. But ﬁrst we discuss an important empirical property of the LLRs, that they are nearly Gaussian. This fact explains and simpliﬁes many of the decoder properties.\nIt is often observed in the literature that the LLR inputs are nearly Gaussian and independent. That they are independent is a consequence of the interleavers before each BCJR module; that they are Gaussian is easily veriﬁed by compiling a pdf. When the codeword symbol BERs are below 0.2, our own observations of the LLR variate z show good agreement with the Gaussian pdf\nwhich becomes more nearly perfect as the LLRs grow. The quantity S here functions as a pseudo-channel SNR: Exactly this distribution would occur if ±1 were the input to an AWGN channel with SNR S and the BCJR observed only the channel LLR. Consequently, apriori and channel-information LLRs are almost indistinguishable.\nwhere y is the AWGN channel observation and ln[Pr(+1)/Pr(−1)] is the apriori symbol LLR z, which is distributed by the form (2). The ISI-BCJR observes both channel and (independent) apriori LLRs, and these sum to another Gaussian. The CC-BCJR does not observe channel outputs, seeing only the apriori LLR term with pdf (2), which could just as well come from an AWGN channel with SNR S. LLR i (n), n = 1, 2, . . . , 2L, the LLR input to the CC-BCJR, is a sequence of independent Gaussians z with pdf of form (2), and similarly for the output from the CC-BCJR module LLR o (n). L is the data block length.\nWe decide symbols in the usual way, namely that the nth symbol is +1 if LLR(n) > 0 and −1 otherwise. From (2), the model symbol probability of error is then Q(\n√ 2S) where Q(α) = ∞ α (1/ √ 2π) exp(u 2 /2)du ≈ (1/2) exp(−S).\nA plot of actual input\u2013output log BERs observed in tests for the ISI-BCJR module, including the instrinsic subtraction, gives the lower line in Fig. 2 when v is (1), one line for each E s /N 0 . A sufﬁcient block length of symbols must be processed (at least 40000) so that a reliable convergence of the BERs to their mean occurs. Essentially the same curve results with real turbo decoding or with artiﬁcial Gaussian input LLRs. Lee et al. [5] observed that a plot without the logs taken is nearly a straight line. The loglog curve is in any case a simple convex one. Its beginning and ending points may be computed theoretically. When the input BER is 0.5 (no apriori information), the ISI-BCJR works only from channel outputs\nand subtracts zero intrinsic information; it achieves an error rate near Q( d 2 min E s /N 0 ) (details on this calculation appear in [6], [8], [10]). 2 . As the input BER tends to zero, apriori information is perfect, and it can be shown that the output BER tends to that of antipodal binary signaling, Q( 2E s /N 0 ).\nThe CC-BCJR BER also displays a linear input\u2013output characteristic, this time in the loglog domain. With either Gaussian apriori LLRs or actual turbo decoding, BERs fall on one of a few straight lines, whose intersections with the ISI characteristic are set mostly by their slopes. 3 The straight line behavior can be justiﬁed as follows. The Gaussian input LLRs are equivalent to an AWGN channel with SNR S. A standard coding gain calculation for the CC-BCJR without intrinsic information subtraction shows that the input symbol BER is converted from about Q(\nS), which has slope ≈ log[Q(\nS)], which is ≈ 1/d H at all but small S. The intrinsic information subtraction forms LLR o (n) − LLR i (n); these are correlated variates, but the differences are again Gaussian, and once interleaved, they are essentially independent over n. The straight line input\u2013output behavior remains but the correlation increases the slope to a less favorable value.\nWe can now portray the iteration to iteration progress in an EXIT-like way as shown in Fig. 2. The output of each module becomes the input of the next until the ISI-BCJR output BER (denoted BER i ) reaches the cross of the two characteristics. Where this occurs depends on the CC-BCJR characteristic. If it is ﬂat enough, the cross will occur at BER i ≈ Q( 2E s /N 0 ). If so, the CC-BCJR data decoder (no intrinsic subtraction) converts this to the much lower value Q( 2d H E s /N 0 ), the convolutional code performance over an AWGN channel with E s /N 0 . If the characteristic is not ﬂat enough, the iterative decoder is in the threshold region and will not reach this performance.\nBefore ﬁnishing we will make the additional assumption that the block length is large (the picture shows the average of 10 length-300000 data bit blocks), so that the statistical means that form the ﬁgure are close to their weak-law asymptotic values. The decoder in Fig. 2 works well at the 6 dB SNR with block lengths as short as 2000\u20134000 data bits, but patterns from successive blocks only roughly follow the CC and ISI characteristics. Long block lengths mean the turbo decoder will almost always stop after the same number of iterations (5 in the ﬁgure). They are also needed when the SNR is near the threshold for the decoder. In any case, we will next base a convolutional code design heuristic on these two characteristics, and the long block assumption means that the decoder progress has a well-deﬁned relationship to them. In effect, we are designing codes for the long block length case.\nWe can conclude from the previous section that a convo- lutional code has a straight-line characteristic that determines how it affects iterative decoding at longer block lengths. A larger free distance d H will lead to a lower data BER at the end of decoding, but the line\u2019s slope has the larger effect since it sets how far left along the ISI-BCJR characteristic the iterations stop. A further-left termination implies a lower BER i at the convolutional decoder, which in turn leads to a lower data BER. Both of these parameters, d H and the CC slope, contribute to error performance; which has the greater effect depends the location of the ISI-BCJR characteristic, which is in turn set by the AWGN channel E s /N 0 .\nWe propose the following search scheme at each code memory.\nMake a short list of codes with the most favorable CC- BCJR characteristic. This is easily done by feeding codes with Gaussian LLRs corresponding to output BERs near .01 and .08. Decide symbols by the LLR sign. This quickly yields two reliable BER statistics and because of the strong straight-line tendency in the loglog plot they give a good indication of the CC location.\nInvestigate the best (lowest) CC slope codes in the list by a more complete simulation.\nFrom the codes with best slope, take codes with best d H . In every case, we have found codes that have both the\nthe approximate best slope and the known best d H for their memory. These are listed in the next section. The outcome of the search does not depend explicitly on the ISI characteristic. ISI characteristics are roughly similar in shape (see e.g. [5]) and their location up or down in the plot depends chieﬂy on SNR, so that the best convolutional slope is the same.\nUnder the Gaussian LLR assumption, the CC slope depends solely on the correlation of the LLR i and LLR o sequences. The can be measured statistically, but the procedure seems no faster than the direct BER method given above. It would be interesting to ﬁnd a way to derive the correlation directly from the code generators.\nThis section lists good codes found by the procedure just given. We have searched over almost all feedforward and RS generators with memory 2\u20134; the skipped codes are those for which both generators have one or more ﬁnal zeros. A more limited search was performed at memory 5. Feedforward generators are considered to be equivalent if g 1 and g 2 are exchanged, or if both are reversed [i.e., (6,4) becomes (5,4)]. These operations create codes with the same codeword weight and bit error weight distributions; that is, the codes have the same numbers of words at weights d H , d H + 1, . . ., and the words at each weight lead to the same number of data bit errors. We found no evidence that codes with the same distributions lead to different turbo equalizer performance. In what follows only one member of each equivalent group is\ngiven. In all cases, both feedforward and RS, a code with maximum textbook free distance was found that also had the minimum slope for the memory and code type.\nWithin codes of the same distance and CC characteristic, it is still possible to ﬁnd codes with different weight or bit error distributions. This does not affect the CC-BCJR input\u2013 output BER characteristic, and therefore does not affect the iterative decoder progress, but it can change the BER at the ﬁnal data bit decoding. 4 The generators listed below have both the optimum d H and slope possible in each memory category, and lead to approximately the best post-threshold data bit BER as measured by tests over an AWGN channel.\nMemory 2 : Only (7,5) is useful. Slope 1/3.9, d H = 5. Memory 3 : (74,54), (64,54). Slope 1/5.2, d H = 6. Memory 4 : (46,72), (66,62). Slope 1/5.2, d H = 7.\nMemory 3 : (54,64), (64,74), (64,54), (54,74). Slope 1/4.4, d H = 6.\nMemory 4 : (46,56), (56,62), slope 1/5.1; (46,72), slope 1/4.85. d H = 7.\nHere are some observations about these results. With feed- forward codes no memory 4 code has better slope than the best memory 3 codes; this is evident in Fig. 2, which shows that both (74,54) and (46,72) intersect the ISI-BCJR characteristic at about the same point. Yet (46,72) has d H = 7, while (74,54) has 6, so when the data symbols are ﬁnally decoded they should have lower BER. Data BER thus stems from two primary sources, the CC-BCJR slope and d H . With RS codes, the memory 4 has better slope than memory 3 \u2014 but at each memory some feedforward code has better (lower) slope than any RS code. It is interesting to observe that RS code (46,66), which is the often used \u201c(23,35)\u201d parallel concatenated turbo convolutional code, has inverse slope 4.8, which is worse than the best memory-4 feedforward codes with the same d H . Finally, we observe that there exist codes in a given category with the best slope but not the best d H , as well as codes with the best d H but not the best slope.\nFigure 3 shows the data BER performance versus E b /N 0 , both for turbo decoding with the target ISI model and for con- volutional decoding alone over the AWGN channel (here the energy/data bit E b = 2E s ). The codes chosen are feedforward (7,5), (74,54) and (46,72), with one point shown for the 32- state code (62,57). The full curves are the convolutional-alone BER, with turbo equalization shown by certain points. Before threshold the turbo BER is high, typically 0.2; after threshold it\nclosely follows the convolutional-alone BER; we do not show test results in either of these regions. The ISI-BCJR is the M- BCJR method of [6], [8], with the M parameter set to 10\u2013300, depending on the code and SNR. At the left is the Shannon BER capacity for the ISI channel. 5 Turbo BER performance approaches within .8\u20131.6 dB of capacity, depending on which code is employed (the product of M-BCJR paths and iterations needed to achieve this is around 3000, and data block length 40000 is required). Note that longer-memory codes do not approach capacity as closely at larger BER ( > 10 − 3 ), but they approach progressively more closely at lower BER.\nFigure 4 compares the BER performance of some good feedforward and RS codes with the same (g 1 , g 2 ), with convolutional-alone decoding. Representatives of all the best code categories in Section IV are included here. The results show that generally speaking RS codes produce somewhat poorer data BER than feedforward codes, once E b /N 0 lies above threshold and the iterative decoding BER settles to the underlying convolutional decoding BER. This is the opposite of the case with parallel concatenation convolutional turbo coding.\nWe have used the near-Gaussian nature of LLRs to create an efﬁcient procedure to ﬁnd good short-memory convolutional codes for use in turbo equalization. What codes are best is strongly driven by the intrinsic information subtraction in the iterative decoding. Consequently, decoder BER depends not only on code minimum distance, but also on an input\u2013output BER slope parameter. The codes are derived for and tested against severe ISI derived from faster than Nyquist signals,\nwhose spectrum includes null regions. With good codes, BER performance is .8\u20131.6 dB from the capacity that applies to these signals.\nThis work was supported by the Swedish Research Council (VR) through Grant 621-2003-3210, and by the Swedish Foundation for Strategic Research (SSF) through its Strategic Center for High Speed Wireless Communication at Lund."},"refs":[{"authors":[{"name":"C. Douillard et al."}],"title":{"text":"Iterative correction of intersymbol interference: Turbo equalization"}},{"authors":[{"name":"M. T ¨uchler"},{"name":"C. Singer"}],"title":{"text":"Turbo equalization: An overview"}},{"authors":[{"name":"G. Colavolpe"},{"name":"G. Ferrari"},{"name":"R. Raheli"}],"title":{"text":"Reduced-state BCJR type algo- rithms"}},{"authors":[{"name":"D. Fertonani"},{"name":"A. Barbieri"},{"name":"G. Colavolpe"}],"title":{"text":"Reduced-complexity BCJR algorithm for turbo equalization"}},{"authors":[{"name":"S.-J. Lee"},{"name":"C. Singer"},{"name":"R. Shanbhag"}],"title":{"text":"Linear turbo equalization analysis via BER transfer and EXIT charts"}},{"authors":[{"name":"B. Anderson"},{"name":"A. Prlja"}],"title":{"text":"Turbo equalization and an M-BCJR algorithm for strongly narrowband intersymbol interference"}},{"authors":[{"name":"M. McGuire"},{"name":"M. Sinha"}],"title":{"text":"Discrete time faster-than-Nyquist signaling"}},{"authors":[{"name":"A. Prlja"},{"name":"B. Anderson"}],"title":{"text":"Reduced-complexity receivers for strongly narrowband intersymbol interference introduced by faster than Nyquist signaling"}},{"authors":[{"name":"F. Rusek"},{"name":"B. Anderson"}],"title":{"text":"Constrained capacities for faster-than-Nyquist signaling"}},{"authors":[{"name":"B. Anderso"},{"name":"A. Svensso"}],"title":{"text":"J"}},{"authors":[{"name":"B. Anderson"},{"name":"F. Rusek"}],"title":{"text":"The Shannon bit error limit for linear coded modulation"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564195.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T6.4","endtime":"16:00","authors":"John B Anderson, Mehdi Zeinali","date":"1341502800000","papertitle":"Best Rate 1/2 Convolutional Codes for Turbo Equalization with Severe ISI","starttime":"15:40","session":"S13.T6: Convolutional and Turbo Codes","room":"Kresge Rehearsal A (033)","paperid":"1569564195"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
