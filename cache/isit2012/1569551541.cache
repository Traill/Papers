{"id":"1569551541","paper":{"title":{"text":"Data Processing Inequalities Based on a Certain Structured Class of Information Measures with Application to Estimation Theory"},"authors":[{"name":"Neri Merhav"}],"abstr":{"text":"Abstract\u2014We study data processing inequalities (DPI\u2019s) that are derived from a certain class of generalized information measures, where a series of convex functions and multiplicative likelihood ratios are nested alternately. A certain choice of the convex functions leads to an information measure that extends the notion of the Bhattacharyya distance: While the ordinary Bhattacharyya distance is based on the geometric mean of two replicas of the channel\u2019s conditional distribution, the more general one allows an arbitrary number of replicas. We apply the DPI induced by this information measure to a detailed study of lower bounds of parameter estimation under additive white Gaussian noise (AWGN) and show that in certain cases, tighter bounds can be obtained by using more than two replicas. While the resulting bound may not compete favorably with the best bounds available for the ordinary AWGN channel, the advantage of the new lower bound, becomes signiﬁcant in the presence of channel uncertainty, like unknown fading. This is explained by the convexity property of the information measure."},"body":{"text":"In classical Shannon theory, DPI\u2019s (in various forms) are frequently used to prove converses to coding theorems and to establish fundamental properties of information measures, like the entropy, the mutual information, and the Kullback\u2013Leibler divergence. For example, the converse to the joint source\u2013 channel coding theorem sets the stage for the separation theorem: When a source with rate\u2013distortion function R(D) is transmitted across a channel with capacity C, the distortion at the decoder must obey R(D) ≤ C, or equivalently, D ≥ R −1 (C).\nZiv and Zakai [17] (see also Csisz´ar [3] for related work) have observed that in order to obtain a wider class of DPI\u2019s, the (negative) logarithm function, that plays a role in the classical mutual information, can be replaced by an arbi- trary convex function Q. This generalized mutual information (GMI), I Q (X; Y ), was further generalized in [15] to be based on multivariate convex functions. In analogy to the classical converse to the joint source\u2013channel coding theorem, one can then deﬁne a generalized rate\u2013distortion function R Q (D) (as the minimum of the GMI between the source and the reproduction, s.t. some distortion constraint) and a generalized channel capacity C Q (as the maximum GMI between the\nchannel input and output) and establish another lower bound on the distortion via the inequality R Q (D) ≤ C Q that stems from the DPI of I Q . While this lower bound obviously cannot be tighter than its classical counterpart in the limit of long blocks (which is asymptotically achievable), Ziv and Zakai have demonstrated that for short block codes sharper lower bounds can be obtained.\nGurantz, in his M.Sc. work [5] (supervised by Ziv and Zakai), continued the work in [17] at a speciﬁc direction: He constructed a special class of generalized information functionals deﬁned by iteratively alternating between appli- cations of convex functions and multiplications by likelihood ratios. After proving that this functional obeys a DPI, Gurantz demonstrated how it can be used to improve on the Arimoto bound for coding above capacity [1] and on the Gallager upper bound of random coding [4] by a pre-factor of 1/2.\nMotivated by the interesting nested structure of Gurantz\u2019 information functional, we continue to investigate this infor- mation measure, ﬁrst of all, in general, on its own right, and then we further study its potential. We begin by putting the Gurantz\u2019 functional in the broader perspective of the other information measures of [15], [17] (Section 2). Speciﬁcally, we show that these GMI\u2019s can be viewed as special cases of the one in [15], which is based on multivariate convex functions.\nWe then focus on a concrete choice of the convex functions (Section 3) in the Gurantz\u2019 information measure which turn out to yield an extension the notion of the Bhattacharyya distance: While the ordinary Bhattacharyya distance is based on the geometric mean of two replicas of the channel\u2019s conditional distribution (see, e.g., [12, eq. (2.3.15)]), the more general measure considered here, allows an arbitrary number of replicas. This generalized Bhattacharyya distance is also intimately related to the Gallager function E 0 (ρ, Q) [4], [12], which is indeed another GMI [3] obeying a DPI (see also [7, Proposition 2]).\nFinally, we apply the DPI, induced by the above described generalized Bhattacharyya distance, to a detailed study of lower bounds on parameter estimation under additive white Gaussian noise (AWGN) and show that in certain cases, tighter bounds can be obtained by using more than two\nreplicas (Section 4). While the resulting lower bound may still not compete favorably with the best available bounds for the ordinary AWGN channel, the advantage of the new lower bound becomes apparent in the presence of channel uncertainty, like the case of an AWGN channel with unknown fading. This different behavior is explained by the convexity property of the information measure.\nIn [5], a generalized information functional was deﬁned in the following manner: Let X and Y be random variables taking on values in alphabets X and Y, respectively. Let x 1 , x 2 , . . . , x k be a given list of symbols (possibly with repeti- tions) from X . Let Q 1 , Q 2 , . . . , Q k be a collection of univari- ate functions, deﬁned on the positive reals, with the following properties, holding for all i: (i) lim t→0 tQ i (1/t) = 0. (ii) |Q i (0)| < ∞. (iii) Either the function ˆ Q i ∆ = Q 1 ◦ Q 2 ◦ . . . ◦ Q i is monotonically non-decreasing and Q i+1 is convex, or ˆ Q i is monotonically non\u2013increasing and Q i+1 is concave (here, the notation ◦ means function composition). Now, deﬁning λ yxx = P Y |X (y|x)/P Y |X (y|x ), the Gurantz\u2019 functional is deﬁned as\nThe DPI associated with the Gurantz\u2019 functional is the following: Let X → Y → Z be a Markov chain and let Q 1 be a convex function which, to- gether with Q 2 , . . . , Q k , complies with (i)\u2013(iii) above. Then, G(Y |x, x 1 , . . . , x k ) ≥ G(Z|x, x 1 , . . . , x k ). Let P (x, x 1 , . . . , x k ) = P X (x)P (x 1 , . . . , x k |x), where P X (·) is the actual distribution of the random variable X and P (x 1 , . . . , x k |x) is an arbitrary conditional distri- bution of (X 1 , . . . , X k ) given X = x. Now, for a given {P (x 1 , . . . , x k |x)}, the Gurantz\u2019 mutual information I G (X; Y ) can be deﬁned as\nwhere the expectation is w.r.t. the above deﬁned joint distribu- tion of the random variables X, X 1 ,..., X k . This GMI is now a well\u2013deﬁned functional of P XY = P X × P Y |X . In principle, one may apply the generalized DPI I G (X; Y ) ≥ I G (X; Z) for any given choice of {P (x 1 , . . . , x k |x)} (consider these as parameters) and then optimize the resulting distortion bound w.r.t. the choice of these parameters.\nOur ﬁrst observation (see the full version of this paper [10] for details) is that I G (X; Y ) is a special case of the Zakai\u2013Ziv GMI [15] (a.k.a. f \u2013dissimalrity [6]), deﬁned as\nwhere Q is a multivariate convex function of k variables and µ i (·, ·), i = 1, 2, . . . , k, are arbitrary measures on X × Y. In view of this observation, one can use the Gurantz mu- tual information to obtain DPI\u2019s for communication systems.\nAccording to [15, Theorems 3.1 and 5.1], the following is true: Let U → X → Y be a Markov chain and let V = g(Y ) where g is a deterministic function. Let µ i (x, y), i = 1, 2, . . . , k, be arbitrary measures and deﬁne µ i (u, y) = P U (u) dxP X|U (x|u)µ i (x, y)/P X (x), µ i (u, v) =\ndyµ i (u, y), i = 1, . . . , k. Then, I ZZ (X; Y ) ≥ I ZZ (U ; V ). Assuming, in addition, that the encoder x = f (u) is deterministic, we can choose (following [15]) µ i (x, y) = P X (x)P Y |X (y|x i ), where x i = f (u i ) is a speciﬁc member in X and then µ(y|u i ) = P Y |X (y|f (u i )). We then obtain (see [10] for details):\nwhere the expectation on the left\u2013hand side is w.r.t. P XY (x, y) i P X (x i ), and the expectation on the r.h.s. is w.r.t. P U V (u, v) i P U (u i ).\nA convenient choice of the functions {Q i } is the following: Q 1 (t) = −t a 1 , and Q i (t) = t a i for i ≥ 2, where 0 ≤ a i ≤ 1, i = 1, . . . , k. We then have so called the Hellinger transform [8]:\nwhere b 0 = 1 − a 1 , b 1 = (1 − a 2 )a 1 , b 2 = (1 − a 3 )a 1 a 2 , etc., and ﬁnally, b k−1 = (1 − a k ) k−1 i=1 a i , and b k = k i=1 a i . Note that b 0 , . . . , b k are all non\u2013negative and they sum to 1. Conversely, for every {b i } with these properties, one can ﬁnd a 1 , . . . , a k , all in [0, 1], using the inverse transformation: a 1 = 1 − b 0 , a 2 = 1 − b 1 /(1 − b 0 ), etc., and ﬁnally, a k = 1 − b k−1 /(1 − k−2 i=0 b i ). This allows us parametrize directly in terms of {b i } without worrying about {a i }. Spe- cializing to the case b i = 1/(k + 1) for all i = 0, 1, . . . , k, eq. (4) extends the Bhattacharyya distance [9]. If, in addition, we assign P X 1 ,...,X k |X 0 (x 1 , . . . , x k |x 0 ) = k i=1 P X (x i ), then I G (X; Y ) = EG(Y, X, X 1 , . . . , X k ) = −e −E 0 (k,P X ) , where\nThus, I G (X; Y ) is related also the Gallager function, albeit only at integer values of the parameter ρ. Indeed, it known that the Gallager function (for every real ρ ≥ 0) is also a GMI [3] and hence it satisﬁes a DPI (see also [7, Proposition 2]). The advantage of working with integer values of ρ, is that an integral raised to an integer power (k + 1) can be expressed in terms of (k + 1)\u2013dimensional integration over the (k + 1) replicas, x 0 ,x 1 ,...,x k , that in turn can be commuted with the integration over Y. In some situations, this is convenient.\nIn this section, we apply the DPI associated with the proposed information measure to obtain a Bayesian lower bound on signal parameter estimation. In particular, our model is the following. The source symbol U , which is uniformly distributed in U = [−1/2, +1/2], is the parameter to be estimated. For mathematical convenience, we deﬁne the dis- tortion measure between a realization u of the source and an estimate v (both in U ) as d(u, v) = [(u − v) mod 1] 2 . where t mod 1 ∆ = t + 1 2 − 1 2 , r being the fractional part of r, that is, r = r− r . In the high\u2013resolution limit (corresponding to the high signal\u2013to\u2013noise (SNR) limit), the modulo 1 operation has a negligible effect, and hence d(u, v) becomes essentially equivalent to the ordinary quadratic distortion.\nThe channel is assumed to be an AWGN channel, namely, the channel output is given by y(t) = x(t, u) + n(t), 0 ≤ t < T , where x(t, u) is an arbitrary waveform of unlimited bandwidth, parametrized by u and n(t) is AWGN with two\u2013 sided spectral density N 0 /2. The energy E = T 0 x 2 (t, u)dt is assumed to be independent of u (for reasons of simplicity). The estimator v is a functional of the channel output waveform {y(t), 0 ≤ t < T }.\nBefore deriving bounds on Ed(U, V ), we ﬁrst need to derive the generalized rate\u2013distortion function and the generalized channel capacity pertaining to the generalized Bhattacharyya distance.\nThe \u201crate\u2013distortion function\u201d R(D) w.r.t. the information measure under discussion is given by the minimum of\nsubject to Ed(U, V ) ≤ D. This problem is solved using calculus of variations (see [10] for details). The result of this is as follows. Deﬁne the functions:\nand for a given s, let us denote D s = C(s)F (s). Then, −R(D s ) = C(s)[G(s)] k+1 , where we have deﬁned\nIn the limits of very low and very high distortion, one can approximate R(D) directly as an explicit function of D. In particular, it is shown in [10] that in the high\u2013resolution regime (R → 0), the behavior depends on whether k = 1, k = 2, or k > 2. For k = 1, the distortion\u2013rate function is approximated as\n2 k\nThe case k = 2 lacks an explicit closed\u2013form di- rect relation between R and D, but it shows that lim D→∞ (log D)/ log[−R(D)] = 1, which means that the relation between R and D is essentially linear, like in the case k > 2, but in a slightly weaker sense.\nThe probability law of the channel from U to Y is given by\nwhere y designates the entire channel output waveform {y(t), 0 ≤ t < T } (represented using some fam- ily of orthnormal basis functions), and ∝ means that the constant of proportionality does not depend on u. Let ρ(u, u ) = 1 E · T 0 x(t, u)x(t, u )dt. The integral\ndy k i=0 [P Y |U (y|u i )] 1/(k+1) is straightforwardly shown to be given by\n \n \nNext, let us take the expectation w.r.t. the randomness of {U i }. As in [15], we resort to a lower bound (hence an upper bound on I G (U ; Y )) based on Jensen\u2019s inequality. Denoting ¯ x(t) =\ndu · x(t, u), it is easily observed that since {U i } are independent, then for all i = j:\nNote that the parameter is always between 0 and 1 and it depends only on the parametric family of signals. Applying Jensen\u2019s inequality on (11), we obtain\n(or equivalently, higher values of the integrated variance of x(t, U )) are expected to yield higher value of I G (U ; Y ), and hence smaller estimation error, at least as far as our bounds predict, and since cannot be negative, the best classes of signals, in this sense, are those for which = 0. Note also that for Jensen\u2019s inequality to be reasonably tight, the random variables {ρ(U i , U j )} should be all close to their expectation\nwith very high probability, and if this expectation vanishes, as suggested, then {ρ(U i , U j )} should all be nearly zero with very high probability. We will get back to classes of signals with this desirable rapidly vanishing correlation property later on.\nWe now equate R(D) to I G (U ; Y ) in order to obtain esti- mation error bounds in the high SNR regime, where the high\u2013 resolution expressions of R(D) are relevant. As discussed above, in this regime, we will neglect the effect of the modulo 1 operation in the deﬁnition of the distortion measure, and will refer to it hereafter as the ordinary quadratic distortion mea- sure. The choice k = 1 yields I G (U ; Y ) ≤ −e −(1− )E/(2N 0 ) (see also [15]), and following eq. (8), this yields\nso, the exponential decay of the lower bound is according to e −(1− )E/N 0 . For k = 2, we have log D ≈ −2(1 −\n)E/(3N 0 ), which means an exponential decay according to e −2(1− )E/(3N 0 ) , which is better. For k ≥ 3, we use (9) and the resulting bound decays according to exp{−(1 − ρ)kE/[(k + 1)N 0 ]}, which is better than the result of k = 1, but not as good as the one of k = 2. Thus, the best choice of k for the high SNR regime is k = 2, namely, a generalized Bhattacharyya distance with k + 1 = 3 replicas, rather than the two replicas of the ordinary Bhattacharyya distance.\nNote that since ≥ 0, as mentioned earlier, then for any family of signals, the exponential function e −2E/(3N 0 ) is a uni- versal lower bound (at high SNR) in the sense that it applies, not only to every estimator of U , but also to every parametric family of signals {x(t, u)}, i.e., to every modulation scheme without being dependent on this modulation scheme (see also [15]). This is in contrast to most of the estimation error bounds in the literature. In other words, it sets a fundamental limit on the entire communication system and not only on the receiver end for a given transmitter. Indeed, for some classes of signals, an MSE with exponential decay in E/N 0 is attainable at least in the high SNR regime, although there might be gaps in the actual exponential rates compared to the above mentioned bound. For example, in [11], it is discussed that in the case of time delay estimation (x(t, u) = x 0 (t − u)), it is possible to achieve an MSE of the exponential order of e −E/(3N 0 ) by allowing the pulse s 0 (t) to have bandwidth that grows exponentially with T . Thus, by improving the lower bound exp(−E/N 0 ) (a special case of the above with k = 1) to exp[−2E/(3N 0 )], we are halving the gap between the exponential rates of the upper bound and the lower bound, from 2E/(3N 0 ) to E/(3N 0 ).\nOur asymptotic lower bound should be compared to other lower bounds available in the literature. One natural candidate would be the Weiss\u2013Weinstein bound (WWB) [13], [14], which for the model under discussion at high SNR, reads [13, p. 66]:\nwhere r(h) = ρ(u, u + h) = T 0 x(t, u)x(t, u + h)dt/E is assumed to depend only on h and not on u. While this is an excellent bound for a given modulation scheme\n{x(t, u), u ∈ U }, it does not seem to lend itself easily to the derivation of universal lower bounds, as discussed above. To this end, in principle, the WWB should be minimized over all feasible correlation functions r(·), which is not a trivial task. A reasonable compromise is to ﬁrst minimize the WWB over r(·) for a given h, and then to maximize the resulting expression over h (i.e., max\u2013min instead of min\u2013 max). Since the expression of the bound is a monotonically increasing function of both r(h) and r(2h), and since both r(h) and r(2h) cannot be smaller than −1, we end up with WWB = e −E/N 0 /[2(1 − e −E/N 0 )] as a modulation\u2013 independent bound. This is a faster exponential decay rate (and hence weaker asymptotically) than that of our proposed bound for k = 2.\nIt is possible, however, to obtain a universal lower bound stronger than both bounds by a simple channel\u2013coding argu- ment, which is in the spirit of the Ziv\u2013Zakai bound [16]. This bound is given by (see [10] for the derivation):\nwhere M is a free parameter, an even integer not smaller than 4, which is subjected to optimization. Throughout the sequel, we refer to this bound as the channel\u2013coding bound. In the high SNR regime, the exponential order of the channel\u2013coding bound (for ﬁxed M ) is exp{−EM/[2N 0 (M − 2)}, which for large enough M becomes arbitrarily close to e −E/(2N 0 ) , and hence better than the DPI bound of e −2E/(3N 0 ) . In view of this comparison, it is natural to ask then what is beneﬁt of our DPI lower bound. The answer is in the next subsection.\nIt turns out that the feature that makes the DPI approach to error lower bounds more powerful, relatively to other approaches, is the convexity property of the GMI (in this case, I G (U ; Y )) w.r.t. the channel P Y |U . Suppose that the channel actually depends on an additional random parameter A (independent of U ), that is known to neither the transmitter nor the receiver, namely,\nwhere P A (a) is the density of A. If we think of I G (U ; Y ) as a functional of P Y |U , denoted I(P Y |U (·|u)), then it is a convex functional, namely,\nThis is a desirable property because the r.h.s. reﬂects a situation where A is known to both parties, whereas the l.h.s. pertains to the situation where A is unknown, so the lower bound associated with the case where A is unknown is always tighter than the expectation of the lower bound pertaining to a known A.\nConsider now the case where A is a fading parameter, drawn only once and kept ﬁxed throughout the entire observation time T . More precisely, our model is the same as before except that now the signal is subjected to fading according to\nwhere a and u are realizations of the random variables A and U , respectively. For the sake of convenience in the analysis, we assume that A is a zero\u2013mean Gaussian random variable with variance σ 2 (other densities are, of course, possible too).\nWe next compare the three corresponding bounds in this case. The overall channel from U to Y is\n \n2  \nwhere θ ∆ = 2σ 2 /[N 2 0 (1 + 2σ 2 E/N 0 )]. On substituting this channel into I G (U ; Y ) and assuming rapidly vanishing cor- relations (in view of the discussion at the end of Subsection IV.B), we eventually obtain in the high SNR regime (see [10] for details):\nApplying the high\u2013resolution approximation of D(R) for k ≥ 3, we get: E(U − V ) 2 ≥ g k σ · N 0 /E, where g k = 1 4 √ 2 (1 − 2/k) k (1 + 1/k) (k+1)/2 . A simple numerical study indicates that {g k } is monotonically increasing and so the best bound is obtained for k → ∞, where the constant is: lim k→∞ g k = 0.03944. Thus, our asymptotic lower bound for high SNR is\nAs can be seen, the WWB decays according to (E/N 0 ) −1 rather than (E/N 0 ) −1/2 and hence inferior to the DPI bound.\nThe channel\u2013coding bound is based on a universal lower bound on the probability of error, which holds for every signal set. The problem is that under fading, we are not aware of such a universal lower bound. The only remaining alternative then is to use a lower bound corresponding to the case where A is known to the receiver, and then to take the expectation w.r.t. A, although one might argue that this comparison is not quite fair. Nonetheless, the derivation of this appears in [10] and the result is\n= 0.001758 σ\nYet another comparison, perhaps more fair, can be made with the Chazan\u2013Zakai\u2013Ziv bound (CZZB) [2]. According to the CZZB, applied to our problem (see [10] for the derivation),\nwhich is again signiﬁcantly smaller than our bound. Thus, we observe that while the WWB and the CZZB are excellent bounds for ordinary channels without fading, when it comes to channels with fading, the proposed DPI bound has an advantage.\nInteresting discussions with Shlomo Shamai are acknowl- edged with thanks."},"refs":[{"authors":[{"name":"S. Arimoto"}],"title":{"text":"On the converse to the coding theorem for discrete memory- less channels"}},{"authors":[{"name":"D. Chazan"},{"name":"M. Zakai"},{"name":"J. Ziv"}],"title":{"text":"Improved lower bounds on signal parameter estimation"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"A class of measures of informativity of observation channels"}},{"authors":[{"name":"R. G. Gallage"},{"name":"J. Wi- le"}],"title":{"text":"Information Theory and Reliable Communication,  Sons, 1968"}},{"authors":[{"name":"I. Gurant"}],"title":{"text":"Application of a Generalized Data Processing Theorem, M"}},{"authors":[{"name":"L. G¨yorﬁ"},{"name":"T. Nemetz"}],"title":{"text":"f \u2013dissimilarity: a generalization of the afﬁnity of several distributions"}},{"authors":[{"name":"G. Kaplan"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"Information rates and error exponents of compound channels with application to antipodal signaling in a fading environment"}},{"authors":[{"name":"L. Le Ca"}],"title":{"text":"Asymptotic Methods in Statistical Decision Theory, Springer\u2013 Verlag, New York, 1986"}},{"authors":[{"name":"K. Matusita"}],"title":{"text":"On the notion of afﬁnity of several distributions and some of its applications"}},{"authors":[{"name":"N. Merhav"}],"title":{"text":"Data processing inequalities based on a certain structured class of information measures with application to estimation theory"}},{"authors":[{"name":"N. Merhav"}],"title":{"text":"Threshold effects in parameter estimation as phase transi- tions in statistical mechanics"}},{"authors":[{"name":"A. J. Viterb"},{"name":"J. K. Omur"}],"title":{"text":"Principles of Digital Communication and Coding , McGraw\u2013Hill, 1979"}},{"authors":[{"name":"A. J. Weis"},{"name":"D. dissertatio"}],"title":{"text":"Fundamental Bounds in Parameter Estimation, Ph"}},{"authors":[{"name":"E. Weinstein"},{"name":"A. J. Weiss"}],"title":{"text":"Lower bounds on the mean square estimation error"}},{"authors":[{"name":"M. Zakai"},{"name":"J. Ziv"}],"title":{"text":"A generalization of the rate-distortion theory and applications"}},{"authors":[{"name":"J. Ziv"},{"name":"M. Zakai"}],"title":{"text":"Some lower bounds on signal parameter esti- mation"}},{"authors":[{"name":"J. Ziv"},{"name":"M. Zakai"}],"title":{"text":"On functionals satisfying a data-processing theorem"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569551541.pdf"},"links":[{"id":"1569566527","weight":4},{"id":"1569565383","weight":9},{"id":"1569566725","weight":4},{"id":"1569566683","weight":4},{"id":"1569559259","weight":4},{"id":"1569566597","weight":4},{"id":"1569566943","weight":4},{"id":"1569556029","weight":4},{"id":"1569564481","weight":4},{"id":"1569565355","weight":9},{"id":"1569565931","weight":9},{"id":"1569551535","weight":9},{"id":"1569564227","weight":4},{"id":"1569566671","weight":4},{"id":"1569566303","weight":4},{"id":"1569564233","weight":4},{"id":"1569564401","weight":4},{"id":"1569558459","weight":4},{"id":"1569565291","weight":4},{"id":"1569560613","weight":4},{"id":"1569566999","weight":4},{"id":"1569566579","weight":4},{"id":"1569564989","weight":4},{"id":"1569566523","weight":4},{"id":"1569564613","weight":4},{"id":"1569566095","weight":13},{"id":"1569565803","weight":4},{"id":"1569566239","weight":4},{"id":"1569561085","weight":4},{"id":"1569566733","weight":4},{"id":"1569566753","weight":4},{"id":"1569558681","weight":9},{"id":"1569555999","weight":4},{"id":"1569566759","weight":4},{"id":"1569567665","weight":4},{"id":"1569564611","weight":4},{"id":"1569565667","weight":4},{"id":"1569561795","weight":9},{"id":"1569566423","weight":4},{"id":"1569567015","weight":4},{"id":"1569566437","weight":4},{"id":"1569566687","weight":4},{"id":"1569566939","weight":4},{"id":"1569553537","weight":4},{"id":"1569565915","weight":9},{"id":"1569553519","weight":4},{"id":"1569566425","weight":4},{"id":"1569554881","weight":4},{"id":"1569554971","weight":4},{"id":"1569566209","weight":4},{"id":"1569566909","weight":4},{"id":"1569565151","weight":4},{"id":"1569558985","weight":4},{"id":"1569565033","weight":9},{"id":"1569564353","weight":9},{"id":"1569565055","weight":4},{"id":"1569565633","weight":9},{"id":"1569565219","weight":4},{"id":"1569565469","weight":4},{"id":"1569565357","weight":4},{"id":"1569565393","weight":4},{"id":"1569562207","weight":4},{"id":"1569566191","weight":4},{"id":"1569567033","weight":4},{"id":"1569565527","weight":4},{"id":"1569566233","weight":4},{"id":"1569560997","weight":4},{"id":"1569566481","weight":4},{"id":"1569565463","weight":9},{"id":"1569562551","weight":4},{"id":"1569566901","weight":4},{"id":"1569551347","weight":4},{"id":"1569555367","weight":4},{"id":"1569566383","weight":4},{"id":"1569566805","weight":4},{"id":"1569565665","weight":9},{"id":"1569557275","weight":4},{"id":"1569565919","weight":4},{"id":"1569565865","weight":4},{"id":"1569566267","weight":4},{"id":"1569564305","weight":4},{"id":"1569566691","weight":4},{"id":"1569565421","weight":9},{"id":"1569566651","weight":4},{"id":"1569565013","weight":13},{"id":"1569551905","weight":18},{"id":"1569565457","weight":4},{"id":"1569556759","weight":4},{"id":"1569566619","weight":4},{"id":"1569561185","weight":4},{"id":"1569565669","weight":9},{"id":"1569565367","weight":4},{"id":"1569564281","weight":4},{"id":"1569566933","weight":4},{"id":"1569563919","weight":9},{"id":"1569565389","weight":4},{"id":"1569559919","weight":4},{"id":"1569566147","weight":4},{"id":"1569555891","weight":4},{"id":"1569565035","weight":4},{"id":"1569565889","weight":9},{"id":"1569563725","weight":4},{"id":"1569551539","weight":4},{"id":"1569565165","weight":4},{"id":"1569566413","weight":4},{"id":"1569565113","weight":4},{"id":"1569566375","weight":4},{"id":"1569564257","weight":18},{"id":"1569566555","weight":4},{"id":"1569564141","weight":4},{"id":"1569566973","weight":4},{"id":"1569565031","weight":4},{"id":"1569564755","weight":4},{"id":"1569565895","weight":4},{"id":"1569564807","weight":4},{"id":"1569566113","weight":18},{"id":"1569566443","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T8.5","endtime":"16:20","authors":"Neri Merhav","date":"1341331200000","papertitle":"Data Processing Inequalities Based on a Certain Structured Class of Information Measures with Application to Estimation Theory","starttime":"16:00","session":"S7.T8: Information Inequalities","room":"Stratton (491)","paperid":"1569551541"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
