{"id":"1569566321","paper":{"title":{"text":"Compressed Sensing on the Image of Bilinear Maps"},"authors":[{"name":"Philipp Walk"},{"name":"Peter Jung"}],"abstr":{"text":"Abstract\u2014For several communication models, the dispersive part of a communication channel is described by a bilinear operation T between the possible sets of input signals and channel parameters. The received channel output has then to be identiﬁed from the image T (X, Y ) of the input signal difference sets X and the channel state sets Y . The main goal in this contribution is to characterize the compressibility of T (X, Y ) with respect to an ambient dimension N . In this paper we show that a restricted norm multiplicativity of T on all canonical subspaces X and Y with dimension S resp. F is sufﬁcient for the reconstruc- tion of output signals with an overwhelming probability from O((S + F ) log N ) random sub-Gaussian measurements. Thus, in this case, the number of degrees of freedom of each output grows only additively instead of multiplicatively with the input dimensions (sparsity) S and F . This is a relevant improvement in the output compressibility and suggests a substantially reduced rate in compressed sampling algorithms."},"body":{"text":"From an abstract point of view, a dispersive communication channel is given as a bilinear operation T (s, h) between the input signal s and the channel parameter h plus additive noise n. For example, on R N this action could be represented as a matrix multiplication, i.e. the received signal is then:\nand the corresponding channel matrix H ∈ R N ×N is given by the action Hs = T (s, h). If the channel matrix is known at the receiver and the possible input signals s exhibit a linear structure the set of all outputs Hs is a linear space. However, if the channel matrix has unknown parameters and the signal set is for example only a union of subspaces, the set of all possible outputs usually looses its linearity and the receiver is thus confronted with the determination of r from a non-linear manifold. Then, in order to provide efﬁcient sampling and reception in such a non\u2013coherent setting, it is of fundamental importance to characterize the complexity of the output set and relate it to structural properties of the channel and of the possible transmit signals. In compressed sensing for example, such an assumption is the sparsity S of the signal set, i.e. the transmitter might operate in a peaky fashion and the data is concentrated only on unknown but small sets of S N sample points. Let us denote this union of so called canonical subspaces with Σ S . Even more, the set of non\u2013zero coefﬁcients in the vector h ∈ R N during transmission can be small as well, i.e. concentrated on subsets of cardinality F \t N such that h ∈ Σ F . The intrinsic dimension for sampling the signal r in (1) is S + F as soon\nthe support of signal and channel coefﬁcients are known to the transmitter, whereby the image of this particular subspaces under T is contained in some subspace up to dimension SF . In both cases an additional factor, logarithmic N , is necessary for the unknown location of the supports. However, in the worst case, the sparsity of the inputs to T behave multiplicatively . The essence of this contribution is to establish conditions on T under which the overall compressibility still behaves only additively in S and F . This has a relevant impact on the complexity of the output set T (Σ S , Σ F ) and hence substantially improve the rate in compressed sampling. Furthermore, understanding the coupling between two sparse sources directly generalizes to the coupling of ﬁnitely many sources.\nIn their recent work [1], Hedge and Baraniuk considered (S, F )-sparse circular convolutions in R N , see Section III, and formulated a restricted isometry property (RIP) for the set of all differences in the union of the image of all (S, F )-sparse circular convolutions. It turns out that their proof approach leads to difﬁcult mathematical problems, which according to the authors\u2019 knowledge are still unsolved. Since the proof in [2, Lemma 5.1] relies on a linear structure, which may not be present for the image of bilinear maps, more strict conditions on T and the input sets X and Y are needed to control the norm of the output set. However, the authors could establish in this work the result of Hedge and Baraniuk for a certain restriction on Euclidean convex cones. Another special case occurs for S and F dimensional subspaces, which are \u201cproperly\u201d separated. In this case, the image under the circular convolution is isomorphic to the set of all simple tensors, which in turn is isomorphic to the set of rank-1 operators from X to Y . Hence the main theorem implies certain results from the theory of rank-1 matrix recovery [3], [4], [5]. Moreover, the developed framework applies to all bilinear operations which have a certain restricted norm multiplicativity property on convex cones in an arbitrary basis. This enables compressed sensing on \u201csparse\u201d output sets, which can not be written as a ﬁnite union of subspaces, and leads to generalized structured sparsity models [6], [7], [8].\nThe outline of this paper is as follow. In Section II the fundamental concept of bilinear maps T on ﬁnite dimen- sional Euclidean spaces is introduced. The authors formulate a sufﬁcient condition in Deﬁnition 1, which ensures a better probability as in [2] for the RIP on the image of such bilinear maps. Some important and simple couplings T for certain communication scenarios are discussed in Section III. Here the\nmain result is applied to some channel models, e.g. circular convolutions, which establishes an additive behavior of the sparsity S and F of the signal inputs and channel states. Section IV concludes this work with a conjecture for the RIP on the image of all (S, F )-sparse circular convolutions.\nEvery bilinear map T : R N × R N → R N is a binary operation on R N and deﬁnes therefore a multiplication on R N . The N dimensional linear space R N , +, ·, T is then called an associative algebra 1 over R. Since we are interested in a stable embedding of the output (image), we have to equip the algebra with a norm · . In our main Theorem 1 we need for the proof technique, based on [2], a nesting in the 2 -norm, i.e. we have to bound the norm of the output z by the product of the norms of the inputs x, y from below and above. This is a very strict property for algebras since this would imply that the nullspace of T is N := (0, X) ∪ (Y, 0), i.e. T is non-singular on X × Y . If we remove all singularity points in X × Y we obtain a subset O ⊂ X × Y on which T is non- singular. But then there could still exist a sequence o n ∈ O\\N such that lim n T (o n ) = 0. The following deﬁnition exclude such sequences and provides how close 0 can be approached.\nx y . \t (3) Remark 1: It is known [9], that for ﬁnite\u2013dimensional al-\ngebras there always exists β < ∞. Obviously, β is always an upper bound on O and this simpliﬁcation will become relevant in the proof of Theorem 1. If α = β(= 1), then the norm is called multiplicative on O , but only few algebras are norm\u2013 multiplicative.\nEssentially, the implicit use of the set O removes the re- dundancy in representing T (X, Y ), i.e. removing unnecessary direction pairs in X × Y . Surely, the exact determination of the set O is a combinatorial hard problem and depends on T as well as on the subsets X and Y . Moreover, this set in general lacks for linear or convex properties.\nIn the following we will only consider R N with standard inner product product and the corresponding Euclidean norm\nx 2 := x, x . For a given subset X ⊆ R N we will denote the shell in X with inner and outer radii α and β by X α,β := { x ∈ X | α ≤ x ≤ β} and abbreviate further X α := X 0,α . The functional x 0 denotes the cardinality of the support of x in the Euclidean basis {e i } N −1 i=0 , i.e. the sparsity of x with respect to the Euclidean basis. A convex set X ⊂ R N is a convex cone if for every x ∈ X and λ ≥ 0 it follows λx ∈ X.\nX has dimensionality S if the space span X has dimension S.\nOur main theorem provides a generalized compressed sens- ing framework by a stable embedding of certain (S, F )-sparse signal models which can not be anymore described by K- sparse signal models Σ K . Since T has the RNMP, there exists by Deﬁnition 1 a subset O ⊂ X × Y such that the representation by T and O leads to a stable embedding of channel outputs T (s, h) received over a ﬁxed but unknown F - sparse channel state h ∈ Y if the difference set of all channel outputs T (s 1 − s 2 , h) is a subset of T (X, Y ).\nTheorem 1: Let 2 ≤ S, F, N, M ∈ N with SF ≤ N and X, Y ⊂ R N are S resp. F dimensional convex cones. If the bilinear map T : X × Y → R N has the restricted norm mul- tiplicativity property with bounds α and β, then a realization of a sub-Gaussian matrix Φ : R N → R M with M ≤ N and [Φ] ij ∼ N (0, 1/M ) fulﬁlls for every z ∈ T (X, Y )\nα) , α = β 12 \t , α = β\n(X + p i )} denotes the covering number of X 1 by the covering sets X . The determination of the covering numbers is a Banach geometrical problem and well studied for various compact subsets of Banach-spaces [10].\nRemark 3: To compute the probability for a mapping Φ which is universal for all L = N S N F ≤ N S+F canonical (S, F ) dimensional convex cone pairs (X, Y ) we apply the union bound technique. Once universal RNMP bounds for all L canonical cone pairs are found, we get the RIP with overwhelming probability for M = O((S + F ) log(N )).\nProof: The main idea follows the technique in [2], where Baraniuk et al. considered a linear subspace Z of R N with a δ/4-net R for Z 1,1 and obtained by the measure concentration phenomenon of Gaussian matrices Φ, that for every r ∈ R and any δ ∈ (0, 1) it holds 2 :\n| Φr − r | ≤ δ 2\nBoth, the constant δ and the dimension of Z determine then the cardinality of R, which is given by the covering number and yields the scaling of the exponential term in (9). This idea can be used again on X and Y as well to get an upper bound\non the cardinality of R, but now in terms of the covering numbers N (X 1 , X δ/d ) and N (Y 1 , Y δ/d ). For this we need to control the norm of z by elements in X × Y which is possible if T has the RNMP, since the set O does not contain \u201dbad\u201d representation pairs for Z := T (X, Y ). It is in fact not necessary to give an explicit parametrization of O. The only information needed for the proof are the bounds α and β.\nEvery normalized z ∈ Z 1,1 can be represented as an element from the image under T of O 1 := { o ∈ O | T (o) = 1} ⊂ X × Y . Since T has the RNMP on O we have by Deﬁnition 1 for (x, y) ∈ O 1 :\nresp. µ −1 > 0 and set ˜ x := x/µ ∈ X 1,1 and ˜ y := µy ∈ Y (X and Y are cones), we get by bilinearity:\nHence there exist a representation set for Z 1,1 by pairs in X 1,1 × Y 1/β,1/α . Since α ≤ 1 ≤ β we can also choose a representation set ˜ O 1 which is contained in the symmetrized set of convex shells X a,b ×Y a,b with common inner and outer radii a := β − 1 2 ≤ 1 ≤ α − 1 2 =: b.\nwhere ˜ O 1,X and ˜ O 1,Y are projections of ˜ O 1 to X resp. Y . These are axial lines in the shells, see Fig. 1. Note, that ˜ O 1 can not be written as a Cartesian product, since only certain pairs are allowed.\nThe algebraic part of the proof goes as follows: Any realization of Φ is a linear map on a ﬁnite\u2013dimensional normed space R N and hence bounded, i.e. there exist A ≥ −1 s.t.\nwhere 1 + A ≥ 0 denotes the smallest upper bound (operator norm of Φ restricted to Z). If we can show that A ≤ δ we have shown the upper bound in (4). Since Φ is linear and Z a cone (not necessarily convex), it is enough to ﬁnd an upper bound in (14) for all z ∈ Z 1,1 . Let P ⊂ X a,b and Q ⊂ Y a,b be -nets for X a,b resp. Y a,b with ∈ (0, 1) and deﬁne R := {T (p, q) : (p, q) ∈ P × Q} ⊂ Z. It follows that |R| ≤ N (X b , X )N (Y b , Y ). Thus, every z ∈ Z 1,1 can now be represented with (13) by a pair (x, y) ∈ X a,b × Y a,b with\nx ∈ X (p) := X + p and y ∈ Y (q) := Y + q. The image T (X (p), Y (q)) is the covering set of the point T (p, q) and the union forms a covering for Z 1,1 by (13). Note that this covering sets in Z are not necessarily convex! By using the triangle inequality and a zero addition in p, q we have for any T (p, q) ∈ R that all z = T (x, y) ∈ T (X (p), Y (q)) ∩ Z 1,1 (i.e. (x, y) ∈ ˜ O 1 ∩ X (p) × Y (q)) satisfy:\n+ Φ(T (x − p, q)) + Φ(T (p, y − q)) . (15) Using the universal bound 1 + A in (14) we obtain:\n+ T (x − p, y − q) \t (16) and since x−p ∈ X and y−q ∈ Y we can apply the universal upper bound β of the RNMP in (2) to get:\nIf we deﬁne the constant c = c(α, β) by: c := β(2b + 1) = β 2/\nα + 1 > 1, \t (18) we obtain the upper bound\nThe main tool of the proof is the measure concentration in (8). But there is no norm nesting T (p, q) since in general (p, q) ∈ ˜ O 1 . Even if (p, q) ∈ ˜ O 1 we don\u2019t get from (13) a tight scaling for vanishing . Therefore we use the continuity property (bilinearity) of T to upper and lower bound\nT (p, q) in terms of for every Cartesian product of two convex covering sets X (p), Y (q). Let us deﬁne the pre- image of T (X (p), Y (q)) ∩ Z 1,1 by\nIf this set is not empty (otherwise (p, q) can be dropped from P × Q), just grap one pair (x, y) ∈ Z −1 (p, q). But then there exist 3 (c, d) ∈ X × Y s.t. (x, y) = (p + c, q + d) and so:\nT (p, q) ≥ T (x, y) − T (x, d) − T (c, y) − T (c, d) ≥ 1 − 2βb − β 2 ≥ 1 − β(2b + 1) = 1 − c\nT (p, q) ≤ T (x, y) + T (x, d) + T (c, y) + T (c, d) ≤ 1 + 2βb + β 2 ≤ 1 + β(2b + 1) = 1 + c .\nLet us discuss the discontinuity of this norm estimation. If we have α = β, hence norm multiplicativity, then we would get c = 3. But in fact, this is to bad, since the shells are now unit spheres and every p, q is normalized and hence by the norm multiplicativity T (p, q). But this gives c = 0. To respect this fact we deﬁne ˜ c and get for all net point pairs\nc , α = β 0 , α = β\nThen we can use the measure concentration in (8) to obtain from (19) and (20) with probability larger than in (9)\nNow, by compactness, there exist a maximal z ∈ Z 1,1 such that equality in (14) is achieved. Hence we get\nLet us proceed by case distinction. If α = β then ˜ c = 0, c = 3. Deﬁning = δ 12 ≤ 1 with δ ∈ (0, 1) we get\nIf we have α = β then ˜ c = c = c(α, β). Deﬁning = δ 7c ≤ 1 with δ ∈ (0, 1) we get from (21):\nα + 1) , α = β 12 \t , α = β\nby considering all z ∈ Z 1,1 we get by inserting (23) and (20) with same probability as in (24)\nThe covering number N (X b , X δ/ ˜ d ) remains the same if we scale both sets X b , X δ/ ˜ d by 1/b =\nα, [10, Lemma 4.16] giving with d := ˜ d/\nBefore using the theorem we will discuss the following observations. A simple coupling T in (1) is given by the pointwise multiplication , e.g. a fading channel:\nT (s, h) = Hs = h s := (h 0 s 0 , . . . , h N−1 s N−1 ) T (28) with diagonal channel matrix H = diag(h). There, we have the norm inequality h s ≤ h s and R N , +, , ·\nbecomes an unital commutative algebra with unit element 1 = (1, . . . , 1) T . Unfortunately, one can not establish a lower norm multiplicativity bound α > 0 on any disjoint convex subsets X, Y ⊂ R N . Hence we can not apply efﬁciently our theorem on these sets. But actually this is not necessary here, since for any S, F dimensional subspaces X, Y the output z has sparsity less than min{S, F } with respect to the Euclidean basis. In this simple case we can immediately apply the original Lemma 5.1 in [2] to establish the RIP on X Y with probability\n> 1 − 2 (12/δ) min{S,F } e −c 0 (δ/2)M . \t (29) This easily extents to pointwise multiplications in another domains, i.e. for some given unitary matrix U we can deﬁne the new product (commutative as well):\nwhich obeys the following 2 -norm inequality: 1\nwith U ∞ := max i,j | e j , Ue i | and s 1 := j | s, e j |. Hence, by commutativity of T , we obtain for any unitary matrix U the upper estimate:\nT (s, h) 2 ≤ N U 2 ∞ min{ s 0 , h 0 } s 2 h 2 . (31) a) Sparse Circular Convolutions: Let us consider multi-\nplication in the frequency domain, i.e. U = F is the Fourier- matrix with [F] lk = e −2πi lk N /\nN for l, k ∈ {0, . . . , N − 1}. Then T = is the circular convolution in the time domain by (30). With F ∞ = 1/\nh s 2 ≤ min{ h 0 , s 0 } h 2 s 2 . \t (32) If s and h are sparse in the Fourier basis, we can proceed as before. But for (S, F )\u2013sparsity in the time domain (with respect to the Euclidean basis) we need RNMP for . We call such a restricted circular convolution an (S, F )-sparse circular convolution, see [1]. In general there doesn\u2019t exists a lower bound α > 0 for sparse circular convolutions, since the nullspace of T can contain elements (s, h) with s = 0 and h = 0. One can prevent this behavior by restricting the domain of T to certain convex sets X, Y ⊂ R N .\nIf we assume the channel is only an on/off channel or a fading channel with positive parameters h i ≥ 0 and the signal parameters are also positive, then we can easily establish the following lower bound:\nThe positive elements in span{e i } i∈I with I ⊂ {0, . . . , N −1} form a canonical (Euclidean) S = |I|-dimensional positive convex cone X. If X and Y are canonical S resp. F dimen- sional positive convex cones, then we can apply our theorem with the norm bounds derived in (33), to establish the RIP on X Y . With a result of Rogers-Zhong [11] and Rogers [12] we can ﬁnd an upper bound for the covering number of S ≥ 3 dimensional positive cones X 1 by N (X 1 , X ) ≤ (4/ ) S 7S log S. For S = 2 we can upper bound this by the covering number with S-dim ˜-balls contained in X , where ˜ = /(2\n2/ ) 2 ˜-balls resp. -cones cover the whole unit ball and hence X 1 . But this number is less than (4/ ) 2 14 log 2 and thus Rogers bound holds also for all S ≥ 2. A rough estimate shows that Rogers bound can be upper bounded for S ≥ 2 by (18/ ) S . Since α = 1 and β = min{S, F } in (33) we get d = ˜ d = 21β with (24). For = δ/d we can hence establish the RIP on X Y by Theorem 1 with probability\nc) Sparse Circular Convolutions and Tensor Products: A main property of the circular convolution is that the image of Euclidean basis vectors (e i , e j ) is again an Eu- clidean basis vector. Let X := span{e i } i∈I and Y := span{e j } j∈J with |I| = S and |J | = F . Then it is easy to show that X Y = span{e k } k∈I⊕J where I ⊕ J := { (i + j) mod N | i ∈ I, j ∈ J }. If |I ⊕ J| = SF then the subspaces X, Y are \u201cproperly\u201d separated [1] and one can show easily that the image is Hilbert isomorph (isometric) to the set of all simple tensor products. This means, there exist isomorphism, s.t. on these extremal pairs (X, Y ) the norm ·\nHere we use the fact, that every simple tensor ˜ s ⊗ ˜ h has a unique representation up to scalars by ˜ s and ˜ h, [13]. This implies a norm multiplicativity, i.e. ˜ s ⊗ ˜ h = ˜ s ˜ h .\nThe covering number for an S dimensional ball X 1 ⊂ R N with X can be upper bounded by N (X 1 , X ) ≤ (3/ ) S [10]. Together with the norm multiplicativity (35), 1 = α = β, we get d = 12 and obtain by Theorem 1 as probability bound\nIn many communication schemes the coupling of channel- and signal parameters is given by bilinear maps T and sparsity is present in both inputs to T . It is therefore of general interest for several engineering applications to characterize the com- pressibility of the whole output set. In this paper we provide such a characterization once the RNMP can be established, uniformly or in some probabilistic setting. However, a uniform treatment of the exemplary case of (S, F )\u2013sparse circular convolutions is still an open problem. It is also not fully understood whether the RNMP is only a sufﬁcient or really a necessary condition for a scaling with O(S +F ). In particular, it is important to know whether to the following conjecture is true: Let (X, Y ) be (S, F )\u2013sparse canonical subspaces in R N (S, F ≥ 2). Does then any realization of a sub-Gaussian matrix Φ : R N → R M with M ≤ N and [Φ] ij ∼ N (0, 1/M ) fulﬁlls for every z ∈ X Y\nwith failure probability p e ≤ 2 (d/δ) S+F e −c 0 (δ/2)M for some ﬁxed constant d > 1 and every δ ∈ (0, 1)?\nThe authors would like to thank Holger Boche and David Gross for their helpful discussion on this topic. This work was partly supported by the Deutsche Forschungsgemeinschaft (DFG) grants Bo 1734/13-1 and JU 2795/1-1&2."},"refs":[{"authors":[{"name":"C. Hegde"},{"name":"R. G. Baraniuk"}],"title":{"text":"Sampling and recovery of pulse streams"}},{"authors":[{"name":"R. G. Baraniuk"},{"name":"M. Davenport"},{"name":"R. DeVore"},{"name":"M. Wakin"}],"title":{"text":"A simple proof of the restricted isometry property for random matrices"}},{"authors":[{"name":"B. Recht"},{"name":"M. Fazel"},{"name":"P. A. Parrilo"}],"title":{"text":"Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization"}},{"authors":[{"name":"E. J. Candes"},{"name":"T. Tao"}],"title":{"text":"The power of convex relaxation: Near-optimal matrix completion"}},{"authors":[{"name":"D. Gross"}],"title":{"text":"Recovering low-rank matrices from few coefﬁcients in any basis"}},{"authors":[{"name":"R. Baraniuk"},{"name":"M. Wakin"}],"title":{"text":"Random projections of smooth manifolds"}},{"authors":[{"name":"M. F. Duarte"},{"name":"Y. C. Eldar"}],"title":{"text":"Structured compressed sensing: From theory to applications"}},{"authors":[{"name":"R. Baraniuk"},{"name":"V. Cevher"},{"name":"M. Wakin"}],"title":{"text":"Low-dimensional models for dimensionality reduction and signal recovery: A geometric perspective"}},{"authors":[{"name":"R. Arens"},{"name":"M. Goldberg"},{"name":"W. Luxemburg"}],"title":{"text":"Multiplicativity factors for seminorms. II"}},{"authors":[{"name":"G. Pisie"}],"title":{"text":"The volume of convex bodies and Banach space geometry"}},{"authors":[{"name":"C. A. Rogers"},{"name":"C. Zong"}],"title":{"text":"Covering convex bodies by translates of convex bodies"}},{"authors":[{"name":"C. Rogers"}],"title":{"text":"A note on coverings"}},{"authors":[{"name":"W. Greu"}],"title":{"text":"Multilinear algebra"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566321.pdf"},"links":[{"id":"1569566799","weight":4},{"id":"1569565227","weight":12},{"id":"1569567005","weight":4},{"id":"1569564469","weight":16},{"id":"1569565123","weight":4},{"id":"1569566497","weight":20},{"id":"1569564337","weight":4},{"id":"1569566167","weight":24},{"id":"1569552251","weight":4},{"id":"1569565559","weight":12},{"id":"1569566913","weight":20},{"id":"1569566245","weight":4},{"id":"1569557715","weight":4},{"id":"1569566983","weight":4},{"id":"1569566873","weight":12},{"id":"1569552037","weight":8},{"id":"1569565353","weight":8},{"id":"1569552025","weight":8},{"id":"1569566715","weight":4},{"id":"1569566755","weight":12},{"id":"1569565425","weight":8},{"id":"1569565529","weight":4},{"id":"1569566001","weight":4},{"id":"1569565367","weight":4},{"id":"1569564281","weight":4},{"id":"1569567691","weight":8},{"id":"1569566635","weight":12},{"id":"1569566611","weight":12},{"id":"1569566125","weight":20},{"id":"1569566825","weight":4},{"id":"1569566443","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T9.4","endtime":"16:00","authors":"Philipp Walk, Peter Jung","date":"1341330000000","papertitle":"Compressed Sensing on the Image of Bilinear Maps","starttime":"15:40","session":"S7.T9: Compressive Sensing","room":"Stratton West Lounge (201)","paperid":"1569566321"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
