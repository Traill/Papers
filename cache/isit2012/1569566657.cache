{"id":"1569566657","paper":{"title":{"text":"High-Rate Sparse Superposition Codes with Iteratively Optimal Estimates"},"authors":[{"name":"Andrew R. Barron"},{"name":"Sanghee Cho"}],"abstr":{"text":"Abstract\u2014 Recently sparse superposition codes with iterative term selection have been developed which are mathematically proven to be fast and reliable at any rate below the capacity for the additive white Gaussian noise channel with power control. We improve the performance using a soft decision decoder with Bayes optimal statistics at each iteration, followed by thresholding only at the ﬁnal step. This presentation includes formulation of the statistics, proof of their distributions, numerical simulations of the performance improvement, and useful identities relating a squared error risk to a posterior probability of error."},"body":{"text":"Sparse superposition codes use a dictionary X consisting of vectors X 1 , X 2 , . . . , X N , each of n coordinates. The codeword vectors Xβ are superpositions β 1 X 1 + β 2 X 2 + . . . + β N X N . The entries of X are independent N(0, 1). The codeword is conveyed through the choice of which L of the coefﬁcients are non-zero, where L matches n to within a log factor, yet L is a small fraction of the dictionary size N . For a channel with additive white Gaussian noise (AWGN), with superposition coding, what is received is Y = Xβ + ε, a vector of length n, where ε is the noise vector with distribution N(0, σ 2 I).\nThe coefﬁcient vector is split into L sections each of size M = N/L, with one non-zero entry in each section. There are M L codewords. With M a power of 2, the encoding from an input bit-string u 1 , u 2 , . . . , u K , with K = L log M , consists of partitioning the string into L substrings of length log M which index the terms chosen to be included in the codeword. Denote these indices {j 1 , j 2 , . . . , j L }. The non-zero coefﬁcient from section takes value β j =\nP , with \t P = P to control the codeword power.\nThe rate of the code is R = (L log M )/n and the capacity of the AWGN channel is C = (1/2) log(1+snr) where snr = P/σ 2 is the signal-to-noise ratio.\nThese sparse superposition codes with an adaptive succes- sive decoder are computationally fast codes for the Gaussian noise channel with any ﬁxed rate below capacity, with error probability proven to be exponentially small. See [1], [2], [3] for this conclusion and the relationship to other literature on compressive sensing, signal recovery, and coding for the Gaus- sian channel. The adaptive successive decoder uses iteratively obtained test statistics related to inner products of the X j with residuals of the previous ﬁts. There, for each ﬁt update, the decoder accepts those terms j for which the statistic is above a threshold, chosen high enough to avoid false alarms.\nThe conditional distribution of such statistics is approxi- mated as a normal random variable shifted, for the true terms compared to the others, by an amount inversely related to\nthe squared distance between the current ﬁt and the true coefﬁcient vector. Accordingly, we seek improved estimates of the coefﬁcients, using the squared error loss, to increase the separation between the distributions of the statistics and thereby improve the reliability of the ﬁnal decision.\nWe use the Bayes optimal coefﬁcient estimates based on the distribution of iteratively obtained statistics, with a uniform prior on the choice of the j sent from each section. These estimates use computed posterior weights of terms in each section. These weights provide a soft decision decoding, rather than the {0, 1} valued weights associated with thresholding.\nWe formulate the statistics, quantify their distribution, and give identities that relate the expected sum of squared distance (Bayes risk) of the estimate to an expected posterior probabil- ity of error. Taking 1 minus it quantiﬁes the rate of success.\nA function g(x) gives the expected success rate on a step if the previous success rate was x. Numerical evaluations show it is higher than the corresponding function from the threshold- based decisions. Indeed, g(x) stays above x for a longer extent of the interval [0, 1] than previously obtained, leading to a smaller fraction of mistakes. As before, this remaining fraction of mistakes are corrected by an optional outer code.\nTo get rates approaching capacity, a variable power alloca- tion is used, with P proportional to e −2 C /L .\nFor R < C, theoretically optimal codes have exponentially small error probability with exponent of order (C −R) 2 n. So far, fast sparse superposition codes in [1], [2], [3] achieve comparable exponents of order (C−R) 2 L, within a logarithmic factor of the optimal form. More speciﬁcally, the bounds on the error probability have the exponents (C M − R) 2 in place of (C−R) 2 , where C M slowly approaches the capacity C, and the bounds are applicable only for R < C M . This motivates additional effort, initiated here, to improve the rate and reli- able tradeoff of sparse superposition codes by improving the adaptive successive decoder.\nIn the process of estimating which terms were sent, the decoder develops a sequence of estimates ˆ β k of the true coefﬁcient vector β, and corresponding ﬁts F k = X ˆ β k of the codeword Xβ. Let J = {1,. . ., N } be the full set of indices.\nThe initial estimate uses stat 0,j = Z 0,j = X T j Y / Y and later estimates use similar inner products with residuals of ﬁts in place of Y . The distribution of stat 0,j is approximately a standard normal shifted by β j n/(σ 2 + P ). The σ 2 + P in the denominator is from the variance of the coordinates\nof Y . The idea of the steps is to use residuals of successive ﬁts to gradually reduce it to σ 2 . This increases the amount by which the distribution is shifted, thereby improving the distinguishability of the true terms from the others.\nDeﬁne the shift factor α = P n/(σ 2 +D). The shift in section takes the form α 1 {j=j } . Initially D 0 = P . For subsequent steps, the role of D is played by ˆ β k − β 2 or its expected value D k = E ˆ β k − β 2 , which quantiﬁes for the statistics we develop the level of remaining interference in the residuals due to inaccuracy of ˆ β k . The associated shift factor is α ,k = P n/(σ 2 +D k ).\nLet ˆ β k be any estimate constructed from statistics stat k−1 = (stat k−1,j , j ∈ J ) computed on the previous step. For instance stat k−1,j could be the inner products of residuals with the columns of X. Initialize G 0 = Y . For k ≥ 1, let F k = X ˆ β k and let G k be the part of the F k orthogonal to G 0 , G 1 , . . . , G k−1 . Assume the current ﬁt X ˆ β k is not in the linear span of the previous ﬁts, so G k > 0. Let Z k,j = X T j G k / G k be the normalized inner product of X j and G k .\nThe Z k = (Z k,j , j ∈ J ) and G k are used to update stat k and then ˆ β k+1 . Require that stat k and ˆ β k+1 be functions of F k = (Z 0 , G 0 , . . . , Z k , G k ). Our ﬁrst lemma provides the conditional distribution of Z k and G k given F k−1 . For k = 0 there is no conditioning F k−1 .\nFor analysis purposes, let β e , ˆ β 1,e ,. . ., ˆ β k,e be the vectors in R N +1 obtained by appending an extra coordinate to the vectors β, ˆ β 1 , . . . , ˆ β k in R N . The value of the extra coordinate for β e is σ and for the ˆ β 1,e , . . . , ˆ β k,e it is 0. The subscript e denotes that the vectors are thus extended.\nLikewise X e denotes an extended dictionary with an ad- ditional column ε/σ. Armed with this extension we have opportunity to use a standard linear model trick representing Y = X e β e . Then the G 0 , G 1 , . . . , G k−1 are the successive orthogonal components of X e β e , X e ˆ β 1,e , . . . , X e ˆ β k−1,e .\nParallel to the development of these vectors G in R n , let b 0,e , b 1,e , . . . , b k,e be deﬁned as the vectors in R N +1 of successive orthonormalization of β e , ˆ β 1,e , . . . , ˆ β k,e and let b 0 ,b 1 , . . . , b k , respectively, be the vectors formed from the corresponding upper N coordinates.\nLet Σ k,e = I − (b 0,e b T 0,e + b 1,e b T 1,e + . . . + b k,e b T k,e ) be the R (N +1)×(N +1) matrix of projection onto the linear space orthogonal to β e , ˆ β 1,e , . . . , ˆ β k,e . The upper left N ×N portion of this matrix denoted Σ k is the conditional covariance matrix below. The suggestion to interpret Σ k as a portion of a projection matrix was made by our colleague Antony Joseph, who credits [4],[5] for some analogous thinking.\nAlso let P roj k be the matrix of projection onto the span of the estimates ˆ β 1 , . . . , ˆ β k , and likewise P roj k,e in which 0 is appended to each of these estimates. Σ k,e differs from I − P roj k,e by accounting for orthogonality to β e .\nLemma 1, proved in the appendix, generalizes conclusions from [3], [2] to handle the present generality.\nLemma 1: For k ≥ 0, the conditional distribution P Z k |F k−1 of Z k given F k−1 is determined by the representation\nwhere Z k = (Z k,j : j ∈ J ) has conditional distribution Normal(0, Σ k ). Here σ 2 0 = σ 2 + P and for k ≥ 1 it is σ 2 k = ˆ β T k Σ k−1 ˆ β k . Moreover, G k 2 /σ 2 k is distributed as a X 2 n−k random variable independent of the Z k and the past F k−1 .\nRelated to the distribution P Z k |F k−1 is the distribution Q Z k |F k−1 which makes the Z k be Normal(0, I − P roj k ). The density ratio between P Z k |F k−1 and Q Z k |F k−1 on R N is uniformly bounded by the constant\nThe Chi-square random variable divided by n is close to the constant 1, except in events of exponentially small probability, as long as the number of steps k is small compared to n. Thus Z k is approximately √ n b k + Z k , a normal shifted by √ n b k . The distribution is further cleaned by addition of an indepen- dent normal of covariance P roj k . This makes the cleaned Z k be distributed N(0, I) with respect to Q. Moreover, as in [2], the boundedness of the density ratio permits replacement of the distribution P with the simpliﬁed distribution that arises from Q, when determining events that have exponentially small probability. Henceforth for this summary we presume the cleaned shifted standard normal distribution for the Z k .\nConsider Z comb k \t = λ k,0 Z 0 − k k =1 λ k,k Z k , where the vector λ k = (λ k,k : 0 ≤ k ≤ k) satisﬁes k k =0 λ k,k = 1. These can be interpreted as shifts of the standard normals Z comb k \t = λ k,0 Z 0 − k k =1 λ k,k Z k , where the shift arises from combinations of the\nk . The task is to choose the coefﬁcients of combination to produce a stat k with total shift of the desired form.\nMotivation comes from the statistics (Y − X ˆ β k,−j ) T X j , or scalings thereof, where ˆ β k,−j is the vector ˆ β k with the contribution from the current j removed. It takes the form (Y − X ˆ β k ) T X j + X j 2 ˆ β k,j . We also ﬁnd motivation from development of approximate Bayes optimality properties. The stat k take the following form, for some choice of vector λ k and some c k typically between σ 2 and σ 2 + P ,\n√ n √ c\nThe combination should be such that these statistics have the representation Z comb k \t +\nHere are three related examples of such statistics. Idealized case [B] has the desired form and case [C] approximately so. Case [A] is similar, but has additional randomness from weights based on Z T k ˆ β k /\nn rather than b T k ˆ β k . [A] Based on residuals: Let\n√ n √\n[B] Idealized: Based on coefﬁcients of orthogonal compo- nents of the ˆ β k , with λ k proportional to\n√ n √\nfor which, in each section , the shift factor is of the desired form α with D k = β − ˆ β k 2 . It suffers from de- pendence of the weights of combination on the unknown β. The b T k ˆ β k depend on β T ˆ β k , for k = 1, 2, . . . , k.\n[C] Simpliﬁed: As in [B] but with each occurrence of β T ˆ β k replaced by its known expected value.\nThe β T ˆ β k is close to its expected value, indeed, within any speciﬁed small positive η, except in an event of probability exponentially small in Lη 2 . This is a consequence of Hoeffd- ing\u2019s inequality, interpreting β T ˆ β k as an average of L bounded independent random variables. Likewise, the ˆ β k − β 2 is close to its expectation, permitting the approximation to its distribution as a shifted normal using D k = E ˆ β k − β 2 in deﬁning the shift factor α ,k as before.\nConsider the choice of the updated coefﬁcient estimates ˆ β k+1 as a function of stat k . We arrange these to be the Bayes optimal posterior mean of β given stat k , as derived here. Use the approximating distribution that the stat k,j are independent Normal α 1 {j=j } , 1 , for j in any section , where α = α ,k . Let φ(s) be the standard normal density and note that φ(s − µ)/φ(s) is proportional to e µs . With the term j chosen according to a uniform distribution over the M choices in section , the posterior distribution of j is\nP 1 j =j . Accordingly, the posterior mean of β j provides the Bayes estimator, E[β j |stat k ] = j ∈sec w k,j\nAt the ﬁnal step, in each section, the decoded term ˆ j may be taken to be the one the highest posterior weight w k,j . The posterior probability of success in a section is the posterior weight of the true term w k,j .\nThe quantity ˆ β T k+1 β/P can be interpreted as a posterior suc- cess rate L =1 (P /P ) w k,j , with a power-weighted average across the sections.\nLemma 2: The posterior success rate has the same ex- pectation as the squared norm ˆ β k+1 2 /P . Consequently, the posterior error rate L =1 P (1 − w k,j ) has the same expectation as the squared distance ˆ β k+1 − β 2 .\nThe proof of Lemma 2 is in the appendix. D. Update Function and its Analysis\nAnalysis of the progression of the adaptive successive decoder is considerably simpliﬁed if one ﬁnds a recursively updated measure of success. The progress may be tracked using either the expected squared distance D k = E ˆ β k − β 2 or the expected posterior success rate which we will denote x k .\nThe above results show that these two quantities are related by D k = (1 − x k )P permitting their recursion as follows.\nConsider, for any realization j 1 , j 2 , . . . , j L , the next step ex- pected posterior success rate x k+1 = L =1 (P /P ) E[w k,j ]. This expected value is the same no matter which j 1 , j 2 , . . . , j L was chosen, so assume here that the ﬁrst term in each section was sent. Accordingly, at α = α ,k ,\nwhere Z 1 , Z 2 , . . . , Z M are independent N(0, 1). What makes this a recursive characterization of progress is that α ,k is a function of the preceding x k via its relationship to the expected squared distance. Indeed, α ,k = α (x k ) where α (x) is\nwhere w 1 (α) = (e α 2 +αZ 1 )/ e α 2 +αZ 1 + M j=2 e αZ j . We ini- tiate investigation of this g(x) and compare it to the corre- sponding update function that arose from the {0, 1} valued weights of the thresholding method. As in [1], [2], [3], it is given by g {0,1} (x) = L =1 (P /P )Φ(α (x) − τ a ), where τ a =\n2 log M + a is the threshold. In that scheme a > 0 is needed to avoid false alarms.\nFor the algorithm to update properly, we need x k+1 > x k where x k+1 = g(x k ). Thus it is desired to have g(x) stay above x (the 45 degree line). In [1], [2], [3], it is conﬁrmed that, for any ﬁxed rate below the capacity, g {0,1} (x) stays above x in an interval [0, x ∗ ], where x ∗ is near 1, though the gap from 1 was of order 1/ log M . We evaluate g(x) to study the performance of the soft decision decoder and to compare it with the {0, 1} valued decoder. Of interest is whether the crossing point x ∗ is moved substantially closer to 1.\nFig. 1 shows our update function with rates at two different fractions of capacity. Observe that, in both cases, the update function is above x on the most of the interval [0, 1]. The step function in the gap in Fig. 1 shows progression of the steps. The gap between g(x) and x affects the number of steps to arrive at a success rate near x ∗ . [Dan Spielman has suggested there is similarity of our use of the function g(x), which is for adaptive successive decoding of sparse superposition codes, with the EXIT charts of [7], used in the study of iterative decoders of turbo codes.]\nA simpliﬁed lower-bound on g(x) is obtained via Jensen\u2019s inequality by replacing the M j=2 e αZ j in the denominator above by its expectation (M −1)e α 2 /2 .\nFig. 2 evaluates different update functions. The highest is g(x) of the new procedure. It is much higher than g {0,1} (x) at realistic thresholds (e.g. a = 1/2) and yet still higher than g {0,1} (x) with the unrealistic idealized threshold a = 0. In the realistic case (a = 1/2) the g {0,1} (x) fails to allow rates at 80% of capacity (for M = 2 9 and snr = 7) because its curve drops below x at a value far from 1. In [1], [2] good performance at reasonable rates required a much larger section size, such as M = 2 16 . In contrast, the new decoder is successful at 80% of capacity with the smaller section size.\nThe value δ = 1 − x ∗ bounds the likely fraction of mistakes of the ﬁnal step of the decoder. It controls the closeness to 1 of the rate of an outer Reed-Solomon code that corrects the remain errors (as described in [1], [2]). Our goal in further research is to establish whether the order of the error 1 − x ∗ is superior to the order 1/ log M established in [2], [3].\nFig. 3 considers the success rate g (x) = E[w j (α (x))] as a function of the section index . It shows an increasing wave of closeness to 1 as x increases.\nAfter a suitable number of steps, the decoder will succeed if the weights w k,j are large enough. It is recommended on the ﬁnal step to decode the sections for which the maximal w k,j is at least 1/2. In contrast, when max j∈sec w k,j < 1/2, the posterior probability of error is more likely than not. In that case it is recommended to leave the section undecoded as an erasure to be corrected by the outer R.S code.\nProof of Lemma 1: Consider the representation of the collec- tion of vectors X j , for 1 ≤ j ≤ N , augmented by one additional vector X N +1 = ε/σ. The Z k ,j = X T j G k / G k for k < k are the coefﬁcients of the representation of X j in the span of the orthonormal G 0 / G 0 , . . . , G k−1 / G k−1 , with an or- thogonal residual vector V k,j , for j in J e = {1, . . . , N, N +1}. Collecting these into a matrix decomposition, it takes the form\nwhere the vectors Z k = (Z k ,j : j ∈ J ) extend to Z k ,e = (Z k ,j : j ∈ J e ) when representing X e .\nUsing these G 0 , G 1 , . . . , G k−1 and the columns of the iden- tity, Gram-Schmidt ﬁlls out a basis of R n with n orthornormal vectors ξ k,0 , ξ k,1 , . . . , ξ k,n−1 , in which the residuals V k,j have representation n−1 i=k V k,j,i ξ k,i , using the last n − k of these orthonormal vectors, with V k,j,i = V T k,j ξ k,i .\nWith the columns of X e assumed to be independent standard normal vectors, we solve for the evolution of the conditional distributions of the Z k,e and G k , using the above representation. The conditional distribution of the Z k,e and G k given F k−1,e = (Z 0,e , G 0 , . . . , Z k−1,e , G k−1 ) has X 2 n−k = G k 2 /σ 2 k distributed chi-square(n − k) and Z k,e = b k,e X n−k + Z k,e with Z k,e distributed N(0, Σ k,e ). The conclusion of the lemma then follows from noting for the Z k that the conditional distribution given F k−1,e only depends on F k−1 , under the assumption that successively the estimates ˆ β k are computed only from the information F k−1 available to the decoder (without knowledge of the noise).\nMoreover, it is claimed that conditionally given F k−1,e , the coordinates V k,j,i of the vectors V k,j in the basis ξ k,i , for i = k, k + 1, . . . , n − 1, are conditionally mean-zero Normal random variables, independent across i, and jointly across j ∈ J e , having covariance Σ k−1,e [where for k = 0 the Σ k−1,e is replaced by the identity matrix].\nThe number of columns is arbitrary. Henceforth in the proof there is no need to make a distinction between the cases with and without the extension, so drop the subscript e.\nProve this claim inductively on k ≥ 0. Initially, V 0,j = X j and the normality of the X j provides for the validity of the distributional claim for V k,j for k = 0. For the induction, assume the claim to be true at step k and derive from it that it is true at the next step k + 1. Along the way, the conditional distribution properties of the G k and Z k in the lemma are established as consequences.\nConcerning G k , note G 0 2 /σ 2 0 is X 2 n distributed. For k ≥ 1, the G k as the part of X ˆ β k orthogonal to the previous parts G 0 , . . . , G k−1 is equal to G k = V k ˆ β k = j ˆ β k,j V k,j since V k is the part of X with columns orthogonal to the previous parts. Representing G k in the basis ξ k,0 , . . . , ξ k,n−1 it has coordinates G k,i equal to 0 for 0 ≤ i ≤ k − 1 and equal to j V k,j,i ˆ β k,j for k ≤ i ≤ n − 1. From the induction hypothesis, these (V k,j,i : j ∈ J ) have conditional distribution Normal(0, Σ k−1 ). Accordingly, these G k,i are independent\nNormal(0, σ 2 k ) where σ 2 k = ˆ β T k Σ k−1 ˆ β k , from which it follows that G k 2 /σ 2 k is X 2 n−k distributed, independent of F k−1 .\nNext, for each j, seek b k,j as a regression coefﬁcient based on the joint distribution of the V k,j and G k (given F k−1 ) to obtain the representation of the vectors\nThis is done in the basis ξ k,k , . . . , ξ k,n−1 where the co- ordinates V k,j,i and G k,i are jointly normal (where across i = k,. . ., n−1 they are independent and identically distributed, conditionally given F k−1 , so they share the same regression coefﬁcient b k,j ). The coordinates of U k,j,i are conditionally normal random variables, independent of the G k,i , and in- dependent for k ≤ i ≤ n − 1. For k = 0 the coefﬁcient b k,j = E[V k,j,i G k,i /σ k ] simpliﬁes to E[X j,i Y i /σ Y ] = β j /σ Y .\nFor k ≥ 1 the b k,j = E[V k,j,i G k,i /σ k ] may be expressed as E[V k,j,i j V k,j ,i ˆ β j ] where the expectation is with respect to the Normal(0, Σ k−1 ) distribution for the (V k,j,i : j ∈ J ). Accordingly, summarize the solution for these coefﬁcients as the vector b k = Σ k−1 ˆ β k /σ k .\nAs for the parameters of the distribution of the (U k,j,i : j ∈ J ), use the identity U k,j,i = V k,j,i − b k,j G k,i /σ k and the conditional distribution of the V and G coordinates to conclude that it has mean 0 and conditional variance Σ k−1 − b k b T k , in agreement with Σ k .\nNote that Z k,j = X T j G k / G k reduces to V T k,j G k / G k , which by the above representation of V k,j takes the form\nThe latter term is what we call Z k,j . The inner product is preserved by switching to the basis ξ k,0 , . . . , ξ k,n−1 . Thus Z k,j = n−1 i=0 α i U k,j,i , with α i = G k,i / G k , which is 0 for 0 ≤ i ≤ k − 1. The sum of squares of the α i is equal to 1. Proceed conditionally on F k−1 . For any ﬁxed α with sum of squares equal to 1, the n−1 i=k α i U k,j,i shares the N(0, Σ k ) distribution, as a result of the independence across i. Accordingly, with α i = G k,i / G k , the conditional distribution of Z k given G k is as indicated, and it does not depend on G k , so the Z k and G k are independent given F k−1 .\nUse G k to update the orthonormal basis of R n by Gram-Schmidt, replacing ξ k,k , ξ k,k+1 , . . . , ξ k,n−1 with G k / G k , ξ k+1,k+1 , . . . , ξ k+1,n−1 .\nThe coefﬁcients of U k,j in this updated basis are U T k,j G k / G k , U T k,j ξ k+1,k+1 , . . . , U T k,j ξ k+1,n−1 , which are denoted U k+1,j,k = Z k,j and U k+1,j,k+1 , . . . , U k+1,j,n−1 , respectively. Recalling the conditional distribution of the U k,j , these coefﬁcients (U k+1,j,i : k ≤ i ≤ n − 1, j ∈ J ) are also normally distributed, conditional on F k−1 and G k , indepen- dent across i from k to n − 1; moreover, for each i from k to n − 1, the (U k+1,j,i : j ∈ J ) inherit a joint N (0, Σ k ) conditional distribution from the conditional distribution that the (U k,j,i : j ∈ J ) have.\nSpecializing the conclusion, separating off the i = k case where the U k+1,j,i is Z k,j , the remaining (U k+1,j,i : k +1 ≤\ni ≤ n, j ∈ J ) have the speciﬁed conditional distribution and are conditionally independent of G k and Z k given F k−1 . It follows that the conditional distribution of (U k+1,j,i : k+1 ≤ i ≤ n − 1, j ∈ J ) given F k = (F k−1 , G k , Z k ) is identiﬁed.\nLikewise, the vector V k,j = b k,j G k /σ k + U k,j has repre- sentation in this updated basis with coefﬁcient Z k,j in place of Z k,j and with V k+1,j,i = U k+1,j,i for i from k+1 to n − 1. So these coefﬁcients (V k+1,j,i : j ∈ J ) have the normal N (0, Σ k ) distribution for each i, independently across i from k + 1 to n, conditionally given F k . Thus the induction is established, which completes the proof of Lemma 1.\nProof of Lemma 2: The random variables in question are sums across the sections. We show equality of the expectations in each section. Fix a section and a step k and let stat = (stat k,j : j ∈ sec ) be the relevant part of the statistics, with index set sec regarded as {1, 2, . . . , M }.\nThe random variables in question have the same expected value no matter which terms j was sent. Accordingly, the expectation taken conditionally on any particular realization j match what is obtained if alternatively one averages with respect to the uniform prior on j . Let P j = P stat|j =j be the conditional distributions and P = (1/M ) M j=1 P stat|j =j be the marginal distribution of stat in section , and likewise let E j and E, respectively, denote corresponding expectations of functions of stat. Now w j = w k,j is the posterior probability P [j = j|stat]. Show that w j and w 2 = M j=1 w 2 j have the same expectation.\nThe P j and P have likelihood ratio M w j . Set j = 1. Calcu- late the expectation E 1 [w 1 ] using the measure P rather than P 1 by incorporating the factor M w 1 . Thus E 1 [w 1 ] = M E[w 2 1 ]. By symmetry, E[w 2 j ] is same across all j and so M E[w 2 1 ] equals E[ M j=1 w 2 j ] = E[ w 2 ] which is (1/M ) M j=1 E j [ w 2 ]. Each term in this sum is the same so it is E 1 [ w 2 ] as claimed. This completes the proof of Lemma 2.\nSpace does not permit full listing and discussion of the relationship of sparse superposition codes to past work in information theory, compressive sensing, and coding. For such the reader is invited to see the discussion and reference lists in [1], [2], [3], [6]."},"refs":[{"authors":[{"name":"R. Barro"},{"name":"A. Josep"}],"title":{"text":"A"}},{"authors":[{"name":"R. Barron"},{"name":"A. Joseph"}],"title":{"text":"Sparse superposition codes: Fast and reliable at rates approaching capacity with Gaussian noise"}},{"authors":[{"name":"R. Barron"}],"title":{"text":"Analysis of fast sparse superposition codes"}},{"authors":[{"name":"M. Bayati"},{"name":"A. Montanari"}],"title":{"text":"The dynamics of message passing on dense graphs, with applications to compressed sensing"}},{"authors":[{"name":"M. Bayati"},{"name":"A. Montanari"}],"title":{"text":"The LASSO risk for gaussian matrices"}},{"authors":[{"name":"A. Joseph"},{"name":"R. Barron"}],"title":{"text":"Least squares superposition codes of moderate dictionary size are reliable at rates up to capacity"}},{"authors":[{"name":"S. ten Brink"}],"title":{"text":"Convergence behavior of iteratively decoded parallel concatenated codes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566657.pdf"},"links":[{"id":"1569565883","weight":11},{"id":"1569565867","weight":11},{"id":"1569565551","weight":11},{"id":"1569565461","weight":11},{"id":"1569555999","weight":22},{"id":"1569564441","weight":11},{"id":"1569566425","weight":11},{"id":"1569554971","weight":11},{"id":"1569566223","weight":11},{"id":"1569566245","weight":11},{"id":"1569566949","weight":11},{"id":"1569565665","weight":11},{"id":"1569566983","weight":22},{"id":"1569565093","weight":11},{"id":"1569565661","weight":11},{"id":"1569566737","weight":11},{"id":"1569565353","weight":11},{"id":"1569566547","weight":11},{"id":"1569566595","weight":11},{"id":"1569566755","weight":22},{"id":"1569564437","weight":11},{"id":"1569565529","weight":11},{"id":"1569566397","weight":11},{"id":"1569565769","weight":11},{"id":"1569567691","weight":22},{"id":"1569565861","weight":22},{"id":"1569562367","weight":11},{"id":"1569565997","weight":22},{"id":"1569561397","weight":11},{"id":"1569565143","weight":11},{"id":"1569566825","weight":11},{"id":"1569566443","weight":22},{"id":"1569566727","weight":11},{"id":"1569565315","weight":22}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S1.T7.1","endtime":"10:10","authors":"Andrew R Barron, Sanghee Cho","date":"1341222600000","papertitle":"High-Rate Sparse Superposition Codes with Iteratively Optimal Estimates","starttime":"09:50","session":"S1.T7: Gaussian Channels","room":"Stratton (407)","paperid":"1569566657"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
