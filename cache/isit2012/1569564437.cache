{"id":"1569564437","paper":{"title":{"text":"Mutual Information for a Deletion Channel"},"authors":[{"name":"Michael Drmota"},{"name":"Wojciech Szpankowski"},{"name":"Krishnamurthy Viswanathan"}],"abstr":{"text":"Abstract\u2014We study the binary deletion channel where each input bit is independently deleted according to a ﬁxed probability. We relate the conditional probability distribution of the output of the deletion channel given the input to the hidden pattern matching problem. This yields a new characterization of the mutual information between the input and output of the deletion channel. Through this characterization we are able to comment on the the deletion channel capacity, in particular for deletion probabilities approaching 0 and 1."},"body":{"text":"A deletion channel with parameter d takes a binary se- quence x := x n 1 = x 1 · · · x n where x i ∈ A = {0, 1} as input and deletes each symbol in the sequence independently with probability d. The output of such a channel is then a subsequence Y = Y (x) = x i 1 ...x i M of x, where M follows the binomial distribution Bi(n, (1 − d)), and the indices i 1 , ..., i M correspond to the bits that are not deleted. Despite signiﬁcant effort [2], [3], [5], [9], [10], [11], [12], [14] the mutual information between the input and output of the deletion channel and its capacity are still unknown. Our goal is to provide a more detailed characterization of the mutual information for memoryless sources (extensions to strongly mixing sources or Markovian sources seem likely). Through this characterization we are able to comment on the channel capacity for two special cases: d → 1 and d → 0. The latter case was already discussed in [10], [9]. We derive our results by relating the the conditional probability distribution of the output of the deletion channel given the input to the so called hidden pattern matching analyzed recently in [1], [7].\nFollowing [4], the channel capacity of the deletion channel with deletion probability d is\nwhere P X n 1 is the distribution of X n 1 , and I(X n 1 ; Y (X n 1 )) is the mutual information between the input and output of the deletion channel. Many bounds have been derived for the capacity (see the survey article by Mitzenmacher [11]).\nLet x = x n 1 ∈ {0, 1} n and w = w 1 w 2 . . . w m ∈ {0, 1} m , m ≤ n, be binary sequences. Let Ω x (w) denote the number of occurrences of w as a subsequence (i.e., not consecutive symbols) of x, that is,\nwhere I A = 1 if A is true and zero otherwise. The problem of counting subsequences in a text is known as the hidden pattern matching problem and was studied in [1], [7]. In this paper, to derive our results we ﬁrst represent the mutual information between the input and output of a deletion channel in terms of the count Ω X (w) for a random sequence X.\nTheorem 1. For any random input X n 1 , the mutual informa- tion satisﬁes\nFrom Theorem 1, we have I(X n 1 ; Y (X n 1 )) \t = S 1 (X n 1 , Y (X n 1 )) − S 2 (X n 1 , Y (X n 1 )) := S 1 − S 2 where\nIn this paper, we focus on memoryless distributions on X n 1 , however, it appears that most of our results extend to larger classes (Markovian). Suppose that X 1 X 2 . . . is an i.i.d. se- quence of Bernoulli random variables with parameter p. For such sequences, let I(d, p) = lim\n; Y (X n 1 )), and λ(d, p) = lim n→∞ 1 n S 1 (X n 1 , Y (X n 1 )).\nTheorem 2. For all 0 ≤ d ≤ 1, and 0 ≤ p ≤ 1, the limit I(d, p) as well as the non-negative limits λ(d, p) and\nwhere, H(·) is the binary entropy function. Further- more, I(d, p) = inf n≥1 1 n I(X n 1 ; Y (X n 1 )), and λ(d, p) = sup n≥1 1 n S 1 (X n 1 , Y (X n 1 )).\nFrom Theorem 2, I(d, p) ≤ I(X 1 1 ; Y (X 1 1 )) = H(p)(1 − d). When optimized over p, this upper bound matches the capacity asymptotically for d → 0 but not for d → 1, as our next result (Theorem 3) shows. This also implies that λ(d, p) ≤ H(1−d). Note that for d → 1 it is just known that C(d) = Θ(1 − d) [2],\n[11], [12]. Our next result is a bound on I(d, p) that implies that, in contrast to the case d → 0, i.i.d. distributions over the inputs X n 1 do not asymptotically achieve capacity as d → 1.\nFinally we demonstrate the strength of our method by re- proving Kanoria and Montanari\u2019s [10] expansion for I(d, p) for d → 0 leading to C(d) = I(d, 1/2) + O(d 3/2−ε ) = 1 + d log d − Ad + O(d 3/2−ε ) (cf. Theorem 4), where A = log(2e) − ≥1 2 − −1 log . Note that the symmetric mem- oryless distribution is asymptotically optimal in this regime.\nIn this section, we ﬁrst prove Theorem 1 and then present a simple proof of the fact that C(d) ≤ 1 − d. A. Proof of Theorem 1\nTo prove Theorem 1, we relate hidden pattern matching to the deletion channel through the following observation. For all\nWe use X and Y to abbreviate X n 1 and Y (X n 1 ) respectively. Using (5), we will compute H(Y ) and H(Y |X) and use I(X; Y ) = H(Y ) − H(Y |X) to prove the theorem. We ﬁrst compute H(Y ). Observe that, from (5) P (Y = w) =\nNext, we compute the conditional entropy H(Y |X). Notice that for x ∈ A n and y ∈ A m we have P (x, y) = P (x)Ω x (y)d n−m (1 − d) m . Combining this with (5) we obtain\nIt is well known that the capacity C(d) of a deletion channel with deletion probability d can be bounded from above by the capacity of an erasure channel with the erasure probability d (e.g., see [3]). We provide a direct proof of this fact. To do so, we ﬁrst compute the expectation of Ω X (w).\n|w| P n (w), where\nwith |w|=m P n (w) = 1. In particular, if X is memoryless, then P n (w) = P (w) where P (w) is the probability that X 1 X 2 ...X |w| = w (see [1] for dynamic X).\nLemma 2. For any distribution on the input binary ran- dom sequence X n 1 , and and deletion probability d ≥ 0, I(X n 1 ; Y (X n 1 )) ≤ n(1 − d).\nProof: Following Theorem 1, we can write I(X n 1 ; Y (X n 1 )) = S 1 − S 2 where S 1 and S 2 are deﬁned in (3)\u2013(4). Since Ω X (w) ≤ n |w| we ﬁrst have\nSince for all m ≥ 0, P n (w) is a probability distribution over w ∈ A m , we have |w|=m P n (w) log(1/P n (w)) ≤ log 2 m = m, and consequently\nSubstituting this in (8) completes the proof, and also estab- lishes an upper bound of C(d) ≤ 1 − d for the capacity.\nWe now restrict the channel input distributions to be mem- oryless over A with p denoting the probability of \u201c0\u201d. We prove Theorems 2 and 3 in this section.\nwhere the sum is taken over all pairs w 1 , w 2 such that their concatenation w 1 w 2 equals w.\nLemma 4. Let z m and a m , 1 ≤ m ≤ M , be non-negative numbers. Then we have\nLemma 5. Let X 1 X 2 . . . be a memoryless random binary sequence. Then\nwhere the last equality follows holds as α(w 1 ) and β(w 2 ) are independent. Let now c n = I(X n 1 ; Y (X n 1 )). Then, by Theorem 1\n(and a similar relation for the sum over w 2 ) we immediately derive c n+k ≤ c n + c k . Note that we have used the property that X n 1 and X n+k n+1 are independent and that X n+k n+1 has the same distribution as X k 1 .\nIn particular, I(d, p) ≤ 1 n I(X n 1 ; Y (X n 1 )) for all n ≥ 1. If we apply this for n = 1, 2 we ﬁnd\nwhere q = 1−p. For example, by looking at the second bound we observe that sup 0≤p≤1 I(d, p) ≤ 1−d 2 + (1 − d) 2 which implies that memoryless input distributions do not meet the general upper bound 1−d when d → 1. Actually we will show that sup 0≤p≤1 I(d, p) is much smaller as d → 1 (Theorem 3).\nWe now prove Theorem 2. As above, we write I(X n 1 ; Y (X n 1 )) = S 1 − S 2 . Also, given two sequences a n and b n , a n ∼ b n if a n /b n → 1 as n → ∞.\nLemma 6. If X n 1 is a memoryless binary sequence with parameter p, then S 2 ∼ n · (H(1 − d) − (1 − d)H(p)) as n → ∞.\nProof: By Theorem 1 and Lemma 1, and by the trivial observation |w|=m P (w) = 1, we have\nThe second term above can be computed directly. By the deﬁnition of the entropy we have |w|=m P (w) log P (w) = −mH(p). Consequently,\nIn order to evaluate the ﬁrst term we apply the results of [6], [8] about the so called binomial sums. Notice that\nThe next step is to show a similar property for S 1 , namely that S 1 ∼ n · λ(d, p), where λ(d, p) is a non-negative constant. The problem is to obtain some information about λ(d, p), but for this we would need precise information about the behavior of Ω X (w).\nLemma 7. Suppose that X 1 X 2 . . . is a binary memoryless sequence and a n = S 1 (X n 1 , Y (X n 1 )). Then a n+k ≥ a n + a k .\nThe superadditivity property of Lemma 7 provides the following convergence result.\nLemma 8. If X = X n 1 is a binary memoryless sequence, then there exists a non-negative constant λ(d, p) ≤ H(1 − d) such that S 1 ∼ n · λ(d, p) as n → ∞.\nProof: Since Ω X (w) is a non-negative integer we cer- tainly have S 1 ≥ 0. Furthermore, since Ω X (w) ≤ n |w| it follows (as in the proof of Lemma 6) that\nBy another application of Fekete\u2019s lemma [13] the sequence a n /n has a limit that equals the supremum sup(a n /n). We have used the property a n+k ≥ a n + a k here.\nThe proof of Theorem 2 is a combination of Lemma 6 and Lemma 8. The lower bound on λ(d, p) follows from the fact that I(d, p) ≥ 0.\nRemark (Extension to Mixing Sources): Most results of this section hold for more general distributions. For example, from the proof of Lemma 6 we conclude that\nwhere ¯ P is the limit of ¯ P n which was deﬁned in Lemma 1 (provided the limit exists). A distribution P (X n 1 ) is said to correspond to a strongly mixing source [13] if for all m ≤ n, there exist constants c 1 , c 2 such that\nFor such distributions, Lemma 7 generalizes to a n+k ≥ a n + a k + K 1 for some constant K 1 , hence Lemma 8 holds as well.\nWe consider the expression in (2). We ﬁrst note that the empty word does not contribute to the sum (2). Next we consider words of length 1. If w = 0 and if X = X n 1 consists of m zeroes and n − m ones then Ω X (w) = m. The situation\nis completely symmetric if w = 1. Hence the contribution of words of length 1 to I(X n ; Y (X n )) is\nmlog m n m\nm log m n m\n= log(np)np + npq np\nPutting all parts together we obtain that T 1 ≤ d n−1 (1 − d) ≤ (1 − d).\nLet T 2 denote the subsum of (2) corresponding to those terms with |w| ≥ 2. By using the trivial estimate Ω X (w) ≤\n≤ 2d n log n n(1 − d) d\nIf n(1 − d) = o(1) this leads to T 2 ≤ C 1 n 2 (1 − d) 2 log n for some absolute constant C 1 > 0. Summing up and using Corollary 1, we obtain that\nI(d, p) ≤ 1 n\nFinally by choosing n = (1 − d) −1/3 we derive the upper bound\nFinally, we comment on the case d → 0 that has been already solved in [10] and [9] where it is shown that I(d, 0.5) = 1 + d log d − Ad + O(d 2−ε ) as d → 0 and C(d) = I(d, 0.5)+O(d 3/2−ε ). The approach presented in [10] is quite different from ours. However, we can use our methods to obtain corresponding bounds. In particular, we easily obtain the following lower bound for I(d, p).\nProof: The lower bound for I(d, p) follows from ideas similar to those in the proof of Theorem 2. Instead of taking the limit of a n /n deﬁned in Lemma 7 we derive lower bounds for a n /n for certain n. We will only consider words w with |w| = n − 1. Then\nSuppose for the moment that w has the form w = 0 i 1 1 j 1 0 i 2 1 j 2 · · · 0 i K 1 j K , where i r , j r ≥ 1; this means that w 1 = 0 and w n−1 = 1 (the other cases can be handled in completely the same way). If |w| = n − 1, then we have Ω X (w) = (for some > 2) if and only if there exists r with\nj r = − 1 and X = 0 i 1 1 j 1 · · · 0 i r 1 j r +1 0 i r+1 · · · 0 i K 1 j K . Hence, by expanding E[Ω X (w) log Ω X (w)],\nwhere i r (w) denotes the length of the r-th 0-run in w and j r (w) the length of the r-th 1-run in w. Now let Z be a new random variable deﬁned on words w of length n − 1 as Z = Z(w) = r≥1 (pI [i r (w)= −1] + qI [j r (w)= −1] ). Then we just have to compute the expected value\nRecall that the expected value E I [i r = −1] = P[i r = − 1] has to be computed according the probability distribution of word W (of length n − 1).\nNext, note that the probability distribution of the length- k 0-run is given by p k q/(1 − q) = p k−1 q and that the number of runs in a string of length n is approximately pqn. Consequently\nE[Z] ∼ npq pp −2 q + qq −2 p and ﬁnally\nNow we choose n = d −ε which ensures that (1−d) n−1 = 1 + O(d 1−ε ). From the deﬁnition of λ(d, p) and (3), this implies that\nSince H(1 − d) = −d log d − (1 − d) log(1 − d) = −d log d + d log(e) + O(d 2 ) we obtain the lower bound (12).\nFor the upper bound we proceed as in the proof of Theo- rem 3. We start with S 1 . Let S 1,n−1 denote the subsum of S 1 corresponding to words of length n − 1. Then it follows from the above calculations that S 1,n−1 = O(nd) (actually we can be much more precise). Furthermore, it follows as in the proof of Theorem 3 that S 1 − S 1,n−1 = O log n d 2 n 2 if dn → 0. Finally, for S 2 we have (see Lemma 6)\n= (1 − d)H(p)−(1 − d) n−1 d log n + O(d) + O log n d 2 n = H(p) + d log d + O(d log log(1/d))."},"refs":[{"authors":[{"name":"J. Bourdon"},{"name":"B. Vall´ee"}],"title":{"text":"Generalized pattern matching statistics"}},{"authors":[{"name":"M. Dalai"}],"title":{"text":"A new bound for the capacity of the deletion channel with high deletion probabilities"}},{"authors":[{"name":"S. Diggavi"},{"name":"M. Grossglauser"}],"title":{"text":"Information transmission over ﬁnite buffer channels"}},{"authors":[{"name":"L. Dobrushin"}],"title":{"text":"Shannon\u2019s theorem for channles with synchronization errors."}},{"authors":[{"name":"A. Iyengar"},{"name":"P. Siegel"},{"name":"J. Wolf"}],"title":{"text":"Modeling and information rates for synchronization error channels"}},{"authors":[{"name":"P. Flajolet"}],"title":{"text":"Singularity analysis and asymptotics of Bernoulli sums"}},{"authors":[{"name":"P. Flajolet"},{"name":"W. Szpankowski"},{"name":"B. Vall´ee"}],"title":{"text":"Hidden word statistics"}},{"authors":[{"name":"P. Jacquet"},{"name":"W. Szpankowski"}],"title":{"text":"Entropy computations via analytic depoissonization"}},{"authors":[{"name":"A. Kalai"},{"name":"M. Mitzenmacher"},{"name":"M. Sudan"}],"title":{"text":"Tight asymptotic bounds for the deletion channel with small deletion probabilities"}},{"authors":[{"name":"Y. Kanoria"},{"name":"A. Montanari"}],"title":{"text":"On the deletion channel with small deletion probability"}},{"authors":[{"name":"M. Mitzenmacher"}],"title":{"text":"A survey of results for deletion channels and related synchronization channels"}},{"authors":[{"name":"M. Mitzenmacher"},{"name":"E. Drinea"}],"title":{"text":"A simple lower bound for the capacity of the deletion channel"}},{"authors":[{"name":"W. Szpankowski"}],"title":{"text":"Average case analysis of algorithms on sequences"}},{"authors":[{"name":"R. Venkataramanan"},{"name":"S. Tatikonda"},{"name":"K. Ramchandran"}],"title":{"text":"Achievable rates for channels with deletions and insertions"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564437.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S14.T6.4","endtime":"18:00","authors":"Michael Drmota, Wojciech Szpankowski, Krishnamurthy Viswanathan","date":"1341510000000","papertitle":"Mutual Information for a Deletion Channel","starttime":"17:40","session":"S14.T6: Applications of Codes in Cryptography","room":"Kresge Rehearsal A (033)","paperid":"1569564437"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
