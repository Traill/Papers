{"id":"1569565889","paper":{"title":{"text":"Achieving Csisz´ar\u2019s Source-Channel Coding Exponent with Product Distributions"},"authors":[{"name":"Adri`a Tauste Campo"},{"name":"Gonzalo Vazquez-Vilar"},{"name":"Albert Guill´en i F`abregas"},{"name":"Tobias Koch"},{"name":"Alfonso Martinez"}],"abstr":{"text":"Abstract\u2014We derive a random-coding upper bound on the average probability of error of joint source-channel coding that recovers Csisz´ar\u2019s error exponent when used with product distributions over the channel inputs. Our proof technique for the error probability analysis employs a code construction for which source messages are assigned to subsets and codewords are generated with a distribution that depends on the subset."},"body":{"text":"We study the problem of transmitting a length-k discrete memoryless source over a discrete memoryless channel using length-n block codes. The source is distributed according to P V (v) = k i=1 P V (v i ), v = (v 1 , . . . , v k ) ∈ V k , where V is a discrete alphabet with cardinality |V|. The channel law is given by P Y |X (y|x) = n i=1 P Y |X (y i |x i ), x = (x 1 , . . . , x n ) ∈ X n , y = (y 1 , . . . , y n ) ∈ Y n , where X and Y are discrete alphabets with cardinalities |X | and |Y|, respectively.\nAn encoder maps the length-k source message v to a length- n codeword x(v), which is then transmitted over the channel. We refer to the ratio t k/n as the transmission rate. Based on the length-n channel output y the decoder guesses which source message was transmitted.\nWe say that an error exponent E > 0 is achievable if there exists a sequence of codes of length n such that the average error probability (averaged over all source messages v) is upper-bounded as\nwhere o(n) satisﬁes lim n→∞ o(n)/n = 0. The error exponent E J of joint source-channel coding is deﬁned as the supremum of all achievable error exponents E.\nLower bounds on the error exponent are often derived by drawing an ensemble of codebooks at random, and by then analyzing the average error probability ¯, averaged over all codebooks in the ensemble. This computation ensures the existence of at least one codebook in the ensemble whose average error probability is at most ¯ [1, Sec. 5.5].\nIn [1, Prob. 5.16], Gallager provides an upper bound on ¯ using maximum a posteriori (MAP) decoding when the codewords corresponding to different source messages are drawn independently according to some distribution P X :\nThe upper bound (2) is derived using similar techniques as Gallager\u2019s channel coding bound [1, p. 135]. In the context of discrete memoryless systems and for P X being a product distribution P X (x) = n i=1 P X (x i ), x ∈ X n , it specializes to\nThus, for a ﬁxed t, the probability of error ¯ vanishes exponen- tially in n with the error exponent given by E 0 (ρ, P Y |X , P X )− tE s (ρ, P V ). By minimizing (5) over P X and ρ, we obtain the following lower bound on the error exponent E J :\nCsisz´ar reﬁned this result using the method of types [2]. Csisz´ar\u2019s approach is different from Gallager\u2019s in several ways. Firstly, Csisz´ar considers ﬁxed composition codes rather than codes that are generated by product distributions \u2013 such codes are constructed by mapping messages within a source type onto sequences within a channel-input type. Secondly, a suboptimal maximum mutual information decoder is used at the receiver. This decoder ﬁrst decides on the source type that is being transmitted and then on the source message within the\ntype. Csisz´ar\u2019s code construction yields the following lower bound on the error exponent:\nwhere e(R, P V ), referred to as the source reliability function [3]\u2013[5], is given by\nwith D(· ·) denoting the divergence, and with E r (R, P Y |X ) denoting the random-coding channel exponent [1]\nA lower bound on the error probability of the best code induces an upper bound on E J . One such upper bound is given by the sphere-packing expoment [2, Lemma 2]\nIt can be checked that E J = E Cs J when the minimum on the right-hand side (RHS) of (11) is attained for a value of R such that E sp (R, P Y |X ) = E r (R, P Y |X ). This holds for values of R above the critical rate of the channel [1].\nIn order to obtain a clearer comparison between (6) and (7), Zhong et al. [6] provide a compact formulation of Csisz´ar\u2019s result. Speciﬁcally, the authors invoke the Fenchel duality theorem [7, Thm. 31.1] to rewrite (7) as\nwhere ¯ E 0 (ρ, P Y |X ) denotes the concave-hull of E 0 (ρ, P Y |X ), deﬁned as the pointwise inﬁmum over the family of afﬁne functions that upper-bound E 0 (ρ, P Y |X ) as a function of ρ ∈ [0, 1] [7, Cor. 12.1.1]. It follows from (12) that E Cs J ≥ E G J , with the inequality possibly being strict.\nIn cases where the above inequality is strict, the difference between E Cs J and E G J is typically small [6]. The methods used to derive each exponent are conceptually different, raising a number of questions. In this paper, we address the question of whether ﬁxed composition codes are necessary in order to achieve Csisz´ar\u2019s exponent. We show that random codes gen- erated by product distributions together with MAP decoding and bounding techniques based on Markov\u2019s inequality can be used to recover Csisz´ar\u2019s exponent, answering the question in the negative.\nThe derivation of the main results involves the following steps:\n1) Deﬁne a partition P k of the message set V k into N k disjoint subsets A 1 , . . . , A N k satisfying N k i=1 A i = V k . We shall refer to these subsets as classes.\n2) Assign a channel input distribution P (i) X to each class A i . Then, for each source message v ∈ A i randomly and independently generate codewords x(v) ∈ X n according to P (i) X .\n3) Upper-bound the probability of error using MAP decod- ing and Gallager\u2019s bounding techniques [1].\nTheorem 1: For every partition P k , for every set of channel- input distributions P (1) X , . . . , P (N k ) X , and for every set of parameters ρ 1 , . . . , ρ N k ∈ [0, 1], the random-coding error probability is upper-bounded by\nThe bound (14) can be optimized over product distributions P (i) X (x) = n j=1 P (i) X (x j ), x ∈ X n , and parameters ρ i ∈ [0, 1] for i = 1, . . . , N k to obtain\nIf the partition P k only has one class, i.e., A 1 = V k for k = 1, 2, . . ., the upper bound (16) recovers Gallager\u2019s bound on the error exponent (6):\nAs we shall see next, with a more judicious choice of P k the upper bound (16) also recovers Csisz´ar\u2019s lower bound on the error exponent (7).\nSpeciﬁcally, (7) can be achieved by identifying the classes A 1 , . . . , A N k with the source-type classes T 1 , . . . , T N k . A source-type class T i is deﬁned as the set of all source messages v ∈ V k with type P i [3, Def. 2.1]. Thus, for a given distribution P i on V, the source-type class T i is the set of all source messages v ∈ V k satisfying\nwhere N (a|v) denotes the number of occurrences of a ∈ V in v.\nCorollary 1: Let the classes A 1 , . . . , A N k of the partition P k be the source-type classes T 1 , . . . , T N k . Then\nBy the type counting lemma [3, Lemma 2.2], there are at most (k+1) |V| different source-type classes. Thus, Corollary 1 demonstrates that partitions P k with not more than (k + 1) |V| classes are sufﬁcient to achieve Csisz´ar\u2019s error exponent. In fact, we have recently showed that Csisz´ar\u2019s error exponent can already be achieved with partitions P k consisting of two classes [8].\nThe random-coding error probability ¯ is upper-bounded by the random-coding union (RCU) bound [9], [10]\n    \n   \n    \n   \nLet {T 1 , . . . , T M k } be the set of all source-type classes in V k . Furthermore, let {T 1 (y), . . . , T M n (y)} be the set of all V-shells of y in X n , where the V-shell T m (y) is deﬁned as the set of all channel inputs x ∈ X n with conditional type V m given y ∈ Y n [3, Def. 2.4]. Thus, for a given conditional type V m given y ∈ Y n , the V-shell T m (y) is the set of all channel inputs x ∈ X n satisfying\nwhere N (a, b|x, y) denotes the number of occurrences of (a, b) ∈ X × Y in (x, y). Note that, by the type counting lemma,\n    \n   \n    \n   \nIt is easy to show that P V (·) is constant within every source- type class T and that P Y |X (y|·) is constant within every V- shell T m (y) of y. This allows us to deﬁne the metric\nfor every y ∈ Y n and = 1, . . . , M k , m = 1, . . . , M n . We further deﬁne\nWe now focus on the last double sum term in (28), which can be upper-bounded as\nfor every y ∈ Y n and = 1, . . . M k , m = 1, . . . , M n . Here (29) follows from the fact that not every x satisfying\nmust be in T m (y), and (30) follows from summing over the entire message set A i and a larger codeword set. Combining (31) with (28) and (21) yields\nwhere in (34) we have used the inequality min{1, x + y} ≤ min{1, x}+min{1, y}, x, y ≥ 0, and in (35) we have used the inequality min{1, x}y ≤ min{1, x}x + min{1, y}y, x, y ≥ 0. By rearranging the terms in (35), we obtain\nfor a, s i > 0. Using that min{1, x} ≤ x ρ i for x ≥ 0 and ρ i ∈ [0, 1], and choosing s i = 1 1+ρ\nBy applying (40) with a = d(y, , m) to (36), we ﬁnally obtain\nTheorem 1 follows then by upper-bounding M k and M n using (24).\nLet the classes A 1 , . . . , A N k of the partition P k be the source-type classes T 1 , . . . , T N k . We ﬁrst note that, by the type counting lemma,\nsince P V (·) is constant within each source-type class A i = T i . Let V i be a random variable whose distribution is the type\nH(V i ) denotes the entropy of V i ), we have the following inequalities [3, Lemmas 2.3 & 2.6]:\nwhere in (48) we have used the deﬁnitions of R i and of the source reliability function (8).\nUsing (44)\u2013(48), and using that t = k/n, we can upper- bound (17) as\nwhere in (50) we have used the deﬁnition of the random- coding channel exponent (10), and where (51) follows from minimizing the exponent over all possible values of R i .\nUsing the deﬁnition of h(k) in (14) and the bound (43), we have that\nSince E r R, P Y |X + te (R/t, P V ) is a decreasing function of R for 0 < R ≤ tH(V ), it follows that the RHS of (53) is equal to the RHS of (7), thus proving Corollary 1.\nOne of the strengths of Gallager\u2019s error bound (2) is that it can be easily generalized to nondiscrete channels without resorting to limiting arguments applied to ever-ﬁner quantiza- tions of X and Y.\nWhile in the derivation of our new bound we mostly used the same techniques as Gallager, there are some steps that rely on the method of types. In particular, to analyze (22), we partitioned V k into source-type classes and X n into V-shells of y ∈ Y n . Nevertheless, these partitions were merely introduced to simplify the analysis and are not essential. Indeed, Csisz´ar\u2019s\nerror exponent can also be obtained by partitioning V k × X n into κ + 1 sets of the form\n(54) for every y ∈ Y n , where κ is a linear function of n, and where α 0 = 0, α = e −nγ e −1 , = 1, . . . , κ, and β = e −nγ e ,\nFollowing the arguments in Sections III-A while treating S 0 and S κ separately we obtain the upper bound\nwhere X i is a random variable with distribution P (i) X , for i = 1, . . . , N k . By judiciously choosing γ and κ, and by max- imizing (55) over product distributions, we recover Csisz´ar\u2019s error exponent.\nA lower bound on E J for nondiscrete channels follows along the same lines by replacing the channel law P Y |X in (54) with the corresponding Radon-Nikodym derivative f Y |X .\nWe have presented an upper bound on the random-coding error probability for joint source-channel coding that recovers Gallager\u2019s and Csisz´ar\u2019s lower bounds on the error exponent for discrete memoryless systems. Thus, the new expression gives the actual error exponent at least in the cases where Csisz´ar\u2019s exponent is tight.\nThe method to obtain the new bound uses a speciﬁc random- coding construction with MAP decoding. Speciﬁcally, we partition the message set into disjoints classes and assign to each class an input distribution according to which the codewords are randomly generated.\nBy partitioning the message set into source-type classes, and by choosing for each class the input distribution to be a product distribution, the new bound on the error probability recovers Csisz´ar\u2019s lower bound on the error exponent, answering the question of whether ﬁxed composition codes are required to achieve Csisz´ar\u2019s exponent in the negative."},"refs":[{"authors":[{"name":"R. G. Gallage"}],"title":{"text":"Information Theory and Reliable Communication"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"Joint source-channel error exponent"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems , 2nd ed"}},{"authors":[{"name":"R. E. Blahut"}],"title":{"text":"Hypothesis testing and information theory"}},{"authors":[{"name":"F. Jeline"}],"title":{"text":"Probabilistic Information Theory"}},{"authors":[{"name":"Y. Zhong"},{"name":"F. Alajaji"},{"name":"L. L. Campbell"}],"title":{"text":"On the joint source-channel coding error exponent for discrete memoryless systems"}},{"authors":[{"name":"R. T. Rockafella"}],"title":{"text":"Conjugate duality and optimization"}},{"authors":[{"name":"A. Tauste Campo"},{"name":"G. Vazquez-Vilar"},{"name":"A. Guill´en i F`abregas"},{"name":"T. Koch"},{"name":"A. Martinez"}],"title":{"text":"Random coding bounds that attain the joint source- channel exponent"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"H. V. Poor"},{"name":"S. Verd´u"}],"title":{"text":"Channel coding rate in the ﬁnite blocklength regime"}},{"authors":[{"name":"A. Tauste Campo"},{"name":"G. Vazquez-Vilar"},{"name":"A. Guill´en i F`abregas"},{"name":"A. Mar- tinez"}],"title":{"text":"Random-coding joint source-channel coding bounds"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565889.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T4.2","endtime":"10:30","authors":"Adrià Tauste Campo, Gonzalo Vazquez-Vilar, Albert Guillén i Fàbregas, Tobias Koch, Alfonso Martinez","date":"1341396600000","papertitle":"Achieving Csiszár's Source-Channel Coding Exponent with Product Distributions","starttime":"10:10","session":"S9.T4: Joint Source-Channel Codes","room":"Stratton 20 Chimneys (306)","paperid":"1569565889"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
