{"id":"1569566839","paper":{"title":{"text":"Design Principles and Speciﬁcations for Neural-like Computation Under Constraints on Information Preservation and Energy Costs as Analyzed with Statistical Theory"},"authors":[{"name":"William B Levy ∗"},{"name":"Toby Berger \u2020"}],"abstr":{"text":"Abstract\u2014Given enough physical constraints, the format of optimal computation may resolve into a rather small set of options, which we call design speciﬁcations. Our interest cen- ters on computational problems that are so intensive, relative to the time and energy available, that they can be solved only in a probabilistic fashion. Here we consider just information and energy in one particular computational format, called neural-like (NL), and characterized as massively parallel, analog computation. Within this format, we consider only the design of a single NL element and the nature of its inputs. Importantly, we provide a speciﬁc mathematical format of a simple NL element. We consider this format to be minimal and generic and, therefore, extendable to structures composed of several NL compartments. Secondly, the information and energy constraints are linked, via Shannon\u2019s entropy, to classical results from mathematical statistics yielding design speciﬁcations that go beyond our initial description of a NL element and its inputs. Critically, for a NL element to preserve all of its relevant input-information at minimal energetic cost, it must transform its inputs so as to create and communicate a minimal sufﬁcient statistic. Then, the assumptions associated with producing such a statistic become new design speciﬁcations for NL computing."},"body":{"text":"In contrast to conventional computing (e.g., Turing- Church limited on a von Neumann architecture) whose underlying computations are boolean-algebraic in nature, we are interested in problems so computationally intensive, relative to two physical constraints, that such problems must be solved in terms of probabilities (e.g., high- dimensional pattern recognition, decision-making, and con- trol); the two physical constraints are (i) the available time and (ii) the available energy. Here we consider just energy and establish the importance of one computational format for the simplest element of a massively parallel, analog computational device, where such a device is most easily described by the appellation neural-like (NL). Speciﬁcally, we will analyze the transformation performed by a simple\nNL element (call it j) in terms of the relevant information- preservation and in terms of the energetic costs.\nGenerically characterized, NL computation uses many asynchronous, analog, computational elements, with large fan-in and fan-out between elements. Both communication between elements and computation within each element has associated costs. Starting with communication between elements, we assume constant amplitude pulses are used over the lossy transmission lines which connect individual elements. Arguably, under this regime of constant am- plitude pulses, the most energy-efﬁcient form of com- munication is continuous-time differential pulse position modulation in which each pulse signals the end of an interpulse interval (ipi) and the beginning of the next such interval. Thus, we have our ﬁrst random variable relative to j\u2019s transformations, the interval T j (k) , the time between pulse k-1 and pulse k.\nThroughout most of this work, until we generalize certain results, we will concern ourselves with the simple NL element j which receives inputs on lines indexed by i and, like j, each of these input lines is delivering pulses, X i ( S) = 1 for a single example, where s denotes a point in continuous time. Driving these input lines is a random la- tent process Λ j ( s) ∈(0,∞). This latent process expresses it- self as the parameter of a Poisson process driving j\u2019s inputs. (This Poisson process is actually an approximation; see Berger and Levy, 2010.) It is j\u2019s problem to communicate something about the states of this latent variable. Instead of the impossible task of communicating a continuous stream of real numbers, element j will communicate information concerning the average value (possible weighted) of this latent variable over an interval deﬁned exactly as a single ipi. In particular, Λ j (k) := t −1 k K(t k − S)Λ j ( S)dS where the integration ranges from the beginning of an ipi to t k (its end), and K(t k − S) is a, ﬁxed, causal (t k > S ≥ 0 ) function with K (0 ) = 0 that is bounded and continuous function of t k − S with ﬁnite derivatives at all arguments\nAn externally arriving pulse is transformed into an internal function h i (τ n i , τ k ) = h(τ k − τ n i ) that is scaled by an amplitude that is dependent on i; here, h(u) is a bounded, continuous function that equals 0 for u ≤ 0 and has ﬁnite derivatives at all u > 0. h(τ k − τ n i ) is known when τ k , τ n i are known. τ n i is the arrival time of the n th i\npulse during interval k, and τ k (≥τ n i ) is the time elapsed within the interval. The scaled h(τ k − τ n i )\u2019s are combined to produce j\u2019s internal excitation, Y j (k,τ k ); thus τ k ∈ [0, t k ). For the k th ipi the transformations are summarized as Λ j (k) → X j (k) → Y j (k) → T j (k) where k implies the full k th ipi, [0,t k ), and where X j (k) is the list of all pulse times for each input line i during the k th interval.\nThe internal excitation is converted into a pulse via a ﬁrst passage time to a deterministic threshold-function θ(k,τ k ) that may or may not vary with time. In any event, knowledge of the ﬁrst passage time t k implies the value of the internal excitation, Y j (k), at this time point. Thus communication of t k to a recipient that has knowledge of the threshold-function is equivalent to communicating {t k , Y j (k)}⇔{t k , Y j (k)/t k }.\nTo complete the model we add band limited noise, thermal and ampliﬁer associated shot-noise which total η(k) and which is in addition to the random Poisson-like arrivals of the individual excitations. For the purpose of mathematical analysis, we associate this noise, with and proportionately apportion it among the individual events. That is, we include noise henceforth via the substitution h i (t k − τ n i ) ← (1 − η n i )h i (t k − τ n i ).\nAt this point we want to consider j as taking sam- ples in order to make some inference about Λ j (k) . Call this set of samples S(k):= {h i (t k − τ n i ): 0 ≤τ n i ≤ t k , n i ∈ {0, 1, ...}}: i∈ {1,...,N}} or ∅ if the preceding set is empty, and denote one of its canonical realizations by s. Now the visualization of the input and j\u2019s transformations is Λ j (k) → X j (k) → S (k) → Y j (k) → T j (k). For now, the function that combines the events in S(k) to produce Y j (k) is an open question.\nWe have deﬁned computation by a simple NL element as a series of transformations. Our design goal is to preserve the Λ j (k) -relevant information at minimum cost, where cost is average energy expended. We will use, and slightly extend via Shannon\u2019s entropy, results from mathematical statistics to infer additional design requirements that are necessary to achieve this pair of goals.\nThe main results in terms of additional speciﬁcations are (1) a probability model of Λ j (k) must be in the exponential class, (2) conditional on Λ j (k) =λ, the relationship between j \u2019s inputs must well-approximate conditional independence, and (3) Y j (k) must be formed additively from the individual events in S(k). These results will be inferred from the preceding deﬁnitions together with the facts that (a) the NL element must use a sufﬁcient statistic in order to preserve the Λ j (k) -relevant information and (b) that representational costs (e.g., states of Y j (k) or states of T j (k)) will be minimized for such a sufﬁcient statistic only when the statistic is a minimal sufﬁcient statistic. Finally, additivity arises from Dynkin\u2019s result concerning minimal sufﬁcient statistics of ﬁxed dimension. Corollaries will be described\nthat extend the results to include a simple NL element that performs Bayesian inference and an extension to non- simple NL elements.\nFully consistent with the NL element, one can consider S(k) to be a data-sampling consisting of the observable events generated by the random process Λ j (k) and noise; moreover, as a reduction of these data, Y j (k) is a statistic. As a design goal, we require that the statistic Y j (k) should contain all the Λ j (k) -relevant information arriving at j. The statistic that preserves all the information from a sampling is called a sufﬁcient statistic (Fisher, 1922; deﬁned below).\nA sufﬁcient statistic can only occur when there is a family of probability models. First, an essentially zeroth- order design speciﬁcation that arises along with these probability models is that any such model is analyt- ically tractable. Speciﬁcally, analytical tractability here shall mean that these distributions must be continuous probability densities that are suitably differentiable and positive everywhere between their endpoints; furthermore, these densities must have a support that is both ﬁxed and known. The densities (or function for any case where normalization is not possible) under discussion are joint densities and associated marginal and conditional densities, e.g.,p(S(k) = s|Λ j (k) = λ), p(Λ j (k) = λ|S(k) = s), p(S(k) = s|Y j (k) = y), inter alia. (Note: t k is implicitly given as a conditioning variable whenever k appears in a probability.)\nDeﬁnition : A statistic Y j (k) (≡Y j (S(k)) ≡Y j (S([0,t k )) ) is sufﬁcient for the model distributions of the data, p(S (k) |Λ j (k) = λ) (where all the distributions of the model reside in the range of λ), if the data conditioned both on Λ j (k)= λ and on Y j (k) = y, are distributed the same way as when they are conditioned only on the value of Y j (k) . In our notation, the requirement that qualiﬁes Y j (k) as a sufﬁcient statistic is\nVia the deﬁnition of a conditional probability, an equivalent requirement is\nGiven this deﬁnition, Koopman (1936) places p(S(k)|Λ j (k) = λ) in the exponential family if and only if there is a sufﬁcient statistic. Furthermore, this result can be combined with a near simultaneous observation by Fisher and Neyman (cited in Kagan et al., 1973) concerning the factorization of the likelihood that must occur when there is a sufﬁcient statistic and the range of Λ j (k) is ﬁxed: p(s|λ) = g(y|λ)φ(s) where g(y|λ) contains no terms of s and φ(s) contains no terms of λ and y.\nTheorem 1 (Scalar parameter form) : Given the above assumptions concerning the ﬁxed support of λ and the other regularity conditions needed (see e.g. Koopman 1936 or Barankin and Maitra, 1963), and given that λ is a scalar,\nthen p(S (k) |Λ j (k) = λ) is in the exponential family if and only if there is a sufﬁcient statistic, e.g., Y j (k) = y, for any value of the parameter λ. That is, the existence of a sufﬁcient statistic and the given regularity conditions require that, for all λ,\n= exp(a(λ) · b(y) + c(S (k)) + d(λ)) \t (3) where the functions a(λ) and d(λ) do not contain terms of the data, h i (τ n i , t k ), and the functions b(S(k)) and c(S(k)) do not contain terms of λ. (Note that d(λ) is the normalization term and may require recalculating in some of the results that follow although we do not modify its explicit form.)\nAt this point let\u2019s turn our attention to the average energy expenditure (cost) and consider a subclass of the sufﬁcient statistics, the minimal sufﬁcient statistics. To relate information-preservation and energy-expenditures, we must complement statistical theory with information- theoretic considerations.\nConsider some space of events deﬁned by sample paths of a random process that has a probability model consistent with the existence of a sufﬁcient statistic for the random variables Y j (k) and T j (k) . Suppose Y j (k) is any minimal sufﬁcient statistic on this space while Y j + (k) is any non- minimal sufﬁcient statistic for this same space and model and that all the sufﬁcient statistics are truncated to some arbitrary precision:\nLemma (encoding inequality): The entropy of a minimal sufﬁcient statistic Y j (k) is strictly less than the entropy of Y j + (k) . (This result will be proved elsewhere where the critical insight arises from a fundamental characterization of entropy; i.e., partitioning an event of positive probability into two distinct events of positive probability increases the entropy of the event space.)\nRelating an information-preserving minimal sufﬁcient statistic to energy costs hinges on an exchangeability of information and energy. Assume a constant minimum cost for any information that is encoded by j. At the joules-per- bit limit, this cost is known: repeating the sense-write-erase cycle so that a bit of information per cycle (i.e., per pulse) is generated (remember information is an average), has a minimum cost of kTlog e 2 joules per bit (fractions of a bit scale accordingly; T. Sagawa and M. Ueda, 2009; just here, k is Boltzmann\u2019s constant and T is absolute temperature). Moreover, even if j does not work at this limit, assume construction of a NL element so that every one of j\u2019s transformations operates at a ﬁxed multiple of this joules- per-bit limit. Then, combining a ﬁxed cost-per-bit with the encoding lemma implies\nLemma (cost minimization): (i) A statistic that preserves information and minimizes cost (average energy use) must be in the equivalence class of minimal sufﬁcient statistics. To say it another way, (ii) failure to use a minimal sufﬁcient statistic leads to information loss and/or excessive cost.\nProof: The ﬁrst part follows from the encoding inequality and the fact that every extra bit produces extra cost. The second part follows from the ﬁrst in light of the deﬁnition of a sufﬁcient statistic.\nThus, the design speciﬁcation is now extended to require creation of a minimal sufﬁcient statistic in the transforma- tions performed by j. Furthermore, a minimal sufﬁcient statistic has a special form when the assumptions of Theorem 1 hold. That is, additivity of singleton events from the sample space is required as the functional form of b(y) when Y j (k) is a minimal sufﬁcient statistic (Dynkin (1950) or see Zacks (1971)). This additivity result follows from independent sampling so this independence (conditional on λ) is also a new design speciﬁcation.\nLemma: A minimal sufﬁcient statistic is additive in the logarithm of the likelihood.\nThen, given (conditional on λ) independent sampling of each singleton member making up a generic data-set, the conditional probability of this data set is the product of the individual probabilities associated with each of the singleton events; that is,\np(S (k) |Λ j (t k ) = λ) =\nbut, assuming the designer is motivated for a particular function b(·) (and recalling also that any invertible function of a minimal sufﬁcient statistic is a minimal sufﬁcient statistic), this can be incorporated into the h i (τ n i , t k ) function allowing the substitution y= i n\nPutting the foregoing results together yields our main theorem.\nTheorem 2 : For arrivals that are conditionally indepen- dent given λ for any value t k >max(τ n i ∈ X j (k)) and any ﬁxed data-set S(k), the transformation X j (k) →Y j (k) that maximizes Λ-information preservation and minimizes cost, takes the form y= i n\nThese results can be extended to a Bayesian form and extended to include dependence on other conditioning vari- ables including dependence on the past sequence of ipi\u2019s. Upgrading the transformations within j for past ipi\u2019s t (k−1) , the system of transformations are Λ j (k) → X j (k) → S(k, t (k−1) ) → Y j (k, t (k−1) ) → T j (k, t (k−1) ), where we have left implicit the delayed feedback from T j (·) to S j (·) and Y j (·) necessary for these localized memories to grow with increasing k.\nBecause a sufﬁcient statistic is information equivalent to the likelihood function and because the likelihood func- tion plus a prior distribution is all that is needed for Bayesian inference, another extension is easily implied because the non-Bayesian deﬁnition of sufﬁcient statistic qualiﬁes as a sufﬁcient statistic within the Bayesian deﬁni- tion (see Blackwell and Ramamoorthi, 1982); speciﬁcally, let\u2019s equip j and its recipients with a timer and a prior distribution.\nBayesian Corollary : Internally, j can make a Bayesian inference at the time of threshold-crossing and, externally, a receiver of j\u2019s time of threshold-crossing can perform a Bayesian inference on λ when we equip j and the recip- ient with a suitable prior distribution, p(Λ j (k)= λ|t (k−1) ), a timer, and knowledge of the deterministic threshold function, θ(t (k) ): Speciﬁcally, when Y j (k, t (k−1) )= θ(t (k) ), for j:\np(Λ j (t k ) = λ|Y j ( t (k) ) = θ( t (k) ), t (k) ) ∝ p(Y j (t (k) ) = θ(t (k) )|Λ j (t k ) = λ, t (k) )\n(Proportionality becomes equality when each r.h.s. is nor- malized by division with the appropriate integral).\nUsing a result of Gutierriz and Muliere (2004) and our earlier reasoning about energy-efﬁciency, a further corollary points out that a conjugate prior is necessary to minimize costs.\nThe ﬁnal extension moves us from a simple NL element to a complex NL element. In this case we deﬁne a complex NL element as being constructed of multiple compartments where each compartment is predicting a different latent variable.\nMulti-compartment corollary : Consider a NL element made up of more than one compartment where, for each compartment, there is a different scalar-valued latent ran- dom process driving the inputs to that compartment and possibly a different sequence of transformations ending with a scalar variate. Then, for each such compartment, the information-preserving and cost-minimizing transfor- mations encoding the transmitted scalar value of that compartment must be a minimal sufﬁcient statistic formed by summation of conditionally independent inputs."},"refs":[{"authors":[{"name":"W. Baranki"},{"name":"P. Maitr"}],"title":{"text":"E"}},{"authors":[{"name":"T. Berge"},{"name":"W. B. Lev"}],"title":{"text":"A mathematical theory of energy efﬁcient neural computation and communication , IEEE Trans"}},{"authors":[{"name":"D. Blackwel"},{"name":"R. V. Ramamoorth"}],"title":{"text":"A Bayes but Not Classically Sufﬁcient Statistic , Ann"}},{"authors":[{"name":"E. B. Dynki"},{"name":"I. Akad"}],"title":{"text":"On sufﬁcient and necessary statistics for families of probability distributions (Russian) , Dok N auk SSSR 75, 161-164, 1950"}},{"authors":[{"name":"R. A. Fishe"}],"title":{"text":"On the Mathematical Foundations of Theoretical Statistics , Phil"}},{"authors":[{"name":"B. O. Koopma"}],"title":{"text":"On Distributions Admitting a Sufﬁcient Statistic, Transactions of the American Mathematical Society, Vol"}},{"authors":[{"name":"T. Sagaw"},{"name":"M. Ued"}],"title":{"text":"Minimal Energy Cost for Thermodynamic Information Processing: Measurement and Information Erasure , Physical Review Letters 102, 250602, 2009"}},{"authors":[{"name":"S. Zak"}],"title":{"text":"The Theory of Statistical Inference, John Wiley and Sons, Inc"}},{"authors":[{"name":"A. M. Kaga"},{"name":"U. V. Lini"},{"name":"C. R. Ra"}],"title":{"text":"Characterization Problems in Mathematical Statistics , Wiley, 1973"}},{"authors":[{"name":"E. Gutierri"},{"name":"P. Mulier"},{"name":"J. of Stat"}],"title":{"text":"Conjugate priors represent strong pre- experimental assumptions , Scand"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566839.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T9.3","endtime":"12:30","authors":"William Levy, Toby Berger","date":"1341576600000","papertitle":"Design Principles and Specifications for Neural-like Computation Under Constraints on Information Preservation and Energy Costs as Analyzed with Statistical Theory","starttime":"12:10","session":"S16.T9: Information Theory in Biology","room":"Stratton West Lounge (201)","paperid":"1569566839"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
