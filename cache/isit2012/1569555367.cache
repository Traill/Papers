{"id":"1569555367","paper":{"title":{"text":"Gaussian Multiple Descriptions with Common and Constrained Reconstruction Constraints"},"authors":[{"name":"Ravi Tandon"},{"name":"Behzad Ahmadi"},{"name":"Osvaldo Simeone"},{"name":"H. Vincent Poor"}],"abstr":{"text":"Abstract\u2014The problem of multiple descriptions for a Gaussian source is considered, in which all the decoders have access to cor- related Gaussian side-information. Two variations of this problem are studied. First, the rate-distortion tradeoff is characterized under the assumption of a common reconstruction constraint, in which the estimate produced at each of the decoders is also exactly recoverable at the encoder. Secondly, a generalization of this setup is studied in which possibly different distortions are tolerable between the estimates at the encoder and the respective estimates at each of the decoders."},"body":{"text":"The rate distortion function for lossy transmission of a memoryless source X to a decoder that has access to correlated side information Y was characterized by Wyner and Ziv in [1]. There are two ways in which the side-information is useful in the Wyner-Ziv problem. First, since the side information Y is correlated with the source X, the encoder can reduce the communication rate via binning; and secondly, the reconstruction ˆ X at the decoder depends on both the digital information received from the encoder and the side- information Y . The second property, i.e., the dependence of the decoder\u2019s estimate ˆ X on the side-information Y (which is unavailable at the encoder) may be undesirable for certain applications. As an example, consider the transmission of sensitive medical records, for which it is of utmost importance that the decoder and the encoder agree upon the estimates of the records.\nTo take such scenarios into account, the problem of common reconstruction ( CR) was proposed by Steinberg in [2], in which the rate-distortion function is characterized for a point- to-point system under a common reconstruction constraint. Formally, a common reconstruction constraint refers to a restriction that the encoder should also be able to reconstruct the estimate at the decoder. The common reconstruction con- straint can be perhaps too restrictive since it precludes the decoder to use its side-information to create the estimate. A relaxed version of the common reconstruction constraint in which some distortion is tolerable between the estimate at the encoder and the estimate at the decoder was studied in [3]. The corresponding rate-distortion function was characterized in [3] (henceforth referred to as the rate-distortion function under constrained reconstruction ( ConR)).\nSome generalizations and extensions of [2] to multi-user settings have been studied recently. In particular, the rate- distortion function for the Heegard-Berger problem with CR\nconstraints has been recently obtained in [4]. Furthermore, the rate-distortion tradeoff for a class of cascade source coding problems under the CR constraint has also been obtained in [4]. It is noted that in the latter case, the problem of determining the rate-distortion tradeoff is open when one does not impose the CR constraint. This demonstrates that the CR constraint may simplify the design problem by limiting the space of possible strategies, as already discussed in [2] and [5]. A source coding problem with complementary side information under a CR constraint has been studied in [6]. Joint source-channel coding for a Gaussian source over a slow fading Gaussian broadcast channel is studied in [5], where it is shown that the natural broadcast strategy coupled with successive reﬁnement source coding is in fact optimal under the CR constraint (also see [2, Sec. IV]).\nIn this paper, we generalize the point-to-point common- reconstruction problem of [2], and the point-to-point con- strained reconstruction problem of [3] to the problem of multiple descriptions ( MD) with decoder side information. We focus on the MD problem with three decoders, in which all the decoders have access to the same correlated side-information Y (see Figure 1). We focus on Gaussian source and side information. Speciﬁcally, an encoder wishes to convey a mem- oryless Gaussian scalar source X to three decoders via two rate-limited orthogonal links. The distortion constraints at all the decoders are assumed to be measured by the mean-squared error (MSE). The correlated side information Y , which is assumed to be jointly Gaussian with X is available at all the three decoders. In the absence of side-information, this problem was solved by Ozarow [7]. Instead, the more general case in which correlated Gaussian side-information is available at all decoders was solved by Diggavi and Vaishampayan [8].\nﬁrst characterize the set of achievable rates and distortion tuples for the Gaussian MD problem under the CR constraint. We next consider the ConR generalization of this setup, in which some pre-speciﬁed MSE distortions are tolerable between the estimates at the encoder and the corresponding estimates at the decoders. It is shown that the optimal strategy is based on separation, in the sense that a combination of coding for the MD problem, followed by binning with respect to side-information Y , is optimal.\nThe encoder observes a memoryless source X n and pro- duces two indices\nwhere { f (n) j : X n → J j } j=1,2 are the encoding functions. The index J 1 (resp. J 2 ) is received at decoder 1 (resp. decoder 2). Both the indices (J 1 , J 2 ) are received at decoder 0. The decoders form their estimates as follows:\nWe call the set of ({f n j } j=1,2 , {g (n) j } j=0,1,2 , {ψ n j } j=0,1,2 ) functions an (n, R 1 , R 2 , D 0 , D 1 , D 2 )-code if |J 1 | ≤ 2 nR 1 , |J 2 | ≤ 2 nR 2 and the reconstruction sequences satisfy\n1 n\nPr (ψ 1 (X n ) = g 1 (J 1 , Y n )) ≤ \t (2) Pr (ψ 2 (X n ) = g 2 (J 2 , Y n )) ≤ \t (3)\nThe non-negative tuple (R 1 , R 2 , D 0 , D 1 , D 2 ) is achievable if for every \t > 0 and sufﬁciently large n, there exists an (n, R 1 + , R 2 + , D 0 + , D 1 + , D 2 + )-code. We denote RD CR as the closure of the set of all achievable (R 1 , R 2 , D 0 , D 1 , D 2 ) tuples.\nWe next extend the CR constraint to the ConR constraint. In this setting, some distortion is permissi- ble between the estimate at the decoder(s) and the re- spective estimate(s) at the encoder. We call the set of ({f n j } j=1,2 , {g (n) j } j=0,1,2 , {ψ n j } j=0,1,2 ) functions an\n(n, R 1 , R 2 , D 0 , D 1 , D 2 , D e,0 , D e,1 , D e,2 )-code if |J 1 | ≤ 2 nR 1 , | J 2 | ≤ 2 nR 2 and the reconstruction sequences satisfy\n1 n\nThe non-negative tuple (R 1 , R 2 , D 0 , D 1 , D 2 , D e,0 , D e,1 , D e,2 ) is achievable if for every \t > 0 and sufﬁciently large n, there exists an (n, R 1 + , R 2 + , D 0 + , D 1 +\n, D 2 + , D e,0 + , D e,1 + , D e,2 + )-code. We denote RD ConR as the closure of the set of all achievable (R 1 , R 2 , D 0 , D 1 , D 2 , D e,0 , D e,1 , D e,2 ) tuples.\nThis paper considers the case in which the source X is Gaussian with variance σ 2 X and the side information Y is jointly Gaussian with X. Under this model, we can write\nwhere Z is a zero-mean, Gaussian random variable with variance σ 2 Z , and independent of X. The distortion measures are quadratic and are deﬁned as follows:\nBefore presenting our results, we recall the rate-distortion region without side-information [7]:\nD No−SI 1 \t ≥ exp(−2R 1 ) \t (8) D No−SI 2 \t ≥ exp(−2R 2 ) \t (9) D No−SI 0 \t ≥ exp(−2(R 1 + R 2 )) \t 1\n(10) where\nΠ No−SI (1 − D No−SI 1 \t )(1 − D No−SI 2 \t ) \t (12) ∆ No−SI D No−SI 1 \t D No−SI 2 \t − exp(−2(R 1 + R 2 )). (13)\nThe rate-distortion region with side-information (without any constraints) [8] is given as\nD SI 1 ≥ exp(−2R 1 ) \t (14) D SI 2 ≥ exp(−2R 2 ) \t (15) D SI 0 ≥ exp(−2(R 1 + R 2 )) \t 1\nwhere the quantities (Π SI , ∆ SI ) are deﬁned as in (12) and (13) with (D No−SI 1 \t , D No−SI 2 \t ), replaced by (D SI 1 , D SI 2 ) and V X|Y = Var (X|Y ) = σ 2 X σ 2 Z σ 2\nTheorem 1: The set RD CR of all achievable (R 1 , R 2 , D 0 , D 1 , D 2 )-tuples for the Gaussian MD problem with decoder side-information and CR constraints is given as\nD CR 1 ≥ exp(−2R 1 ) \t (18) D CR 2 ≥ exp(−2R 2 ) \t (19) D CR 0 ≥ exp(−2(R 1 + R 2 )) \t 1 1 − ( √ Π CR − √ ∆ CR ) 2 , (20)\nD j + σ 2 Z , j = 0, 1, 2. (21) Π CR (1 − D CR 1 )(1 − D CR 2 ) \t (22) ∆ CR D CR 1 D CR 2 − exp(−2(R 1 + R 2 )). \t (23)\nTheorem 2: The set RD ConR of all achievable (R 1 , R 2 , D 0 , D 1 , D 2 , D e,0 , D e,1 , D e,2 )-tuples \t for \t the Gaussian MD problem with decoder side-information and ConR constraints is given as follows:\nD ConR 1 ≥ exp(−2R 1 ) \t (24) D ConR 2 ≥ exp(−2R 2 ) \t (25) D ConR 0 ≥ exp(−2(R 1 + R 2 )) \t 1\n(26) where for j = 0, 1, 2, we have deﬁned\n  \n \n(27) and\nΠ ConR (1 − D ConR 1 )(1 − D ConR 2 ) \t (28) ∆ ConR D ConR 1 D ConR 2 − exp(−2(R 1 + R 2 )). \t (29)\nThe proof of Theorem 2 is omitted due to space limitations and can be found in [9].\nRemark 1: The optimal coding schemes achieving the tradeoffs in Theorems 1 and 2 are based on a separation based strategy, i.e., a combination of coding for the MD problem, followed by binning with respect to side-information Y . This is evident by comparing the expressions in Theorems 1 and 2 to the corresponding tradeoff regions in (8)-(10) and (14)- (16). It is interesting to note that all four regions are described by a similar set of constraints, except that the deﬁnition of the effective distortions D No−SI , D SI , D CR and D ConR differ depending on the problem under consideration. We also note that as D e,j → 0, for j = 0, 1, 2, we have D ConR j \t → D CR j , and the region in Theorem 2 collapses to the one in Theorem 1.\nWe illustrate these tradeoffs in Figure 2 for the symmetric case in which D 1 = D 2 = D 0 = D sym , and R 1 = R 2 = R sym , D e,1 = D e,2 = D e,0 = D e,sym = 0.05, with σ 2 X = 4 and σ 2 Z = 1. From Figure 2, it is worth noting that for values\nof D sym below a certain threshold, the ConR constraints are automatically satisﬁed, and hence the minimal rate under ConR coincides with the minimal rate without any constraints.\nThis paper has focused on the generalization of the Gaus- sian multiple-description problem with side-information at the decoders by imposing a common reconstruction constraint as in [2] and also constrained reconstruction constraints as in [3]. These additional distortion constraints effectively control the amount of side-information to be used in creating the source estimate at each decoder. The complete rate-distortion tradeoff has been characterized for the case of common-reconstruction constraints. This result is then generalized to the constrained- reconstruction setting with MSE distortion measures at each of the decoders. These tradeoffs reﬂect the penalty in terms of additional communication rates of the encoder, if it seeks to agree with the estimates at the decoders.\nThe work of H. V. Poor and R. Tandon was supported in part by the Air Force Ofﬁce of Scientiﬁc Research MURI Grant FA- 9550-09-1-0643 and in part by the National Science Foundation Grant CNS- 09-05398. The work of O. Simeone was supported by the U.S. National Science Foundation under Grant CCF- 0914899.\n1) Coding Scheme with CR: The following region is achievable for the MD problem with a CR constraint:\nR 1 ≥ I(X; U 1 |Y ) R 2 ≥ I(X; U 2 |Y )\nwhere the random variables (U 1 , U 2 , X, Y ) satisfy the Markov condition (U 1 , U 2 ) → X → Y , and there exist functions\n2) Gaussian MD with CR constraints: For the case in which (X, Y ) are jointly Gaussian, we select\nwhere (N 1 , N 2 ) are jointly Gaussian, independent of X, with a covariance matrix as follows:\nThe estimates ˆ X i are selected as the minimum MSE (MMSE) estimators of X given U 1 , U 2 and (U 1 , U 2 ) for decoders 1, 2 and decoder 0, respectively. Note that such a selection automatically satisﬁes the common reconstruction constraints for all the three decoders. The achievable distortions are hence given as\nWe select σ 2 1 and σ 2 2 such that the distortion constraints in (32) are met with equality, i.e., we select\n− D j , j = 1, 2. \t (34) Also, note that we have for j = 1, 2\nR 1 + R 2 ≥ I(X; U 1 , U 2 |Y ) + I(U 1 ; U 2 |Y ) \t (38) = h(U 1 |Y ) + h(U 2 |Y ) − h(U 1 , U 2 |X) (39)\nσ 2 1 σ 2 2 (1 − ρ 2 ) \t (40) = 1 2 log \t 1 D CR\nSubstituting this value of ρ in the expression for D 0 in (33) and using (32), we arrive at\nD CR 1 ≥ exp(−2R 1 ) \t (44) D CR 2 ≥ exp(−2R 2 ) \t (45) D CR 0 ≥ exp(−2(R 1 + R 2 )) \t 1\n3) Converse Proof under the CR Constraint: We note that R 1 and R 2 must always satisfy the following bounds:\nwhere D CR is as deﬁned in (21). Hence, from (47) and (49), we have the following bounds:\nWe now obtain a lower bound on the sum rate R 1 + R 2 as follows:\n= H(J 1 ) + H(J 2 ) \t (51) ≥ H(J 1 |Y n ) + H(J 2 |Y n ) \t (52) = H(J 1 , J 2 |Y n ) + H(J 1 |Y n ) + H(J 2 |Y n ) − H(J 1 , J 2 |Y n )\n(53) = H(J 1 , J 2 |Y n ) + I(J 1 ; J 2 |Y n ) \t (54) = I(X n ; J 1 , J 2 |Y n ) + I(J 1 ; J 2 |Y n ) \t (55) ≥ nR CR (D 0 ) + I(J 1 ; J 2 |Y n ). \t (56)\nD CR 1 ≥ exp(−2R 1 ) \t (58) D CR 2 ≥ exp(−2R 2 ) \t (59) D CR 0 ≥ exp(−2(R 1 + R 2 ))T. \t (60)\nWe now obtain a lower bound on T by using Ozarow\u2019s technique of inducing conditional independence [7]. For this purpose, we deﬁne an artiﬁcial Gaussian random variable as follows:\nwhere the sequence of random variables { W i } n i=1 is inde- pendent and identically distributed with each element being a zero-mean, Gaussian random variable with variance η, and W n is assumed to be independent of (X n , Y n ). To lower bound T , we have the following sequence of inequalities:\n≥ I(J 1 ; J 2 |Y n ) − I(J 1 ; J 2 |Y n , S n ) \t (62) = I(J 1 ; S n |Y n ) − I(J 1 ; S n |Y n , J 2 ) \t (63)\n− h(S n |J 1 , Y n ) − h(S n |J 2 , Y n ) \t (64) = h(S n |Y n ) + h(S n |J 1 , J 2 , Y n )\n− h(X n + W n |J 1 , Y n ) − h(X n + W n |J 2 , Y n ) (65) ≥ h(S n |Y n ) + h(S n |J 1 , J 2 , Y n )\n(68) The bounding step (66) follows from the following sequence of inequalities:\n≤ h(X n + W n | ˆ X n 1 , Y n ) \t (69) = h(S n | ˆ X n 1 , Y n ) \t (70) = h(S n |ψ 1 (X n ), ˆ X n 1 , Y n ) + I(ψ 1 (X n ); S n | ˆ X n 1 , Y n ) (71) ≤ h(S n |ψ 1 (X n ), ˆ X n 1 , Y n ) + n \t (72) ≤ h(S n |ψ 1 (X n ), Y n ) + n \t (73) = h(X n + W n |ψ 1 (X n ), Y n ) + n \t (74) ≤ n 2 log (D ∗ 1 + η) + n , \t (75)\nwhere (72) follows from the CR constraint and application of Fano\u2019s inequality, and (75) follows from arguments as in [2, Sec. V-A eqn. 69 − 72] and [2, Sec. V-C]. Next, consider the second term in (67):\n(76) = exp 2 n h(X n |J 1 , J 2 , Y n ) + (2πe)(η), \t (77)\nwhere in (76), we have used the fact that W n is independent of (X n , Y n , J 1 , J 2 ) and the conditional version of the entropy power inequality (EPI).\nWe now note the following sequence of inequalities: h(X n |J 1 , J 2 , Y n )\n= h(X n |Y n ) − I(X n ; J 1 , J 2 |Y n ) \t (78) = h(X n |Y n ) − H(J 1 , J 2 |Y n ) \t (79) = h(X n |Y n ) − H(J 1 |Y n ) − H(J 2 |Y n ) + I(J 1 ; J 2 |Y n )\n(80) ≥ h(X n |Y n ) − H(J 1 ) − H(J 2 ) + I(J 1 ; J 2 |Y n ) \t (81) ≥ h(X n |Y n ) − nR 1 − nR 2 + I(J 1 ; J 2 |Y n ), \t (82)\nHence, from (77) and (84), and using (67), we obtain the following lower bound on T :\nfor j = 1, 2. The following choice of η ∗ maximizes the right hand side above:\nwhich upon substitution, yields the following lower bound on T :"},"refs":[{"authors":[{"name":"A. D. Wyne"},{"name":"J. Ziv"}],"title":{"text":"The rate distortion function for source coding with side information at the decoder"}},{"authors":[{"name":"Y. Steinberg"}],"title":{"text":"Coding and common reconstruction"}},{"authors":[{"name":"A. Lapidot"},{"name":"A. Mala"},{"name":"M. Wigger"}],"title":{"text":"Constrained Wyner-Ziv coding"}},{"authors":[{"name":"B. Ahmad"},{"name":"R. Tando"},{"name":"O. Simeon"},{"name":"H. V. Poor"}],"title":{"text":"Heegard-Berger and cascade source coding problems with common reconstruction constraints [arXiv 1112"}},{"authors":[{"name":"C. Tia"},{"name":"A. Steine"},{"name":"S. Shama"},{"name":"S. N. Diggavi"}],"title":{"text":"Successive reﬁnement via broadcast: Optimizing expected distortion of a Gaussian source over a Gaussian fading channel"}},{"authors":[{"name":"R. Tim"},{"name":"A. Gran"},{"name":"G. Kramer"}],"title":{"text":"Rate-distortion functions for source coding with complimentary side information"}},{"authors":[{"name":"L. Ozarow"}],"title":{"text":"On a source coding problem with two channels and three receivers"}},{"authors":[{"name":"S. N. Diggav"},{"name":"V. A. Vaishampayan"}],"title":{"text":"On multiple description source coding with decoder side information"}},{"authors":[{"name":"R. Tando"},{"name":"B. Ahmad"},{"name":"O. Simeon"},{"name":"H. V. Poor"}],"title":{"text":"Gaussian multiple descriptions with common and constrained reconstruction constraints"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569555367.pdf"},"links":[{"id":"1569566381","weight":8},{"id":"1569566485","weight":4},{"id":"1569565383","weight":12},{"id":"1569566725","weight":12},{"id":"1569564635","weight":8},{"id":"1569565867","weight":4},{"id":"1569559617","weight":4},{"id":"1569566683","weight":4},{"id":"1569559259","weight":4},{"id":"1569566597","weight":8},{"id":"1569566943","weight":8},{"id":"1569552245","weight":4},{"id":"1569565931","weight":8},{"id":"1569566373","weight":4},{"id":"1569566765","weight":4},{"id":"1569564245","weight":8},{"id":"1569564227","weight":4},{"id":"1569565837","weight":4},{"id":"1569564233","weight":4},{"id":"1569559541","weight":4},{"id":"1569565123","weight":4},{"id":"1569566941","weight":4},{"id":"1569558459","weight":12},{"id":"1569565291","weight":4},{"id":"1569564203","weight":4},{"id":"1569556713","weight":4},{"id":"1569566751","weight":4},{"id":"1569566467","weight":4},{"id":"1569566999","weight":4},{"id":"1569566843","weight":8},{"id":"1569565455","weight":16},{"id":"1569566709","weight":4},{"id":"1569564989","weight":4},{"id":"1569566523","weight":12},{"id":"1569565953","weight":4},{"id":"1569564189","weight":4},{"id":"1569564271","weight":8},{"id":"1569565907","weight":4},{"id":"1569566239","weight":4},{"id":"1569563981","weight":8},{"id":"1569566905","weight":4},{"id":"1569566753","weight":12},{"id":"1569566311","weight":4},{"id":"1569566063","weight":4},{"id":"1569558681","weight":4},{"id":"1569555999","weight":4},{"id":"1569565213","weight":4},{"id":"1569566511","weight":4},{"id":"1569565841","weight":4},{"id":"1569565833","weight":8},{"id":"1569564611","weight":4},{"id":"1569565667","weight":4},{"id":"1569561795","weight":4},{"id":"1569566325","weight":12},{"id":"1569566423","weight":4},{"id":"1569566437","weight":4},{"id":"1569553909","weight":33},{"id":"1569553537","weight":4},{"id":"1569552251","weight":4},{"id":"1569553519","weight":4},{"id":"1569566231","weight":4},{"id":"1569554881","weight":4},{"id":"1569566371","weight":4},{"id":"1569558985","weight":8},{"id":"1569564333","weight":8},{"id":"1569566629","weight":4},{"id":"1569565033","weight":12},{"id":"1569563897","weight":16},{"id":"1569565887","weight":4},{"id":"1569565633","weight":4},{"id":"1569555879","weight":4},{"id":"1569558509","weight":4},{"id":"1569564851","weight":4},{"id":"1569564969","weight":4},{"id":"1569566043","weight":4},{"id":"1569561245","weight":4},{"id":"1569566505","weight":4},{"id":"1569565393","weight":4},{"id":"1569567033","weight":4},{"id":"1569566695","weight":4},{"id":"1569566673","weight":4},{"id":"1569565311","weight":4},{"id":"1569566233","weight":4},{"id":"1569566667","weight":8},{"id":"1569564097","weight":4},{"id":"1569566481","weight":4},{"id":"1569566387","weight":4},{"id":"1569562551","weight":4},{"id":"1569563395","weight":8},{"id":"1569551347","weight":4},{"id":"1569566383","weight":8},{"id":"1569565885","weight":4},{"id":"1569565611","weight":4},{"id":"1569566479","weight":8},{"id":"1569565397","weight":4},{"id":"1569565919","weight":4},{"id":"1569565181","weight":4},{"id":"1569565661","weight":4},{"id":"1569566267","weight":4},{"id":"1569564131","weight":4},{"id":"1569566917","weight":4},{"id":"1569566691","weight":4},{"id":"1569566651","weight":4},{"id":"1569566823","weight":4},{"id":"1569566137","weight":4},{"id":"1569565375","weight":4},{"id":"1569565293","weight":12},{"id":"1569551905","weight":8},{"id":"1569556759","weight":4},{"id":"1569566619","weight":4},{"id":"1569561185","weight":8},{"id":"1569560235","weight":4},{"id":"1569566817","weight":8},{"id":"1569566435","weight":4},{"id":"1569566299","weight":4},{"id":"1569565769","weight":4},{"id":"1569563919","weight":4},{"id":"1569557851","weight":12},{"id":"1569566147","weight":4},{"id":"1569559597","weight":4},{"id":"1569567013","weight":8},{"id":"1569561861","weight":8},{"id":"1569550425","weight":8},{"id":"1569565889","weight":4},{"id":"1569563725","weight":4},{"id":"1569566375","weight":4},{"id":"1569564257","weight":8},{"id":"1569564141","weight":4},{"id":"1569564509","weight":4},{"id":"1569551541","weight":4},{"id":"1569566663","weight":4},{"id":"1569564419","weight":4},{"id":"1569566067","weight":4},{"id":"1569566609","weight":4},{"id":"1569563007","weight":4},{"id":"1569565315","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S8.T4.4","endtime":"18:00","authors":"Ravi Tandon, Behzad Ahmadi, Osvaldo Simeone, H. Vincent Poor","date":"1341337200000","papertitle":"Gaussian Multiple Descriptions with Common and Constrained Reconstruction Constraints","starttime":"17:40","session":"S8.T4: Multiple Description Coding","room":"Stratton 20 Chimneys (306)","paperid":"1569555367"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
