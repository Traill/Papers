{"id":"1569566893","paper":{"title":{"text":"An Achievable Rate Region for Three-Pair Interference Channels with Noise"},"authors":[{"name":"Bernd Bandemer"}],"abstr":{"text":"Abstract\u2014An achievable rate region for certain noisy three- user-pair interference channels is proposed. The channel class under consideration generalizes the three-pair deterministic in- terference channel (3-DIC) in the same way as the Telatar\u2013Tse noisy two-pair interference channel generalizes the El Gamal\u2013 Costa injective channel. Speciﬁcally, arbitrary noise is introduced that acts on the combined interference signal before it affects the desired signal. This class of channels includes the Gaussian case.\nThe rate region includes the best-known inner bound on the 3-DIC capacity region, dominates treating interference as noise, and subsumes the Han\u2013Kobayashi region for the two-pair case."},"body":{"text":"The interference channel is one of the canonical models in network information theory, and has withstood all attempts to solve it in general. In recent years, signiﬁcant progress has been made for the case with two sender\u2013receiver pairs. The best known achievable rate region is achieved by the Han\u2013Kobayashi coding scheme [1], for which a compact formulation was given in [2]. Much less is known in the case with more than two user pairs. Major lines of work exist in the areas of interference alignment [3, 4], and deterministic models as pioneered in [5, 6]. The key idea in the latter is to ﬁrst investigate a simpliﬁed interference channel that does not contain noise, and then proceed to transfer the insight to more practically relevant channels with noise.\nIn this paper, we apply this idea to the three-user-pair deterministic interference channel (3-DIC) ﬁrst introduced in [7]. We consider the noisy version of the 3-DIC depicted in Figures 1 and 2. The channel consists of three sender\u2013 receiver alphabet pairs (X l , Y l ), loss functions g lk that model the links between each sender and receiver, and a conditional probability mass function (pmf) at each receiver that maps the three impinging signals into the receiver observation Y l , for indices k, l ∈ {1:3}. The pmfs have the special structure depicted in Figure 2 for the ﬁrst receiver. They consist of two deterministic stages, namely an interference combining function h l and a receiver function f l . We assume that the functions h l and f l are injective in each argument, that is, they become one-to-one when either one of their arguments is ﬁxed. For example, for Y 1 = f 1 (X 11 , S 1 ), this assumption is equivalent\nto H(X 11 ) = H(Y 1 | S 1 ) and H(S 1 ) = H(Y 1 | X 11 ) for every pmf p(x 11 , s 1 ). An example of a function that is injective in each argument (but not injective) is regular addition. Deviating from the deterministic nature of the 3-DIC, we introduce noise between the two combining stages. It acts on the combined interference signal S l and is characterized by the discrete memoryless channel p(s l |s l ). (Note that setting S l = S l for all l recovers the 3-DIC setting.)\nEach sender l wishes to convey an independent message M l at rate R l to its corresponding receiver. We deﬁne a (2 nR 1 , 2 nR 2 , 2 nR 3 , n) code, probability of error, achievability of a rate triple (R 1 , R 2 , R 3 ), and the capacity region in the standard way (see [8]). The capacity region is not known, but in this paper, we make progress towards characterizing it.\nThe channel model under consideration is interesting since it contains the Gaussian interference channel as a special case. Characterizing its capacity region thus has immediate consequences for practical wireless communications systems\nwhere simultaneous transmissions use the same radio spectrum. Moreover, we follow along the footsteps of previous work in the case with two user pairs. The capacity region of the two-pair version of 3-DIC was found in [9]. A modiﬁcation of this channel in which noise affects the interfering signal was studied in [10]. The authors establish inner and outer bounds to the capacity region which differ only by a constant gap, akin to the Gaussian case studied in [11].\nThus we are motivated to generalize the results for the 3-DIC to its own noisy cousin. Let us brieﬂy review the 3- DIC results. The best known achievable rate region is given in [12]. The underlying scheme combines insights from the transmitter-centric view of communication with disturbance constraints [13] and the receiver-centric view of interference decoding [14]. All results of [12]\u2013[14] are also contained and discussed in detail in [15].\nWe extend the achievable rate region in [12] to the channel under consideration. It turns out that the key properties of 3-DIC are preserved and allow us to apply the same coding scheme, which consists of rate splitting, Marton coding, and superposition coding. The analysis of the probability of error is more involved than in the deterministic case due to the noise. The resulting inner bound to the capacity region includes all previous results for the 3-DIC. It simpliﬁes to the Han\u2013 Kobayashi inner bound when one of the three user pairs is not present. Finally, unlike the interference decoding inner bound for the 3-DIC with noisy observations [14], the present bound is larger than the one that results from using point-to-point random codes and treating interference as noise.\nIn order to state the inner bound to the capacity region of the channel under consideration, we need the following deﬁnitions. Fix a joint pmf for (Q, U 1 , X 1 , U 2 , X 2 , U 3 , X 3 ) of the form\n(R 10 , R 11 , R 12 , R 13 , ˜ R 12 , ˜ R 13 , R 20 , R 22 , R 23 , R 21 , ˜ R 23 , ˜ R 21 ,\n˜ R 12 − R 12 + ˜ R 13 − R 13 ≥ I(X 12 ; X 13 | U 1 , Q), (2) ˜ R 12 − R 12 + ( ˜ R 13 − R 13 )/2 ≤ I(X 12 ; X 13 | U 1 , Q), (3)\n( ˜ R 12 − R 12 )/2 + ˜ R 13 − R 13 ≤ I(X 12 ; X 13 | U 1 , Q), (4) ˜ R 12 ≥ R 12 , \t (5) ˜ R 13 ≥ R 13 , \t (6)\nIn (7), symbols like r 1i , c 1i , and t 1i are placeholders for the terms speciﬁed in Tables 1, 2, and 3, respectively. For example, for i = 3, j = 3, and k = 2, condition (7) becomes\nR 20 + ˜ R 21 + I(S 1 ; S 1 | X 2 , U 3 , Q), ˜ R 31 + I(S 1 ; S\nR 20 + ˜ R 31 + I(S 1 ; S 1 | U 2 , X 3 , Q), R 20 + ˜ R 21 + ˜ R 31\nSimilarly, deﬁne the regions R 2 (p) and R 3 (p) by making the subscript replacements 1 → 2 → 3 → 1 and 1 → 3 → 2 → 1 in the deﬁnition of R 1 (p), respectively.\nDeﬁne a Fourier\u2013Motzkin elimination operator FM that maps a convex 18-dimensional set of rate vectors of the form (1) to a 3-dimensional region by letting R l = 3 ν=0 R lν , for l ∈ {1:3}, and projecting on the coordinates (R 1 , R 2 , R 3 ). Let S denote the convex hull of S .\nWe are now ready to state the main result as follows. Theorem 1 (Achievable rate region). The region\nwhere p = p(q) p(u 1 , x 1 |q) p(u 2 , x 2 |q) p(u 3 , x 3 |q), is achiev- able in the interference channel under consideration.\nThe regions R 1 (p), R 2 (p), and R 3 (p) in the theorem represent decodability conditions at the ﬁrst, second, and third\nreceiver, correspondingly. The regions and their intersection are generally nonconvex. By the time-sharing argument, we are allowed to convexify, as shown in the theorem. This convex hull operation is nontrivial even for a ﬁxed pmf p, and therefore, it is not automatically achieved by including Q. Due to the explicit convex hull operation, the Fourier\u2013Motzkin reduction FM cannot be evaluated symbolically.\nWe outline the coding scheme that attains the inner bound of Theorem 1. To simplify the notation, we omit the time-sharing auxiliary random variable Q throughout this section.\nCodebook generation and encoding: The transmitter codebooks are generated as in [12], inspired by communication with disturbance constraints [13]. The intuition is that the interfer- ence channel under consideration is sufﬁciently deterministic in nature such that the results from the deterministic case of communication with disturbance constraints still apply. In particular, disturbance is measured before the combining functions h l , and thus before the noise appears.\nFix a pmf p(u 1 , x 1 ) p(u 2 , x 2 ) p(u 3 , x 3 ). Consider the ﬁrst transmitter. Split the rate as R 1 = R 10 + R 11 + R 12 + R 13 , and deﬁne the auxiliary rates ˜ R 12 ≥ R 12 and ˜ R 13 ≥ R 13 . Let ε > 0, and deﬁne the set partitions\n{1:2 n ˜ R 12 } = L 12 (1) ∪ · · · ∪ L 12 (2 nR 12 ), {1:2 n ˜ R 13 } = L 13 (1) ∪ · · · ∪ L 13 (2 nR 13 ),\nwhere L 12 (·) and L 13 (·) are indexed sets of size 2 n( ˜ R 12 −R 12 ) and 2 n( ˜ R 13 −R 13 ) , respectively.\n2) For each l 12 ∈ {1 : 2 n ˜ R 12 }, generate x n 12 (m 10 , l 12 ) according to n i=1 p(x 12i | u 1i (m 10 )). Likewise, for each l 13 ∈ {1 : 2 n ˜ R 13 }, generate a sequence x n 13 (m 10 , l 13 ) according to n i=1 p(x 13i | u 1i (m 10 )).\n3) For each triple (m 10 , m 12 , m 13 ), let S(m 10 , m 12 , m 13 ) be the set of all pairs (l 12 , l 13 ) from the product set L 12 (m 12 ) × L 13 (m 13 ) such that (x n 12 (m 10 , l 12 ), x n 13 (m 10 , l 13 )) ∈ T (n) ε (X 12 , Z 13 | u n 1 (m 10 )).\n4) For each (m 10 , l 12 , l 13 ) and m 11 ∈ {1 : 2 nR 11 }, generate a sequence x n 1 (m 10 , l 12 , l 13 , m 11 ) accord- ing to n i=1 p(x 1i | u 1i (m 10 ), x 12i (l 12 ), x 13i (l 13 )), if\n(l 12 , l 13 ) ∈ S(m 10 , m 12 , m 13 ). Otherwise, generate it according to Unif(X n 1 ).\n5) Draw a random pair uniformly from S(m 10 , m 12 , m 13 ) and denote it as (l (m 10 ,m 12 ,m 13 ) 12 \t , l (m 10 ,m 12 ,m 13 ) 13 \t ). If S(m 10 , m 12 , m 13 ) is empty, use (1, 1) instead.\nCodebooks for the second and third transmitter are generated analogously by applying the subscript replacements 1 → 2 → 3 → 1 and 1 → 3 → 2 → 1 in each step of the procedure.\nDecoding: The receivers use simultaneous non-unique decod- ing [8]. The ﬁrst receiver observes y n 1 . Deﬁne the tuple\nDeclare that ˆ m 1 = ( ˆ m 10 , ˆ m 12 , ˆ m 13 , ˆ m 11 ) has been sent if it is the unique message such that\nAnalysis of error probability: Each triple (i, j, k) in (7) corresponds to a certain error event, the probability of which must asymptotically vanish. This can be ensured by any one of several conditions, indexed by j and k . The details are deferred to the appendix.\nDifferent (j , k ) in (7) correspond to various ways of inter- ference signal saturation, as ﬁrst discussed in [14]. Saturation takes place when the total number of interfering codewords exceeds the number of distinguishable sequences, and thus it is not possible to decode the interfering messages. This is illustrated by the example in (8). Let us compare the ﬁrst and the last terms in the min expression, i.e.,\nIn the noiseless case, S 1 equals S 1 , and the ﬁrst term becomes H(S 1 | U 3 , Q). In logarithmic scale, this is the size of the set of typical interfering sequences that can appear under the error event in question (i = 3, j = 3, k = 2). Saturation occurs if the interfering rate R 20 + ˜ R 21 + ˜ R 31 exceeds this quantity, and thus increasing the rates does not further increase the set of observed interference sequences. On the other hand, when noise is present, we have\nI(S 1 ; S 1 | U 3 , Q) = H(S 1 | U 3 , Q) − H(S 1 | U 3 , S 1 , Q) ≤ H(S 1 | U 3 , Q),\nwhich implies that saturation starts to occur at lower rates than in the noiseless case. Loosely speaking, each interfering sequence takes up more of the observed signal space due to channel noise. Along similar lines, the remaining terms in the min expression in (8) correspond to other modes of (partial) saturation. Thus, we can interpret the choice of (j , k ) in condition (7) as switching between different regimes of saturation, and treating the saturated sequences as i.i.d. noise as appropriate. Keep in mind, however, that the entire inner bound is achieved by the same simultaneous non-unique typicality decoder. The distinction of saturation regimes appears only through different ways of analyzing the error probability of the same decoding rule.\nIt is interesting to note that Theorem 1 contains the following three special cases.\n1) 3-DIC inner bound: It is not hard to see that Theorem 1 subsumes previously known results for the 3-DIC case where S l = S l for all l. The inequalities in (7) for a given triple (i, j, k) are implied by the conditions in [12, Corollary 1]. In fact, Theorem 1 slightly improves upon the results in the deterministic case: For example, comparing the min terms in [12, equation (19)] with those in (8) reveals that terms such as H(X 21 | U 2 , Q) + H(X 31 | U 3 , Q) can be replaced by H(S 1 | U 2 , U 3 , Q). The improvement comes from the reﬁned proof technique in the appendix.\n2) Point-to-point codes with treating interference as noise: Inspecting (7), note that\nI(X 1 , X 2 , X 3 ; Y 1 | c 1i , c 21j , c 31k , Q) − I(S 1 ; S 1 | c 21j , c 31k , Q)\nwhere the last step relies on the Markov chains c 21j −c 21j −Y 1 and c 31k − c 31k − Y 1 . Therefore, an equivalent way to write condition (7) is\nThis implies that Theorem 1 includes the achievable rate region attained by point-to-point random codes and treating interference as noise. To see this, set U l = X l , let R 12 =\n˜ R 12 = R 13 = ˜ R 13 = R 11 = 0 and R 10 = R 1 , and likewise for the other transmitters. Only the cases with i = 5 remain, and we choose j = k = 1. Then condition (9) is implied by\nwhich is the rate achievable by using point-to-point (non- layered) random codes and treating interference as noise. The same inclusion does not hold in the case of the interference decoding inner bound for the 3-DIC with noisy observations studied in [14], where the noise in the channel acts on Y l instead of S l . In that case, the saturation effects are exploited without taking the noise into account, which leads to an artiﬁcial separation between the channel noise and the combined interference even if the latter is to be treated as noise. In the present case, however, since the noise directly affects the combined interference signal S l , saturation and noise can be treated jointly as discussed above, and the inefﬁciency of artiﬁcially separating them is avoided.\n3) Han\u2013Kobayashi inner bound: Finally, when one of the three user pairs is not present, say, the third one (X 13 = X 23 = X 3 = ∅), Theorem 1 recovers the Han\u2013Kobayashi inner bound for the interference channel that consists of the ﬁrst and second user pair. To see this, let R 12 = ˜ R 12 , R 13 = ˜ R 13 = 0, and R 11 = 0. The encoding scheme then degenerates to a superposition code with coarse layer according to U 1 and ﬁne layer according to X 1 , and component rates R 10 and R 12 , respectively, and likewise at the second transmitter. The receivers in the general scheme perform simultaneous non- unique decoding and will thus act correctly even when the codebook structure simpliﬁes to superposition codes.\nThe author is grateful to Professors Abbas El Gamal and Young-Han Kim for helpful discussions about the material presented in this paper.\nThe analysis proceeds analogously to [12] (and more completely, [15]). In particular, conditions (2) to (6) arise from analyzing the error events related to codebook generation. As an example for an error event arising at the decoder, consider\nU n 2 (m 20 ), X n 21 (m 20 , l 21 ), U n 3 (1), X n 31 (1, l 31 ), S n 1 (m 20 , l 21 , 1, l 31 ), Y n 1 ∈ T (n) ε\nAs a representative special case, we show that the condition indexed by i = 3, j = 3, and k = 2 in Theorem 1 as detailed in (8) implies that the probability of this event vanishes asymptotically. In particular, we are going to show the case j = 2, k = 1, i.e., we prove that convergence is a consequence of the second min term in (8). Using the union bound as in [15, Section 5.2.2], the relevant probability satisﬁes\nwhere E c e denotes that no encoding error has occurred (ensured by conditions (2) to (6)), E eq is deﬁned as in [15], and\nP 2 = P (u n 1 , x n 12 , X n 13 (1, l 13 ), X n 1 (1, L (1,1,1) 12 , l 13 , m 11 ), u n 2 , X n 21 (m 20 , l 21 ), u n 3 , X n 31 (1, l 31 ), y n 1 ) ∈ T (n) ε\nwhere the probability is increased by omitting parts of the typicality requirement. This step replaces the application of Corollary A.2 in [15] and simpliﬁes the proof. It also leads to the slight improvement in the deterministic case as discussed above. The following steps are fairly conventional.\nH(S 1 | U 2 , U 3 , S 1 ) = H(S 1 | S 1 ), H(Y 1 | U 1 , X 12 , U 3 , X 1 , X 2 , X 3 ) = H(S 1 | S 1 ),\nand subtracting this term from both sides of the last inequality leads to\nWhen transitioning from (10) to (11), the union bound was applied to the indices m 11 , l 13 and m 20 , while the indices l 21 and l 31 where handled later by omitting terms from the typicality requirement. The latter omission is the technical reason for the saturation effects. By varying which subset of indices {m 20 , l 21 , l 31 } is treated by the union bound, we can obtain the remaining lines in (8), corresponding to other modes of saturation. In all cases, the indices m 11 and l 13 are treated by the union bound, since they correspond to the intended message and thus saturation is not desirable."},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566893.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S13.T2.5","endtime":"16:20","authors":"Bernd Bandemer","date":"1341504000000","papertitle":"An Achievable Rate Region for Three-Pair Interference Channels with Noise","starttime":"16:00","session":"S13.T2: Interference Channels","room":"Kresge Auditorium (109)","paperid":"1569566893"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
