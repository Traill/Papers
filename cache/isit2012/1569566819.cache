{"id":"1569566819","paper":{"title":{"text":"Quantized Stochastic Belief Propagation: Efﬁcient Message-Passing for Continuous State Spaces"},"authors":[{"name":"Nima Noorshams"},{"name":"Martin J. Wainwright ,2"}],"abstr":{"text":"Abstract\u2014Belief propagation (BP) is a widely used algorithm for computing the marginal distributions in graphical models. However, in applications involving continuous random variables, the messages themselves are real-valued functions, which leads to signiﬁcant computational bottlenecks. In this paper, we propose a low complexity method for performing belief propagation for con- tinuous state space problems. Our algorithm, which we refer to as quantized stochastic belief propagation (QSBP), is a randomized variant of BP in which each node only passes stochastically chosen information at each round. The most attractive feature of QSBP is its signiﬁcant gain in computational and communication efﬁciencies. In addition, we provide some theoretical guarantees including almost sure convergence and the rate of convergence for the case of tree-structured graphical models."},"body":{"text":"Graphical models provide a general framework for capturing statistical dependencies in large databases. In a wide range of applications, including signal and image processing, channel and source coding, computer vision, and bioinformatics, a fundamental challenge is the computation of marginal distri- butions over some subset of variables. Such marginalization problems, if naively approached, are computationally pro- hibitive. The sum-product algorithm, also known as belief propagation (BP), is a message-passing method for calculating exact marginals on tree-structured graphs, and approximate marginals for graphs with cycles. For more details on graphical models and belief propagation, we refer the reader to the papers and books [7], [12], [1], [13].\nMany applications of graphical models involve random vari- ables that take continuous values. Examples of such problems include disparity estimation and stereo in computer vision, tracking problems in sensor networks, vehicle localization, image geotagging, protein folding among other problems. With certain exceptions (such as multivariate Gaussian problems), it is usually expensive to compute, store and transmit BP messages for such continuous problems. Motivated by this challenge, researchers have proposed different techniques to reduce complexity of BP in different applications (e.g., see the papers [2], [11], [4], [5], [3], [6], [10], [8] and references therein). Various types of quantization schemes [3], [6] have been proposed as a way to reduce the effective state space, thereby lowering the complexity. In another line of work, researchers have proposed stochastic methods inspired by par- ticle ﬁltering [2], [11], [4], [5]. These techniques are typically based on approximating the messages as weighted sum of\nparticles [4], [5], or mixture of Gaussians [11]. Song et al. [10] proposed a combined technique, using kernel methods, for learning and performing inference in a simultaneous manner.\nIn this paper, we present a low-complexity alternative to the belief propagation for the case of continuous state space problems. Our method, which we refer to as quantized stochastic belief propagation (QSBP), is an approximated and randomized version of the BP, where each node only passes randomly selected partial information to its neighbors at each round. Thus, the QSBP updates lead to substantial improve- ments in communication and computational efﬁciencies. Our main contributions are to analyze the convergence properties of the QSBP algorithm, and to provide rigorous bounds on its approximation error as a function of the associated com- putational complexity. In particular, for tree-structured graphs, we estabish almost sure convergence, and provide an explicit inverse polynomial convergence rate. The algorithm of this paper is an extension of a method previously introduced in our recent work [8] for the case of discrete state spaces.\nIn this section, we provide some background on graphical models as well as the belief propagation algorithm.\nConsider a random vector X := {X 1 , X 2 , . . . , X n }, where for each u = 1, 2, . . . , n, the variable X u takes values in some space X . An undirected graphical model, also known as a Markov random ﬁeld, deﬁnes a family of joint probability distributions over this random vector by associating the index set {1, 2, . . . , n} with the vertex set V of an undirected graph G = (V, E). In addition to the vertex set, the graph consists of a collection of edges E ⊂ V ×V, where a pair (u, v) ∈ E if and only if nodes u and v are connected by an edge. The structure of the graph describes the statistical dependencies among the different random variables. More precisely, consider the set of all possible probability distributions, continuous with respect to an arbitrary measure µ, that factorizes as\nwhere ψ u : X → (0, ∞) is the node potential function, and ψ uv : X ×X → (0, ∞) is the edge potential function. In terms\nof drawing conclusions about the data, the central object is the marginal distribution i.e.\nsimilarly deﬁned for all nodes u ∈ V. The integrals involved in the marginalization problem are not tractable. A method for computing the marginals is an algorithm known as the belief propagation, to which we now turn.\nBelief propagation, also known as the sum-product algo- rithm, is an iterative algorithm consisting of a set of local message-passing rounds, for computing either exact or ap- proximate marginal distributions. Here we provide a very brief treatment, referring the reader to various standard sources [7], [1], [12] for further background.\nIn order to deﬁne the message-passing updates, we re- quire some further notation. For each node u ∈ V, let N (u) := {w | (w, u) ∈ E} denote its set of neighbors, and let E := {(u → v) | u ∈ V, v ∈ N (u)} denote the set of all directed edges in the graph. Furthermore, let S denote the set of all feasible probability densities deﬁned on the space X . In belief propagation algorithm, one message m uv ∈ S is assigned to every directed edge (u → v) ∈ E. With slight abuse of notation, we denote the set of all messages by m = {m uv } (u→v)∈E .\nAt each round t = 0, 1, . . ., every node u ∈ V calculates a message m t +1 uv ∈ S to be sent to its neighbor v ∈ N (u). In mathematical terms, this operation can be represented as an update of the form m t +1 uv = F uv (m t ), where F uv : S 2|E| → S is the local update operator. In more detail, for each x ∈ X , we have\nm t +1 uv (x) = [F uv (m t )](x) = κ\nEquation (2) is basically an iterative way of solving a set of ﬁxed point equations. More precisely, by concatenating the local updates (2), we obtain a global update operator F : S 2|E| → S 2|E| of the form F (m) = {F uv (m)} (u→v)∈E . The goal of message-passing is to obtain a ﬁxed point, meaning a vector m ∗ ∈ S 2|E| such that F (m ∗ ) = m ∗ . Given a ﬁxed point m ∗ , node v computes its marginal (approximation) τ ∗ v by combining the local potential func- tion ψ v with a product of all incoming messages as\nτ ∗ v (x) = κ ψ v (x) u ∈N (v) m ∗ uv (x), where κ is a normaliza- tion constant chosen so that X τ ∗ v (x) µ(dx) = 1.\nFor tree-structured (cycle-free) graphs, it is known that the update (2) has a unique ﬁxed point and BP computes the exact marginals in a ﬁnite number of iterations. However, the same message-passing updates can also be applied to more general\ngraphs, and are known to be extremely effective for computing approximate marginals in numerous applications.\nWe now turn to the description of the quantized stochastic belief propagation for general state spaces as well as the statement of our main theoretical result.\nAs mentioned previously, when belief propagation is applied to continous random variables, every node is required to compute and transmit a real-valued function (message) to its neighbors at every round. With certain exceptions (such as multivariate Gaussians), these continuous-valued messages do not have ﬁnite representations, so that this approach is not computationally feasible. We are thus motivated to develop a randomized and approximate algorithm, one which is motivated by the following observation: the message-passing updates can be formulated as an expectation involving a reweighted compatibility function. Here the expectation is taken place over a probability distribution that is a function of the messages and changes from iteration to iteration. Based on this perspective, we are naturally led to an adaptively randomized variant of BP: instead of computing and transmitting the full expectation, which incurs computing an intractable integral and transmitting a function, QSBP simply draws samples form the probability distribution and perform a simple adaptive update. Moreover, in order to have an efﬁcient representation, we approximate the continuous messages by truncating their expansion over an appropriately chosen basis of functions.\nIn order to make these ideas precise, we require some notation. Let {φ i } ∞ i =1 be a set of basis functions orthonor- mal in L 2 (µ), meaning that φ i 2 2 := X |φ i (x)| 2 µ(dx) = 1, and φ i , φ j := X φ i (x)φ j (x)µ(dx) = 0 for all i = 1, 2, . . ., and j = i. Assuming that the BP messages can be well approximated by the ﬁrst r basis functions, we quantize the messages by projecting them onto the space Q r := span{φ 1 , φ 2 , . . . , φ r }. More precisely, let m uv = \t ∞ i =1 a uv ;i φ i be the message expansion in the {φ i } ∞ i =1 system, then ˆ m uv = r i =1 a uv ;i φ i denotes its projection onto the space Q r . Quantizing the messages, we can equivalently consider the r-dimensional vector a uv := [a uv ;1 , . . . , a uv ;r ] T ∈ R r as the message transmitted by node u to node v.\nFurthermore, for all directed edges (u → v), deﬁne the \t marginal \t effect of the \t poten- tials \t β uv (y) := ψ u (y) X ψ uv (x, y) µ(dx), \t as well as the normalized compatibility function Γ uv (·, y) := ψ uv (·, y)/ x ∈X ψ uv (x, y) µ(dx). Let ˆ Γ uv (·, y) denote the projection of Γ uv (·, y) onto the quantization space Q r with the approximation error 1\nQSBP for continuous state spaces: Now we are ready for a precise description of the QSBP algorithm.\n(1) Initialize the message coefﬁcients a uv ;i = 1 for all i = 1, 2, . . . , r and directed edges (u → v) ∈ E.\n(2) For iterations t = 0, 1, . . ., and for each directed edge (u → v) ∈ E:\n(a) Compute ˆ m t wu = r i =1 a t wu ;i φ i and form the modi- ﬁed message\n(b) Draw samples Y t +1 := {Y t +1 1 , Y t +1 2 , . . . , Y t +1 k } from the probability density\n(d) For the step size η t = 1/(t + 1), update the message coefﬁcients via\nNote that after projecting the messages onto the space Q r , they may not stay positive every where on X . Therefore, to prevent such unpleasant events, we modify the messages (4).\nAs a concrete example, consider the case where edge potentials are symmetric, non-negative deﬁnite kernels 2 . Then, by Mercer\u2019s theorem we have ψ uv (x, y) = ∞ j =1 λ uv ;j φ j (x) φ j (y), \t for \t non- negative eigenvalues {λ uv ;j } ∞ j =1 . Compute the function g uv (y) := X ψ uv (x, y)µ(dx) ofﬂine. Doing some algebra in order to simplify the integral (6) yields\nTherefore, the coefﬁcients (6) can be calculated with only O(kr) summations and/or multiplications. On the other hand, using a standard technique such as accept-reject sampling [9], we can draw a sample using O(rρ max ) operations, where ρ max denotes the maximum degree of the graph. Finally, the update equation (7) requires O(r) operations. Putting the pieces together, QSBP only requires O(k r ρ max ) summations and/or multiplications per iteration per edge, which is substantially less than the case of BP. Moreover, since g uv and the basis functions φ i are known to v, node u only requires to transmit the k samples {Y t +1 ℓ } k ℓ =1 to node v, demonstrating QSBP\u2019s signiﬁcant gain in communication efﬁciency.\nBefore stating the theoretical results, it is worth taking a pause to provide some intuition as to why the update (7) is ex- pected to work. Taking the conditional expectation of the equa- tion (6) given ˆ m t or equivalently a t := [a t uv ] (u→v)∈E ∈ R 2|E|r , we ﬁnd that E[b uv ;i (Y t +1 )|a t ] is equal to\nfor all i = 1, 2, . . . , r. Doing some algebra simplifying the last expression yields\nwhere we used the heuristic that m is a good approximation of m. Therefore, in an average sense, update (7) is equivalent of the damped version of the usual BP updates projected onto the quantization space Q r . The technical difﬁculty lies in showing that despite ﬂuctuations and the approximation error, the QSBP updates converge to the BP ﬁxed point.\nIn this section, we state some theoretical results regarding the QSBP algorithm. Our theorem provides guarantees on the special but very important case of tree-structured graphs. Belief propagation is known to have a unique ﬁxed point on trees. Denote this ﬁxed point by m ∗ , so we have F (m ∗ ) = m ∗ .\nLet ˆ m ∗ uv = r i =1 a ∗ uv ;i φ i be the projection of the BP ﬁxed point onto the quantization space Q r with expansion coef- ﬁcients {a ∗ uv ;i } r i =1 . Having the QSBP messages {a t uv ;i } r i =1 at time t, we deﬁne the local error on the directed edge (u → v) as e t uv := r i =1 |a t uv ;i − a ∗ uv ;i | 2 . In addition, by concatenating the local errors, we form the global error vector e t := [e t uv ] (u→v)∈E ∈ R 2|E| . Similarly, we deﬁne the 2|E|-dimensional vector\nwhere the edge-based error δ uv was previously deﬁned (3). Note that this vector of approximation errors is a function of the number of coefﬁcients r in our basis approximation.\nSome further remarks on notation before proceeding: for a set B we say e t → B as t → ∞ if and only if inf e ∈B |e t − e| → 0. We also make use of the element wise inequality based on positive orthant. In particular, for vectors x, y ∈ R r we say x y if and only if x(i) ≤ y(i) for all i = 1, 2, . . . , r, with a similar interpretation for x y. Finally, recall that for some integer ℓ, a square matrix D is said to be nilpotent of degree ℓ if D ℓ = 0.\nOur results to follow, are based on the assumption that the compatibility functions Γ uv (x, y), are ﬁnite and bounded away from zero. Formally, we assume:\nA: for all directed edges (u → v), we have inf x,y ∈X Γ uv (x, y) > 0, and sup x,y ∈X Γ uv (x, y) < ∞.\nTheorem 1. Consider the sequence of messages {a t } ∞ t =0 gen- erated by the QSBP on a tree-structured graphical model. Un- der condition A, there exists a nilpotent matrix D ∈ R 2|E|×2|E| such that\nwhere B := {e ∈ R 2|E| | |e| D(I − D) −1 ∆}. Furthermore, we have\nSome comments are in order regarding the interpretation and consequences of these results. By Parseval\u2019s theorem, we know e t uv = ˆ m t uv − ˆ m ∗ uv 2 2 . As shown by Theorem 1 (a), the error is guaranteed to almost surely converge to the ball B, regardless of the node and edge potentials. Thus, the eventual distance between the QSBP message and the quantized BP ﬁxed point is of the order D(I − D) −1 ∆ ∞ , where we recall that ∆ = ∆(r) is the approximation error introduced by truncating to the ﬁrst r basis functions. This approximation error can be arbitrarily small by increasing the number of expansion coefﬁcients r, albeit with an associated increase in computational complexity. In addition, as given by the second part of the theorem, the rate of convergence at most O(1/t), neglecting logarithmic factors.\nIt is interesting to compare these guarantees to those as- sociated with certain types of particle ﬁltering methods, for which it is possible to establish consistency as the number of particles tends to inﬁnity [4] or ﬁnite-length results inversely proportional to the square root of the number of particles [5]. The computational complexity of particle-based BP scales quadratically with the number of particles [5]. Therefore, obtaining ǫ-accurate solution requires O(1/ǫ 2 ) operations. In contrast, we have ﬁnite-length bounds on the error at iteration t in terms of the number of expansion coefﬁcients and the associated approximate node/edge potentials. The complexity per iteration scales linearly with the number of expansion coefﬁcients; thus, obtaining ǫ-accurate solution to the quantized BP ﬁxed point requires O(r/ǫ) operations. For sufﬁciently smooth compatibility functions, the approximation error ∆(r) ∞ drops off rapidly as a function of r, so that this yields a favorable trade-off.\nDue to space constraints, we present only rough proof sketches, leaving the details to the long version. Let b t uv ;i denote the basis function expansion coefﬁcients of F uv (m t ) i.e. F uv (m t ) = ∞ i =1 b t uv ;i φ i . Also deﬁne the deviation ζ t +1 uv ;i := b t uv ;i (Y t +1 ) − b t uv ;i . Subtracting a ∗ uv ;i from both sides of the update (7) and unwrapping the recursion, we obtain\nSquaring the last equality and using the Cauchy Schwartz inequality yields that r i =1 |a t +1 uv ;i − a ∗ uv ;i | 2 is at most\n(9) The proof consists of three major parts: upper bounding the deterministic term G t +1 uv ;1 in terms of the message expansion coefﬁcients a τ uv ;i , controlling the stochastic term G t +1 uv ;2 , and ﬁnally establishing convergence and the rate of convergence by exploiting the results of the previous two parts.\na) Upper bounding the term G t +1 uv ;1 : By Parseval\u2019s theo- rem one can relate G t +1 uv ;1 to the term F uv (m t ) − F uv (m ∗ ) 2 . On the other hand, under condition A, we can prove that F uv is a Lipschitz operator. Doing some algebra, exploiting these facts, we obtain\nforms a bounded martingale difference sequence with respect to the ﬁltration F t = σ(a 0 , a 1 , . . . , a t ). Therefore, by standard convergence theorems, we have G t +1 uv ;2 → 0 almost surely as t → ∞. Furthermore, by integrating the tail bound we can show that E[G t +1 uv ;2 ] = O(1/t).\nc) Establishing \t the \t convergence: \t Combining the scalar inequalities (9) in conjunction with the results established so far yields the vector inequality e t +1 \t 1 t +1 t τ =1 D e τ + D ∆ + ν t +1 , where D denotes a 2|E| × 2|E| matrix with entries d uv,wu , indexed by pairs of directed edges, and ν t +1 is a small term satisfying ν t +1 a.s. −→ 0. From Lemma 1 in the paper [8] we know that D is a nilpotent matrix with order ℓ = diam(G). Finally, unwrapping the recursive vector inequality for a total of ℓ = diam(G) times and doing some algebra yields the claims.\nTo demonstrate the effectiveness of the QSBP algorithm, some experimental results are presented in this section. We consider the simple but popular mixture of Gaussians model for node and edge potentials. More speciﬁcally, for all (u, v) ∈ E and u ∈ V, assume ψ uv (x, y) = 3 i =1 p uv ;i exp − (x − y) 2 /(2σ 2 uv ;i ) , and ψ u (x) = 3 i =1 p u ;i exp − (x − µ u ;i ) 2 /(2σ 2 u ;i ) , \t where\np uv ;i = 3 i =1 p u ;i = 1. The mixture parameters are randomly chosen from the range σ 2 u ;i , σ 2 uv ;i ∈ (0, 0.5] and µ u ;i ∈ [−3, 3] for all (u, v) ∈ E, u ∈ V, and i = 1, 2, 3.\nFor a chain of size n = 100 and ﬁxed parameters, we ﬁrst run the belief propagation to compute the ﬁxed point m ∗ . In\norder to do so, we approximate the integral update (2) with its Riemann sum over the range X = [−5, 5] and with 100 samples per unit time. Then we run the quantized stochastic belief propagation 3 using the ﬁrst r = 10 Fourier basis coef- ﬁcients and k = 5 samples to ﬁnd the sequence of messages {m t } ∞ t =0 . Having found the QSBP messages, we calculate the normalized square error (u→v)∈E m t uv − m ∗ uv 2 2 / 2|E|. Figure 1 illustrates the error versus the number of iterations for 10 different sample paths. In our next experiment, we run QSBP for different number of expansion coefﬁcients r ∈ {2, 5, 10} on the same graph. Figure 2 shows a typical BP message and the corresponding QSBP approximates after t = 100 iterations. As expected, the approximation error becomes negligible for large number of expansion coefﬁcients.\nIn this paper, we presented quantized stochastic belief propagation as a low-complexity alternative to the belief\npropagation algorithm for the case of continuous state space problems. Quantized stochastic belief propagation is inspired by the observation that the sum-product update can be formu- lated as the expectation of a compatibility function. Instead of computing the expectation that in general is intractable, the idea of QSBP is to draw samples from the appropriate distribution and update the messages accordingly. One of the additional complications of the continuous case is having an efﬁcient representation of the continuous messages. To resolve this issue, we expanded the continuous messages in some basis functions system and truncated the result. The approximation error incurred by the quantization leads to an offset in the error. This offset cannot be overcome by any method; however it can be made arbitrarily small by a proper choice of basis functions and the number of expansion coefﬁcients. In addition, we provided some theoretical guarantees including the rate of convergence for the case of tree-structured graphs.\nThere are a number of problems that remain to be studied, among which is conducting more extensive and realistic ex- periments. Moreover, it would be interesting to develop some theory for the general graphical models.\nWork was partially supported by ONR MURI grant: N00014-11-1-0688."},"refs":[{"authors":[{"name":"S. M. Aj"},{"name":"R. J. McEliece"}],"title":{"text":"The generalized distributive law and free energy minimization"}},{"authors":[{"name":"M. S. Arulampala"},{"name":"S. Maskel"},{"name":"N. Gordo"},{"name":"T. Clapp"}],"title":{"text":"A tutorial on particle ﬁlters for online nonlinear/non-Gaussian Bayesian tracking"}},{"authors":[{"name":"J. Coughla"},{"name":"H. Shen"}],"title":{"text":"Dynamic quantization for belief propagation in sparse spaces"}},{"authors":[{"name":"A. Douce"},{"name":"N. de Freita"},{"name":"N. Gordon"}],"title":{"text":"Sequential Monte Carlo methods in practice "}},{"authors":[{"name":"A. T. Ihle"},{"name":"D. McAllester"}],"title":{"text":"Particle belief propagation"}},{"authors":[{"name":"M. Isar"},{"name":"J. MacCormic"},{"name":"K. Achan"}],"title":{"text":"Continuously-adaptive discretization for message-passing algorithms"}},{"authors":[{"name":"F. R. Kschischan"},{"name":"B. J. Fre"},{"name":"H. A. Loeliger"}],"title":{"text":"Factor graphs and the sum-product algorithm"}},{"authors":[{"name":"N. Noorsham"},{"name":"M. J. Wainwright"}],"title":{"text":"Stochastic belief propagation: Low complexity message-passing with guarantees"}},{"authors":[{"name":"C. P. Rober"},{"name":"G. Casella"}],"title":{"text":"Monte Carlo statistical methods"}},{"authors":[{"name":"L. Son"},{"name":"A. Gretto"},{"name":"D. Bickso"},{"name":"Y. Lo"},{"name":"C. Guestrin"}],"title":{"text":"Kernel belief propagation"}},{"authors":[{"name":"E. B. Suddert"},{"name":"A. T. Ihle"},{"name":"W. T. Freema"},{"name":"A. S. Willsky"}],"title":{"text":"Non- parametric belief propagation"}},{"authors":[{"name":"M. J. Wainwrigh"},{"name":"M. I. Jordan"}],"title":{"text":"Graphical Models, Exponential Families, and Variational Inference "}},{"authors":[{"name":"J. S. Yedidi"},{"name":"W. T. Freema"},{"name":"Y. Weiss"}],"title":{"text":"Constructing free energy approximations and generalized belief propagation algorithms"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566819.pdf"},"links":[{"id":"1569566683","weight":2},{"id":"1569566855","weight":5},{"id":"1569565551","weight":2},{"id":"1569566761","weight":8},{"id":"1569566943","weight":2},{"id":"1569552245","weight":2},{"id":"1569564469","weight":2},{"id":"1569565775","weight":2},{"id":"1569564849","weight":2},{"id":"1569566739","weight":2},{"id":"1569565809","weight":2},{"id":"1569566311","weight":5},{"id":"1569559805","weight":2},{"id":"1569565559","weight":2},{"id":"1569566809","weight":2},{"id":"1569566853","weight":2},{"id":"1569567235","weight":5},{"id":"1569566901","weight":2},{"id":"1569565155","weight":2},{"id":"1569559199","weight":2},{"id":"1569565665","weight":10},{"id":"1569566983","weight":2},{"id":"1569565093","weight":2},{"id":"1569565661","weight":2},{"id":"1569566887","weight":2},{"id":"1569566917","weight":5},{"id":"1569566595","weight":5},{"id":"1569566137","weight":2},{"id":"1569566639","weight":8},{"id":"1569564787","weight":8},{"id":"1569566171","weight":5},{"id":"1569567691","weight":2},{"id":"1569562367","weight":2},{"id":"1569560785","weight":2},{"id":"1569564961","weight":2},{"id":"1569565165","weight":2},{"id":"1569565031","weight":2},{"id":"1569558697","weight":2},{"id":"1569564807","weight":2}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T7.3","endtime":"15:40","authors":"Nima Noorshams, Martin J. Wainwright","date":"1341328800000","papertitle":"Quantized Stochastic Belief Propagation: Efficient Message-Passing for Continuous State Spaces","starttime":"15:20","session":"S7.T7: Approximate Belief Propagation","room":"Stratton (407)","paperid":"1569566819"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
