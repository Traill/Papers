{"id":"1569566299","paper":{"title":{"text":"Lossy Communication over Multiple-Access Channels with Feedback"},"authors":[{"name":"Paolo Minero"},{"name":"Saﬁtha Jayaraj"}],"abstr":{"text":"Abstract\u2014We consider the problem of lossy communication of correlated sources over multiple access channels with noiseless causal feedback. A sufﬁcient condition for lossy transmission is presented, which generalizes several previous results on commu- nication over multiple access channels, recovering as special cases channel coding results by King, Cover, and Leung, and extending a previous sufﬁcient condition for lossless communication by Cover, El Gamal, and Salehi. The proposed coding scheme is based on a recently developed analog-digital hybrid coding scheme for joint source\u2013channel coding combined with block- Markov coding."},"body":{"text":"We study the problem of lossy communication of two correlated discrete memoryless sources (2-DMS) (S 1 , S 2 ) over a discrete memoryless multiple-access channel (DM-MAC) (X 1 × X 2 , p(y 1 , y 2 |x), Y) with noiseless causal feedback (see Fig. 1). Here each separate encoder observes the feedback from the channel output of the previous time instants as well as one of the two source components, which it wishes to communicate to the common receiver so that two functions of the sources can be reconstructed with desired distortions D 1 and D 2 . This problem models communication scenarios where spatially separated senders cooperate in transmitting correlated information over a shared medium and serves as a building block to study the tradeoff between cooperation and competition among users in more general communication settings.\nThe formal deﬁnition of the problem is as follows. A (|S 1 | n , |S 2 | n , n) joint source\u2013channel code consists of\n\u2022 two encoders, where at time i ∈ [1 : n] encoder j = 1, 2 assigns a symbol x j,i (s n j , y i −1 ) ∈ X j to each sequence s n j ∈ S n j and past received output sequence y i −1 ∈ Y i −1 , and\n\u2022 a decoder that assigns an estimate (ˆ s n 1 , ˆ s n 2 ) ∈ ˆ S n 1 × ˆ S n 2 to each sequence y n ∈ Y n .\nLet d 1 (s 1 , ˆ s 1 ) and d 2 (s 2 , ˆ s 2 ) be two distortion measures. The average per-letter distortion d j (s n j , ˆ s n j ), j = 1, 2, is deﬁned as d(s n j , ˆ s n j ) = (1/n) n i =1 d(s ji , ˆ s ji ). A distortion pair (D 1 , D 2 ) is said to be achievable for communication of the 2-DMS (S 1 , S 2 ) over the DM-MAC p(y|x 1 , x 2 ) with feedback if there exists a sequence of (|S 1 | n , |S 2 | n , n) joint source\u2013channel codes such that\nThe optimal distortion region is deﬁned as the closure of the set of all achievable distortion pairs (D 1 , D 2 ). A computable characterization of the optimal distortion region is not known in general.\nSpecial instances of this problem have been previously stud- ied in the literature. Ong and Motani [1] studied the lossless communication problem where the receiver is interested in reconstructing the sources with vanishing error probability and derived sufﬁcient conditions for reliable transmission. Lapi- doth and Tinguely analyzed the problem of sending a bivariate Gaussian source over a Gaussian multiple access with feed- back under quadratic distortion measures [2] and characterized tight scaling results in the limiting regimes of low and high signal-to-noise (SNR) ratios. King [3] investigated the special case where S 1 = (W 0 , W 1 ) and S 2 = (W 0 , W 2 ), where W 0 , W 1 , and W 2 are three mutually independent random variables of entropies R 0 , R 1 , and R 2 , respectively. In this case, the set of triples (R 0 , R 1 , R 2 ) that can be reliably transmitted is referred to as the feedback capacity region of the DM- MAC. Assuming that the entropy of R 0 is zero, Ozarow [4] characterized the feedback capacity of a Gaussian MAC with feedback, while Cover and Leung [5] established an inner bound on the feedback capacity of a general DM-MAC which is suboptimal in general [4], [6], [7] but tight for a special class of channels [8]. Finally, an inner bound to the optimal distortion region in the case where there is no output feedback was derived in [9].\nIn this paper, we develop an inner bound on the optimal distortion region for general multiple access channels with feedback. This inner bound generalizes the result by Lim et al. [9] to the case where the sources have a common part \u2013\nin the sense of Gács\u2013Körner [10] and Witsenhausen [11] \u2013 and noiseless output feedback is available at the transmitters, and the result by Ong and Motani [1] to the case of lossy communication. When we specialize our result to the prob- lem studied by Lapidoth and Tinguely, the proposed coding scheme achieves the optimal scaling results characterized in [2]. The achievable region that we obtain includes the feedback capacity inner bounds by King [3] and by Cover and Leung [5]. However, in the special of case of the Gaussian MAC, our inner bound does not achieve the feedback capacity characterized by Ozarow [4]. Two special cases of channels and sources for which our approach leads to tight conditions are presented. The associated coding scheme is based on the analog-digital hybrid coding scheme for joint source\u2013channel coding proposed in [9], [12]. The novelty of our construction lies in combining hybrid coding with a block-Markov coding similar to the one used in the analysis of the Cover and Leung\u2019s inner bound [13].\nThe rest of the paper is organized as follows. In the next section, we present the main result of the paper and its relationship with existing results in the literature. A sketch of the proof of the main coding theorem is given in Section III. Section IV concludes the paper. Throughout the paper, we use the notation in [13].\nTheorem 1: A distortion pair (D 1 , D 2 ) is achievable for communication of the 2-DMS (S 1 , S 2 ) with common part K over a DM-MAC p(y|x 1 , x 2 ) with noiseless feedback if\nI(U 1 ; S 1 |U 0 , U 2 ) < I(U 1 ; Y |U 0 , U 2 ), I(U 2 ; S 2 |U 0 , U 1 ) < I(U 2 ; Y |U 0 , U 1 ),\nfor some pmf p(u 0 |k)p(u 1 |s 1 , u 0 )p(u 2 |s 2 , u 0 ), two encod- ing functions x 1 (s 1 , u 0 , u 1 ), x 2 (s 2 , u 0 , u 2 ), and two decod- ing functions ˆ s 1 (u 0 , u 1 , u 2 , y) and ˆ s 2 (u 0 , u 1 , u 2 , y) such that E(d j (S j , ˆ S j )) ≤ D j , j = 1, 2.\nA few remarks are in order. The inequalities that appear in Theorem 1 have a natural interpretation \u2013 the terms at the left-hand side denote the rates at which the source (S 1 , S 2 ) is compressed (described) by the codewords U 0 , U 1 , and U 2 , while the terms at the right-hand side denote the rates at which these codewords are communicated over the channel. Thus, Theorem 1 states that a distortion pair (D 1 , D 2 ) is achievable if the source compression rates do not exceed the channel communication rates. At a high level, a similar condition can also be obtained by performing separate source-channel coding, by concatenating, for instance, the distributed source coding scheme in [14] with the channel coding scheme in [3]. The main difference with the separation-base approach is that here the same codewords U 0 , U 1 , and U 2 are used for both source coding and channel coding.\nIn Theorem 1, the codeword U 0 is available at both senders and is used for jointly compressing the common part K of the\nsource (S 1 , S 2 ) as well as for coherently communicating over the DM-MAC. To illustrate the role played by U 0 , consider the following two extreme cases:\na) Degraded \u201csource\u201d set problem : On the one hand, con- sider the degraded \u201csource\u201d set problem where encoder 1 has access to both source sequences. This setup can be captured by considering transmission of a source ( ˜ S 1 , ˜ S 2 ) such that ˜ S 1 = (S 1 , S 2 ), ˜ S 2 = S 2 . For this problem, the sufﬁcient condition in Theorem 1 is also necessary.\nCorollary 1: A distortion pair (D 1 , D 2 ) is achievable for communication of the degraded source 2-DMS ( ˜ S 1 , ˜ S 2 ) over a DM-MAC p(y|x 1 , x 2 ) with noiseless feedback if\nI( ˆ S 1 ; S 1 | ˆ S 2 ) < I(X 1 ; Y |X 2 ), I( ˆ S 1 , ˆ S 2 ; S 1 , S 2 ) < I(X 1 , X 2 ; Y ),\nfor some pmf p(ˆ s 2 |s 2 )p(ˆ s 1 |s 1 , s 2 )p(x 1 , x 2 ). Conversely, if a distortion pair (D 1 , D 2 ) can be achieved, then the above in- equalities with \u201c <\u201d replaced by \u201c≤\u201d have to be simultaneously satisﬁed for some p(ˆ s 2 |s 2 )p(ˆ s 1 |s 1 , s 2 )p(x 1 , x 2 ).\nProof: For the proof of the direct part, take U 0 = ( ˆ S 2 , X 2 ), U 1 = ( ˆ S 1 , X 1 ), and U 2 = ∅ in Theorem 1 for some joint pmf p(ˆ s 2 |s 2 )p(ˆ s 1 |s 1 , s 2 )p(x 1 , x 2 ). The proof of the converse follows from standard techniques and therefore it is not reported here.\nNotice that in the proof of Corollary 1 U 0 is chosen equal to two codewords ˆ S 2 and X 2 . ˆ S 2 is used by the senders to jointly compress the common source S 2 , while X 2 is used to cooperatively communicate the (digital) compression index to the receiver. In this case, the special structure of the source allows the senders to fully cooperate and so the channel output feedback is useless. It is easily veriﬁed, in fact, that the sufﬁcient condition in Corollary 1 can be directly obtained from the coding scheme in [9], which does not make use of channel feedback. This shows that in this case feedback does not enlarge the set of achievable distortion pairs for communication over a MAC.\nb) Independent sources: On the other hand, consider the special case where the sources S 1 and S 2 are independent and the DM-MAC belongs to the class of multiple access channels considered by Willems [8], in which one channel input is a deterministic function of the channel output and the other channel input. For this problem, the sufﬁcient condition in Theorem 1 is also necessary.\nCorollary 2: A distortion pair (D 1 , D 2 ) is achievable for communication of the 2-DMS (S 1 , S 2 ) over a DM-MAC p(y|x 1 , x 2 ) in the class studied in [8] and with noiseless feedback if\nI( ˆ S 1 ; S 1 | ˆ S 2 ) < I(X 1 ; Y |U, X 2 ), I( ˆ S 2 ; S 2 | ˆ S 1 ) < I(X 2 ; Y |U, X 1 ),\nfor some pmf p(ˆ s 1 |s 1 )p(ˆ s 2 |s 2 )p(u)p(x 1 |u)p(x 2 |u). Conversely, if a distortion pair (D 1 , D 2 ) can be\nachieved, then the above inequalities with \u201c <\u201d replaced by \u201c ≤\u201d have to be simultaneously satisﬁed for some p(ˆ s 1 |s 1 )p(ˆ s 2 |s 2 )p(u)p(x 1 |u)p(x 2 |u).\nProof: For the proof of the direct part, take U 0 = U , U 1 = ( ˆ S 1 , X 1 ), and U 2 = ( ˆ S 2 , X 2 ) in Theorem 1 for some joint pmf p(ˆ s 2 |s 2 )p(ˆ s 1 |s 1 )p(u)p(x 1 |u)p(x 2 |u). The proof of the converse follows by application of [15, Theorem 2] and [8], but it can also be proved directly using standard converse techniques.\nIn this case, the sources are independent and so cooperation between the senders can only take place at the channel-coding level, through the channel output feedback, while the source- coding has to be performed in a distributed manner. In the proof of Corollary 2 notice that U 0 is chosen independent of the sources and is only used to induce coherent transmissions over the channel, exactly as in the channel coding scheme in [5]. It should also be remarked that the same sufﬁcient condition can be obtained by separately performing source and channel coding, ﬁrst compressing the sources using the dis- tributed source coding scheme by Berger [16] and Tung [17] and then encoding the resulting sequences using the channel coding scheme for the DM-MAC with feedback in [5].\nWhen specialized to the lossless case, wherein d 1 and d 2 are Hamming distortion measures and D 1 = D 2 = 0, Theorem 1 yields the following sufﬁcient condition for lossless communication of a 2-DMS over a DM-MAC with feedback:\nCorollary 3: A 2-DMS (S 1 , S 2 ) with common part K can be communicated losslessly over a DM-MAC p(y|x 1 , x 2 ) with noiseless feedback if\nH(S 1 |S 2 ) < I(X 1 ; Y |X 2 , S 2 , Q), H(S 2 |S 1 ) < I(X 2 ; Y |X 1 , S 1 , Q), H(S 1 , S 2 ) < I(X 1 , X 2 ; Y ),\nProof: Take U 0 = (K, Q), U j = (X j , S j ), and ˆ s j = s j , j = 1, 2, in Theorem 1 under the distribution p(q)p(x 1 |s 1 , q)p(x 2 |s 2 , q).\nCorollary 3 generalizes the sufﬁcient condition derived by Cover, El Gamal, and Salehi [18] for lossless communication of correlated sources over a DM-MAC to the case where noiseless feedback is available at the encoders.\nWe can further specialize the result in Corollary 3 to the channel coding problem studied by King [3] where S 1 = (W 0 , W 1 ) and S 2 = (W 0 , W 2 ), with H(W j ) = R j , j = 0, 1, 2. By choosing (X 1 , X 2 ) to be independent of (W 0 , W 1 , W 2 ), Corollary 3 yields King\u2019s inner bound [3] to the capacity region, which states that a rate triple (R 0 , R 1 , R 2 ) is achievable if it satisﬁes\nR 1 < I(X 1 ; Y |X 2 , Q) R 2 < I(X 2 ; Y |X 1 , Q)\nfor some pmf p(q)p(x 1 |q)p(x 2 |q). It can be shown that this inner bound achieves capacity for the class of multiple access channels studied by Willems in [8]. Moreover, by setting R 0 = 0 we recover the inner bound of Cover and Leung [5] for the problem of reliable communication of two independent messages over a DM-MAC with feedback. We remark, how- ever, that the resulting region does not include the inner bounds derived by Bross and Lapidoth [6] and by Venkataramanan and Pradhan [7], which strictly outperform the inner bound by Cover and Leung, as well as the result by Ozarow for the Gaussian MAC with feedback [4].\nFinally, when we can specialize Theorem 1 to the problem of transmitting a Gaussian source over a Gaussian MAC with feedback under mean-square distortion, we recover the low and high SNR scaling results derived in [2]. On the one hand, Lapidoth and Tinguely proved that at low SNR uncoded transmission is optimal. We can recover this result by setting in Theorem 1 the encoding function x 1 ( x 2 ) proportional to s 1 ( s 2 ) and the decoding function ˆ s 1 ( ˆ s 2 ) equal to the minimum mean-square estimate of s 1 ( s 2 ) given y. On the other hand, they showed that at high SNR the source-channel separation scheme obtained concatenating the Gaussian multiterminal source coding scheme in [19], [20] with the channel coding scheme by Ozarow is asymptotically optimal. It can be shown that the same asymptotic performance can also be obtained after replacing the Ozarow\u2019s scheme with the (continuous- alphabet version of the) scheme by Cover and Leung, and thus it can be achieved by Theorem 1 .\nThe coding scheme used for the proof of achievability in Theorem 1 uses the hybrid coding technique for joint source- channel coding proposed in [9], [12], combined with a block Markov coding technique. We provide here a sketch, while the full prove can be found in the extended version of this paper [21]. Coding is performed in blocks of n transmissions. Within each block, the senders select a codebook to use for hybrid coding based on the received channel output feedback in the previous block. Decoding is performed using backward decoding starting from the last transmission block. At a high level, the sufﬁcient condition is obtained combining the con- ditions for distributed source encoding with those for channel decoding. Speciﬁcally, in each block the source encoding operation is successful if\nR 0 > I(U 0 ; S 1 , S 2 ), R 1 > I(U 1 ; S 1 |U 0 ), R 2 > I(U 2 ; S 2 |U 0 ),\nwhile the channel decoding operation across two consecutive blocks is successful if\nR 1 < I(U 1 ; Y, U 2 |U 0 ), R 2 < I(U 2 ; Y, U 1 |U 0 ),\nTheorem 1 is then established by eliminating the in- termediate rate triple (R 0 , R 1 , R 2 ) from the above in- equalities and by simplifying the resulting inequalities us- ing the fact that the underlying joint pmf factorizes as p(u 0 |k)p(u 1 |s 1 , u 0 )p(u 2 |s 2 , u 0 ). We remark that the deriva- tion of the conditions for channel coding requires the use of a technique developed in [9] to deal with the dependency between the transmitted messages and the codebook. In the re- maining of this section, we ﬁrst describe the random codebook generation and encoding/decoding scheme, then we outline the analysis of the probability of error.\nCodebook generation: Fix ǫ > ǫ \u2032 > 0 and a pmf p(u 0 |k) p(u 1 |u 0 , s 1 ) p(u 2 |u 0 , s 2 ), two encoding functions x 1 (s 1 , u 0 , u 1 ) and x 2 (s 2 , u 0 , u 2 ), and two reconstruction functions ˆ s 1 (u 0 , u 1 , u 2 , y) and ˆ s 2 (u 0 , u 1 , u 2 , y). As in block Markov coding, we randomly and independently generate a codebook for each block. For j ∈ [1 : b], randomly and in- dependently generate 2 n (R 0 +R 2 ) sequences u n 0 (m 0,j , m 2,j−1 ), m 0,j ∈ [1 : 2 nR 0 ] and m 2,j−1 ∈ [1 : 2 nR 2 ], each ac- cording to n i =1 p U 0 (u 0i ). For each (m 0,j , m 2,j−1 ), randomly and conditionally independently generate 2 nR 1 sequences u n 1 (m 1,j |m 0,j , m 2,j−1 ), m 1,j ∈ [1 : 2 nR 1 ], each according to n i =1 p U 1 |U 0 (u 1,i |u 0,i (m 0,j , m 2,j−1 )) and 2 nR 2 sequences u n 2 (m 2,j |m 0,j , m 2,j−1 ), m 2,j ∈ [1 : 2 nR 2 ], each according to\nEncoding: Let (s n 1 (j), s n 2 (j)) be the source sequences to be sent in block j ∈ [1, b]. Upon observing s n 2 (j), en- coder 2 ﬁnds an index m 0,j ∈ [1 : 2 nR 0 ] such that\nk n (j), u n 0 (m 0,j , m 2,j−1 ) ∈ T (n) ǫ \u2032 , where by convention m 2,0 = 1. If there is more than one such index, it chooses one of them at random 1 . If there is no such index, it chooses an arbitrary index at random from [1 : 2 nR 0 ]. Next, it ﬁnds an index m 2,j ∈ [1 : 2 nR 2 ] such that\nIf there is more than one such index, it chooses one of them at random. If there is no such index, it chooses an arbitrary index at random. Finally, encoder 2 transmits\n˜ m 2,j−2 ), u n 2 ( ˜ m 2,j−1 |m 0,j−1 , ˜ m 2,j−2 ), y n (j − 1) ∈ T (n) ǫ , where by convention ˜ m 2,0 = 1. In block j, encoder 1 ﬁnds an index m 0,j ∈ [1 : 2 nR 0 ] such that k n (j), u n 0 (m 0,j , ˜ m 2,j−1 ) \t ∈ T (n) ǫ \u2032 . Next, it ﬁnds an index m 1,j ∈ [1 : 2 nR 1 ] such that\nIf there is more than one such index, it chooses one of them at random. If there is no such index, it chooses an arbitrary index at random. Finally, encoder 1 transmits x 1,i s 1,i (j), u 0,i (m 0,j , ˜ m 2,j−1 ), u 1,i (m 1,j |m 0,j , ˜ m 2,j−1 ) for\nDecoding: Decoding at the receiver is done by backward decoding after b blocks are received. For j ∈ [b : 1], the decoder ﬁnds the unique index triple ( ˆ m 0,j , ˆ m 1,j , ˆ m 2,j−1 ) such that u n 0 ( ˆ m 0,j , ˆ m 2,j−1 ), u n 1 ( ˆ m 1,j | ˆ m 0,j , ˆ m 2,j−1 ), u n 2 ( ˆ m 2,j | ˆ m 0,j , ˆ m 2,j−1 ), y n (j) \t ∈ T (n) ǫ , with the initial condition ˆ m 2,b = 1, and sets the reproduction sequences as ˆ s 1,i (j) = ˆ s 1 (u 0,i ˆ m 0,j , ˆ m 2,j−1 ), u 1,i ( ˆ m 1,j | ˆ m 0,j , ˆ m 2,j−1 ), u 2,i ( ˆ m 2,j | ˆ m 0,j , ˆ m 2,j−1 ),\ny i (j) and ˆ s 2,i (j) = ˆ s 2 (u 0,i ˆ m 0,j , ˆ m 2,j−1 ), u 1,i ( ˆ m 1,j | ˆ m 0,j , ˆ m 2,j−1 ), u 2,i ( ˆ m 2,j | ˆ m 0,j , ˆ m 2,j−1 ), y i (j) for all i ∈ [1 : n].\nAnalysis of the probability of error: We bound the distortion averaged over (S n 1 (j), S n 2 (j)), the random codebook, and the random index assignments at the encoders. Let M 0,j , M 1,j , and M 2,j be random variables denoting the chosen indixes at the encoders in block j ∈ [1 : b]. Deﬁne the \u201cerror\u201d event:\nE(j) = S n 1 (j), S n 2 (j), U n 0 ( ˆ M 0,j , ˆ M 2,j−1 ), U n 1 ( ˆ M 1,j | ˆ M 0,j , ˆ M 2,j−1 ),\nand partition it into six events, three of which account for errors in the source encoding procedure\nE 0 (j) = {(U n 0 (m 0,j , ˜ M 2,j−1 ), K n ) ∈ T (n) ǫ \u2032 for all m 0,j }, E 1 (j) = {(S n 1 (j), U n 0 (M 0,j , ˜ M 2,j−1 ),\nU n 1 (m 1,j |M 0,j , ˜ M 2,j−1 )) ∈ T (n) ǫ \u2032 for all m 1,j }, E 2 (j) = {(S n 2 (j), U n 0 (M 0,j , M 2,j−1 ),\nwhile the remaining three are channel decoding error events E 3 (j) = { ˜ M 2,j−1 = M 2,j−1 },\nE 4 (j + 1) = { ˆ M 0,j+1 = M 0,j+1 , ˆ M 1,j+1 = M 1,j+1 , ˆ M 2,j = M 2,j },\nIt can be shown by the covering lemma and the conditional typicality lemma that the ﬁrst three terms in (2) tend to zero as n → ∞ if\nR 0 > I(U 0 ; K) + δ(ǫ), \t (3) R 1 > I(U 1 ; S 1 |U 0 ) + δ(ǫ), \t (4) R 2 > I(U 2 ; S 2 |U 0 ) + δ(ǫ), \t (5)\nwhere I(U 0 ; K) = I(U 0 ; S 1 , S 2 ) under the distribution p(s 1 , s 2 )p(u 0 |k).\nNext, consider E 3 (j), i.e., the event that encoder 1 fails in decoding the message M 2,j−1 transmitted by encoder 2 in block j − 1. Combining similar steps as in the analysis of hybrid coding [9] with those in the analysis of multi-hop coding for the relay channel [13], it can be shown that the probability of decoding error for encoder 1 at the end of block j − 1 goes to zero as n → ∞ if\nSince by assumption ˜ M 2,0 = 1, by forward induction P{E 3 (j)} → 0 as n → ∞ for every j ∈ [1 : b] if (6) is satisﬁed.\nSimilarly, by combining the hybrid coding analysis with the analysis of backward decoding [13], it can be shown that the probability of E 4 (j), i.e., the event that the decoder fails in decoding the messages M 0,j , M 1,j , and M 2,j−1 , goes to zero as n → ∞ if (6) is satisﬁed as well as\nBy convention M 2,b+1 = M 0,b+1 = M 1,b = 1, so by backward induction P{E 4 (j)} → 0 as n → ∞ for every j ∈ [1 : b] if (6), (7), and (8) are satisﬁed. Finally, by the Markov lemma [13], P(E 5 (j) ∩ 4 k =0 E c k (j) ∩ E c 4 (j + 1)) → 0 as n → ∞.\nIt follows if conditions (3)-(8) are satisﬁed for some (R 0 , R 1 , R 2 ), then each term in (2) tends to zero as n → ∞ and thus the probability of \u201cerror\u201d in each block j tends to zero as n → ∞. By the law of total expectation and the typical average lemma, it then follows that\nP(E(j)) E(d(S n l (j), ˆ S n l (j))|E(j)) + P(E(j) c ) E(d(S n l (j), ˆ S n l (j))|E(j) c )\nfor l = 1, 2, and hence the average distortion over the random codebooks generation and index assignment can be bounded as desired. Thus, there exists at least one sequence of codes achieving the desired distortion.\nIn this paper, we considered the problem of lossy com- munication over multiple access channels with feedback. By combining the recently proposed analog-digital hybrid coding scheme for joint source-channel coding [9], [12] with block Markov coding, we presented an inner bound to the optimal distortion region which was shown to include and generalize several existing results in the literature. The fundamental\nquestion that is leitmotif of our future research is how to re- solve the tradeoff between competition and cooperation among separated senders in a network. This tradeoff is currently fully understood only in few special cases of sources and channels.\nThis work was supported by the National Science Founda- tion (NSF) through the grant CCF-1117728."},"refs":[{"authors":[{"name":"L. Ong"},{"name":"M. Motani"}],"title":{"text":"Coding strategies for multiple-access channels with feedback and correlated sources"}},{"authors":[{"name":"A. Lapidoth"},{"name":"S. Tinguely"}],"title":{"text":"Sending a bivariate Gaussian source over a Gaussian MAC with feedback"}},{"authors":[{"name":"R. King"}],"title":{"text":"Multiple-access channels with generalized feedback"}},{"authors":[{"name":"L. H. Ozarow"}],"title":{"text":"Coding and capacity for additive white Gaussian noise multi-user channels with feedback"}},{"authors":[{"name":"T. M. Cover"},{"name":"C. S. K. Leung"}],"title":{"text":"An achievable rate region for the multiple-access channel with feedback"}},{"authors":[{"name":"S. Bross"},{"name":"A. Lapidoth"}],"title":{"text":"An improved achievable region for the discrete memoryless two-user multiple-access channel with noiseless feedback"}},{"authors":[{"name":"R. Venkataramanan"},{"name":"S. Pradhan"}],"title":{"text":"A new achievable rate region for the multiple-access channel with noiseless feedback"}},{"authors":[{"name":"F. M. J. Willems"}],"title":{"text":"The feedback capacity region of a class of discrete memoryless multiple access channels"}},{"authors":[{"name":"S. H. Lim"},{"name":"P. Minero"},{"name":"Y.-H. Kim"}],"title":{"text":"Lossy communication of correlated sources over multiple access channels"}},{"authors":[{"name":"P. Gács"},{"name":"J. Körner"}],"title":{"text":"Common information is far less than mutual information"}},{"authors":[{"name":"H. S. Witsenhausen"}],"title":{"text":"On sequences of pairs of dependent random variables"}},{"authors":[{"name":"P. Minero"},{"name":"S. H. Lim"},{"name":"Y.-H. Kim"}],"title":{"text":"Joint source-channel coding via hybrid coding"}},{"authors":[{"name":"A. El Gama"},{"name":"Y.-H. Ki"}],"title":{"text":"Network Information Theory"}},{"authors":[{"name":"A. B. Wagner"},{"name":"B. G. Kelly"},{"name":"Y. Altu˘g"}],"title":{"text":"Distributed rate-distortion with common components"}},{"authors":[{"name":"C. Tian"},{"name":"J. Chen"},{"name":"S. N. Diggavi"},{"name":"S. Shamai"}],"title":{"text":"Optimality and approximate optimality of source-channel separation in networks"}},{"authors":[{"name":"T. Berger"}],"title":{"text":"Multiterminal source coding"}},{"authors":[{"name":"S.-Y. Tung"}],"title":{"text":"Multiterminal source coding"}},{"authors":[{"name":"T. M. Cover"},{"name":"A. El Gamal"},{"name":"M. Salehi"}],"title":{"text":"Multiple access channels with arbitrarily correlated sources"}},{"authors":[{"name":"Y. Oohama"}],"title":{"text":"Gaussian multiterminal source coding"}},{"authors":[{"name":"A. B. Wagner"},{"name":"S. Tavildar"},{"name":"P. Viswanath"}],"title":{"text":"Rate region of the quadratic Gaussian two-encoder source-coding problem"}},{"authors":[{"name":"P. Minero"},{"name":"S. Jayaraj"}],"title":{"text":"Lossy communication over multiple-access channels with feedback"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566299.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S17.T2.1","endtime":"15:20","authors":"Paolo Minero, Safitha Jayaraj","date":"1341586800000","papertitle":"Lossy Communication over Multiple-Access Channels with Feedback","starttime":"15:00","session":"S17.T2: Communication and Computation over Multiple Access Channels","room":"Kresge Auditorium (109)","paperid":"1569566299"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
