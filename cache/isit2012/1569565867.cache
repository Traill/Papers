{"id":"1569565867","paper":{"title":{"text":"Non-coherent Network Coding: An Arbitrarily Varying Channel Approach"},"authors":[{"name":"Mahdi Jafari Siavoshani ∗"},{"name":"Shenghao Yang \u2020"},{"name":"Raymond W. Yeung \u2021"}],"abstr":{"text":"Abstract\u2014In this paper, we propose an \u201carbitrarily vary- ing channel\u201d (AVC) approach to study the capacity of non- coherent transmission in a network that employs randomized linear network coding. The network operation is modeled by a matrix channel over a ﬁnite ﬁeld where the transfer matrix changes arbitrarily from time-slot to time-slot but up to a known distribution over its rank. By extending the AVC results to this setup, we characterize the capacity of such a non-coherent transmission scheme and show that subspace coding is optimal for achieving the capacity.\nBy imposing a probability distribution over the state space of an AVC, we obtain a channel which we called \u201cpartially arbitrar- ily varying channel\u201d (PAVC). In this work, we characterize the \u201crandomized\u201d as well as the \u201cdeterministic\u201d code capacity of a PAVC under the average error probability criterion. Although we introduce the PAVC to model the non-coherent network coding, this extension to an AVC might be of its own interest as well."},"body":{"text":"Randomized linear network coding [1] is an efﬁcient and practical approach to implement network coding [2], [3] in large dynamically changing networks because it does not require a priori the knowledge of the network topology. However, in order to enable the receivers to decode, to each packet a coding vector is appended to learn the transfer matrix induced by the network.\nA different approach, other than using coding vectors, is to assume a non-coherent scenario for communication, as proposed in [4], where neither the source(s) nor the receiver(s) have any knowledge of the network topology or the network nodes operations. Non-coherent communication allows cre- ation of end-to-end systems that are completely oblivious to the network state. In [4], the authors proposed communications via choosing subspaces and they introduced a subspace chan- nel called \u201coperator channel\u201d (a channel which has subspaces as input and output symbols). Then, they focused on algebraic subspace code constructions over a Grassmannian for the operator channel.\nFollowing [4], different probabilistic models have been pro- posed to model the non-coherent randomized linear network coding channel, where these models enable one to deﬁne and characterize the capacity for such a channel. In all of these works, when there are no errors in the network, the\nnon-coherent linear network coding channel is modeled by a multiplicative matrix channel.\nMontanari et al. [5] introduced a probabilistic model to capture the end-to-end functionality of non-coherent network coding operation, with a focus on the case of error correction capabilities. Jafari et al. [6], [7], [8] modeled the non-coherent network coding channel by assuming that the transfer matrix has i.i.d. entries selected uniformly at random in every time- slot. They showed that coding over subspaces is sufﬁcient to achieve the capacity. Moreover, they obtained the channel capacity as a solution of a convex optimization problem over O(min[M, N ]) variables and when the ﬁeld size is greater than a threshold, they characterized the capacity by solving the optimization problem. Silva et al. [9] derived the capacity of the multiplicative ﬁnite ﬁeld matrix channel under the assumption that the transfer matrix is square and chosen uniformly at random among all full-rank matrices. Similarly, in this model the coding over subspaces is sufﬁcient to achieve the capacity. Yang et al. [10], [11] (see also [12], [13]) considered a completely general scenario, making no assumption on the distribution of the transfer matrix. They obtained upper and lower bounds on the channel capacity, and give a sufﬁcient condition on the distribution of the transfer matrix such that coding over subspaces is capacity achieving. They also studied the achievable rates of coding over subspaces. Nobrega et al. [14] considered the case where the probability distribution of the rank of the transfer matrix is arbitrary; however all matrices with the same rank are equiprobable. Then, following an approach similar to [8], they expressed the capacity as the solution of a convex optimization problem over O(min[M, N ]) variables. They also observed that in this case the subspace codes are sufﬁcient to achieve the capacity.\nIn most of the previous works, only certain probability models for the channel transfer matrix have been discussed. However, in practice a complete probabilistic characterization of the matrix channel is difﬁcult and the network may not follow a given probability model. Instead of assuming a complete probability model, we consider in this paper that only a partial knowledge about the probabilistic model of the channel is known.\nMore precisely, we assume that the rank distribution of the transfer matrix is known a priori, but the distribution of matrices among each rank is unknown and arbitrary. Though very similar to the arbitrarily varying channel (AVC) model introduced in [16] (refer to [17] and the references therein), but this non-coherent network coding model is not exactly an AVC. We introduce a \u201cpartially arbitrary varying channel\u201d (PAVC) to capture the statistical property of this non-coherent network coding model.\nBy extending results for the AVC, we obtain the capacities of the PAVC for randomized and deterministic codes (The- orem 1 and 2). We further show that the randomized and the deterministic code capacities of the non-coherent network coding model are the same (Theorem 3), and that subspace coding is sufﬁcient to achieve the capacity (Corollary 4). This AVC approach to the non-coherent network coding provides a justiﬁcation for the optimality of subspace coding in a more general setting.\nWe use bold letters to denote vectors and matrices. For convenience of notation, we use [i : j] to denote the set {i, i + 1, . . . , j − 1, j} where i, j ∈ Z. Let Uni(M) denote the uniform distribution over the set M. For m × n matrices over F q , we use Uni(F m×n q , r) to denote the uniform distribution over all m × n matrices with rank r.\nConsider a unicast communication over a network where the relay nodes perform random linear network coding over a ﬁnite ﬁeld F q . Suppose that time is slotted and the channel is block time-varying. At every time-slot, the source injects M packets X 1 [t], . . . , X M [t] of length T symbols from F q into the network, i.e., X i [t] ∈ F T q . The receiver collects N packets Y 1 [t], . . . , Y N [t] and aims to decode the transmitted packets.\nWe use matrices X[t] and Y [t] to denote respectively, the transmitted and received packets, i.e., the ith column of these matrices represent the ith transmitted and received packets, respectively. For a unicast communication, at time-slot (block) t, the receiver observes\nwhere X[t] ∈ F T ×M q , Y [t] ∈ F T ×N q , and H[t] ∈ F M ×N q . We assume that the channel transfer matrix H[t] is unknown to both the transmitter and the receiver and it changes arbitrarily from one block to another block with a constraint on its rank. More precisely, the ranks of H[t], t = 1, 2, . . ., are indepen- dent and follow the same distribution of a random variable R. The conditional distribution of H[t] given rk (H[t]) is unknown and changes arbitrarily for different t. However, we assume that the distribution of the random variable R is known. We may consider the channel transfer matrix as the channel state. For given h[1 : n] the channel transition\nThe above model is very similar to an arbitrarily varying channel (AVC) model (refer to [17] for more information about AVC) but it does not completely ﬁt into that model. In this work, we will show that it is indeed possible to extend the AVC concepts and results for the above channel model and characterize its capacity.\nBefore deﬁning a partially arbitrarily varying channel (PAVC), let us ﬁrst consider an AVC model. Let X ∈ X and Y ∈ Y denote the input and output symbol of a channel where X and Y are ﬁnite sets denoting the channel input and output alphabets, respectively. Let us consider a transmission scenario where the channel parameters vary arbitrarily from symbol to symbol during the course of a transmission. More precisely, for the channel transition matrix, we can write\nwhere s = (s 1 , . . . , s n ), s i ∈ S, and W : X × S → Y is a given stochastic matrix. S is a ﬁnite set, often referred to as the state space. This model, called a \u201cdiscrete memoryless arbitrarily varying channel,\u201d will be referred to as an AVC.\nNow, we deﬁne a PAVC as an AVC with a probability constraint over the state space S. Deﬁne a function q : S → Q where Q {0, . . . , m} and deﬁne a random variable Q with alphabet Q whose distribution is known by the encoder and the decoder. For a PAVC, we have q(S t ), t = 1, 2 . . ., are independent and follow the same distribution of Q. In other words,\nwhere q(S) \t (q(S 1 ), . . . , q(S n )). We call this model a \u201cdiscrete memoryless partially arbitrarily varying channel,\u201d and will refer to it as a PAVC.\nIn this work, we are interested in characterizing the capacity of a PAVC. However, we ﬁrst have to deﬁne the capacity. As there are different notions of capacity for an AVC based on different error criteria, the same is true for a PAVC (for more information refer to [17]).\nLet the message set of a code be M = {1, . . . , K}. A length n block code is deﬁned by a pair of mappings (ψ, φ), where ψ : M → X n is the encoder mapping, and φ : Y n → M ∪ {0} is the decoder mapping, where the output 0 indicates a decoding error. When this code is used on a PAVC and the state sequence is s, the error probability for message i is\nAccordingly, the average probability of error for a state sequence s is\nDeﬁnition 1. A number R > 0 is called an achievable rate for the given PAVC (for deterministic code and average error probability criterion) if for every > 0, δ > 0, and sufﬁciently large n, there exists a length n block code (ψ, φ) with\nn log K > R − δ, \t and max\nwhere P Q n (q) \t n t=1 P Q (q t ). The maximum achievable rate is called the capacity of the PAVC and is denoted by C d,a pavc (where superscript \u201c a\u201d denotes the average error probability criterion in (3) and \u201cd\u201d denotes determinist code).\nRemark: Note that if there is no probability constraint on the state space in Deﬁnition 1 (i.e., P S instead of P S|q(S) is unknown), then by replacing P S|q(S) in the maximization by P S , we recover the average error criterion for an AVC, namely, max P S E [¯ e d (S)] ≤ is equivalent to max s ¯ e d (s) ≤ .\nIn contrast to using deterministic codes, there exists another communication technique called randomized coding which can provide improvement in performance if a common source of randomness is available between the source and the receiver.\nPrecisely, a randomized code (Ψ, Φ) is a random variable with values in the family of all length n block codes (ψ, φ), deﬁned earlier in this section, with the same message set M. Let us deﬁne\nWhen this code is used on a PAVC and the state sequence is s, the error probability for message i is\nSimilar to Deﬁnition 1, we deﬁne the capacity C r,a pavc by replacing the function ¯ e d (s) with ¯ e r (s).\nOur main goal is to characterize the capacity of the non- coherent network coding channel described in §II-B. Toward this end, we ﬁrst determine the capacity of a general PAVC.\nBefore stating the deterministic code capacity of a PAVC, we need the following deﬁnition.\nDeﬁnition 2. A PAVC is called symmetrizable if for some channel U : X × Q → S, and for every x, x , and y we have\nLet U (X × Q → S) be the set of all such channels. If U (X × Q → S) = ∅ then the PAVC is called non-symmetrizable.\nThe following two theorems, which are proved using tech- niques similar to those in [18], [19], characterize the capacity of the PAVC for the average error criterion. Due to space limit, we refer the reader to [20] for the proof of Theorem 1 and 2. Theorem 1. For the deterministic code capacity C d,a pavc we have C d,a pavc > 0 if and only if the PAVC is non-symmetrizable. If C d,a pavc > 0, then\n(4) where\nand I(P X , ¯ W S ) I(X; Y ) such that Y is connected to X through the channel ¯ W S .\nTheorem 2. The randomized code capacity of the PAVC, denoted by C r,a pavc , is given by (4).\nRemark: Same as an AVC, the randomized code capacity of a PAVC for the maximum and the average error probability criteria are the same.\nAccording to the deﬁnition of the PAVC in §II-C, the non- coherent network coding model deﬁned by (1) is a PAVC whose deterministic code capacity, as stated in Theorem 1, can be characterized as follows.\nCorollary 1. The deterministic code capacity of the channel (1) is non-zero and is given by\nif and only if the channel is non-symmetrizable, i.e., if there is no stochastic matrix U : X × [0 : min[M, N ]] → H such that we have\nSimilarly, using Theorem 2, the randomized code capacity of the non-coherent network coding deﬁned by (1) is stated in the following corollary.\nCorollary 2. The randomized code capacity of the channel deﬁned by (1) is given by (5).\nIt is hard to show directly that the channel deﬁned by (1) is non-symmetrizable. Instead, we prove this indirectly in the next lemma by showing the existence of a coding scheme that gives a non-zero transmission rate over the channel.\nLemma 1. If E [R] > 0, the channel deﬁned by (1) is non- symmetrizable, and so by Corollary 1, its capacity is non-zero and is given by (5). If E [R] = 0, then the capacity is zero.\nProof: The case for E [R] = 0 follows because H[t] is the zero matrix with probability one. To show the non- symmetrizability of the channel deﬁned by (1) when E [R] > 0, we construct a deterministic coding scheme that can achieve a strictly positive rate. The idea is to degrade the channel deﬁned by (1) to a binary memoryless Z-channel with a known cross-over probability.\nwith uniform i.i.d. components. Deﬁne a binary-input binary- output channel as follows. Let B[t] be the input of the channel at time t, which takes the value 0 or 1 in F q . The output of the channel at the time t is Y [t] = rk (B[t]G[t]H[t]). Since the dimension of the matrix B[t]G[t]H[t] is 1 × N , Y [t] takes the integer value 0 or 1. Let us check the transition matrix of this channel. If B[t] = 0, then Y [t] = 0. If B[t] = 1, then Y [t] = rk (G[t]H[t]). Note that rk (G[t]H[t]) is a random variable whose distribution only depends on the distribution of rk (H[t]) ∼ R (see the computation in [10, Section IV]). Since rk (H[t]), t = 1, 2, . . . are independent, the channel is a binary memoryless Z channel.\nWhat remains is to check the cross over probability of the Z channel given by\nDeﬁnition 3 ([14]). A random matrix is called u.g.r. (uniform given rank) if any two matrices with the same rank are equiprobable.\nLemma 2. For any M × N random matrix H, AHB is u.g.r. with the same rank distribution as of H, where A ∼ Uni(F M ×M q , M ) and B ∼ Uni(F N ×N q , N ) are uniform and full-rank random matrices, and A, B, and H are inde- pendent 1 .\nwhere P A (a) and P B (b) respectively do not depend on a and b. Now, for another instance g of G with g = U gV for some full rank matrices U and V , we can see that P G (g) = P G (g ). In the following we show that if rk (g) = rk (g ), then there exist full rank matrices U and V such that g = U gV .\nFix two decompositions g = bc and g = b c with rk (b) = rk b = rk (g), which implies rk (c) = rk (c ) = rk (g). Then there exist full rank square matrices U and V such that U b = b and cV = c . Hence, g = U gV .\nLemma 3. In the capacity expression (5), the u.g.r. distribu- tion for P H| rk(H) is a minimizer for the expression.\nProof: Let P ∗ H| rk(H) be the distribution that minimizes (5). Now consider a new channel deﬁned by AHB where A ∼ Uni(F M ×M q , M ) and B ∼ Uni(F N ×N q , N ) are uniform full rank random matrices (note that A, B, and H are independent). Then by Lemma 2, the rank distribution of AHB is the same as that of H, but AHB has a u.g.r. distribution.\nBy the data processing inequality, the mutual information between the input and output of the new channel is less than or equal to the original channel. So if P ∗ H| rk(H) is a minimizer, then the u.g.r. distribution with the same rank distribution is also a minimizer.\nFrom Corollary 1, Corollary 2, Lemma 1, and Lemma 3 we obtain the following theorem.\nTheorem 3. The randomized and deterministic code capaci- ties of the non-coherent network coding model, i.e., the matrix channel deﬁned by (1), are the same and are equal to the capacity of the matrix channel Y = ¯ HX where ¯ H has the same rank distribution as H but has uniform distribution among matrices having the same rank, i.e.,\nTheorem 3 shows that, if only the knowledge of the rank distribution of the transfer matrix is available, the maximum rate that we can communicate over the channel deﬁned by (1) is equal to the communication rate over a channel which has the same rank distribution but the channel transfer matrix is u.g.r.\nNow, it is shown in [14, Theorem 16] that for a matrix multiplicative channel with u.g.r. distribution over the transfer matrix, the subspace coding is sufﬁcient to achieve the capac- ity. So we have the following result.\nTheorem 4. Subspace coding [4] is sufﬁcient to achieve the capacity (randomized and deterministic) of the non-coherent network coding channel discussed in §II-B.\nAlthough determining the exact value of the capacity in Theorem 3 is still open, as shown in [14], the capacity can be\nexpressed as the solution of a convex optimization problem with only O (min[M, N ]) parameters which is computation- ally tractable. Indeed, to achieve the optimal communication rate, the u.g.r. distribution over the input symbols (input matrices X) is sufﬁcient.\nHere, we present an alternative proof for Theorem 3 that is not based on the derived PAVC results 2 .\nWe may consider the non-coherent network coding channel introduced in §II-B as a matrix channel where the transfer matrix is chosen arbitrarily by an adversary, in such a way that the constraint on the rank distribution is preserved. Let this channel is denoted by Ch adv . Then we deﬁne Ch ugr to denote the discrete memory-less channel deﬁned by Y [t] = X[t]H[t] with a u.g.r. transfer matrix [14].\nNow we can state the following facts. First, for Ch adv , the adversary can always emulate Ch ugr by choosing the transfer matrix according to the u.g.r. distribution of Ch ugr . Secondly, by using the randomization described in Lemma 2, we may degrade Ch adv to Ch ugr for whatever strategy the adversary has been performing. Note that we need to apply a similar technique in Lemma 2 to show that the degradation is memoryless. Thus, from the above argument, for every deﬁnition of the capacity of Ch adv , it must be equal to the capacity of Ch ugr with the same rank distribution.\nAlthough the above argument provides a simpler proof for the capacity of Ch adv , we believe that our AVC approach gives a different insight and enables us to study more general models without the knowledge of the full probabilistic characterization of the rank distribution.\nIn this work, we proposed an arbitrarily varying channel (AVC) approach to model the non-coherent network coding by a matrix channel where the rank distribution of the transfer matrix is known and apart from that the transfer matrix can be changed arbitrarily from time-slot to time-slot. We believe that this AVC approach better ﬁts to model a complex, dynamically changing network where relay nodes perform randomized network coding.\nIn order to characterize the capacity of such a channel, we deﬁned a new class of channels, called partially AVC (PAVC), with a partial probabilistic constraint over the state space. By extending the previous result on AVC to PAVC, we proved that the subspace coding is optimal to achieve the capacity of non-coherent network coding.\nThe authors would like to thank Ning Cai and Emre Telatar for many useful discussions. The work of M. Jafari Siavoshani was supported by the Swiss National Science Foundation through Grant PP00P2-128639. The work of S. Yang and R. W. Yeung was partially supported by a grant from the\nUniversity Grants Committee of the Hong Kong Special Administrative Region, China (Project No. AoE/E-02/08). The work of S. Yang was partially supported by the National Basic Research Program of China through Grant 2011CBA00300, 2011CBA00302."},"refs":[{"authors":[{"name":"T. Ho"},{"name":"M. Medard"},{"name":"R. Koetter"},{"name":"D. R. Karger"},{"name":"M. Effros"},{"name":"J. Shi"},{"name":"B. Leong"}],"title":{"text":"A random linear network coding approach to multicast"}},{"authors":[{"name":"R. Ahlswede"},{"name":"N. Cai"},{"name":"Y. R. Li"},{"name":"R. W. Yeung"}],"title":{"text":"Network informa- tion ﬂow"}},{"authors":[{"name":"S.-Y. R. Li"},{"name":"N. Cai"},{"name":"R. W. Yeung"}],"title":{"text":"Linear network coding"}},{"authors":[{"name":"R. Koetter"},{"name":"F. R. Kschischang"}],"title":{"text":"Coding for errors and erasures in random network coding"}},{"authors":[{"name":"A. Montanari"},{"name":"R. Urbanke"}],"title":{"text":"Coding for network coding"}},{"authors":[{"name":"M. Jafari Siavoshani"},{"name":"C. Fragouli"},{"name":"S. Diggavi"}],"title":{"text":"Non-coherent multi- source network coding"}},{"authors":[{"name":"M. Jafari Siavoshani"},{"name":"S. Mohajer"},{"name":"C. Fragouli"},{"name":"S. Diggavi"}],"title":{"text":"On the capacity of non-coherent network coding"}},{"authors":[{"name":"M. Jafari Siavoshani"},{"name":"S. Mohajer"},{"name":"C. Fragouli"},{"name":"S. N. Diggavi"}],"title":{"text":"On the capacity of noncoherent network coding"}},{"authors":[{"name":"D. Silva"},{"name":"F. R. Kschischang"},{"name":"R. Koetter"}],"title":{"text":"Communication over ﬁnite- ﬁeld matrix channels"}},{"authors":[{"name":"S. Yang"},{"name":"S.-W. Ho"},{"name":"J. Meng"},{"name":"E.-hui Yang"},{"name":"R. W. Yeung"}],"title":{"text":"On Linear operator channels over ﬁnite ﬁelds"}},{"authors":[{"name":"S. Yang"},{"name":"S.-W. Ho"},{"name":"J. Meng"},{"name":"E.-hui Yang"}],"title":{"text":"Symmetric properties and subspace degradations of linear operator channels over ﬁnite ﬁelds"}},{"authors":[{"name":"S. Yang"},{"name":"J. Meng"},{"name":"E. hui Yang"}],"title":{"text":"Coding for linear operator chan- nels over ﬁnite ﬁelds"}},{"authors":[{"name":"S. Yang"},{"name":"S.-W. Ho"},{"name":"J. Meng"},{"name":"E.-h. Yang"}],"title":{"text":"Optimality of subspace coding for linear operator channels over ﬁnite ﬁelds"}},{"authors":[{"name":"R. W. Nobrega"},{"name":"B. F. Uchoa-Filho"},{"name":"D. Silva"}],"title":{"text":"On the capacity of mul- tiplicative ﬁnite-ﬁeld matrix channels"}},{"authors":[{"name":"R. W. Nobrega"},{"name":"D. Silva"},{"name":"B. F. Uchoa-Filho"}],"title":{"text":"On the Capacity of Multiplicative Finite-Field Matrix Channels"}},{"authors":[{"name":"D. Blackwell"},{"name":"L. Breiman"},{"name":"A. J. Thomasian"}],"title":{"text":"The Capacities of Certain Channel Classes Under Random Coding"}},{"authors":[{"name":"A. Lapidoth"},{"name":"P. Narayan"}],"title":{"text":"Reliable communication under channel uncertainty"}},{"authors":[{"name":"I. Csiszar"},{"name":"P. Narayan"}],"title":{"text":"Arbitrarily varying channels with constrained inputs and states"}},{"authors":[{"name":"I. Csiszar"},{"name":"P. Narayan"}],"title":{"text":"The capacity of the arbitrarily varying channel revisited: Positivity, constraints"}},{"authors":[{"name":"M. Jafari Siavoshani"},{"name":"S. Yang"},{"name":"R. W. Yeung"}],"title":{"text":"Non-coherent net- work coding: an arbitrarily varying channel approach"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565867.pdf"},"links":[{"id":"1569566485","weight":2},{"id":"1569565883","weight":5},{"id":"1569565223","weight":2},{"id":"1569566725","weight":5},{"id":"1569565663","weight":2},{"id":"1569565377","weight":2},{"id":"1569566385","weight":5},{"id":"1569566605","weight":2},{"id":"1569566683","weight":2},{"id":"1569566855","weight":2},{"id":"1569559259","weight":10},{"id":"1569566697","weight":5},{"id":"1569566597","weight":2},{"id":"1569565551","weight":5},{"id":"1569566943","weight":2},{"id":"1569567045","weight":2},{"id":"1569564481","weight":2},{"id":"1569566081","weight":2},{"id":"1569565613","weight":2},{"id":"1569565355","weight":5},{"id":"1569564469","weight":2},{"id":"1569565931","weight":7},{"id":"1569566765","weight":5},{"id":"1569565461","weight":2},{"id":"1569564731","weight":2},{"id":"1569566207","weight":2},{"id":"1569558325","weight":13},{"id":"1569565837","weight":13},{"id":"1569566119","weight":7},{"id":"1569564233","weight":5},{"id":"1569560427","weight":2},{"id":"1569566941","weight":2},{"id":"1569564203","weight":2},{"id":"1569556713","weight":5},{"id":"1569566157","weight":2},{"id":"1569566903","weight":7},{"id":"1569566999","weight":5},{"id":"1569566843","weight":2},{"id":"1569558483","weight":2},{"id":"1569564387","weight":10},{"id":"1569565455","weight":5},{"id":"1569566795","weight":10},{"id":"1569566963","weight":2},{"id":"1569561679","weight":18},{"id":"1569566709","weight":5},{"id":"1569564989","weight":5},{"id":"1569566015","weight":7},{"id":"1569565953","weight":2},{"id":"1569566895","weight":7},{"id":"1569564613","weight":5},{"id":"1569565321","weight":7},{"id":"1569566193","weight":5},{"id":"1569565785","weight":2},{"id":"1569563981","weight":2},{"id":"1569566617","weight":2},{"id":"1569566905","weight":2},{"id":"1569566063","weight":2},{"id":"1569555999","weight":5},{"id":"1569566759","weight":2},{"id":"1569566657","weight":2},{"id":"1569565213","weight":5},{"id":"1569566511","weight":7},{"id":"1569565841","weight":10},{"id":"1569565833","weight":5},{"id":"1569565535","weight":15},{"id":"1569561795","weight":5},{"id":"1569566325","weight":5},{"id":"1569559805","weight":5},{"id":"1569566811","weight":2},{"id":"1569553909","weight":7},{"id":"1569559111","weight":2},{"id":"1569553519","weight":2},{"id":"1569566885","weight":2},{"id":"1569564441","weight":2},{"id":"1569566425","weight":2},{"id":"1569554881","weight":5},{"id":"1569554971","weight":2},{"id":"1569566445","weight":23},{"id":"1569566209","weight":2},{"id":"1569565655","weight":2},{"id":"1569565151","weight":2},{"id":"1569558985","weight":2},{"id":"1569566473","weight":2},{"id":"1569564333","weight":2},{"id":"1569566809","weight":2},{"id":"1569566629","weight":2},{"id":"1569565033","weight":2},{"id":"1569565817","weight":2},{"id":"1569557083","weight":2},{"id":"1569566141","weight":2},{"id":"1569555879","weight":2},{"id":"1569558509","weight":2},{"id":"1569566003","weight":28},{"id":"1569565185","weight":10},{"id":"1569566037","weight":10},{"id":"1569565095","weight":2},{"id":"1569566223","weight":5},{"id":"1569566553","weight":2},{"id":"1569562207","weight":2},{"id":"1569566191","weight":2},{"id":"1569567033","weight":2},{"id":"1569565527","weight":2},{"id":"1569566695","weight":5},{"id":"1569565467","weight":2},{"id":"1569566673","weight":2},{"id":"1569565311","weight":2},{"id":"1569566297","weight":5},{"id":"1569560997","weight":5},{"id":"1569566407","weight":5},{"id":"1569566857","weight":2},{"id":"1569565961","weight":2},{"id":"1569566387","weight":7},{"id":"1569566245","weight":2},{"id":"1569560503","weight":2},{"id":"1569563395","weight":2},{"id":"1569565415","weight":2},{"id":"1569555367","weight":2},{"id":"1569566383","weight":2},{"id":"1569565885","weight":5},{"id":"1569565493","weight":2},{"id":"1569566805","weight":2},{"id":"1569559199","weight":2},{"id":"1569565665","weight":2},{"id":"1569566983","weight":2},{"id":"1569566779","weight":5},{"id":"1569566479","weight":7},{"id":"1569566873","weight":2},{"id":"1569565765","weight":2},{"id":"1569565435","weight":2},{"id":"1569557275","weight":5},{"id":"1569566129","weight":2},{"id":"1569565093","weight":5},{"id":"1569565919","weight":2},{"id":"1569565241","weight":2},{"id":"1569566267","weight":2},{"id":"1569564919","weight":2},{"id":"1569565511","weight":2},{"id":"1569566737","weight":5},{"id":"1569566253","weight":2},{"id":"1569565353","weight":5},{"id":"1569564305","weight":2},{"id":"1569566691","weight":5},{"id":"1569566547","weight":2},{"id":"1569566823","weight":13},{"id":"1569566595","weight":2},{"id":"1569566137","weight":2},{"id":"1569565013","weight":2},{"id":"1569566529","weight":5},{"id":"1569565375","weight":15},{"id":"1569566639","weight":2},{"id":"1569566755","weight":2},{"id":"1569566813","weight":7},{"id":"1569564247","weight":2},{"id":"1569564437","weight":5},{"id":"1569563975","weight":10},{"id":"1569565457","weight":2},{"id":"1569565529","weight":5},{"id":"1569556759","weight":2},{"id":"1569565271","weight":2},{"id":"1569566397","weight":2},{"id":"1569560235","weight":2},{"id":"1569566817","weight":2},{"id":"1569566911","weight":2},{"id":"1569564923","weight":18},{"id":"1569565367","weight":2},{"id":"1569566299","weight":5},{"id":"1569564769","weight":2},{"id":"1569565769","weight":2},{"id":"1569566601","weight":13},{"id":"1569566933","weight":5},{"id":"1569563919","weight":5},{"id":"1569557851","weight":15},{"id":"1569567691","weight":2},{"id":"1569565861","weight":5},{"id":"1569562367","weight":2},{"id":"1569565997","weight":5},{"id":"1569567013","weight":10},{"id":"1569565853","weight":13},{"id":"1569564505","weight":2},{"id":"1569565165","weight":7},{"id":"1569565635","weight":2},{"id":"1569566797","weight":15},{"id":"1569566413","weight":5},{"id":"1569565707","weight":2},{"id":"1569565143","weight":2},{"id":"1569565583","weight":2},{"id":"1569566555","weight":5},{"id":"1569564141","weight":5},{"id":"1569566973","weight":10},{"id":"1569558697","weight":2},{"id":"1569565139","weight":21},{"id":"1569565579","weight":5},{"id":"1569566825","weight":2},{"id":"1569566609","weight":2},{"id":"1569566443","weight":2},{"id":"1569566727","weight":5},{"id":"1569565315","weight":5},{"id":"1569560581","weight":2}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T1.4","endtime":"12:50","authors":"Mahdi Jafari Siavoshani, Shenghao Yang, Raymond Yeung","date":"1341405000000","papertitle":"Non-coherent Network Coding: An Arbitrarily Varying Channel Approach","starttime":"12:30","session":"S10.T1: Network Coding: Capacity and Bounds","room":"Kresge Rehearsal B (030)","paperid":"1569565867"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
