{"id":"1569565181","paper":{"title":{"text":"Recognition Capacity Versus Search Speed in Noisy Databases"},"authors":[{"name":"Ertem Tuncel"}],"abstr":{"text":"Abstract\u2014The tradeoff between the number of distinguishable objects and search speed in a data management system is inves- tigated in an information-theoretic framework. In the discussed scenario, incoming high-dimensional (and noisy) data vectors are enrolled in (possibly multiple) clusters to be accessed later. Upon receiving a random query, which is the noisy version of an enrolled vector, the search engine retrieves only a subset of the clusters to compare against the query. This creates tension between the search speed (determined by the expected number of retrieved entries) and recognition capacity (maximum possible number of entries that can be reliably recognized). A single- letter achievable rate region is characterized and it is shown with examples that search can be performed much faster in the discussed scenario than in non-clustered linear scan without compromising maximum recognition capacity."},"body":{"text":"With the proliferation of high-dimensional data used for identiﬁcation purposes, such as ﬁngerprints, iris scans, or other biometric or behavioral patterns, comes the need for huge disk storage space, and more importantly, the need for data management systems (DMSs) with fast search algorithms for reliable identiﬁcation of the entries. In any such system, there is obvious tension between (i) the number of entries that can be reliably identiﬁed, (ii) the speed of search, and (iii) the amount of storage in the disk. For example, more data entries imply slower search and larger disk space. Similarly, if the storage space is reduced, it will result in less number of entries that can be reliably identiﬁed due to the inherent information loss.\nThe information-theoretic analysis of the performance of DMSs was initiated by [3], [7], where the maximum growth rate of number of entries that can be reliably identiﬁed (i.e., the capacity) was characterized. Later, in [4], [6], the analysis was generalized to the tradeoff between capacity and storage space. Other extensions include analysis of the tradeoff between capacity, storage, and reconstruction distortion [5], and the capacity-storage tradeoff over multiple databases where the query is a function of one entry from each database [2].\nAs for search speed, Willems [8] introduced a clustering- based DMS, whereby entries are clustered (in an overlapping fashion) using a quantizer, and upon receiving a query, only one cluster is retrieved. The tradeoff between the capacity, the number of clusters, and the number of entries in each cluster was derived. It turns out that in this scheme, the optimal tradeoff comes at the expense of a signiﬁcant amount of redundancy in storage.\nIn this paper, we take a different approach and propose a system in which we allow more than one cluster to be retrieved, and characterize an achievable tradeoff of only the capacity versus the search speed. Although we also allow an entry to be enrolled in multiple clusters, this assignment is probabilistic, and the resultant tradeoff enrolls each entry in only one cluster on average. Somewhat surprisingly, we observe from the resultant tradeoff that up to a certain point, the reduction in search speed does not come with any loss in the capacity of the database. This is in contrast with the capacity-storage tradeoff where any reduction from the lossless compression rate comes at the expense of reduced recognition capacity [4], [6].\nWe assume that M data vectors X n (m) ∈ X n for m = 1, 2, . . . , M are generated independently and according to\nIn the enrollment phase, noisy versions of X n (m), denoted by Y n (m), are observed and enrolled in one or more of the K clusters. We model Y n (m) as the output of a discrete memoryless channel (DMC) governed by P Y |X with the output alphabet Y. The enrollment is performed using the enrollment function\nwhere K = {1, 2, . . . , K} and 2 K is the set of all subsets of K. Now let\nThat is, C k is the collection of Y n (m) which are enrolled in cluster k.\nLet W be independent from {X n (m), Y n (m)} M m=1 , uni- formly distributed in M = {1, 2, . . . , M }, and unknown to the user of the data management system. In the identi- ﬁcation phase, the user observes Z n ∈ Z n , which is the noisy version of X n (W ) corrupted by the DMC P Z|X with Z n − X n (W ) − Y n (W ) forming a Markov chain, and desires\nto identify W . For that purpose, only a subset of the clusters indicated by the selector function\nis retrieved and compared against Z n . For any z n ∈ Z n , we denote by D(z n ) the union of the data in the selected clusters, i.e.,\nIdentiﬁcation of the unknown W is performed by the recog- nition function\nwhere for any set A, A ∗ denotes the set of arbitrarily many Cartesian products of A, i.e., A ∗ = ∞ i=0 A i . More speciﬁ- cally, for a random query Z n , h n computes\nas the estimate of W . The quality of this estimate is measured by\nFor ﬁnite n, the goal of the designer of a system should be to minimize the expected number of data vectors retrieved from disk for each query, which is given by\nsubject to a given number of entries M , while ensuring P e ≤ for some small . Note that (1) is in general larger than\nE (|D(Z n )|) because of potentially multiple countings of the same Y n (m) in (1). We will use (1) because it reﬂects the\nreal cost of retrieving data from disk during a sequential scan of the selected clusters.\nDeﬁnition 1: An (M, K, L, n, ) data management system consists of an enrollment function f n , using which M data entries of dimension n are enrolled into K ≤ M clusters, a selector function g n , and a recognition function h n such that\nRemark 1: The data management systems that were fully treated in [3] and [7] correspond to the special case of (M, 1, M, n, ) systems here.\nOur goal in this paper is to characterize the asymptotic behavior of data management systems in terms of rates of exponential growth of the database size M and of the average time spent for answering the query. For the time spent, we propose to use a cost model which takes into account not only the total number of retrieved entries, but also the number of clusters. It is logical to assume that the higher the number of clusters, the more time it takes to compute the selector function g n . For example, it is easy to conceive a model where g n operates by comparing Z n with cluster representatives (such as mean, median, etc.) In this model, cluster representatives, since exponentially many, would probably be stored in the disk also, and therefore will have comparable cost to retrieve. Thus, we propose a cost model given by\nwhich imposes the exponential rate of growth 1\nDeﬁnition 2: A rate pair (R i , R s ) is achievable if for every > 0, there exists large enough n such that there is an\nR i − ≤ 1 n\nWe close this section by introducing our typicality notation. We use the strong typicality deﬁnitions of [1]. That is, a sequence x n is δ-typical with respect to P X if\nfor all x, where π(x|x n ) is the frequency of occurrences of x in x n . The set of all typical sequences will be denoted as S n [X] . Dependence of this set on δ will be suppressed with the understanding that it will be synchronized with other small entities such as .\nWe are ready to present our main result which is a single- letter achievable rate region in the (R i , R s )-plane. Although this result is not accompanied with a converse, and hence is not complete, it will be crucial in understanding the advantage of structured data management.\nTheorem 1: A pair (R i , R s ) is achievable if there exists an auxiliary random variable T such that T − Y − X − Z and\nR i ≤ I(Y ; Z) \t (2) R s ≥ I(Y ; T ) \t (3)\nProof: Fix > 0. For a random variable T that satisﬁes (2)-(4) for given (R i , R s ), randomly generate vectors T n (k) for k = 1, 2, . . . , K − 1 in an i.i.d. fashion according to P T , where K (to be chosen later) satisﬁes\nn log K ≤ R s + . \t (5) These vectors will serve as the cluster representatives as discussed in the previous section. Then enroll M noisy data entries Y n (m) using the enrollment function f n as described below, where\nEnrollment: For any given y n ∈ Y n , the enrollment function f n checks for all k whether y n and T n (k) are jointly typical, i.e., whether\nFor each k where the pair is typical, a random decision is made: k is included in the output set f n (y n ) with prob- ability q n , where q n is to be determined later. If every k = 1, 2, . . . , K − 1 ends up being excluded from the output set, then f n (y n ) is set to {K} where the sole purpose of this last cluster K is to collect the outlier data for which the system concedes an identiﬁcation error from the beginning.\nCluster selection: Similar to the enrollment phase, for given z n ∈ Z n , the cluster selection function g n checks whether\nIdentiﬁcation: The identiﬁcation function h n goes through all y n in its argument set, and checks whether any of them satisﬁes\nExpected number of retrieved vectors: For large enough n,\nwhere (a) follows from symmetry in W , and (b) follows because even when Z n is not typical, which happens with probability less than any > 0 for large enough n, the number of retrieved data vectors cannot be larger than KM . Also, in (c), k = K is excluded from the summation because C K is never retrieved. Now, the sum in (7) can be split into two parts: m = 1 and m = 1. For m = 1, since Z n , Y n (m), and T n (k) are independent,\nAs for m = 1, because of the assumption W = 1, (Y n (1), Z n ) is generated i.i.d.∼ P Y Z . Therefore, due to the Markov lemma, if (Y n (1), T n (k)) is jointly typical, so is (Z n , T n (k)) with high probability. Hence, we use an upper bound which cannot be improved exponentially:\nPr[Y n (1) ∈ C k , k ∈ g n (Z n )|W = 1, Z n ∈ S n [Z] ] ≤ q n Pr[(Y n (1), T n (k)) ∈ S n [Y T ] ]\nBringing everything together, and further upper bounding K − 1 and M − 1 by K and M , respectively,\nand therefore, 1 n\nlog L ≤ 3 + 1 n\nlog K + 1 n\nfor large enough n. Now impose on K the condition 1\nto further obtain 1\nwhere (a) follows from (4), and (b) follows from (6) whenever is chosen to satisfy ≤ 3 8 .\nProbability of identiﬁcation error: Due to symmetry, we can assume W = 1 again. The possible events that are sources of error are\nE 3 = {∀k < K, Y n (1) ∈ C k or (Z n , T n (k)) ∈ S n [ZT ] } E 4 (m) = {(Z n , Y n (m)) ∈ S n [ZY ] and ∃k < K with\nNow, Pr[E 1 ] → 0 because (Z n , Y n (1)) is generated i.i.d.∼ P ZY . Deﬁning\nPr[E 2 ] = (1 − p n q n ) K−1 ≤ exp(−Kp n q n )\nwhich can be brought to 0 if < . The third error event E 3 conditioned on E c 2 can be understood as \u201cfor all clusters k in which Y n (1) is stored, T n (k) is not jointly typical with Z n .\u201d The probability of this also vanishes as n → ∞ because of the fact that for all such clusters Y n (1) is jointly typical with T n (k) and using the Markov lemma.\nwhere Y n (2), Z n , and T n (1) are independently generated. It is then easy to see\nThe ﬁrst thing to notice about Theorem 1 is that the choice T =constant achieves the point\nwhich is the same recognition capacity and retrieval cost as in unstructured (i.e., non-clustered and linear scan) data management systems analyzed in [3], [7].\nSecondly, the resemblance of the region of Theorem 1 to [4, Theorem 1] can be exploited in the analysis of the achievability region. More speciﬁcally, in [4, Theorem 1], it was shown that a compression/identiﬁcation rate pair (R c , R i ) is achievable if and only if\nR c ≥ I(Y ; U ) R i ≤ I(Z; U )\nfor some U − Y − X − Z. Deﬁning the recognition capacity as\nR i ≤ I(Y ; Z) \t (8) R i − R s ≤ C(R s ) . \t (9)\nOf course, a direct application of [4, Lemma 1] to the current problem yields that the achievability region in (2)-(4) is convex, and can be characterized completely by focusing on alphabets T of size not exceeding |Y| + 1.\nAnother immediate conclusion is that even when R i = I(Y ; Z), i.e., we do not want to compromise from maximum recognition capacity, the search rate R s can be signiﬁcantly small compared to I(Y ; Z), which was the minimum search rate in all the uncompressed and unstructured data storage schemes before [7], [6], [4]. To achieve this maximum R i , it follows from (8) and (9) that R s must satisfy\nDenote by R s,min the minimum R s that satisﬁes (10). Since the left-hand side of (10) is increasing in R s , we need to solve\nExample 1: P X (x) = 1 2 for x ∈ X = {0, 1}, and P Y |X and P Z|X are binary symmetric channels (BSC) with crossover probabilities p 1 and p 2 , respectively. This implies that P Y (y) = 1 2 for y ∈ {0, 1} and P Z|Y is a also a BSC with crossover probability γ = p 1 p 2 , where a b = a(1 − b) + b(1 − a). Therefore,\nwhere H(p) is the binary entropy function. It was shown in [4] that\nfor all 0 ≤ R s ≤ 1, where H −1 returns values between 0 and 1 2 . Bringing (11), (12), and (13) together, R s,min is the solution to\nOf course, this is not easy to solve in general, but it is not hard to see that at the extreme of γ → 0 (no noise during enrollment or identiﬁcation), R s,min → 0.5 and I(Y ; Z) → 1.\nExample 2: P X (x) = 1 2 for x ∈ X = {0, 1}, Y = X, and P Z|X is the erasure channel with erasure probability . It is clear that"},"refs":[{"authors":[{"name":"A. E. Gama"},{"name":"Y.-H. Kim"}],"title":{"text":"Network Information Theory, Cambridge, 2012"}},{"authors":[{"name":"D. Gunduz"},{"name":"E. Tuncel"},{"name":"A. Goldsmith"},{"name":"H. V. Poor"}],"title":{"text":"Identiﬁcation over multiple databases"}},{"authors":[{"name":"J. A. O\u2019Sullivan"},{"name":"N. A. Schmid"}],"title":{"text":"Large deviations performance analysis for biometrics recognition"}},{"authors":[{"name":"E. Tuncel"}],"title":{"text":"Capacity/storage tradeoff in high-dimensional identiﬁcation systems"}},{"authors":[{"name":"E. Tuncel"},{"name":"D. Gunduz"}],"title":{"text":"Identiﬁcation and lossy reconstruction in noisy databases"}},{"authors":[{"name":"M. B. Westover"},{"name":"J. A. O\u2019Sullivan"}],"title":{"text":"Achievable rates for pattern recognition"}},{"authors":[{"name":"F. Willems"},{"name":"T. Kalker"},{"name":"J. Goseling"},{"name":"J.-P. Linnartz"}],"title":{"text":"On the capacity of a biometrical identiﬁcation system"}},{"authors":[{"name":"F. Willems"}],"title":{"text":"Search methods for biometric identiﬁcation systems: Fun- damental limits"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565181.pdf"},"links":[{"id":"1569566381","weight":21},{"id":"1569566527","weight":7},{"id":"1569565383","weight":14},{"id":"1569565223","weight":7},{"id":"1569566725","weight":14},{"id":"1569566385","weight":7},{"id":"1569564635","weight":7},{"id":"1569559617","weight":7},{"id":"1569559259","weight":7},{"id":"1569566597","weight":7},{"id":"1569552245","weight":7},{"id":"1569565931","weight":7},{"id":"1569566373","weight":7},{"id":"1569564731","weight":7},{"id":"1569566119","weight":7},{"id":"1569563411","weight":7},{"id":"1569559541","weight":14},{"id":"1569566941","weight":7},{"id":"1569565291","weight":7},{"id":"1569564203","weight":14},{"id":"1569566821","weight":7},{"id":"1569562685","weight":7},{"id":"1569566467","weight":7},{"id":"1569566999","weight":7},{"id":"1569566843","weight":7},{"id":"1569565455","weight":21},{"id":"1569566709","weight":7},{"id":"1569566523","weight":7},{"id":"1569551763","weight":7},{"id":"1569565953","weight":7},{"id":"1569565321","weight":7},{"id":"1569566905","weight":7},{"id":"1569566753","weight":7},{"id":"1569566311","weight":7},{"id":"1569558681","weight":7},{"id":"1569565841","weight":7},{"id":"1569565833","weight":7},{"id":"1569566325","weight":7},{"id":"1569566423","weight":7},{"id":"1569566437","weight":7},{"id":"1569566811","weight":7},{"id":"1569553909","weight":7},{"id":"1569566687","weight":7},{"id":"1569552251","weight":7},{"id":"1569553519","weight":14},{"id":"1569566209","weight":7},{"id":"1569565655","weight":7},{"id":"1569564333","weight":21},{"id":"1569566629","weight":7},{"id":"1569565033","weight":7},{"id":"1569563897","weight":7},{"id":"1569565633","weight":7},{"id":"1569555879","weight":21},{"id":"1569565219","weight":7},{"id":"1569558509","weight":7},{"id":"1569564851","weight":7},{"id":"1569566553","weight":7},{"id":"1569564969","weight":7},{"id":"1569565029","weight":7},{"id":"1569567033","weight":7},{"id":"1569565441","weight":7},{"id":"1569564097","weight":7},{"id":"1569566501","weight":14},{"id":"1569560503","weight":7},{"id":"1569563395","weight":14},{"id":"1569551347","weight":7},{"id":"1569555367","weight":7},{"id":"1569566383","weight":7},{"id":"1569565885","weight":7},{"id":"1569566479","weight":7},{"id":"1569566129","weight":7},{"id":"1569565919","weight":7},{"id":"1569566267","weight":7},{"id":"1569564131","weight":7},{"id":"1569561221","weight":7},{"id":"1569566917","weight":7},{"id":"1569566651","weight":7},{"id":"1569566823","weight":14},{"id":"1569566137","weight":21},{"id":"1569565013","weight":7},{"id":"1569565375","weight":7},{"id":"1569565293","weight":7},{"id":"1569566641","weight":7},{"id":"1569551905","weight":7},{"id":"1569566619","weight":7},{"id":"1569561185","weight":7},{"id":"1569565233","weight":7},{"id":"1569566911","weight":7},{"id":"1569566299","weight":7},{"id":"1569557851","weight":28},{"id":"1569555891","weight":7},{"id":"1569566847","weight":7},{"id":"1569560459","weight":7},{"id":"1569565889","weight":7},{"id":"1569556327","weight":7},{"id":"1569566375","weight":7},{"id":"1569564257","weight":7},{"id":"1569564931","weight":7},{"id":"1569551751","weight":7},{"id":"1569564419","weight":7},{"id":"1569566067","weight":7},{"id":"1569564807","weight":7},{"id":"1569563007","weight":7}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S14.T7.1","endtime":"17:00","authors":"Ertem Tuncel","date":"1341506400000","papertitle":"Recognition Capacity Versus Search Speed in Noisy Databases","starttime":"16:40","session":"S14.T7: Topics in Shannon Theory","room":"Stratton (407)","paperid":"1569565181"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
