{"id":"1569565489","paper":{"title":{"text":"The Peak Constrained Additive Inverse Gaussian Noise Channel"},"authors":[{"name":"Andrew W. Eckford"},{"name":"K. V. Srinivas"},{"name":"Raviraj S. Adve"}],"abstr":{"text":"Abstract\u2014In molecular communication, messages are conveyed in patterns of particles (e.g., arranged in time), which propagate from transmitter to receiver by means of Brownian motion. If there is drift from transmitter to receiver, the ﬁrst arrival time of the particles has the Inverse Gaussian distribution, leading to the additive inverse Gaussian noise channel. In this paper, we give a closed-form upper bound on capacity for this channel when the maximum waiting time is constrained, building on previous work in which only the mean waiting time was constrained."},"body":{"text":"In diffusion-mediated molecular communication, messages are carried by molecules (or other particles), which propagate from transmitter to receiver by Brownian motion [1]. Mimick- ing the communication among bacteria and cells in the body, molecular communication has excellent properties in terms of energy efﬁciency, simplicity, and biocompatibility, making it a potential solution to the problem of nanoscale networking [2], especially in biomedical applications [3].\nConsider a molecular communication system in which the transmitter encodes a message in the timing of molecule releases. For example, say the message consists of one bit: a molecule is released at time T 0 to represent 0, or at T 1 to represent 1. The message is distorted by the random prop- agation time n between the transmitter and receiver, known as the ﬁrst arrival time: the receiver observes T 0 + n if 0 was sent, or T 1 + n if 1 was sent. Thus, the ﬁrst arrival time acts as additive noise in the timing channel, analogous to additive noise in conventional communication systems. In a one-dimensional ﬂuid medium, in which there is a net drift from the transmitter to the receiver, the ﬁrst arrival time has the Inverse Gaussian (IG) distribution [4]. A molecular communication timing channel in which the ﬁrst arrival time is IG distributed is known as an Additive Inverse Gaussian Noise (AIGN) channel. (Note that this does not include the case of Brownian motion without drift.)\nMany closed-form properties of the IG distribution are known, which can be exploited in the analysis of the AIGN channel. In previous work [5], we considered detection, mod- ulation, and capacity of AIGN channels, in which there was a constraint on mean waiting time. However, if only the mean is constrained, the maximum waiting time is unlimited. In this paper, we consider a more practical version of the AIGN\nchannel with a peak constraint: i.e., there exists a time T at which the communication session must end. As a result of this restriction, many of the results in [5] require modiﬁcation.\nThe main contribution of this paper is a closed-form upper bound on the capacity of the peak-constrained AIGN channel. Few analytical results exist for this family of channels, and our result represents an important step towards the mathematical understanding of their capabilities and limitations.\nOur paper adds to a rapidly expanding body of research on the information theory of molecular communication, for which an excellent survey is found in [1]. Information-theoretic analysis of the molecular communication problem has been considered in several recent papers: diffusion of concentra- tions has been considered in [6]; \u201cactive\u201d propagation (e.g., using molecular motors) has been analyzed in [7], [8]; and analysis of models including biological components has been performed in [9], [10]. Our paper contributes to this growing body of literature by providing an analytical result, which (in future work) will provide a basis for performance measure- ment and optimal communication strategies (e.g., modulation techniques).\nOur system consists of a one-dimensional medium, with a transmitter at the origin and a receiver located at position d > 0, as depicted in Figure 1. Our results can be applied to three-dimensional media where the Brownian motions in all three dimensions are independent, and where the receiver is an inﬁnite plane; in this case, only the Brownian motion in the dimension perpendicular to the plane is relevant. Molecules released into the medium propagate by Brownian motion, with a drift velocity v > 0 from transmitter to receiver. Drift velocity is the expected value of displacement, divided by the time over which that displacement occurs; it can be caused by the net motion of molecules in the medium, such as blood ﬂow in capillaries.\nThis system is used to transmit information from transmitter to receiver. Throughout this paper, we assume the molecular communication system is ideal:\n2) The transmitter perfectly controls the release time of the molecules;\n3) The receiver perfectly measures the arrival time of the molecules; and\n4) On ﬁrst arrival at the receiver, molecules are absorbed and removed from the system. (If this arrival time occurs after the time limit T , then the molecules remain in the system.)\nThese assumptions may be challenging to achieve in practice, but they lead to information-theoretically useful results: in [11], it was shown that relaxing any of them leads to a reduction in capacity.\nMathematically, we model the system as follows. The position of the molecule is a function of time, written B(t). Suppose the molecule is released at time t = 0. Then where B(t) = 0 for all t ≤ 0, and for each t > 0, B(t) is a Gaussian random variable with mean µ, given by\nµ = vt, \t (1) and variance of σ 2 t = 4Dt. (If the molecule is released at time t = τ , the same properties hold, substituting t − τ for t.)\nThe ﬁrst arrival time n of a Brownian motion B(t) is deﬁned as\nn = min{t : B(t) = d}. \t (2) Since B(t) is a random process, n is a random variable, and its distribution f N (n) is known as the ﬁrst arrival time distribution.\nrepresent the Inverse Gaussian distribution with parameters µ and λ, where\nwhich is constant with respect to n. If v > 0, with µ given by (1) and letting\nWe will use the differential entropy of the IG distribution h(N ), which can be given in closed form. For f N (n) given by (7), and measured in nats,\nwhere K γ (·) is the order-γ modiﬁed Bessel function of the second kind. This expression is found in [5], a special case of results found in [4]. (An expression for the derivative of the Bessel function with respect to its order is found in [12].)\nIn a Peak-Constrained AIGN (PCAIGN) channel, there is some maximum waiting time T , at which point the receiver is required to make a decision on its observation. We call this \u201cpeak-constrained\u201d because the signal is expressed in the magnitude of the timing signal; thus, the peak of the time signal must be less than or equal to T . Letting x represent the release time of a molecule, and letting n represent an Inverse Gaussian random variable, the observation y is given by\nThe use of y = ∞ in (10) represents the event that the receiver never observes the molecule.\nThe message alphabet need not be binary, and can be any arbitrary set; for instance, we can send M -ary with release times T 0 , T 1 , . . . , T M−1 . As a result, and for mathematical convenience, we measure information in nats throughout this paper.\nCapacity C, in information units per channel use, is given by\nThroughout this paper, we measure information in nats. In the AIGN channel, a channel use consists of the release of a single molecule at the transmitter, together with its observation at the receiver. Thus, capacity per channel use is equivalent to capacity per molecule. Our main result is a closed-form upper bound on capacity.\nLet Γ(α, z) represent the upper incomplete Gamma func- tion, deﬁned as\n, x ≥ e e , \t (14) and let\nFinally, let p T = Pr(n ≤ T ) represent the probability that the ﬁrst arrival time n is not greater than the maximum waiting time T . The deﬁnitions in (12)-(15) are required to state our main result; the signiﬁcance of these quantities will become apparent later.\nTheorem 1: Capacity C, measured in nats per molecule, satisﬁes\nThe remainder of the paper is dedicated to the proof of Theorem 1.\nAs a general outline of our strategy, capacity is given by C = max\nThe quantity H(Y |X), the uncertainty in the noise given the input, is intuitively equal to the entropy in the noise, h(N ); however, we know that n ≤ T so that the particle does not get lost, so we must calculate h(N |n ≤ T ). (This intuitive\nexplanation hides many details that we resolve later.) We can also choose an upper bound on H(Y ) to get a bound on capacity.\nOur ﬁrst step is to ﬁnd h(N |n ≤ T ). Since h(N ) is given by (9), it will be most convenient to write\n(18) and ﬁnd bounds for the second term.\nWe start by proving some bounds on integrals involving ig(n; λ, µ) log ig(n; λ, µ). Note that\nThe latter two terms require integrals of the form n α ig(n; λ, µ), for α = 1 and α = −1, respectively; in Lemma 1, we give an integral relation for this case.\n(20) Proof: For the integral in (20), the index of integration is\nz, and it is always true that z ≥ x. For z ≥ x > 0, we have that\nThe lemma follows from multiplying the terms in (21-22) by Kz α−3/2 and integrating, with the appropriate changes of variables.\nWe deal with the log n term in (19) by bounding it with a monomial; we can then use Lemma 1 to deﬁne a bound. We ﬁrst have:\nlog z ≤ z (x) . \t (23) Proof: First, for any 0 < z ≤ 1, log z ≤ 0 while z α > 0\n(for any α). Thus, we can restrict ourselves to z > 1 for the remainder of the proof.\nWe now show that log z ≤ z 1/e , which must hold for all z > 1. If log z ≤ z 1/e , then log log z − (1/e) log z ≤ 0, with equality when z = e e . Taking the ﬁrst derivative,\nze log z (e − log z), (24) for which the sign is determined by (e − log z); if z < e e , (e − log z) < 0, and if z > e e , then (e − log z) > 0. Thus, z = e e is a maximum point, so log log z − (1/e) log z ≤ 0.\nFinally, we show that log z ≤ z log log x/ log x , which must hold for all z ≥ x ≥ e e . This statement is equivalent to\nand equality is obviously achieved when z = x. For z > x, taking the ﬁrst derivative,\nFor any x ≥ e e , log log x ≥ 1, and for any z > x, log z/ log x > 1. Thus, the right hand side of (26) is negative for all z > x ≥ e e , which is sufﬁcient to show that (25) is correct.\n(log z)ig(z; λ, µ)dz \t (27) ≤ φ (x) (x; λ, µ). \t (28)\n(log x)ig(z; λ, µ) ≤ (log z)ig(z; λ, µ) \t (29) for z ≥ x. Further, from Lemma 2,\n(log z)ig(z; λ, µ) ≤ z (x) ig(z; λ, µ). \t (30) The lemma follows from integrating (29) and (30), using the bounds from Lemma 1.\nWe now give upper and lower bounds on the integral. (The upper bound is not used any further in this paper, but is given for completeness.)\nProof: The proof follows directly from Lemmas 1-3. Figure 2 gives an illustration of the bounds.\nThe integral bounds in Theorem 2 are similar to the form required to calculate the differential entropy h(N |N ≤ T ). In the following corollary, we complete the calculation to obtain a lower bound on this quantity.\nCorollary 1: Suppose n is an IG random variable with parameters λ and µ, and entropy H(N ). Then\nh(N |n ≤ T ) \t (34) ≥ log p T + 1 p\nOne could also construct an upper bound on differential entropy using Theorem 2. However, as we can see from Figure 2, the upper bound is much looser than the lower bound.\nThere are two challenges to be overcome in order to calculate capacity. First, the conditional entropy H(Y |X) can no longer be reduced to the noise differential entropy h(N ) (from (9)), since n can\u2019t be obtained from y − x, as in the unconstrained case. Second, the distribution of the channel output y has both continuous (i.e., y = x + n) and discrete (i.e., y = ∞) components.\nTo address the ﬁrst challenge, consider an alternative chan- nel model: instead of the communication session ending at time T , as in the PCAIGN, we suppose that the molecule \u201cexpires\u201d exactly T seconds after release. In this channel, letting n represent the IG channel noise, and letting\ny ∗ = x + n ∗ . \t (36) That is, the noise (rather than the channel output) is truncated at time T . We have the following result.\nProof: From (36) and (10), y can be written as a function of y ∗ , i.e., y = ψ(y ∗ ), where\nTo address the second challenge, we obtain an auxiliary random variable u ∗ , given by\nThus, u ∗ is a discrete random variable, and given that u ∗ = 0, y ∗ is a continuous random variable. Since u ∗ is a function of y ∗ ,\nI(X; Y ∗ ) = I(X; U ∗ , Y ∗ ) \t (39) = I(X; Y ∗ |U ∗ ), \t (40)\nwhere (40) follows since u ∗ is only dependent on n ∗ , and is independent of x. Furthermore, note that\nSince Pr(Y ∗ = ∞|U ∗ = 1) = 1, then H(Y ∗ |U ∗ = 1) = 0, and\nProof: From Lemma 4 and (38)-(44), I(X; Y )\n(45) = (H(Y ∗ |U ∗ = 0) − H(Y ∗ |X, U ∗ = 0)) p T .\n(46) If u ∗ = 0, then y ∗ is a continuous random variable on the interval [0, T ]; thus,\nFurthermore, given x and u ∗ = 0, and for an IG random variable n with parameters λ and µ,\nwhere (49) follows from the corollary to Theorem 2. The right hand side of equation (16) is obtained by substituting (47) and (49) into (46). Finally, noting that this upper bound is independent of the input distribution f (x), it must apply to all f (x), and is therefore an upper bound on capacity C.\nExample upper bounds are given in Figure 3. As expected, the upper bounds in Figure 3 increase as T increases. At low\nvalues of T , the \u201cdip\u201d indicates that the bound is not tight in this region; capacity must be nondecreasing with T . However, at larger values of T , we believe the bound to be a good approximation of the true capacity, due to the accuracy of the integral bounds."},"refs":[{"authors":[{"name":"S. Hiyama"},{"name":"Y. Moritani"}],"title":{"text":"Molecular communication: Harnessing biochemical materials to engineer biomimetic communication systems"}},{"authors":[{"name":"I. F. Akyildiz"},{"name":"F. Brunetti"},{"name":"C. Blazquez"}],"title":{"text":"Nanonetworks: A new communication paradigm"}},{"authors":[{"name":"Y. Moritani"},{"name":"S. Hiyama"},{"name":"T. Suda"}],"title":{"text":"Molecular communication for health care applications"}},{"authors":[{"name":"R. S. Chhikar"},{"name":"L. Folk"}],"title":{"text":"The Inverse Gaussian Distribution: Theory, Methodology, and Applications"}},{"authors":[{"name":"K. V. Srinivas"},{"name":"R. S. Adve"},{"name":"A. W. Eckford"}],"title":{"text":"Molecular commu- nication in ﬂuid media: The additive inverse Gaussian noise channel"}},{"authors":[{"name":"D. J. Spencer"},{"name":"S. K. Hampton"},{"name":"P. Park"},{"name":"J. P. Zurkus"},{"name":"P. J. Thomas"}],"title":{"text":"The diffusion-limited biochemical signal-relay channel"}},{"authors":[{"name":"N. Farsad"},{"name":"A. W. Eckford"},{"name":"S. Hiyama"},{"name":"Y. Moritani"}],"title":{"text":"Quick system design of vesicle-based active transport molecular communication by using a simple transport model"}},{"authors":[{"name":"M. J. Moore"},{"name":"T. Suda"},{"name":"K. Oiwa"}],"title":{"text":"Molecular communication: Mod- eling noise effects on information rate"}},{"authors":[{"name":"P. J. Thomas"},{"name":"D. J. Spencer"},{"name":"S. K. Hampton"},{"name":"P. Park"},{"name":"J. P. Zurkus"}],"title":{"text":"The diffusion-limited biochemical signal-relay channel"}},{"authors":[{"name":"A. Einolghozati"},{"name":"M. Sardari"},{"name":"F. Fekri"}],"title":{"text":"Capacity of diffusion- based molecular communication with ligand receptors"}},{"authors":[{"name":"A. W. Eckford"}],"title":{"text":"Molecular communication: Physically realistic models and achievable information rates"}},{"authors":[{"name":"Y. A. Brychko"}],"title":{"text":"Handbook of Special Functions: Derivatives, Integrals, Series and Other Formulas"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565489.pdf"},"links":[{"id":"1569565663","weight":12},{"id":"1569565091","weight":4},{"id":"1569564311","weight":4},{"id":"1569565365","weight":28},{"id":"1569565469","weight":4},{"id":"1569566853","weight":4},{"id":"1569566301","weight":4},{"id":"1569566457","weight":4},{"id":"1569555891","weight":8},{"id":"1569565113","weight":8},{"id":"1569564807","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T9.4","endtime":"12:50","authors":"Andrew Eckford, K. V. Srinivas, Raviraj Adve","date":"1341577800000","papertitle":"The peak constrained additive inverse Gaussian noise channel","starttime":"12:30","session":"S16.T9: Information Theory in Biology","room":"Stratton West Lounge (201)","paperid":"1569565489"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
