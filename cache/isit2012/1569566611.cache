{"id":"1569566611","paper":{"title":{"text":"Sparse Signal Separation in Redundant Dictionaries"},"authors":[{"name":"Céline Aubel ∗"},{"name":"Christoph Studer \u2020"},{"name":"Graeme Pope ∗"},{"name":"Helmut Bölcskei ∗"}],"abstr":{"text":"Abstract\u2014We formulate a uniﬁed framework for the separa- tion of signals that are sparse in \u201cmorphologically\u201d different redundant dictionaries. This formulation incorporates the so- called \u201canalysis\u201d and \u201csynthesis\u201d approaches as special cases and contains novel hybrid setups. We ﬁnd corresponding coherence- based recovery guarantees for an 1 -norm based separation algorithm. Our results recover those reported in Studer and Baraniuk, ACHA, submitted, for the synthesis setting, provide new recovery guarantees for the analysis setting, and form a basis for comparing performance in the analysis and synthesis settings. As an aside our ﬁndings complement the D-RIP recovery results reported in Candès et al., ACHA, 2011, for the \u201canalysis\u201d signal recovery problem"},"body":{"text":"We consider the problem of splitting the signal x = x 1 +x 2 into its constituents x 1 ∈ C d and x 2 ∈ C d \u2014assumed to be sparse in \u201cmorphologically\u201d different (redundant) dic- tionaries [1]\u2014based on m linear, nonadaptive, and noisy measurements y = Ax + e. Here, A ∈ C m×d , m ≤ d, is the measurement matrix, assumed to be known, and e ∈ C m is a noise vector, assumed to be unknown and to satisfy e 2 ≤ ε, with ε known.\nRedundant dictionaries [2], [3] often lead to sparser repre- sentations than nonredundant ones, such as, e.g., orthonormal bases, and have therefore become pervasive in the sparse signal recovery literature [3]. In the context of signal separation, redundant dictionaries lead to an interesting dichotomy [1], [4], [5]:\n\u2022 In the so-called \u201csynthesis\u201d setting, it is assumed that, for = 1, 2, x = D s , where D ∈ C d×n (d < n) is a redundant dictionary (of full rank) and the coefﬁcient vector s ∈ C n is sparse (or approximately sparse in the sense of [6]). Given the vector y ∈ C m , the problem of ﬁnding the constituents x 1 and x 2 is formalized as [7]:\n\u2022 In the so-called \u201canalysis\u201d setting, it is assumed that, for = 1, 2, there exists a matrix Ψ ∈ C n×d such that\nΨ x is sparse (or approximately sparse). The problem of recovering x 1 and x 2 from y is formalized as [5]:\nThroughout the paper, we exclusively consider redundant dic- tionaries as for D , = 1, 2, square, the synthesis setting can be recovered from the analysis setting by taking Ψ = D −1 .\nThe problems (PS) and (PA) arise in numerous applications including denoising [8], super-resolution [8], inpainting [9]\u2013 [11], deblurring [11], and recovery of sparsely corrupted signals [12]. Coherence-based recovery guarantees for (PS) were reported in [7]. The problem (PA) was mentioned in [5]. In the noiseless case, recovery guarantees for (PA), expressed in terms of a concentration inequality, are given in [13] for A = I d and Ψ 1 and Ψ 2 both Parseval frames [2].\nwhich encompasses (PS) and (PA). To recover (PS) from (P), one sets A = AD and Ψ = [I d 0 d,n−d ] T , for = 1, 2. (PA) is obtained by choosing A = A, for = 1, 2. Our main contribution is a coherence-based recovery guarantee for the general problem (P). This result recovers [7, Th. 4], which deals with (PS), provides new recovery guarantees for (PA), and constitutes a basis for comparing performance in the analysis and synthesis settings. As an aside, it also complements the D-RIP recovery guarantee in [5, Th. 1.2] for the problem\nby delivering a corresponding coherence-based recovery guar- antee. Moreover, the general formulation (P) encompasses novel hybrid problems of the form\nNotation: Lowercase boldface letters stand for column vectors and uppercase boldface letters denote matrices. The transpose, conjugate transpose, and Moore-Penrose inverse of the matrix M are designated as M T , M H , and M \u2020 , respectively. The jth column of M is written [M] j , and\nthe entry in the ith row and jth column of M is [M] i,j . We let σ min (M) denote the smallest singular value of M, use I n for the n × n identity matrix, and let 0 k×m be the k × m all zeros matrix. For matrices M and N, we let ω min (M) \t min j [M] j 2 , ω max (M) \t max j [M] j 2 , ω min (M, N) min{ω min (M), ω min (N)}, and ω max (M, N)\nmax{ω max (M), ω max (N)}. The kth entry of the vector x is written [x] k , and x 1 \t k |[x] k | stands for its 1 -norm. We take supp k (x) to be the set of indices corresponding to the k largest (in magnitude) coefﬁcients of x. Sets are designated by uppercase calligraphic letters; the cardinality of the set S is |S| and the complement of S (in some given set) is denoted by S c . For a set S of integers and n ∈ Z, we let n + S {n + p : p ∈ S}. The n × n diagonal projection matrix P S for the set S ⊂ {1, . . . , n} is deﬁned as follows:\nand we set M S P S M. We deﬁne σ k (x) to be the 1 -norm approximation error of the best k-sparse approximation of x, i.e., σ k (x) \t x−x S 1 where S = supp k (x) and x S P S x.\nCoherence deﬁnitions in the sparse signal recovery litera- ture [3] usually apply to dictionaries with normalized columns. Here, we will need coherence notions valid for general (un- normalized) dictionaries M and N, assumed, for simplicity of exposition, to consist of nonzero columns only.\nDeﬁnition 1 (Coherence): The coherence of the dictio- nary M is deﬁned as\nDeﬁnition 2 (Mutual coherence): The mutual coherence of the dictionaries M and N is deﬁned as\nThe main contribution of this paper is the following recov- ery guarantee for (P).\nTheorem 1: Let y = A 1 x 1 + A 2 x 2 + e with e 2 ≤ ε and let Ψ 1 ∈ C n 1 ×p 1 and Ψ 2 ∈ C n 2 ×p 2 be full-rank matrices. Let x = [x T 1 x T 2 ] T , ˆ µ 1 = ˆ µ(A 1 Ψ \u2020 1 ), ˆ µ 2 = ˆ µ(A 2 Ψ \u2020 2 ), ˆ µ m = ˆ µ m (A 1 Ψ \u2020 1 , A 2 Ψ \u2020 2 ), and ˆ µ max = max{ˆ µ 1 , ˆ µ 2 , ˆ µ m }. Without loss of generality, we assume that ˆ µ 1 ≤ ˆ µ 2 . Let k 1 and k 2 be nonnegative integers such that\n(3) Then, the solution (x ∗ 1 , x ∗ 2 ) to the convex program (P) satisﬁes\nwhere C 0 , C 1 ≥ 0 are constants that do not depend on x 1 and x 2 and where x ∗ = [x ∗ 1 T x ∗ 2 T ] T .\nNote that the quantities ˆ µ 1 , ˆ µ 2 , and ˆ µ m characterize the in- terplay between the measurement matrix A and the sparsifying transforms Ψ 1 and Ψ 2 .\nAs a corollary to our main result, we get the following statement for the problem (P ∗ ) considered in [5].\nCorollary 2: Let y = Ax + e with e 2 ≤ ε and let Ψ ∈ C n×p be a full-rank matrix. Let k be a nonnegative integer such that\nThe proofs of Theorem 1 and Corollary 2 can be found in the Appendix.\nWe conclude by noting that D-RIP recovery guarantees for (P ∗ ) were provided in [5]. As is common in RIP-based\nrecovery guarantees the restricted isometry constants are, in general, hard to compute. Moreover, the results in [5] hinge on the assumption that Ψ forms a Parseval frame, i.e., Ψ H Ψ = I d ; a corresponding extension to general Ψ was provided in [14]. We ﬁnally note that it does not seem possible to infer the coherence-based threshold (5) from the D-RIP recovery guarantees in [5], [14].\nWe analyze an image-separation problem where we remove a ﬁngerprint from a cartoon image. We corrupt the 512 × 512 greyscale cartoon image depicted in Fig. 1(a) by adding a ﬁngerprint 2 and i.i.d. zero-mean Gaussian noise.\nCartoon images are constant apart from (a small number of) discontinuities and are thus sparse under the ﬁnite difference operator ∆ deﬁned in [15]. Fingerprints are sparse under the application of a wave atom transform, W, such as the redun- dancy 2 transform available in the WaveAtom toolbox 3 [16]. It is therefore sensible to perform separation by solving the problem (PA) with Ψ 1 = ∆, Ψ 2 = W, and A = I d . For our simulation, we use a regularized version of ∆ and we employ the TFOCS solver 4 from [17].\nFig. 1(c) shows the corresponding recovered image. We can see that the restoration procedure gives visually satisfactory results.\nFor simplicity of exposition, we ﬁrst present the proof of Corollary 2 and then describe the proof of Theorem 1.\nWe deﬁne the vector h = x ∗ − x, where x ∗ is the solution to (P ∗ ) and x is the vector to be recovered. We furthermore set S = supp k (Ψx).\n1) Prerequisites: Our proof relies partly on two important results developed earlier in [5], [6] and summarized, for completeness, next.\nProof: Since x ∗ is the minimizer of (P ∗ ), the inequality Ψx 1 ≥ Ψx ∗ 1 holds. Using Ψ = Ψ S + Ψ S c and x ∗ =\n≥ Ψx ∗ 1 = Ψ S x + Ψ S h 1 + Ψ S c x + Ψ S c h 1 ≥ Ψ S x 1 − Ψ S h 1 + Ψ S c h 1 − Ψ S c x 1 .\nLemma 4 (Tube constraint [5], [6]): The vector Ah satis- ﬁes Ah 2 ≤ 2ε.\nProof: Since both x ∗ and x are feasible (we recall that y = Ax + e with e 2 ≤ ε), we have the following\n2) Bounding the recovery error: We want to bound h 2 from above. Since σ min (Ψ) > 0 by assumption (Ψ is assumed to be full-rank), it follows from the Rayleigh-Ritz theorem [18, Th. 4.2.2] that\nSince Q is the set of indices of the k largest (in magnitude) coefﬁcients of Ψh and since Q and S both contain k elements, we have Ψ S h 1 ≤ Ψ Q h 1 and Ψ Q c h 1 ≤ Ψ S c h 1 , which, combined with the cone constraint in Lemma 3, yields\n+ 2 Ψ S c x 1 Ψ Q h 1 k\nk u 2 for k-sparse 5 u and (12b) is a consequence of 2xy ≤ x 2 + y 2 , for x, y ∈ R.\nk \t (13a) ≤\nwhere (13a) is a consequence of (12b) and (13b) results from x 2 + y 2 ≤ x + y, for x, y ≥ 0.\n3) Bounding the term Ψ Q h 2 in (14): In the last step of the proof, we bound the term Ψ Q h 2 in (14). To this end, we ﬁrst bound AΨ \u2020 Ψ Q h 2 2 , with Ψ \u2020 = (Ψ H Ψ) −1 Ψ H , using Geršgorin\u2019s disc theorem [18, Th. 6.2.2]:\nwhere θ min ω 2 min − µ(k − 1) and θ max ω 2 max + µ(k − 1) with\nUsing Lemma 4 and (15) and following the same steps as in [20, Th. 2.1] and [7, Th. 1], we arrive at the following chain of inequalities:\n= (Ah) H AΨ \u2020 Ψ Q h − (AΨ \u2020 Ψ Q c h) H AΨ \u2020 Ψ Q h \t (17a) ≤ |(Ah) H AΨ \u2020 Ψ Q h| + |(Ψ Q c h) H (AΨ \u2020 ) H AΨ \u2020 (Ψ Q h)| ≤ Ah 2 AΨ \u2020 Ψ Q h 2\n(17d) ≤ 2ε θ max Ψ Q h 2 + µk Ψ Q h 2 2\nwhere (17a) follows from Ψ Q h = Ψh − Ψ Q c h and Ψ \u2020 Ψ = I d , (17b) is a consequence of the Cauchy-Schwarz inequality, (17c) is obtained from (15), Lemma 4, and the deﬁnition of µ in (16), (17d) results from (11), and (17e) comes from u 1 ≤ √\nIf h = 0, then Ψ Q h 2 = 0, since Ψ is assumed to be full-rank and Q is the set of indices of the k largest (in magnitude) coefﬁcients of Ψh, and therefore, the inequality between θ min Ψ Q h 2 2 and (17e) simpliﬁes to\n4) Recovery guarantee: Using Deﬁnition 1, we get ˆ µ = ˆ µ(AΨ \u2020 ) = µ/ω 2 min . Combining (14) and (18), we therefore conclude that for\nk < 1 2\nˆ µ \t (19) we have\n(1 + ˆ µ(k − 1)) 1 − ˆ µ(2k − 1)\nWe start by transforming (P) into the equivalent problem (P ∗ ) minimize\nby amalgamating Ψ 1 , Ψ 2 and A 1 , A 2 into the matrices Ψ and A as follows:\nwhere p = 2d in the analysis setting, p = 2n in the synthesis setting, and p = d + n in hybrid settings. The corresponding measurement vector is y = Ax + e, where we set x = [x T 1 x T 2 ] T .\nA recovery condition for (P) could now be obtained by simply inserting A and Ψ in (20), (21) above into (5). In certain cases, we can, however, get a better (i.e., less restrictive) threshold following ideas similar to those reported in [7] and detailed next.\nWe deﬁne the vectors h 1 = x ∗ 1 − x 1 , h 2 = x ∗ 2 − x 2 , the sets Q 1 \t supp k\n(Ψ 2 h 2 ), and h = [h T 1 h T 2 ] T , Q = Q 1 ∪ Q 2 , and set k = k 1 + k 2 .\nWe furthermore let, for = 1, 2, µ = max\nWith the deﬁnitions of Q 1 and Q 2 , we have from (15) AΨ \u2020 Ψ Q h 2 2 = AΨ \u2020 Ψ Q 1 h 2 2 + AΨ \u2020 Ψ Q 2 h 2 2\nwith θ min, \t ω 2 min (A Ψ \u2020 ) − µ (k − 1) and θ max, ω 2 max (A Ψ \u2020 ) + µ (k − 1), for = 1, 2.\nIn addition, the last term in (22) can be bounded as |(AΨ \u2020 Ψ Q 1 h) H AΨ \u2020 Ψ Q 2 h|\n≤ µ m Ψ Q 1 h 1 Ψ Q 2 h 1 \t (25a) ≤ µ m k 1 k 2 Ψ Q 1 h 2 Ψ Q 2 h 2 \t (25b) ≤ µ m\nwhere (25a) follows from the deﬁnition of µ m , (25b) results from u 1 ≤\nk u 2 , for k-sparse u, and (25c) is a conse- quence of the arithmetic-mean geometric-mean inequality.\nwhere θ min ω 2 min −f (k 1 , k 2 ), θ max ω 2 max +f (k 1 , k 2 ), ω min ω min (A 1 Ψ \u2020 1 , A 2 Ψ \u2020 2 ), ω max ω max (A 1 Ψ \u2020 1 , A 2 Ψ \u2020 2 ), and\nNext, we bound g(k 1 , k 2 ) from below by a function of k = k 1 +k 2 . This can be done, e.g., by looking for the minimum [7]\nTo ﬁnd ˆ g(k) in (26) or in (27), we need to distinguish between two cases:\n\u2022 Case 1: µ 1 (k 1 − 1) ≤ µ 2 (k 2 − 1) In this case, we get\nA straightforward calculation reveals that the minimum of g is achieved at\nσ min (Ψ)ˆ g(k) and\nwhere we used Deﬁnitions 1 and 2 to get a threshold depending on the coherence parameters only.\n\u2022 Case 2: µ 2 (k 2 − 1) ≤ µ 1 (k 1 − 1) Similarly to Case 1, we get\nSince ˆ µ 1 ≤ ˆ µ 2 , by assumption, the inequality in (30) is tighter than the one in (29). We complete the proof by combining the thresholds in (19) and (29) to get (3)."},"refs":[{"authors":[{"name":"R. Rubinstein"},{"name":"A. M. Bruckstein"},{"name":"M. Elad"}],"title":{"text":"Dictionaries for sparse representation modeling"}},{"authors":[{"name":"O. Christense"},{"name":"J. J. Benedett"}],"title":{"text":"An Introduction to Frames and Riesz Bases, ser"}},{"authors":[{"name":"M. Ela"}],"title":{"text":"Sparse and Redundant Representations \u2013 From Theory to Applications in Signal and Image Processing "}},{"authors":[{"name":"P. Milanfar"},{"name":"R. Rubinstein"}],"title":{"text":"Analysis versus synthesis in signal priors"}},{"authors":[{"name":"E. J. Candès"},{"name":"Y. C. Eldar"},{"name":"D. Needell"},{"name":"P. Randall"}],"title":{"text":"Compressed sens- ing with coherent and redundant dictionaries"}},{"authors":[{"name":"E. J. Candès"},{"name":"J. Romberg"},{"name":"T. Tao"}],"title":{"text":"Stable signal recovery from incomplete and inaccurate measurements"}},{"authors":[{"name":"C. Studer"},{"name":"R. Baraniuk"}],"title":{"text":"Stable restoration and separation of approximately sparse signals"}},{"authors":[{"name":"S. G. Malla"}],"title":{"text":"A Wavelet Tour of Signal Processing: The Sparse Way"}},{"authors":[{"name":"M. Elad"},{"name":"J.-L. Starck"},{"name":"P. Querre"},{"name":"D. L. Donoho"}],"title":{"text":"Simultaneous cartoon and texture image inpainting using morphological component analysis (MCA)"}},{"authors":[{"name":"J. Fadili"},{"name":"J.-L. Starck"},{"name":"M. Elad"},{"name":"D. L. Donoho"}],"title":{"text":"MCALab: Repro- ducible research in signal and image decomposition and inpainting"}},{"authors":[{"name":"J.-F. Cai"},{"name":"S. Osher"},{"name":"Z. Shen"}],"title":{"text":"Split Bregman methods and frame based image restoration"}},{"authors":[{"name":"C. Studer"},{"name":"P. Kuppinger"},{"name":"G. Pope"},{"name":"H. Bölcskei"}],"title":{"text":"Recovery of sparsely corrupted signals"}},{"authors":[{"name":"G. Kutyniok"}],"title":{"text":"Data separation by sparse representations"}},{"authors":[{"name":"Y. Liu"},{"name":"T. Mi"},{"name":"S. Li"}],"title":{"text":"Compressed sensing with general frames via optimal-dual-based 1 -analysis"}},{"authors":[{"name":"S. Nam"},{"name":"M. Davies"},{"name":"M. Elad"},{"name":"R. Gribonval"}],"title":{"text":"The cosparse analysis model and algorithms"}},{"authors":[{"name":"L. Demanet"},{"name":"L. Ying"}],"title":{"text":"Wave atoms and sparsity of oscillatory patterns"}},{"authors":[{"name":"S. Becker"},{"name":"E. J. Candès"},{"name":"M. Grant"}],"title":{"text":"Templates for convex cone problems with applications to sparse signal recovery"}},{"authors":[{"name":"R. A. Hor"},{"name":"C. R. Johnso"}],"title":{"text":"Matrix Analysis"}},{"authors":[{"name":"T. T. Cai"},{"name":"L. Wang"},{"name":"G. Xu"}],"title":{"text":"New bounds for restricted isometry constants"}},{"authors":[],"title":{"text":"Stable recovery of sparse signals and an oracle inequality"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566611.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T9.4","endtime":"11:10","authors":"Céline Aubel, Christoph Studer, Graeme Pope, Helmut Bölcskei","date":"1341485400000","papertitle":"Sparse Signal Separation in Redundant Dictionaries","starttime":"10:50","session":"S11.T9: L1-Regularized Least Squares and Frames","room":"Stratton West Lounge (201)","paperid":"1569566611"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
