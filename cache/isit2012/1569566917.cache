{"id":"1569566917","paper":{"title":{"text":"Message-passing sequential detection of multiple change points in networks"},"authors":[{"name":"XuanLong Nguyen"},{"name":"Arash Ali Amini"},{"name":"Ram Rajagopal"}],"abstr":{"text":"Abstract\u2014We propose a probabilistic formulation that enables sequential detection of multiple change points in a network setting. We present a class of sequential detection rules for func- tionals of change points, and prove their asymptotic optimality properties in terms of expected detection delay time. Drawing from graphical model formalism, the sequential detection rules can be implemented by a computationally efﬁcient message- passing protocol which may scale up linearly in network size and in waiting time. The effectiveness of our exact and approximate inference algorithms are demonstrated by simulations."},"body":{"text":"Classical sequential detection is the problem of detecting changes in the distribution of data collected sequentially over time [1]. In a decentralized network setting, the decentralized sequential detection problem concerns with data sequences aggregaged over the network, while sequential detection rules are constrained to the network structure (see, e.g., [2], [3], [4]). The focus was still on a single change point variable taking values in (discrete) time. In this paper, our interests lie in sequential detection in a network setting, where multiple change point variables may be simultaneously present.\nAs an example, quickest detection of trafﬁc jams concerns with multiple potential hotspots (i.e., change points) spatially located across a highway network. A simplistic approach is to treat each change point variables independently, so that the sequential analysis of individual change points can be applied separately. However, it has been shown that accounting for the statistical dependence among the change point variables can provide signiﬁcant improvement in reducing both false alarm probability and detection delay time [5].\nThis paper proposes a general probablistic formulation for the multiple change point problem in a network setting, adopting the perspective of probabilistic graphical models for multivariate data [6]. We consider estimating functionals of multiple change points deﬁned globally and locally across the network. The probablistic formulation enables the borrowing of statistical strengh from one network site (associated with a change point variable) to another. We propose a class of sequential detection rules, which can be implemented in a message-passing and distributed fashion across the network. The computation of the proposed sequential rules scales up linearly in both network size and in waiting time, while an approximate version scales up constantly in waiting time. The proposed detection rules are shown to be asymptotically optimal in a Bayesian setting. Interestingly, the expected\ndetection delay can be expressed in terms of Kullback-Leibler divergences deﬁned along edges of the network structure. We provide simulations that demonstrate both statistical and computational efﬁciency of our approach.\nRelated Work. The rich statistical literature on sequential analysis tends to focus almost entirely on the inference of a single change point variable [1]. There are recent formulations for sequential diagnosis of a single change point, which may be associated with multiple causes [7], or multiple sequences [8]. Another approach taken in [9] considers a change propa- gating in a Markov fashion across an array of sensors. These are interesting directions but the focus is still on detecting the onset of a single event. Graphical models have been considered for distributed learning and decentralized detection before, but not in the sequential setting [10], [11]. This paper follows the line of work of [5], [12], but our formulation based on graphical models is more general, and we impose less severe constraints on the amount of information that can be exchanged across network sites.\nIn this section, we shall formulate the multiple change point detection problem, where the change point variables and observed data are linked using a graphical model. Consider a sensor network with d sensors, each of which is associated with a random variable λ j ∈ N, for j ∈ [d] := {1, 2, . . . , d}, representing a change point, the time at which a sensor fails to function properly. We are interested in detecting these change points as accurately and as early as possible, using the data that are associated with (e.g., observed by) the sensors. Taking a Bayesian approach, each λ j is independently endowed with a prior distribution π j (·).\nA central ingredient in our formalism is the notion of a statistical graph , denoted as G = (V, E), which speciﬁes the probabilistic linkage between the change point variables and observed data collected in the network. The vertex set of the graph, V = [d] represents the indices of the change point variables λ j . The edge set E represents pairings of change point variables, E = {e = {s 1 , s 2 } | s 1 , s 2 ∈ V }. With each vertex and each edge, we associate a sequence of observation variables,\nX j = (X 1 j , X 2 j , . . . ), j ∈ V, \t (1) X e = (X 1 e , X 2 e , . . . ), e ∈ E, \t (2)\nwhere the superscript denotes the time index. The X j models the private information of node j, while X e models the shared information of nodes connected by e. We will use the notation X n j = (X 1 j , . . . , X n j ) and similarly for X n e ; notice the distinction between X n j , the observation at time n, versus bold X n j , the observations up to time n, both at node j. The aggregate of all the observations in the network is denoted as X ∗ = (X j , j ∈ V, X e , e ∈ E). Similarly, X n ∗ represents all the observations up to time n. We will also use λ ∗ = (λ j , j ∈ V ).\nThe joint distribution of λ ∗ and X n ∗ is given by a graphical model,\n(3) Given λ j = k, we assume X 1 j , . . . , X k −1 j to be i.i.d. with density g j and X k j , X k +1 j , . . . to be i.i.d. with density f j . Given (λ s 1 , λ s 2 ), we assume that the distribution of X n e only depends on λ e := λ s 1 ∧ λ s 2 , the minimum of the two change points; hence we often write P (X n e |λ e ) instead of P (X n e |λ s 1 , λ s 2 ). Given λ e = k, X 1 e , . . . , X k −1 e are i.i.d. with density g e and X k e , X k +1 e , . . . are i.i.d. with density f e . All the densities are assumed to be with respect to some underlying measure µ. These speciﬁcations can be summarized as,\nand similarly for P (X n e |λ e ). We will assume the prior on λ j to be geometric with parameter ρ j ∈ (0, 1), i.e. π j (k) := (1 − ρ j ) k −1 ρ j , for k ∈ N. Note that these change point variables are dependent a posteriori, despite being independent a priori.\nAlthough our primary interest is in sequential estimation of the change points λ ∗ = (λ j ), we are in general interested in the following functionals,\nfor some subset S ⊂ [d]. Examples include a single change point S = {j}, the earliest among a pair S = {i, j} and the earliest in the entire network S = [d]. Let F n = σ(X n ∗ ) be the σ-algebra induced by the sequence X n ∗ . A sequential detection rule for φ is formally a stopping time τ with respect to ﬁltration (F n ) n ≥0 . To emphasize the subset S, we will use τ S to denote a rule when the functional φ = λ S . For example τ 1 is a detection rule for λ 1 and τ 12 is a rule for λ 12 = λ 1 ∧λ 2 .\nIn choosing τ , there is a trade-off between the false alarm probability P (τ ≤ φ) and the detection delay E(τ −φ) + . Here, we adopt the Neyman-Pearson setting to consider all stopping rules for φ, having false alarm at most α,\nand pick a rule in ∆ φ that has minimum detection delay. It is worth mentioning that there are non-Bayesian optimality criteria for the single change point problem, e.g. [13], and it\nwould be an interesting direction to study our multiple change point model in such settings.\nAnother ingredient of our formalism is the notion of a communication graph representing constraints under which the data can be transmitted across network to compute a particular stopping rule, say τ j . In general, such a rule depends on all the aggregated data X n ∗ . We are primarily interested in those rules that can be implemented in a distributed fashion by passing messages from one sensor only to its neighbors in the communication graph. Although, conceptually, the statistical graph and communication graphs play two distinct roles, they usually coincide in practice and this will be assumed throughout this paper. See Fig. 1 for an illustration.\nWe suspect that it is not feasible to derive strictly optimal sequential stopping rules in closed from (say by stochastic dynamic programming) for the multiple change point problem introduced earlier. More crucially, even if such rules are obtained, they are not computationally tractable for large networks, due to the exponential complexity of the state-space. In this section, we shall present a class of detection rules that scale linearly in the size of the network, d, and can be implemented in a distributed fashion by message passing.\nWe propose to stop at the ﬁrst time γ n S [n] goes above a threshold,\nwhere α is the maximum tolerable false alarm. It is easily veriﬁed that these rules have a false alarm at most α.\nMore interestingly, we will show that τ S is asymptotically optimal for detecting λ S . To do so, let us extend the edge set to E := E ∪ {{j} : j ∈ V }. This allows us to treat the private data associated with node j, i.e. X j , as (shared) data associated with a self-loop in the graph (V, E). For any e ∈ E, let I e := f e log f e g e dµ be the KL divergence between f e and g e . For φ = λ S , let\nwhere the sum runs over all e ∈ E which are subsets of S. For example, for a chain graph on {1, 2, 3} with node 2 in the middle, E = {{1, 2}, {2, 3}, {1}, {2}, {3}} and we have I λ 12 := I 1 + I 2 + I 12 while I λ 13 := I 1 + I 3 . (Here, we abuse notation to write I 12 instead of I {1,2} and so on.)\nRecall the geometric prior on λ j (with parameter ρ j ) and the deﬁnition of φ = λ S as the minimum of λ j , j ∈ S. Then, φ is geometrically distributed a priori with parameter 1 − e −q φ := 1 − j ∈S (1 − ρ j ). We can now state our main result on asymptotic optimality.\n| ≤ M for all e ∈ E. Then, τ S is asymptotically optimal for φ = λ S ; more speciﬁcally, as α → 0,\nq φ + I φ (1 + o(1)) = inf\nRemark 1. A notable feature of this result is the decompo- sition (10) of information along the edges of the graph. For example, in the case of a paired delay φ = λ 12 , for which the information I φ = I 1 + I 2 + I 12 1 {{1,2}∈E} increases (hence the asymptotic delay decreases) if there is an edge between nodes 1 and 2. This has no counterpart in the classical theory where one looks at change points independently.\nRemark 2. Another feature of the result is observed for a single delay, say φ = λ 1 , where one has I φ = I 1 regardless of whether there is an edge between nodes 1 and 2. Thus, the asymptotic delay for the threshold rule which bases its decision on the posterior probability of λ 1 given all the data in the network (X n ∗ ) is the same as the one which bases its decision on the posterior given only private data of node 1 (X n 1 ). Al- though this rather counter-intuitive result holds asymptotically, the simulations show that even for moderately low values of α, having access to extra information in X n 12 does indeed improve performance as one expects. (cf. Section VI).\nIt is relatively simple to adapt the well-established belief propagation algorithm, also known as sum-product, to the graphical model (3). The algorithm produces exact values of the posterior γ n S , as deﬁned in (7), in the cases where G is a polytree (and provides a reasonable estimate otherwise.) In this section, we provide the details for S = {j} or S = {i, j} ∈ E.\nOne issue in adapting the algorithm is the possible inﬁnite support of γ n S . Thanks to a \u201cconstancy\u201d property of the likelihood, it is possible to lump all the states after n when computing γ n S [n].\nLemma 2. Let {i 1 , i 2 , . . . , i r } ⊂ [d] be a distinct collection of indices. The function\nThe algorithm is invoked at each time step n, by passing messages between nodes according to the following protocol: a node sends a message to one of its neighbors (in G) when and only when it has received messages from all its other neighbors. Message passing continues until any node can be linked to any other node by a chain of messages, assuming a connected graph. For a tree, this is usually achieved by designating a node as root and passing messages from the root to the leaves and then backwards.\nThe message that node j sends to its neighbor i, at time n, is denoted as m n ji = [m n ji (1), . . . , m n ji (n + 1)] ∈ R n +1 and computed as\nfor k ∈ [n + 1], where π j (k) := π j (k) for k ∈ [n] and π j (n + 1) := π j [n] c = ∞ k =n+1 π j (k), and ∂j is the neighborhood set of j. Once the message passing ends, γ n j and γ n ij are readily available. We have\nIt also holds for k = n + 1 if the LHS is interpreted as γ n j [n] c . Similarly, when {i, j} ∈ E, P (λ i = k 1 , λ j = k 2 |X n ∗ ) is proportional to\nπ i (k 1 )π j (k 2 )P (X n i |k 1 )P (X n j |k 2 )P (X n ij |k 1 ∧ k 2 ) ×\nLemma 3. When G is a tree, the message passing algorithm produces correct values of γ n j and γ n ij at time step n, with computational complexity O ((|V | + |E|)n).\nWe now turn to an approximate message passing algo- rithm which, at each time step, has computational complexity O (|V | + |E|). Let us deﬁne binary variables\nThe idea is to compute P (Z n ∗ |X n ∗ ) = P (Z n ∗ |X n ∗ , X n −1 ∗ ) recursively based on P (Z n −1 ∗ |X n −1 ∗ ). The former is propor- tional (in Z n ∗ ) to P (Z n ∗ , X n ∗ |X n −1 ∗ ) and we have\nLet u e (z; ξ) := [g e (ξ)] 1−z [f e (ξ]) z for e ∈ E, z ∈ {0, 1}. Then, P (X n j |Z n j ) = u j (Z n j ; X n j ), and P (X n ij |Z n i , Z n j ) = u ij (Z n i ∨ Z n j ; X n ij ). It remains to express P (Z n ∗ |X n −1 ∗ ) in terms of P (Z n −1 ∗ |X n −1 ∗ ). This is possible at a cost of O(2 |V | ), but we omit the details for brevity. To obtain a fast algorithm (i.e., O (poly(|V |))), we instead approximate\n(14) where ν (z; β) := β z (1 − β) 1−z . By constancy Lemma 2, Bayes rule and algebra, we get the recursion\nThus, at time n, the RHS of (14) is known based on values computed at time n − 1 (with initial value γ 0 j [0] = 0, j ∈ V ). Inserting this RHS into (13) in place of P (Z n ∗ |X n −1 ∗ ), we obtain a graphical model in variables Z n ∗ (instead of λ ∗ ) which has the same form as (3) with ν (Z n j ; γ n −1 j [n]) playing the role of the prior π (λ j ).\nHence, we can apply a message passing algorithm similar to that described in Section IV, to marginalize this approximate version of P (Z n ∗ , X n ∗ |X n −1 ∗ ), providing approximate values of γ n j [n] = P (Z n j = 1|X n ∗ ) and γ n ij [n]. The message update equations are similar to those of Section IV and are omitted. The difference is that the messages are now binary and do not grow in size with n. Fig. 2 shows examples of posterior tracking by approximate algorithm. Theoretical analysis of the algorithm will appear in a longer version of this paper.\nWe present simulation results as depicted in Fig. 3. The setting is that of graphical model (3) on d = 4 nodes, where the statistical graph is a star with node 2 in the middle. Condi- tioned on λ ∗ , all the data sequences, X ∗ , are assumed Gaussian of variance 1, with pre-change mean 1 and post-change mean zero. All priors are geometric with parameters ρ j = 0.1. Fig. 3 shows plots of expected delay over | log α|, against | log α|, for\nthree methods: the message-passing algorithm of Section IV (MP), approximate algorithm of Section V (APPROX) and the method which bases its inference on posteriors calculated based only on each node\u2019s private information (SINGLE). This latter method estimates a single change point λ j by τ j := inf{n : P (λ j ≤ n|X n j ) ≥ 1 − α} and a paired λ ij = λ i ∧ λ j by τ i ∧ τ j . Also shown in the ﬁgure is the limiting value of the normalized expected delay as predicted by Theorem 1. All plots are generated by Monte Carlo simulation over 5000 realizations.\nIn estimating single change points, MP, which takes shared information into account, has a clear advantage over SINGLE, for high to relatively low false alarm values (even, say, around α ≈ e −5 ); though, both methods seem to converge to the same slope in the α → 0 limit, as suggested by Theorem 1. (The particular value is (− log 0.9 + 0.5) −1 = 1.6519.) Also note that the advantage of MP over SINGLE is more emphasized for node 2, as expected by its access to shared information from all the three nodes. We also note that APPROX does a reasonable job at approximating MP, with delays between those of SINGLE and MP, getting closer to MP as α → 0.\nFor paired change points, the advantage of MP and AP- PROX over SINGLE is more emphasized. It is also interesting to note that while MP seems to converge to the expected theoretical limit (−2 log 0.9 + 3 · 0.5) −1 = 0.5845, SINGLE\nseems to converge to a higher slope (with a reasonable guess being 1.6519 as in the case of single change points).\nIn regard to false alarm probability, nonzero values were only observed for the ﬁrst few values of α considered here, and those were either below or very close to the speciﬁed tolerance.\nWe provide a proof sketch for the case d = 2 here (the full proof is quite technical, and can be found in [14]). Fix some φ = τ S and consider the likelihood ratio\nLet P k φ = P( · | φ = k). Our asymptotic analysis hinges on the asymptotic behavior of 1 n log D k φ (X n ∗ ), as n → ∞, under probability measure P k φ . In particular, building on the results of [15], it is straightforward to derive the following sufﬁcient condition. Let P m 1 ,m 2 λ\nbe the probability measure conditioned on λ 1 = m 1 and λ 2 = m 2 . Also, let π k φ (m 1 , m 2 ) := P k φ (λ 1 = m 1 , λ 2 = m 2 ). Suppose that for all (m 1 , m 2 ) with positive probability under π k φ (m 1 , m 2 ), we can show the \u201cconcentration inequality\u201d\nfor all n ≥ 1 ε p (m 1 , m 2 , k ) for polynomials p(·) and q(·). Then, if both π k φ (·, ·) and P(φ = ·) have ﬁnite polynomial moments, which is the case for our geometric priors, conclusions of Theorem 1 hold for φ. We say that |n −1 log D k φ (X n ∗ )−I φ | ≤ ε holds with high probability, abbreviated w.h.p.\nNext, we deﬁne R n p (e) := R n p (X e ) := n t =p f e (X e ) g e (X e ) if e ∈ E and p ≤ n, and R n p (X e ) = 1 otherwise. Similarly let I e be deﬁned as in (10) if e ∈ E and I e = 0 otherwise. We note that\nwith some abuse of notation. In addition, for a collection E = {e 1 , . . . , e J }, deﬁne\nWe will say that a n ε ≍ b n if |a n − b n | ≤ ε for n ≥ c 0 ε . This relation is transitive and stable under addition, subtraction and taking maximum. Furthermore,\nn −1 log(a n + b n ) ε ≍ max{n −1 log a n , n −1 log b n }. (15) At the heart of the proof are two concentration inequalities:\nwhere the ﬁrst holds conditioned on λ e ≤ m w.h.p. and the second conditioned on λ e ≤ m for all e ∈ E, w.h.p.\nLet us focus on the case φ = λ 12 . For k < ∞, π k φ (k 1 , k 2 ) ∝ ρ k 1 1 ρ k 2 2 1 {k 1 ∧k 2 =k} where ρ j := 1 − ρ j . Let\nConsider T 1 and note that n −1 log T 1 ε ≍ n −1 log R n k {1} + max{ 1 n log S n,n k +1 {2}, log ρ 2 } where the ﬁrst term is ε ≍ I 1 w.h.p. by (16). Similarly, n −1 log S n,n k +1 {2} equals\nand since log ρ 1 < 0, we have n −1 log T 1 ε ≍ I 1 + I 2 . Other terms are dealt with similarly, and we get, w.h.p."},"refs":[{"authors":[{"name":"T. L. Lai"}],"title":{"text":"Sequential analysis: Some classical problems and new challenges (with discussion)"}},{"authors":[{"name":"V. V. Veeravalli"},{"name":"T. Basar"},{"name":"H. V. Poor"}],"title":{"text":"Decentralized sequential detection with a fusion center performing the sequential test"}},{"authors":[{"name":"Y. Mei"}],"title":{"text":"Asymptotic optimality theory for decentralized sequential hy- pothesis testing in sensor networks"}},{"authors":[{"name":"X. Nguyen"},{"name":"M. J. Wainwright"},{"name":"M. I. Jordan"}],"title":{"text":"On optimal quantiza- tion rules in some problems in sequential decentralized detection"}},{"authors":[{"name":"R. Rajagopal"},{"name":"X. Nguyen"},{"name":"S. Ergen"},{"name":"P. Varaiya"}],"title":{"text":"Distributed online simultaneous fault detection for multiple sensors"}},{"authors":[{"name":"M. I. Jordan"}],"title":{"text":"Graphical models"}},{"authors":[{"name":"S. Dayanik"},{"name":"C. Goulding"},{"name":"H. V. Poor"}],"title":{"text":"Bayesian sequential change diagnosis"}},{"authors":[{"name":"Y. Xie"},{"name":"D. Siegmund"}],"title":{"text":"Sequential multi-sensor change-point detec- tion"}},{"authors":[{"name":"V. Raghavan"},{"name":"V. V. Veeravalli"}],"title":{"text":"Quickest change detection of a markov process across a sensor array"}},{"authors":[{"name":"M. Cetin"},{"name":"L. Chen"},{"name":"J. W. Fisher III"},{"name":"A. Ihler"},{"name":"R. Moses"},{"name":"M. Wainwright"},{"name":"A. Willsky"}],"title":{"text":"Distributed fusion in sensor networks: A graphical models perspective"}},{"authors":[{"name":"O. P. Kreidl"},{"name":"A. Willsky"}],"title":{"text":"Inference with minimum communication: a decision-theoretic variational approach"}},{"authors":[{"name":"R. Rajagopal"},{"name":"X. Nguyen"},{"name":"S. Ergen"},{"name":"P. Varaiya"}],"title":{"text":"Simultaneous sequential detection of multiple interacting faults"}},{"authors":[{"name":"A. G. Tartakovsky"},{"name":"M. Pollak"}],"title":{"text":"Nearly minimax changepoint detec- tion procedures"}},{"authors":[{"name":"A. A. Amini"},{"name":"X. Nguyen"}],"title":{"text":"Sequential detection of multiple change points in networks: A graphical model based approach"}},{"authors":[{"name":"A. G. Tartakovsky"},{"name":"V. V. Veeravalli"}],"title":{"text":"General asymptotic bayesian theory of quickest change detection"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566917.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T7.4","endtime":"11:10","authors":"XuanLong Nguyen, Arash Amini, Ram Rajagopal","date":"1341485400000","papertitle":"Message-passing sequential detection of multiple change points in networks","starttime":"10:50","session":"S11.T7: Message Passing Algorithms","room":"Stratton (407)","paperid":"1569566917"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
