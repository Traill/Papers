{"id":"1569561123","paper":{"title":{"text":"Decoding of Cyclic Codes over Symbol-Pair Read Channels"},"authors":[{"name":"Eitan Yaakobi ∗\u2020"},{"name":"Jehoshua Bruck ∗"},{"name":"Paul H. Siegel \u2020"}],"abstr":{"text":"Abstract\u2014Symbol-pair read channels, in which the outputs of the read process are pairs of consecutive symbols, were recently studied by Cassuto and Blaum. This new paradigm is motivated by the limitations of the reading process in high density data stor- age systems. They studied error correction in this new paradigm, speciﬁcally, the relationship between the minimum Hamming dis- tance of an error correcting code and the minimum pair distance, which is the minimum Hamming distance between symbol-pair vectors derived from codewords of the code. It was proved that for a linear cyclic code with minimum Hamming distance d H , the corresponding minimum pair distance is at least d H + 3.\nOur main contribution is proving that, for a given linear cyclic code with a minimum Hamming distance d H , the minimum pair distance is at least d H + d H 2 . We also describe decoding al- gorithms, based upon bounded distance decoders for the cyclic code, whose pair-symbol error correcting capabilities reﬂects the larger minimum pair distance. In addition, we consider the case where a read channel output is a prescribed number, b > 2, of consecutive symbols and provide some generalizations of our results. We note that the symbol-pair read channel problem is a special case of the sequence reconstruction problem that was introduced by Levenshtein."},"body":{"text":"The traditional approach in information theory to analyze noisy channels is to parse the message into independent infor- mation units, called symbols. Even though in many works the error correlation and interference between the symbols is stud- ied, the process of writing and reading each symbol is usually assumed to be performed independently. However, in some of today\u2019s storage technologies as well as future ones, this is no longer an accurate assumption and symbols can only be writ- ten and read together. This brings us to study a model, recently proposed by Cassuto and Blaum [1], for channels whose out- puts are overlapping pairs of symbols.\nThe rapid progress in high density data storage technologies paved the way for high capacity storage with reduced price. However, since the bit size at high densities is so small, one of the fundamental problems is to successfully read the indi- vidual bits recorded on the storage medium; for more details, see [1]. The channel model studied by Cassuto and Blaum [1], and later by Cassuto and Litsyn [2], mimics the reading pro- cess of such storage technologies. On each reading operation, the value of two consecutive symbols is read, called a pair- read symbol . This new model changes the requirement on the error correction capability of error-correction codes. There is already a signiﬁcant amount of redundancy as every symbol is read twice. Furthermore, the errors are no longer symbol errors, but, rather, pair-symbol errors, where in a pair-symbol error at least one of the symbols is erroneous. The main task now becomes to combat these pair-symbol errors by designing codes with large minimum pair distance.\nThe works in [1], [2] studied the case of pair-read sym- bols. However, this model can be easily generalized such that on every read operation, multiple, say b > 2, consecutive symbols are read and thus every symbol is read b times. In essence, we receive multiple estimations of the same stored word. This connection brings us to the sequence reconstruc- tion problem, which was introduced by Levenshtein [5]\u2013[7]. In this model, the same codeword is transmitted over multiple channels. Then, a decoder receives all channel outputs, which are guaranteed to be different from each other, and outputs an estimation of the transmitted word. The original motivation did not come from storage systems but rather from other ﬁelds, such as molecular biology and chemistry, where the amount of redundancy in the information is too low and thus the only way to combat errors is by repeatedly transmitting the same message. This model is very relevant for storage technologies we described above or any other storage where the stored in- formation is read multiple times. Furthermore, we note that the model by Levenshtein was recently studied and extended in the context of associative memories [11].\nIn the channel model described by Levenshtein, all channels are (almost) independent from each other as it is only guaran- teed that the channel outputs are all different. If the transmitted message c belongs to a code with Hamming distance d H and the number of errors in every channel can be strictly greater than d H −1 2 , then Levenshtein studied the minimum number of channels that are necessary to construct a successful de- coder. This value was studied in [6] for the Hamming metric as well as other distance metrics and was later analyzed for a distance metric over permutations, e.g. [3], [4], and error graphs [8].\nFor the Hamming distance, the following result was proved in [6]. Assume the transmitted word belongs to a code with minimum Hamming distance d H and the number of errors, t, in every channel is greater than d H −1 2 . Then, in order to construct a successful decoder, the number of channels has to be greater than\nFor example, if t = d H −1 2 + 1, i.e., only one more than the error correction capability, then the number of channels has to be at least ( 2t t ) + 1. If t > d H −1 2 + 1 then this number is even a function of the message length. This disappointing re- sult is a consequence of the arbitrary errors that may occur in every channel. In practice, especially for storage systems, we can take advantage of the fact that the errors are constrained.\nIn the symbol-pair read channel, there are in fact two chan- nels. If the stored information is x = ( x 0 , . . . , x n−1 ) , then the pair-read vector of x is\nmain contribution in this work is proving that d p d H + d H 2 .\nAccording to [1], this permits correction of 3d H / 4 − 1 symbol-pair errors. Thus, in contrast to Levenshtein\u2019s results on independent channels, on the symbol-pair read channel we can correct a large number of symbol-pair errors. In or- der to establish this result, we explicitly construct a decoder that can correct this number of symbol-pair errors.\nThe rest of the paper is organized as follows. In Section II, we review the symbol-pair read channel and some basic prop- erties. In Section III, we show that cyclic codes can correct a large number of symbol-pair errors and in Section IV, de- coders for such codes are given. Section V generalizes some of the results on the symbol-pair read channel to channels that sense b consecutive symbols on each read, where b > 2. Fi- nally, Section VI concludes the paper.\nIn this section, we brieﬂy review the model and deﬁnition of the symbol-pair read channel. If a length- n vector is stored in the memory then its pair-read vector is also a length- n vector, while every entry consists of two consecutive symbols of the stored vector. More formally, if x = ( x 0 , . . . , x n−1 ) ∈ Σ n is a length- n vector over some alphabet Σ, then the symbol-pair read vector of x, denoted by π ( x ) , is deﬁned to be\nWe will focus in this work on binary vectors, so Σ = { 0, 1 } . Note that π ( x ) ∈ ( Σ × Σ ) n , and for x, y ∈ Σ, π ( x + y ) = π ( x ) + π ( y ) . Unless stated otherwise, in this paper, all in- dices are taken modulo n. The Hamming distance between two vectors x and y is denoted by d H ( x, y ) . Similarly, the Hamming weight of a vector x is denoted by w H ( x ) . The pair distance between x and y is denoted by d p ( x, y ) and is deﬁned to be\nAccordingly, the pair weight of x is w p ( x ) = w H ( π ( x )) . A symbol-pair error in the i-th symbol of π ( x ) changes at least one of the two bits ( x i , x i+1 ) . Note that the following connection between the pair distance and pair weight holds.\nA ﬁrst observation on the connection between the Hamming distance and pair distance was proved in [1].\nProposition 2. For x, y ∈ Σ n , let 0 < d H ( x, y ) < n be the Hamming distance between x and y. Then,\nFor a code C , we denote its minimum Hamming distance by d H (C) . The symbol-pair code of C is the code\nThen, similarly, the minimum pair distance of C , d p (C) , is the minimum Hamming distance of π (C) , i.e.,\nFrom Proposition 2, the following connection between d H (C) and d p (C) is established [1]\nThe goal in constructing codes for the pair-read channel is to achieve high minimum pair distance with respect to the min- imum Hamming distance. It was shown in [1] that interleaving two codes with minimum Hamming distance d H generates a code with the same minimum Hamming distance d H but with minimum pair distance is 2d H . Even though this construction generates codes with the largest possible minimum pair dis- tance with respect to their minimum Hamming distance, it is less attractive as, in general, the interleaving method suffers from a poor Hamming distance relative to its resulting code- word length.\nYet another interesting family of codes that was analyzed in [1] are the linear cyclic codes. It was proved that for a lin- ear cyclic code C with minimum Hamming distance d H , its minimum pair distance is at least d H + 2. Using the Hartmann- Tzeng bound, this lower bound was improved to d H + 3, when the code length is a prime number. Our main goal in the next section is to show an improved lower bound on the minimum pair distance of linear cyclic codes.\nThe goal of this section is to show that linear cyclic codes have high minimum pair distance. In order to do so, we ﬁrst give a method to calculate the pair weight of x. In fact, a similar property was proved in [1] (Theorem 2) but using a different notation.\nThe key idea to notice is that if x i = 1 then there are two non-zero symbols in π ( x ) , the i-th and ( i − 1 ) -st symbol. However if x i−1 = 1, then the ( i − 1 ) -st symbol is already non-zero as a result of x i−1 = 1. Hence, if ( x i−1 , x i ) = ( 0, 1 ) we have two non-zero symbols in π ( x ) as a result of x i and if ( x i−1 , x i ) = ( 1, 1 ) we have only a single non-zero sym- bol in π ( x ) . Therefore, in order to determine the weight of π ( x ) , one needs to count the number of ( 0, 1 ) sequences in the vector x, which we now show how to calculate.\nLemma 3. For any x ∈ Σ n , w p ( x ) = w H ( x ) + w H ( x )/ 2. Proof: Let\nFor all 0 i n − 1, i ∈ S 1 if and only if x i+1 = 1 and x i = 0. Thus, x i + x i+1 = 1 or x i = 1, where x i = 0. Hence, we get\n| S 1 | = |{ i : x i+1 = 1 and x i = 0 }| . Note that for any x ∈ Σ n ,\n|{ i : x i+1 = 1 and x i = 0 }| = |{ i : x i+1 = 0 and x i = 1 }| , and the sum of the cardinality of the two sets is w H ( x ) . Hence, | S 1 | = w H ( x ) 2 and\nUsing the property we proved in Lemma 3, we are now ready to show an improved lower bound on the minimum pair distance of linear cyclic codes.\nTheorem 4. Let C be a linear and cyclic code of dimension greater than one. Then,\nProof: Let x = ( x 0 , . . . , x n−1 ) be a codeword in C . As- sume that x is not the all-ones vector. Since the code is cyclic then ( x 1 , . . . , x n−1 , x 0 ) ∈ C and thus x ∈ C . The weight of x is even and hence w H ( x ) 2 d H (C)/ 2 . Furthermore, w H ( x ) d H (C) . Together, these facts imply that\nWe conclude by noting that if x = 1, the all-ones vector, then the inequality above is easily veriﬁed. This completes the proof.\nTheorem 4 shows that linear cyclic codes are attractive for symbol-pair read channels as their minimum pair distance is large, allowing the correction of a large number of symbol-pair errors. An interesting problem which thus arises is to construct efﬁcient decoders for these codes.\nAfter ﬁnding codes with large minimum pair distance we now show an efﬁcient decoder for such codes. Given a linear cyclic code C , with minimum distance d H (C) = 2t + 1, we assume it has a decoder D C that can correct up to t errors. We will show how to use this decoder in order to construct a decoder for the code π (C) which corrects up to t 0 = 3t+1 2\nWe deﬁne the decoder D C as a map D C : Σ n → C ∪ { F } and the notation D C ( y ) = c indicates that the decoder\u2019s input is a received word y and its output is a decoded codeword c or the decoder failure symbol F. If c ∈ C is the transmitted word and d H ( c, y ) t, then it is guaranteed that c = c. However, if d H ( c, y ) > t, then either c = F, indicating that more than t errors have occurred, or c is a codeword different from c , whose Hamming distance from the received word y is at most t, i.e., d H ( c, y ) t.\nLet us introduce another code that will serve us in this decoder construction. The double-repetition code of C is the code \t C\nNote that its length is 2n and its minimum Hamming distance satisﬁes d H (C 2 ) = 2d H (C) . The code C 2 can correct up to 2t errors and we assume that it has a decoder D C 2 : Σ n × Σ n →\nΣ n ∪ { F } having the same properties as the decoder D C . Ev- ery codeword in C 2 consists of two identical codewords from C and thus, for simplicity of notation, we assume that the de- coder D C 2 returns only one copy of the decoded codeword from C . We will address at the end of the section the problem of constructing the decoder D C 2 .\nLet c ∈ C and let π ( c ) ∈ π (C) be its symbol-pair vector. Let y = π ( c ) + e be a received word, where e ∈ ( Σ × Σ ) n is the error vector and w H ( e ) \t 3t+1 2 = t 0 . We will show a decoder D π : ( Σ × Σ ) n → { 0, 1 } n which receives the word y and returns c.\ny = ( y 0,0 , y 0,1 ) , ( y 1,0 , y 1,1 ) , . . . , ( y n−1,0 , y n−1,1 ) and deﬁne the following three vectors\ny L = ( y 0,0 , . . . , y n−1,0 ) , y R = ( y 0,1 , . . . , y n−1,1 ) ,\nSince the vector y suffers at most t 0 pair-symbol errors, the vectors y L and y R each have at most t 0 errors as well. Note that the vector y S has at most t 0 errors with respect to the codeword c = ( c 0 + c 1 , . . . , c n−1 + c 0 ) . In general, the knowledge of the codeword c does not uniquely determine the value of c. However, in this scenario it does. This ob- servation, which we will use of in the decoder algorithm, is proved in the following lemma.\nLemma 5. If the codeword c ∈ C is successfully decoded then we can recover the codeword c.\nProof: The codeword c satisﬁes c i = c 0 + ∑ i−1 j=0 c j . Hence if we deﬁne c = [ c 0 , . . . , c n−1 ] by c i = ∑ i−1 j=0 c j then the code- word c is either c or c + 1, depending on the value of c 0 . The distance between y L and c is at most t 0 and d H ( c, c + 1 ) = n. Hence, if d H ( y L , c ) < d H ( y L , c + 1 ) then c = c and other- wise c = c + 1. In any case, we can recover the codeword c.\nAccording to Lemma 5, it is possible to recover the code- word c from the codeword c . By abuse of notation, we denote by c ∗ an operator that calculates, as explained in Lemma 5, the codeword c from c , and so c ∗ = c.\nThe number of symbol-pair errors in the vector y is at most t 0 . Each symbol-pair error corresponds to one or two bit error in the symbol-pair. We let E 1 be the number of single-bit pair- symbol errors and E 2 be the number of double-bit pair-symbol errors, where E 1 + E 2 t 0 . Thus, the number of errors in y S is E 1 and the number of errors in ( y L , y R ) is E 1 + 2E 2 . An- other property which we will use in the decoder construction is proved in the next lemma.\nProof: If E 1 t then the decoder D C ( y S ) is successful. Otherwise, E 1 t + 1 and E 2 t 0 − ( t + 1 ) , so the number of errors in ( y L , y R ) satisﬁes\nAccording to the last lemma we know that at least one of the two decoders succeeds. However, we cannot determine easily which one of them does and the main task of the de- coder construction for π (C) is to ﬁnd the successful decoder. The decoder\u2019s output D π ( y ) = c is calculated as follows:\nStep 2. c 2 = D C 2 ( y L , y R ) , e 2 = d H (( c 2 , c 2 ) , ( y L , y R )) . Step 3. If c 1 = F or w H ( c 1 ) is odd then c = c 2 . Step 4. If e 1 \t t+2 2 , then c = c ∗ 1 .\nStep 5. If e 1 > t+2 2 , let e 1 = t+2 2 + a, (1 a \t t 2 − 1) a) If e 2 t 0 + a then c = c 2 , b) Otherwise, c = c ∗ 1 .\nThe correctness of the decoder is proved in the next theorem. Theorem 7. The decoder output satisﬁes D π ( y ) = c = c.\nProof: According to Lemma 6, at least one of the two de- coders in Steps 1 and 2 succeeds. Steps 3\u20135 help to determine which of the two decoders succeeds.\nStep 3: Since y S is a noisy version of the codeword c , in the decoding operation on the ﬁrst step we try to decode the codeword c . Remember that the weight of c is even. Hence, if c 1 = F or the Hamming weight of c 1 is odd, then this de- coding operation fails and thus the decoder in Step 2 succeeds. If we reach Steps 4 and 5 then w H ( c 1 ) is even.\nwhich is a contradiction as the number of errors in y S is at most t 0 . Therefore, in this case the decoding operation c 1 = D C ( y S ) = c succeeds, and according to Lemma 5, we get\nStep 5: We are left with the case where e 1 > t+2 2 . Since e 1 t, let e 1 = t+2 2 + a, where 1 a \t t 2 − 1.\nAssume the decoding in Step 2 fails. According to Lemma 6, the decoding operation c 1 = D C ( y S ) succeeds,\nSince the decoder D C 2 (( y L , y R )) fails and the minimum dis- tance of C 2 is 4t + 2, we get that the weight of the error vector in Step 2, e 2 , would have to satisfy\ne 2 4t + 2 − ( E 1 + 2E 2 ) 4t + 2 − 5t + 1 2 \t − a 3t + 1\nHence, we conclude that if e 2 t 0 + a then necessarily the decoder in Step 2 succeeds.\nAssume the decoding in Step 1 fails. As in Step 4, since D C ( y S ) fails, the number of errors E 1 in y S is at least\nSince E 1 + E 2 t 0 , the value of E 2 satisﬁes 0 E 2 a − 1, and E 1 + 2E 2 , the total number of errors in ( y L , y R ) , satisﬁes\nThus, the decoding operation D C 2 (( y L , y R )) succeeds, and t 0 − ( a − 1 ) e 2 t 0 + a − 1.\nHence, if e 2 > t 0 + a then the decoder in Step 1 succeeds. That explains a) and b) of Step 5.\nTo complete this section, let us go back to the construction of the decoder D C 2 . This decoder receives two vectors y 1 = ( y 1,0 , . . . , y 1,n−1 ) , y 2 = ( y 2,0 , . . . , y 2,n−1 ) . Each is a noisy version of some codeword c ∈ C , and the goal is to correct a total of 2t errors in the two vectors. We deﬁne the vector y = ( y 0 , . . . , y n−1 ) such that for all 0 i i − 1, y i = y 1,i if y 1,i = y 2,i , and otherwise y i = ? to indicate an erasure. If the number of errors in y is τ and the number of erasures is ρ, then we have 2τ + ρ 2t = d (C) − 1, which is within the error and erasure correcting capability of C . We are left only with the problem of deﬁning a decoder that corrects errors and erasures for cyclic codes. For that, we refer to [9], [10].\nwe extend some of our results on the symbol-pair read chan- nel to the case where more than two symbols are sensed on each read\nIn this section, we extend some of our results on the symbol- pair read channel to the case where more than two symbols are sensed on each read. For b 3, the b-symbol read vector of x = ( x 0 , x 1 , . . . , x n−1 ) ∈ Σ n is deﬁned to be\nπ b ( x ) = [( x 0 , . . . , x b−1 ) , . . . , ( x n−1 , x 0 , . . . , x b−2 )] ∈ Σ b n . The b-distance between x and y is denoted by d b ( x, y ) and is deﬁned to be d b ( x, y ) = d H ( π b ( x ) , π b ( y )) . In analogy to Proposition 2, it is possible to show that\nOur main goal here is to generalize Lemma 3 for arbitrary b 3. For a vector x, let us deﬁne the vector x by inverting every sequence of less than b − 1 zeros in x. More formally, if ( x i , x i+1 , . . . , x i+k , x i+k+1 ) = ( 1, 0, . . . , 0, 1 ) for some 0\ni n − 1 and k b − 2, then x j = 1 for i + 1 j i + k. For all other values of j, x j = x j .\nProof: Let us ﬁrst show that w b ( x ) = w b ( x ) . Consider a sequence of k b − 2 consecutive zeros,\nNext, we ﬁnd the value of w b ( x ) , making use of the fact that any run of zeros in x is of length at least b − 1. Let\nLet us show that for all 2 \t b − 1, | S 1 | = | S | . If i ∈ S 1 then ( x i , x i+1 ) = ( 0, 1 ) . Since there is no sequence of less than b − 2 consecutive zeros\nWe would next like to construct codes with large b-distance. As in the case of symbol-pair codes, we deﬁne the b-symbol read code of C as the code π b (C) = { π b ( c ) : c ∈ C} , and the minimum b-distance of C , d b (C) , as d b (C) = d H ( π b (C)) .\nThe interleaving scheme given in [1] constructs codes C that satisfy d p (C) = 2d H (C) . This construction can be ex- tended for arbitrary b. It can be shown that the interleaving of b codes, all with minimum distance d H , generates a code C with minimum Hamming distance d H (C) = d H and minimum b-distance d b (C) = b · d H (C) .\nA decoder for the interleaving construction works very sim- ilarly to the one given for the interleaving scheme for pair- symbols in [1]. The majority decoder is a decoder which out- puts for every bit its majority value among its b received val- ues, or ? in case of equality between the number of zeros and ones. Then, it is possible to decode every interleaved code- word independently.\nThe following two lemmas show properties of some special codes. In the ﬁrst lemma, the code is Σ n , so its minimum dis- tance is one. The second lemma analyzes the Hamming code. Lemma 9. If C = Σ n , then for b 3 the minimum b-distance satisﬁes d b (C) = b and it is possible to correct b−1 2 symbol errors by the majority decoder.\nProof: Assume x is a non-zero word which is not the all-ones vector. Then, we have x = 0 and thus w H ( x ) 1 and w H ( x ) 2. According to Lemma 8, we get w b ( x )\n1 + ( b − 1 ) · 2 2 = b. In case x = 1, then w b ( x ) = n and the inequality above holds as well.\nIf there are b−1 2 symbol errors, then every bit of the vec- tor x is in error in π b ( x ) at most b−1 2 times. Thus, the majority decoder succeeds.\nLemma 10. If C is the cyclic Hamming code of length n = 2 m − 1 then d b (C) = 2b + 1, where b + 2 m.\nProof: Let x ∈ C be a non-zero codeword and assume that x = 1. Hence, w H ( x ) 3. If w H ( x ) 4, then accord- ing to Lemma 8, we get w b ( x ) 3 + ( b − 1 ) · 4 2 = 2b + 1.\nNow assume that w H ( x ) = 2, so x has a single continuous run of ones, and assume it has length . We notice that \t m. Otherwise, the non-zero entries of the codeword x are conﬁned to at most m − 1 locations. If g ( x ) is a generator polynomial of degree m for the cyclic Hamming code, then there exists a non-zero polynomial of degree at most m − 1 which is a mul- tiple of g ( x ) . Thus, we get a contradiction. Therefore, in this case, we get w b ( x ) m + ( b − 1 ) · 2 2 = m + b − 1 2b + 1. To conclude this part of the proof, note that if x = 1, then w b ( x ) = n 2b + 1.\nTo show that the minimum distance is 2b + 1, we see that if we take a codeword of weight three with two consecutive ones, then its b-weight is exactly 2b + 1.\nIn this paper, we studied the symbol-pair read channel. Af- ter reviewing the channel model and basic properties, we then showed that linear cyclic codes are very effective in correcting symbol-pair errors. The main part of the paper was devoted to the construction of an effective decoding algorithm for such codes. Finally, we extended the model and some of the results to the b-symbol read channel, where b > 2.\nThe authors thank to Yuval Cassuto for helpful dis- cussions on the symbol-pair read channels. This research was supported in part by the ISEF Foundation, the Lester Deutsch Fellowship, the University of California Lab Fees Research Program, Award No. 09-LR-06-118620-SIEP, the National Science Foundation under Grant CCF-1116739, the Center for Magnetic Recording Research at UCSD, and the NSF Expeditions in Computing Program under grant CCF-0832824."},"refs":[{"authors":[{"name":"Y. Cassuto"},{"name":"M. Blaum"}],"title":{"text":"Codes for symbol-pair read channels"}},{"authors":[{"name":"Y. Cassuto"},{"name":"S. Litsyn"}],"title":{"text":"Symbol-pair codes: algebraic constructions and asymptotic bounds"}},{"authors":[{"name":"E. Konstantinova"}],"title":{"text":"Reconstruction of signed permutations from their dis- torted patterns"}},{"authors":[{"name":"E. Konstantinova"},{"name":"I. Levenshtein"},{"name":"J. Siemons"}],"title":{"text":"Reconstruc- tion of permutations distorted by single transposition errors"}},{"authors":[{"name":"I. Levenshtein"}],"title":{"text":"Reconstructing objects from a minimal number of dis- torted patterns"}},{"authors":[{"name":"I. Levenshtein"}],"title":{"text":"Efﬁcient reconstruction of sequences"}},{"authors":[{"name":"I. Levenshtein"}],"title":{"text":"Efﬁcient reconstruction of sequences from their sub- sequences or supersequences"}},{"authors":[{"name":"I. Levenshtein"},{"name":"J. Siemons"}],"title":{"text":"Error graphs and the reconstruction of elements in groups"}},{"authors":[{"name":"E. Orsini"},{"name":"M. Sala"}],"title":{"text":"Correcting errors and erasures via the syndrome variety"}},{"authors":[{"name":"H. Shahri"},{"name":"K. Tzeng"}],"title":{"text":"On error-and-erasure decoding of cyclic codes"}},{"authors":[{"name":"E. Yaakobi"},{"name":"J. Bruck"}],"title":{"text":"On associative memories and the sequences reconstructions problem"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569561123.pdf"},"links":[{"id":"1569565711","weight":6},{"id":"1569566871","weight":6},{"id":"1569565317","weight":13},{"id":"1569566739","weight":6},{"id":"1569565609","weight":6},{"id":"1569564249","weight":13},{"id":"1569565809","weight":6},{"id":"1569566787","weight":6},{"id":"1569566015","weight":6},{"id":"1569566895","weight":6},{"id":"1569566369","weight":6},{"id":"1569566423","weight":20},{"id":"1569558901","weight":13},{"id":"1569564857","weight":13},{"id":"1569565817","weight":13},{"id":"1569565847","weight":20},{"id":"1569565393","weight":6},{"id":"1569565765","weight":6},{"id":"1569565925","weight":20},{"id":"1569565263","weight":13},{"id":"1569565385","weight":6},{"id":"1569564919","weight":60},{"id":"1569564595","weight":20},{"id":"1569564861","weight":13},{"id":"1569566075","weight":6},{"id":"1569567483","weight":6},{"id":"1569561713","weight":6},{"id":"1569565373","weight":6}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T6.2","endtime":"12:10","authors":"Eitan Yaakobi, Jehoshua Bruck, Paul H. Siegel","date":"1341575400000","papertitle":"Decoding of Cyclic Codes over Symbol-Pair Read Channels","starttime":"11:50","session":"S16.T6: Cyclic Codes and Symbol-Pair Codes","room":"Kresge Rehearsal A (033)","paperid":"1569561123"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
