{"id":"1569566773","paper":{"title":{"text":"Hybrid Generalized Approximate Message Passing with Applications to Structured Sparsity"},"authors":[{"name":"Sundeep Rangan"},{"name":"Alyson K. Fletcher"},{"name":"Vivek K Goyal"},{"name":"Philip Schniter"}],"abstr":{"text":"Abstract\u2014Gaussian and quadratic approximations of message passing algorithms on graphs have attracted considerable atten- tion due to their computational simplicity, analytic tractability, and wide applicability in optimization and statistical inference problems. This paper summarizes a systematic framework for incorporating such approximate message passing (AMP) methods in general graphical models. The key concept is a partition of dependencies of a general graphical model into strong and weak edges, with each weak edge representing a small, linearizable coupling of variables. AMP approximations based on the central limit theorem can be applied to the weak edges and integrated with standard message passing updates on the strong edges. The resulting algorithm, which we call hybrid generalized approx- imate message passing (Hybrid-GAMP), can yield signiﬁcantly simpler implementations of sum-product and max-sum loopy belief propagation. By varying the partition between strong and weak edges, a performance\u2013complexity trade-off can be achieved. Structured sparsity problems are studied as an example of this general methodology where there is a natural partition of edges."},"body":{"text":"Message passing algorithms on graphical models have be- come widely-used in high-dimensional optimization and infer- ence problems in a range of ﬁelds [1], [2]. The fundamental principle of graphical models is to factor high-dimensional problems into sets of problems of lower dimension. The factorization is represented via a graph where the problem variables and factors are represented by the graph vertices, and the dependencies between them represented by edges. Message passing methods such as loopy belief propagation (BP) use this graphical structure to perform approximate inference or optimization in an iterative manner. In each iteration, inference or optimization is performed \u201clocally\u201d on the sub-problems associated with each factor, and \u201cmessages\u201d are passed between the variables and factors to account for the coupling between the local problems.\nAlthough effective in a range of problems, loopy BP is only as simple as the problems in the constituent factors; if the fac- tors themselves are of high dimensions, exact implementation of loopy BP will be computationally intractable.\nTo reduce the complexity of loopy BP, this paper presents a hybrid generalized approximate message passing (Hybrid- GAMP) algorithm for what we call graphical models with\nlinear mixing . The basic idea is that when factors depend on large numbers of variables, the dependencies are often through aggregates of small, linearizable contributions. In the proposed framework, these weak, linear interactions are identiﬁed by partitioning the graph edges into weak and strong edges, with the dependencies on the weak edges being described by a linear transform. Under the assumption that the components of the linear transform are small, it is argued that the computations for the messages of standard loopy BP along the weak edges can be signiﬁcantly simpliﬁed. Approximate messages along the weak edges are integrated with standard messages on the strong edges.\nThe Hybrid-GAMP methodology can be applied to any variant of loopy BP, including the sum-product algorithm for inference (e.g., computation of a posterior mean) and the max-sum algorithm for optimization (e.g., computation of a posterior mode). For the sum-product loopy BP algorithm, we show that the messages along the weak edges can be approxi- mated as Gaussian random variables and the computations for these messages can be simpliﬁed via the central limit theorem. For max-sum loopy BP, we argue that one can use quadratic approximations of the messages and perform the computations via a simple least-squares solution.\nThese approximations can dramatically simplify the com- putations. The complexity of standard loopy BP generically grows exponentially with the maximum degree of the factor nodes. With the GAMP approximation, however, the com- plexity is exponential only in the maximum degree from the strong edges, while it is linear in the number of weak edges. As a result, Hybrid-GAMP algorithms on a graphical model with linear mixing can remain tractable even with very large numbers of weak, linearizable interactions.\nGaussian and quadratic approximations for message passing algorithms with linear dependencies are not new. The purpose of this paper is to provide a systematic and general framework for these approximations that incorporates and extends many earlier algorithms. Many previous works have considered Gaussian approximations of loopy BP for the problem of estimating vectors with independent components observed through noisy, linear measurements [3]\u2013[9]. In the terminology of this paper, these algorithms apply to graphs where all the non-trivial edges are weak. By enabling graphs that have mixes of both strong and weak edges, the framework of this paper signiﬁcantly generalizes these methods. For example, instead of the unknown vector simply having independent components, the presence of strong edges can enable the vector to have any prior describable with a graphical model.\nThe approach here of combining approximate message passing methods and standard graphical models with linear mixing is closest to the methods developed in [10]\u2013[13] for wavelet image denoising and turbo equalization. These works also considered graphical models that had both linear and nonlinear components, and applied approximate message passing techniques along the lines of [7], [8] to the lineariz- able portions while maintaining standard BP updates in the remainder of the graph. The use of approximate message passing methods on portions of a factor graph has also been applied with joint parameter estimation and decoding for CDMA multiuser detection in [14]; in a wireless interference coordination problem in [15], and proposed in [16, Section 7] in the context of compressed sensing. The framework pre- sented here uniﬁes and extends all of these examples and thus provides a systematic procedure for incorporating Gaussian approximations of message passing in a modular manner in general graphical models.\nThe remainder of this paper develops only the sum-product case; the reader is referred to [17] for parallel development of the max-sum case as well as proofs omitted for brevity and more examples and details.\nx = (x ∗ 1 , . . . , x ∗ n ) ∗ , \t z = (z ∗ 1 , . . . , z ∗ m ) ∗ , \t (1) and consider a function of these vectors of the form\nwhere, for each i, f i (·) is a real-valued function; α(i) is a sub- set of the indices {1, . . . , n}; and x α (i) is the concatenation of the vectors {x j , j ∈ α(i)}. We are interested in computations subject to linear constraints of the form\nz i = n j =1 A ij x j = A i x , \t (3) where each A ij is a real-valued matrix and A i is the block column matrix with components A ij . We will also let A be the block matrix with components A ij so that we can write the linear constraints as z = Ax.\nThe function F (x, z) is naturally described via a graphical model as shown in Fig. 1. Speciﬁcally, we associate with F (x, z) a bipartite factor graph G = (V, E) whose vertices V consist of n variable nodes corresponding to the (vector- valued) variables x j , and m factor nodes corresponding to the factors f i (·) in (2). There is an edge (i, j) ∈ E in the graph if and only if the variable x j has some inﬂuence on the factor f i (x α (i) , z i ). This inﬂuence can occur in one of two mutually exclusive ways:\n\u2022 The index j is in α(i), so that the variable x j directly appears in the sub-vector x α (i) in the factor f i (x α (i) , z i ). In this case, (i, j) will be called a strong edge, since x j can have an arbitrary and potentially-large inﬂuence on the factor.\n\u2022 The matrix A ij is nonzero, so x j affects f i (x α (i) , z i ) through its linear inﬂuence on z i in (3). In this case, (i, j)\nwill be called a weak edge, since the approximations we will make in the algorithms below assume that A ij is small. The set of weak edges into the factor node i will be denoted β(i).\nTogether α(i) and β(i) comprise the set of all indices j such that the variable node x j is connected to the factor node f i (·) in the graph G. The union ∂(i) = α(i) ∪ β(i) is thus the neighbor set of f i (·). Similarly, for any variable node x j , we let α(j) be the set of indices i such that that the factor node f i (·) is connected to x j via a strong edge, and let β(j) be the set of indices i such that there is a weak edge. We let ∂(j) = α(j) ∪ β(j) be the union of these sets, which is the neighbor set of x j .\nGiven these deﬁnitions, we are interested in the Expectation problem P-EXP : Given a function F (x, z) of the form (2), a matrix A, and scale factor u > 0, deﬁne the joint distribution\nwhere Z(u) is a normalization constant called the partition function (it is a function of u). For this distribution, compute the expectations\nP-EXP arises naturally in statistical inference: Suppose we are given a probability distribution p(x) of the form (4) for some function F (x, z). The function F (x, z) may depend implicitly on some observed vector y, so that p(x) represents the posterior distribution of x given y. In this context, the solution (x, z) to the problem P-EXP is precisely the minimum mean squared error (MMSE) estimate when u = 1. The function ∆ j (x j ) is the log marginal distribution of x j .\nIn the analysis below, we will assume that, for all factor nodes f i (·), the strong and weak neighbors, α(i) and β(i), are disjoint. This assumption introduces no loss of generality: If an edge (i, j) is both weak and strong, we can modify the function f i (x α (i), z i ) to move the inﬂuence of x j from the term z i into the direct term x α (i) [17].\nEven when the dependence of a factor f i (x α (i) , z i ) on a variable x j is only through the linear term z i , we may still wish to move the dependence to a strong edge to improve accuracy at the expense of greater computation.\nSince A ij = 0 only when j ∈ β(i), we may sometimes write the summation (3) as\nwhere x β (i) is the sub-vector of x with components j ∈ β(i) and A i,β (i) is the corresponding portion of the ith block-row of A.\nThe sum-product loopy BP algorithm is based on iteratively passing estimates of the log marginals ∆ j (x j ) in (6). We index the iterations by t = 0, 1, . . ., and denote the estimate \u201cmessage\u201d from the factor node f i to the variable node x j in the tth iteration by ∆ i →j (t, x j ) and the reverse message by ∆ i ←j (t, x j ). The messages in loopy BP are equivalent up to a constant factor. That is, adding any constant term that does not depend on x j to either the message ∆ i →j (t, x j ) or ∆ i ←j (t, x j ) has no effect on the algorithm. We will thus use the notation\nfor some constant C that does not depend on x. Similarly, we write p(x) ∝ q(x) when p(x) = Cq(x) for some constant C. We ﬁx the scale factor u > 0 and, for any function ∆(·), we will write E[g(x) ; ∆(·)] to denote the expectation of g(x) with respect to a distribution speciﬁed indirectly by ∆(·):\nFor each edge (i, j) ∈ E, the factor node update is a computation of ∆ i →j (t, x j ) by integration over variables x r with r ∈ ∂(i) and r = j. The variable node update is a computation of ∆ i ←j (t + 1, x j ) by combining inﬂuences of ∆ ℓ →j (t, x j ) for ℓ ∈ ∂(j) \\ i followed by computation of a point estimate x j (t+1) as a scalar expectation.\nWhen the graph G is acyclic, the sum-product loopy BP algorithm converges to the exact solution of P-EXP . For graphs with cycles, the loopy BP algorithm is, in general, only approximate; see, for example, [2], [18], [19].\nBrute force solutions to P-EXP involve an expectation over all n variables x j . Loopy BP reduces this \u201cglobal\u201d problem to a sequence of \u201clocal\u201d problems associated with each of\nthe factors f i (·). The local expectation problems may be signiﬁcantly lower in dimension than the global problem. In particular, the factor f i (x α (i) , z i ) is a function of d i = |∂(i)| variables x j , either through one of the |α(i)| strong edges or |β(i)| weak edges. For each j ∈ ∂(i), the factor node update involves an integration over d i − 1 variables. The complexity in general grows exponentially in d i . Thus, standard loopy BP is only typically tractable when the degrees d i of the factor nodes are small or the factors have some convenient form.\nThe Hybrid-GAMP algorithm reduces the cost of loopy BP by exploiting complexity-reducing approximations of the cumulative effect of the weak edges. We saw in the previous section that the loopy BP update at each factor node f i (·) has a cost that may be exponential in the degree d of the node, which consists of |α(i)| strong edges and |β(i)| weak edges. The Hybrid-GAMP algorithm with edge partitioning uses the linear mixing property to eliminate the exponential dependence on the |β(i)| weak edges, so the only exponential dependence is on the |α(i)| strong edges. Thus, the edge partitioning makes Hybrid-GAMP computationally tractable as long as the number of strong edges is small. The number of weak edges can be arbitrary. In particular, the mixing matrix A can be dense.\nThe basis of the Hybrid-GAMP approximation is to assume that the matrix A ij is small along any weak edge (i, j). Under this assumption, one can apply a Gaussian approximation of the weak edge messages and use the central limit theorem at the factor nodes. A heuristic derivation of the Hybrid-GAMP approximations is given in [17, App. B].\nWe need additional notation: The Hybrid-GAMP algorithm produces a sequence of estimates x j (t) and z i (t) for the variables x j and z i . Several other intermediate variables p i (t), s i (t) and r j (t) are also produced. Associated with each of the variables are matrices Q x j (t), Q z i (t), . . ., that represent certain covariances. When we need to take the inverse of the matrices, we will use the notation Q −x j (t) to mean (Q x j (t)) −1 . Finally, for any positive deﬁnite matrix Q and vector a, we will let\nAlgorithm 1: Hybrid-GAMP: Consider the problem P-EXP for some function F (x, z) of the form (2), matrix A, and scale factor u > 0.\n1) Initialization: Set t = 0 and select some initial values ∆ i →j (t − 1, x j ) for all strong edges (i, j) and values r j (t−1) and Q r j (t−1) for all variable node indices j.\n2) Variable node update, strong edges: For all strong edges (i, j), compute\n(t−1) . \t (9) 3) Variable node update, weak edges: For all variable nodes\nx j (t) = E (x j ; ∆ j (t, ·)) , \t (12) Q x j (t) = u var (x j ; ∆ j (t, ·)) . \t (13)\n4) Factor node update, linear step: For all factor nodes i, compute\nz i (t) = \t j ∈β(i) A ij x j (t), \t (14a) p i (t) = z i (t) − Q p i (t)s i (t−1), \t (14b)\n5) Factor node update, strong edges: For all strong edges (i, j), compute:\n(15) where the integral is over z i and all components x r with r ∈ α(i) \\ j, and p i →j (0, x j ) is the probability distribution function\n6) Factor node update, weak edges: For all factor nodes i, compute\nwhere z i is the component of the pair (x α (i) , z i ) with the joint distribution\ns i (t) = Q −p i (t) z 0 i (t) − p i (t) , \t (20a) Q s i (t) = Q −p i (t) − Q −p i (t)D −z i (t)Q −p i (t). (20b)\n7) Variable node update, linear step: For all variables nodes j compute\nIncrement t and return to step 2 for some number of iterations.\nAlthough the Hybrid-GAMP algorithm above appears more complicated than standard loopy BP, Hybrid-GAMP can be computationally dramatically cheaper. The main computa- tional difﬁculty of loopy BP is the factor update; this involves an expectation over |∂(i)| variables, where ∂(i) is the set of\nall variables connected to the factor node i. In the Hybrid- GAMP algorithm, these computations are replaced by (15), where the expectation is over the strong edge variables α(i). If the number of edges is large, the computational savings can be dramatic. The other steps of the Hybrid-GAMP algorithms are all linear least-squares operations, or componentwise nonlinear functions on the individual variables.\nFor illustration, we have only presented one form of the Hybrid-GAMP procedure. Variants with discrete distributions and other message scheduling are possible.\nHybrid-GAMP is a ﬂexible and general methodology. To enable comparisons against existing algorithms, we consider the group sparse estimation problem [20], [21]. This is a highly-structured problem in which dependencies beyond lin- ear mixing are captured by binary latent variables ξ k as shown in Fig. 2. The setting and the specialization of Algorithm 1 to this case are described in detail in [17].\nIn addition to its generality, the Hybrid-GAMP procedure is among the most computationally efﬁcient. Consider the special case when there are K non-overlapping groups of d elements each; that is, each ξ k is connected to d variables x j , and the sets are disjoint. In this case, the total vector dimension for x is n = Kd. We consider the non-overlapping case since there are many algorithms that apply to this case that we can compare against. For non-overlapping uniform groups, Table I compares the computational cost of the Hybrid-GAMP algorithm to other methods. Because of the block separability of this special case, the analysis in [22] applies. For d = 2, correlated variables can be considered the real and imaginary parts of a complex variable, enabling the methods of [23].\nOf course, a complete comparison requires that we consider the number of iterations, not just the computation per iteration.\nThis comparison requires further study beyond the scope of this paper. However, it is possible that the Hybrid-GAMP procedure will be favorable in this regard. Our simulations below show good convergence after only 10\u201320 iterations.\nFig. 3 shows the results of a simple simulation comparison of algorithms. The simulation used a vector x with n = 100 groups of size d = 4 and i.i.d. Bernoulli variables ξ k with ρ = Pr(ξ k = 1) = 0.1. The matrix was i.i.d. Gaussian and the observations were with AWGN noise at an SNR of 20 dB. The number of measurements m was varied from 50 to 200, and the plot shows the MSE for each of the methods. The Hybrid- GAMP method was run with 20 iterations. In group LASSO, at each value of m, the algorithm was simulated with several values of the regularization parameter and the plot shows the minimum MSE. In Group-OMP, the algorithm was run with the true value of the number of nonzero coefﬁcients. It can be seen that the Hybrid-GAMP method is consistently as good or better than the other methods. All code for the simulations can be found in the SourceForge open website [27].\nA general model for optimization and statistical inference based on graphical models with linear mixing was presented. The linear mixing components of the graphical model account for interactions through aggregates of large numbers of small, linearizable contributions. Gaussian and second-order approx- imations are shown to greatly simplify the implementation of loopy BP for these interactions, and the Hybrid-GAMP framework presented here enables systematic incorporation of these approximations in general graphical models. Simulations were presented for group sparsity where the Hybrid-GAMP method has equal or superior performance to existing methods. However, the generality of the method will enable Hybrid- GAMP to be applied to much more complex models where few algorithms are available. In addition to experimenting with such models, future work will focus on establishing rigorous theoretical analyses along the lines of [9], [28]."},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566773.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T7.2","endtime":"15:20","authors":"Sundeep Rangan, Alyson Fletcher, Vivek K Goyal, Philip Schniter","date":"1341327600000","papertitle":"Hybrid Generalized Approximate Message Passing with Applications to Structured Sparsity","starttime":"15:00","session":"S7.T7: Approximate Belief Propagation","room":"Stratton (407)","paperid":"1569566773"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
