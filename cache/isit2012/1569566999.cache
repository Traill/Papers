{"id":"1569566999","paper":{"title":{"text":"Achievable Rates of Gaussian Channels with Realistic Duty Cycle and Power Constraints"},"authors":[{"name":"Hui Li"},{"name":"Dongning Guo"}],"abstr":{"text":"Abstract\u2014Many wireless communication systems are subject to duty cycle constraint, that is, a radio only actively transmits signals over a fraction of the time. For example, it is desirable to have a small duty cycle in some low-power systems; a half- duplex radio cannot keep transmitting if it wishes to receive useful signals; and a cognitive radio needs to listen to the channel frequently to detect primary users. Zhang and Guo have shown that the capacity of a Gaussian channel subject to an idealized duty cycle constraint as well as average transmission power constraint is achieved by discrete independent and identically distributed (i.i.d.) on-off signaling in lieu of Gaussian signaling. This paper extends the previous results by considering a more realistic duty cycle constraint where the extra cost of transitions between transmissions and nontransmissions due to pulse shaping is accounted for. The capacity-achieving input is no longer independent over time and is hard to compute. A lower bound of the input-output mutual information as a function of the input distribution is developed, which is shown to be maximized by a ﬁrst-order Markov process, the distribution of which is also discrete and can be computed efﬁciently. Simulation results show that the Markov input is superior to i.i.d. inputs for the Gaussian channel subject to the realistic duty cycle and average power constraints."},"body":{"text":"In many wireless communication systems, radios are de- signed to be active only for a fraction of the time, which is known as the duty cycle. For example, in the ultra-wideband system in [1], short bursts of signals are transmitted to trade bandwidth for power savings. Another example, the physical half-duplex constraint also requires a radio to stop transmis- sion over a frequency band from time to time if it wishes to receive useful signals over the same band. Thus wireless relays are subject to duty cycle constraint, so do cognitive radios which have to listen to the channel frequently to avoid causing interference to primary users [2].\nThe impact of duty cycle constraint on capacity-achieving signaling has been studied in [3], where the duty cycle con- straint is regarded as a requirement on the minimum fraction of zero symbols in each codeword. A unique discrete independent and identically distributed (i.i.d.) input is shown to achieve the capacity of an additive white Gaussian noise (AWGN) channel subject to duty cycle and power constraints. The results of [3] rely on a crucial idealization that the analog waveform\nof a pulse spans exactly one symbol interval. However this assumption is unrealistic because pulse waveform required for band-limited channels is usually larger than one symbol interval. As such, during a transition between zero and nonzero symbol, the pulse waveform of the nonzero symbol leaks into the interval of the zero symbol. A realistic duty cycle constraint must include the extra transmission time due to transitions between zero and nonzero symbols. In this paper we investigate the optimal signaling for achieving the capacity subject to the realistic duty cycle and power constraints. The mathematical model of the input-constrained channels is described in Section II.\nThe capacity of a discrete-time channel subject to various input constraints is a classical problem. It is well-known that Gaussian signaling achieves the capacity of a Gaussian channel with average input power constraint only. Zamir [4] also shows that the achievable rate using a white Gaussian input never incurs a loss of more than half a bit per sample with respect to the power constrained capacity. The interesting result of Smith [5] indicates that the capacity-achieving distribution for scalar AWGN channels under both average power and peak power constraint is discrete and consists of a ﬁnite number of mass points. A list of channels with discrete capacity-achieving measures is provided in [6], including Poisson channels and Rayleigh fading channels (see also [7]\u2013[11]).\nThe main results of this paper are summarized in Section III. As long as all costs associated with the constraints can be decomposed into per-letter costs, the optimal input is i.i.d.. The challenge in this work is that the realistic duty cycle concerns consecutive symbols, so that the capacity-achieving input is no longer i.i.d. and is hard to compute. In this paper, we develop a good lower bound of the input-output mutual information as a function of the input distribution. It is proved that, under the realistic duty cycle constraint, a discrete ﬁrst-order Markov input maximizes the lower bound, the distribution of which can be efﬁciently computed.\nWe devote Section V to the numerical methods and results. In order to compute the achievable rate of the Markov input, a Monte Carlo method is introduced to numerically compute the differential entropy rate of hidden Markov processes. Numer- ical results show that the rate achieved by the Markov input is substantially higher than that achieved by any i.i.d. input and that by deterministically scheduled Gaussian signaling.\nWe consider the equivalent baseband discrete-time model. The received signal over a block of n symbols can be described by\nY i = X i + N i \t i = 1, · · · , n \t (1) where X i denotes the transmitted symbol at time i and N 1 , · · · , N n are independent standard Gaussian random vari- ables. For simplicity, we assume no inter-symbol interference is at receiver. Each symbol modulates a continuous-time pulse waveform for transmission. If the width of a pulse were exactly of one symbol interval, which is denoted by T , the duty cycle is equal to the fraction of nonzero symbols in a codeword. In practice, however, the pulse is usually wider than T , so that the support of the transmitted waveform is greater than the sum of the intervals corresponding to nonzero symbols. To be speciﬁc, suppose the width of a pulse is (1 + 2c)T , then each transition between zero and nonzero symbols incurs an additional cost up to c T in terms of actual transmission time.\nLet 1 − q denote the maximum duty cycle allowed. In this paper, we require every codeword (x 1 , x 2 , · · · , x n ) to satisfy\n1 n\nwhere 1 {·} is the indicator function, and the number of transitions from zero to nonzero symbols is equal to the number of transitions from nonzero to zero symbols. For subsequent convenience, (2) is deﬁned in a circular manner using the modular operation. As a consequence, (x n , x 1 ) is also regarded as a transition if x n = 0 and x 1 = 0. This of course has vanishing impact as n → ∞ and thus no impact on the capacity. From now on, we refer to (2) as duty cycle constraint (q, c). Note that the constraint assumed in [3] is the special case of duty cycle constraint (q, 0). If c ∈ [0, 1 2 ], then the left hand side of (2) is equal to the actual duty cycle. If c > 1 2 , the left hand side of (2) is an overestimate of the duty cycle. Nonetheless, we use the simple constraint (2) because it concerns only pairs of consecutive symbols. In addition, we consider the usual average input power constraint,\nLet µ denote the probability distribution of the process X 1 , X 2 , · · · . We use µ X i to denote the marginal distribution of X i , and µ X i ,X j to denote the joint probability distribution of (X i , X j ). Denote the set of n-dimension distribution which satisfy duty cycle constraint (q, c) and power constraint γ by Λ n (γ, q, c) =\nµ : 1 n\nwhere µ X i ,X j ({0} × (R\\{0})) = P (X i = 0, X j = 0) denotes the probability of a zero-to-nonzero transition.\nIn this paper, let X n k denote the subsequence (X k , X k+1 , · · · , X n ), where X ∞ k \t = (X k , X k+1 , · · · ). We also use shorthand X n = X n 1 . The capacity of the AWGN channel (1) with duty cycle constraint (q, c) and power constraint γ is\nThe capacity is in fact achieved by a stationary input process. This is justiﬁed in Section IV-A by showing that any nonstationary input process has a stationary counterpart with equal or greater input-output mutual information per symbol. Let us denote the set of stationary distributions which satisfy duty cycle constraint (q, c) and power constraint γ by\nwhere I(X; Y ) is the mutual information of the additive white Gaussian noise channel between the input symbol X, which follows distribution µ X 1 , and the corresponding output Y . The following properties hold:\nb) The maximum of L(·) is achieved by a discrete ﬁrst-order Markov process, denoted by µ ∗ ;\nc) Deﬁne B i = 1 {X i =0} , i = 1, 2, · · · . Then µ ∗ satisﬁes the following property: for every i, conditioned on B i and B i+1 , the variables X i and X i+1 are independent, and\nThe optimal input distribution should exhaust the power budget γ. This is because that increasing the input power by scaling the input linearly not only maintains its duty cycle, but also increases the mutual information.\nWe ﬁrst establish the fact that a stationary distribution achieves the capacity of the AWGN channel with the real- istic duty cycle constraint and power constraint. Because the AWGN channel (1) is memoryless, over n uses of the channel,\nwhere p Y |X (·|·) is the conditional probability density function (pdf) of the output given the input of the AWGN channel.\nProof: Let T k (·) as a k-cyclic-shift operator on µ ∈ Λ n (γ, q, c), deﬁned as\nwhere k = 1, · · · , n − 1, and speciﬁcally T 0 (µ) = µ. Deﬁne a stationary distribution ν on X n as\nν = 1 n\nBased on the concavity of the mutual information and (8) it is easy to prove I(ν) ≥ I(µ) for any µ in Λ n (γ, q, c).\nAccording to Proposition 1, for any n, I(X n ; Y n ) is maxi- mized by a stationary distribution. Therefore with n → ∞, the capacity in (4) is achieved by a stationary input distribution.\nProposition 2: Let the input follows distribution µ ∈ Λ(γ, q, c). The limit of the input-output mutual information per symbol as a function of µ can be expressed as\nwhere I(X; Y ) is the mutual information of the AWGN channel between the input X, which follows distribution µ X 1 and the corresponding output Y , h(Y ) is the differential entropy of Y and h( Y ) is the differential entropy rate of output process {Y i }.\nProof: The mutual information between X n and Y n can be expressed using relative entropies\nI(X n ; Y n ) = D(P Y n |X n P Y n |P X n ) \t (13) = D(P Y n |X n P Y 1 × · · · × P Y n |P X n )\nWhen the input is an i.i.d. random process, the output is also i.i.d., h(Y ) = h( Y ). This implies the following corollary.\nCorollary 1: Among all i.i.d. input, the one that maximizes the mutual information under duty cycle constraint (q, c) and power constraint γ can be solved by\nsubject to P X (0) − 2cP X (0)(1 − P X (0)) ≤ q, E{X 2 } ≤ γ.\nIn the special case of no transition cost, i.e., c = 0, the result of (17) is equal to the result in [3].\nThe mutual information expressed by (12) is hard to opti- mize, even if the input is restricted to Markov processes. To simply the matter, we introduce a lower bound of I(µ), which is given by L(µ) in (6).\nProperty (a): Using the fact that processing reduce relative entropy and µ is a stationary distribution, we have\n1 n\n1 n\nusing the fact that the Ces´aro mean of sequence I(X 1 , X k 2 ) is I(X 1 ; X ∞ 2 ). Applying (12), (14) and (20),\nProperty (b): For ∀µ ∈ Λ(γ,q,c), which is not Markov in general, its ﬁrst-order Markov approximation ν is deﬁned by\nEvidently, ν and µ have identical marginal distributions: ν X i = µ X i , and also identical joint distributions of any consecutive pairs: ν X i ,X i+1 = µ X i ,X i+1 . Since µ ∈ Λ(γ, q, c), we have ν ∈ Λ(γ, q, c). Let {X i } follow distribution µ and {Z i } follow distribution ν. Then\nwhere equality holds if and only if {X i } is a ﬁrst-order Markov process. By (23), L(ν) ≥ L(µ). So for any µ which maximizes L(µ), ν can be deﬁned by (22) with L(ν) ≥ L(µ). L(µ) must be maximized by a ﬁrst-order Markov process.\nProperty (c): Suppose ν is a stationary ﬁst-order Markov process, sufﬁciently denote as ν = (X , P X 2 |X 1 ), where X is the state space of ν and P X 2 |X 1 is the transition prob- ability distribution. Let S 1 = X \\ {0}, α = P X 2 |X 1 (0|0), β = P (X 2 ∈ S 1 |X 1 ∈ S 1 ) and η = P (X ∈ S 1 ). Deﬁne a\nDeﬁnition 1: Let ¯ ν, deﬁned on the same state space X as ν, be a ﬁrst-order Markov process denoted by (X , P Z 2 |Z 1 ), where\n      \n     \nα \t z 1 = 0 z 2 = 0, 1 − β \t z 1 = 0 z 2 = 0,\nIt is easy to prove that the stationary distributions P Z = P X . Moreover, ¯ ν satisﬁes the same power and duty cycle con- straint as ν, i.e., ¯ ν ∈ Λ(γ, q, c). Let B i = 1 {X i =0} , then P B 2 |B 1 (0|0) = α and P B 2 |B 1 (1|1) = β. Let b i = 1 {z i =0} . Since\nZ i and Z i+1 are independent given B i = 1 {Z i =0} and B i+1 = 1 {Z i+1 =0} . Based on (24) and (25), it is easy to see that\nThe discreteness of the optimized input distribution is proved in the following. According to the above proof lower bound L(·) is maximized by a ﬁrst-order Markov process. The transition probability distribution P X 2 |X 1 can be expressed as\nwhere b i = 1 {x i =0} , P X = µ X and P X 2 |X 1 = µ X 2 |X 1 . Then the maximum of L(µ) can be achieved by the optimization\nwhere P B = P B 1 = P B 2 . Since given any q 0 , I X (q 0 )−I B (q 0 ) can be maximized by the maximum of I X (q 0 ) and the minimum of I B (q 0 ) respectively, the maximization of (28) must be achieved by P X , which maximizes I(X; Y ) for given q 0 . Therefore given q 0 , the maximization in (29) is similar to the problem in [3]. The difference to [3] is that in (29) the distribution P X satisﬁes P X (0) = q 0 , however in [3] the distribution P X satisﬁes P X (0) ≥ q. Following a similar development as the proof in [3], P X can be proved to be discrete. Therefore, the maximum of the lower bound L(·) is achieved by a discrete ﬁrst-order Markov process.\nBased on Theorem 1, in order to ﬁnd the lower bound of the capacity, we can maximize L(µ) and obtain an optimized discrete ﬁrst-order Markov input µ ∗ in Λ(γ, q, c). Let µ 0 denote the capacity-achieving distribution, then\nIn order to numerically calculate the mutual informa- tion (12), it is important to compute the differential entropy rate of a HMP generated by Markov input through the AWGN channel. Computing the (differential) entropy rate of HMPs is a hard problem. Most works in this area focus on the entropy rate of the binary Markov input through various channels. Reference [13] and [14] presented two different methods to approximate the entropy of the HMP output generated by binary Markov input. Based on these existing studies, a Monte Carlo algorithm is provided in this paper to compute the differential entropy rate of HMPs generated from a m-state Markov chain (m ≥ 3) through the AWGN channel. We sketch the main ideas in our algorithm in this subsection.\nBased on Blackwell\u2019s work [15], the entropy rate of HMPs can be expressed in terms of the distribution of the conditional\ndistribution of X 0 given the past observations Y 0 −∞ . In order to estimate P X 0 |Y 0\nwhere m is the number of the states of Markov Chain, X (i) ∈ X is the ith state and X is the state space of Markov Chain. It is obviously that L (0) n ≡ 0. Then given L n = {L (0) n , L (1) n , · · · , L (m−1) n \t },\nwhere R (i) (·) and F (i) (·) are speciﬁed functions relative to the Markov chain. Due to space limitation, the details of the deduction is not given in this paper. For the hidden Markov processes observed through the AWGN channel (1), the entropy of HMPs can be computed as [15]\nIn order to compute the entropy rate of HMPs based on (36), the key is to estimate the probability distribution of L n , P L n . we can evolve the distribution of L n based on (35) from any initial distribution P L 0 . Due to space limitations, the detailed algorithm is presented in [16].\nBased on the results in Section IV and V-A, we ﬁrst seek a discrete Markov chain with ﬁnite alphabet that maximizes L(µ). Once the optimal Markov distribution µ ∗ is determined, we compute the achievable rate I(µ ∗ ) according to (12).\nFig. 1 shows the stationary (marginal) distribution for sub- optimal Markov input. In order to compensate the transition cost, additional fraction of zero symbol should be transmitted, P X (0) > q. As the SNR increases, more and more weights are put on distant constellation points, where less and less weights are put on the zero letter.\nIn Fig. 2, the rates achieved by various optimized input distributions are plotted against the SNR. The rate achieved by the optimized Markov input is larger than that of subop- timal i.i.d. input calculated by formula (17) with duty cycle constraint (q, c). The lower bound L(µ) is quite tight and can be regarded as a good approximation of mutual information of ﬁrst-order Markov inputs.\nFig. 3 and 4 demonstrate the sensitivity of the achievable rates to the duty cycle q and the transition cost c, respectively.\nThe performance of Markov inputs is superior to i.i.d. inputs as well as Gaussian signaling with deterministic schedule. Fig 3 shows that the performance of i.i.d. input is similar to the deterministic schedule, which implies that different from the\ncase in [3], i.i.d. input is not a good choice under the realistic duty cycle constraint."},"refs":[{"authors":[{"name":"D. Julian"},{"name":"S. Majumdar"}],"title":{"text":"Low power personal area communication"}},{"authors":[{"name":"W.-Y. L. I. F. Akyildiz"},{"name":"M. C. Vuran"}],"title":{"text":"Next generation/dynamic spectrum access/cognitive radio wireless networks: A survey"}},{"authors":[{"name":"L. Zhang"},{"name":"D. Guo"}],"title":{"text":"Capacity of gaussian channels with duty cycle and power constraints"}},{"authors":[{"name":"R. Zamir"}],"title":{"text":"A gaussian input is not too bad"}},{"authors":[{"name":"J. G. Smith"}],"title":{"text":"The information capacity of amplitude and variance- constrained scalar Gaussian channels"}},{"authors":[{"name":"T. H. Chan"},{"name":"S. Hranilovic"},{"name":"F. R. Kschischang"}],"title":{"text":"Capacity-achieving probability measure for conditionally Gaussian channels with bounded inputs"}},{"authors":[{"name":"S. Shamai (Shitz)"}],"title":{"text":"Capacity of a pulse amplitude modulated direct detection photon channel"}},{"authors":[{"name":"S. Shamai (Shitz)"},{"name":"I. Bar-David"}],"title":{"text":"The capacity of average and peak- power-limited quadrature Gaussian channels"}},{"authors":[{"name":"I. C. Abou-Faycal"},{"name":"M. D. Trott"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"The capacity of discrete-time memoryless Rayleigh-fading channels"}},{"authors":[{"name":"M. Katz"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"On the capacity-achieving distribution of the discrete-time noncoherent and partially coherent AWGN chan- nels"}},{"authors":[{"name":"M. C. Gursoy"},{"name":"H. V. Poor"},{"name":"S. Verd´u"}],"title":{"text":"The noncoherent Rician fading channel-part I: Structure of the capacity-achieving input"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of information theory"}},{"authors":[{"name":"E. Ordentlich"},{"name":"T. Weissman"}],"title":{"text":"Approximations for the entropy rate of a hidden markov process"}},{"authors":[{"name":"J. Luo"},{"name":"D. Guo"}],"title":{"text":"On the entropy rate of hidden markov processes observed through arbitrary memoryless channels"}},{"authors":[{"name":"D. Blackwell"}],"title":{"text":"The entropy of functions of ﬁnite-state markov chains"}},{"authors":[{"name":"L. Zhang"},{"name":"H. Li"},{"name":"D. Guo"}],"title":{"text":"Capacity of gaussian channels with duty cycle and power constraints."}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566999.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S2.T7.3","endtime":"12:30","authors":"Hui Li, Dongning Guo","date":"1341231000000","papertitle":"Achievable Rates of Gaussian Channels with Realistic Duty Cycle and Power Constraints","starttime":"12:10","session":"S2.T7: Capacity of Gaussian Channels","room":"Stratton (407)","paperid":"1569566999"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
