{"id":"1569564703","paper":{"title":{"text":"Corrupted and missing predictors: Minimax bounds for high-dimensional linear regression"},"authors":[{"name":"Po-Ling Loh"},{"name":"Martin J. Wainwright"}],"abstr":{"text":"Abstract\u2014Missing and corrupted data are ubiquitous in many science and engineering domains. We analyze the information- theoretic limits of recovering sparse vectors under various models of corrupted and missing data. In particular, consider a high- dimensional linear regression model y = Xβ ∗ + , where y ∈ R n is the response vector, X ∈ R n×p is a random design matrix with p \t n and rows distributed i.i.d. as N (0, Σ x ), β ∗ ∈ R p is the unknown regression vector, and ∼ N (0, σ 2 I) is independent additive noise. Whereas a traditional approach assumes that the covariates X are fully observed, we assume only that a corrupted version Z is observed. Our main contribution is to establish minimax rates of convergence for estimating β ∗ in squared 2 - loss, assuming β ∗ is k-sparse. Our upper and lower bounds in both additive noise and missing data cases scale as k log(p/k) n , with prefactors depending only on the corruption and/or missing pattern of the data."},"body":{"text":"The goal of prediction is to estimate a mapping between a vector x ∈ R p of covariates and a response variable y ∈ R. In standard prediction problems, the data are fully observed, meaning estimators are constructed based on a collection of pairs (x i , y i ) ∈ R p × R. In many applications, however, the assumption of complete observations is unrealistic: elements of x ∈ R p may be corrupted (e.g., due to noise and other disturbances), or partly missing (e.g., due to failures to respond in survey data). In mathematical terms, such corruptions may be modeled as random additive or multiplicative perturbations. For instance, simple measurement error may be modeled as an additive perturbation; and missing data may be modeled as a multiplicative perturbation, where each entry is multiplied by a Bernoulli random variable that results in missingness with a certain probability. In contrast to the classical setup, such pre- diction problems in which both the covariate and response are observed subject to noise are less well understood, and blindly applying classical methods to the corrupted variables will generally yield estimators that are not statistically consistent for the true population parameter. Although there are a variety of heuristic methods for problems with corrupted/missing data (e.g., see Little and Rubin [4]), the theory is still lacking.\nThis paper focuses on fundamental or information-theoretic limitations of prediction under various forms of corrupted co- variates, particularly in the context of sparse high-dimensional linear regression, where the number of predictors p is allowed\nto grow with and possibly exceed the sample size n, but the number k of relevant predictors is substantially less than p. Our approach consists of a two-pronged attack: on the statistical side, we demonstrate an efﬁcient estimator for our model and prove upper bounds on 2 -error between the estimator and the population parameter; while on the information-theoretic side, we establish lower bounds on 2 -error that hold for any estimator derived from the data. Our upper and lower bounds in the additive noise setting agree up to constant factors, demonstrating that our proposed estimator is minimax optimal.\nwhere the x i \u2019s are p-dimensional covariates, the y i \u2019s are re- sponse variables, the i \u2019s are independent noise, and β ∗ ∈ R p is the unknown vector. In matrix form, we write y = Xβ ∗ + , where X ∈ R n×p and y, ∈ R n . Since we are working in a high-dimensional setting (p n), we must impose additional structure on β ∗ . Henceforth, we assume that β ∗ 0 ≤ k, meaning β ∗ has at most k nonzero entries.\nIn the traditional linear regression framework, one would es- timate β ∗ based on observations (y, X). However, we assume that only the pair (y, Z) is available, where Z is a version of X corrupted by noise. We analyze the following settings:\n(a) Additive noise: For each i, observe z i = x i + w i , where w i is independent of x i .\n(b) Missing data: For each i and each component j, inde- pendently observe z ij = x ij with probability 1 − α, and z ij = with probability α, where α ∈ [0, 1).\nIn both cases, we assume the x i \u2019s and i \u2019s are drawn i.i.d. from the distributions N (0, σ 2 x I) and N (0, σ 2 I), respectively. We assume the w i \u2019s are drawn i.i.d. from N (0, σ 2 w I) in the additive noise case.\nOur analysis focuses on the minimax squared 2 -error M(n, p, k) := inf\n2 unit ball, whereas the inﬁmum is taken over all measureable functions β of the observed data (y, Z). In Theorems 1 and 3,\nwe derive upper bounds for M by analyzing a modiﬁed version of the Lasso for corrupted covariates. In Theorems 2 and 4, we derive lower bounds via information-theoretic techniques, where we ﬁrst reduce the estimation problem to a hypothesis testing problem and then apply Fano\u2019s inequality to lower-bound the error probability. This type of reduction is standard in minimax statistical analysis (e.g., [2, 8, 7]).\nThe remainder of the paper is organized as follows. In Section II, we state the upper and lower bounds obtained in the additive noise and missing data settings. In Section III, we provide proofs of our results.\nFollowing standard conventions, we use f (n) \t g(n) to denote that f (n) ≤ cg(n) for a universal constant c > 0, and similarly, f (n) g(n) to denote that f (n) ≥ c g(n) for a universal constant c > 0. We write f (n) \t g(n) when f (n) g(n) and f (n) g(n) hold simultaneously.\nWe now state our main results. Following Loh and Wain- wright [5], we deﬁne the surrogate Γ ∈ R p×p for Σ x , deﬁned in the additive noise and missing cases as\nAssumption 1 (Lower-RE condition). For some α > 0, we have θ T Γθ ≥ α θ 2 2 whenever θ 1 ≤ c 0\nBy Lemmas 1 and 3 of Loh and Wainwright [5], Assump- tion 1 holds w.h.p. for α \t σ 2 x in both settings of interest.\nWe begin by stating an upper bound for the additive noise setting, when X and W are Gaussian with covariance σ 2 x I and σ 2 w I, respectively. We write σ 2 z := σ 2 x + σ 2 w and κ := σ 2 w σ 2\nTheorem 1. In the additive noise setting, if Γ satisﬁes As- sumption 1 and n k log(p/k), we have\nNote that when σ w = 0, corresponding to the classical case of fully-observed covariates, the upper bound reduces to\nPast work has established bounds of this form for the Lasso and related estimators [3, 1], and this rate has been shown to be minimax optimal [6]. In the more general setting with σ w > 0, the bound (1) has a qualitatively similar form, with the prefactor growing with the magnitude of σ w .\nWe now turn to a lower bound that matches the upper bound up to a constant factor. The probability for the lower bound is\nchosen to be 1/2; it may be replaced by a constant arbitrarily close to 1, by a suitable modiﬁcation of the universal constants.\nTheorem 2. In the additive noise setting, if 8 ≤ k ≤ p/2 and n k log(p/k), we have\nis bounded above by a constant and α \t σ 2 x , the bounds in Theorems 1 and 2 match up to constant factors, identifying minimax optimal rates for the additive noise setting. The assumption of bounded κ merely requires the SNR to be bounded away from zero.\nIn the missing data setting, we assume x i ∼ N (0, σ 2 x I), and α ∈ [0, 1) is the probability that a given entry is missing. We have the following upper bound:\nTheorem 3. In the missing data setting, if Γ satisﬁes Assump- tion 1 and n \t 1 (1−α) 2 k log(p/k), we have\nTheorem 4. In the missing data setting, if 8 ≤ k ≤ p/2 and n \t 1 (1−α) 2 k log(p/k), we have\nn \t , \t (3) with probability at least 1/2.\nNote that when α = 0, corresponding to no missing data, Theorem 3 again reduces to known results. Furthermore, both upper and lower bounds grow as the inverse of (1 − α), agreeing with intuition\u2014as the proportion of missing entries increases, the estimation problem increases in difﬁculty. How- ever, a gap of a factor of (1 − α) remains between the scaling in Theorems 3 and 4.\nIn an earlier paper [5], we propose a family of modiﬁed Lasso estimators for use in high-dimensional linear regression problems, when the unknown vector β ∗ is sparse and the covariates are observed subject to additive noise or missing data. We use the same estimators to establish upper bounds on 2 -error rates in the present paper; however, our analysis yields sharper bounds. We improve the asymptotic scaling in the squared 2 -error from k log p n to k log(p/k) n , and tighten the prefactor so it achieves known minimax results in the limit of no corruption. Note, however, that the upper bounds in the previous paper apply more generally to sub-Gaussian matrices with nondiagonal covariances, whereas our current analysis\nonly applies when covariates are Gaussian and covariances are multiples of the identity. Our proof techniques for lower bounds closely follow those of Raskutti et al. [6].\nIt sufﬁces to demonstrate an estimator for β ∗ which, w.h.p., has small 2 -norm error. We use the following estimator proposed in Loh and Wainwright [5]:\nfor (Σ x , Cov(x i , y i )), and λ \t log(p/k) n is chosen appropri- ately. We show that if β ∗ ∈ B 0 (k) ∩ B 2 (1), then β − β ∗ 2 2 satisﬁes the proper upper bound.\nSince β ∗ is feasible and β is optimal, we have 1\nk ν 2 , we may lower- bound the LHS of inequality (4) using Assumption 1.\nTo upper-bound the RHS of inequality (4), we use the following combinatorial lemma, a slight generalization of Lemma 11 in Loh and Wainwright [5]:\nwhere cl{·} and conv{·} denote the topological closure and convex hull, respectively.\nto obtain u ⊆ (1 + 2c) cl{conv{B 0 (k) ∩ B 2 (1)}}, so |ν T (γ − Γβ ∗ )| ≤ (1 + 2c) ν 2 \t sup\nClearly, the sup may be taken over u ∈ B 0 (k) ∩ B 2 (1). Furthermore, we may use a standard discretization argument for each support set S with |S | ≤ k, followed by a union bound over all choices of S . Since the discretization gives a factor of c k and the union bound gives a factor of p k ≤ p k k , it sufﬁces to bound the sup w.h.p. for an arbitrary ﬁxed unit vector u with u 0 ≤ k. This yields a bound of the form\nwith probability at least 1 − c 1 exp(−c 2 k log(p/k)), where ϕ is a function of the problem parameters, derived below.\nCombining this with inequality (4) and the lower-RE bound then implies\nk α\nmax ϕ log(p/k) n\nHence, choosing λ \t ϕ log(p/k) n , we obtain the bound M ≤ cϕ 2 α 2 k log(p/k) n . The remaining component is to ﬁnd an appropriate choice of the prefactor ϕ.\nT y n\nwith probability at least 1 − 2 exp(−cnt 2 ), using standard tail bounds for sub-Gaussian matrices. Taking ϕ = (σ σ z + σ w σ z ) and t = k log(p/k) n \t gives\nn \t , w.h.p. Finally, we bound\nFor lower bounds, we follow a standard argument [2, 7, 8] to transform the estimation problem into a hypothesis testing problem. Namely, given any δ-packing {β 1 , . . . , β M } of the target set B 0 (k) ∩ B 2 (1), we have the inequality\n(5) where B is uniformly distributed over {β 1 , . . . , β M } and β is an estimator. We then lower-bound the RHS using Fano:\nlog M \t . \t (6) In order to upper-bound the mutual information I(y; B), let P β denote the distribution of y given B = β (when Z is observed). For conciseness of notation, denote P j = P β j . Since y is distributed as the mixture 1 M j P j , we have\nexploiting the convexity of the KL divergence in the last in- equality. Finally, we upper-bound the pairwise KL divergences D(P j P ) explicitly, and then choose an appropriate value of δ to ensure that P(β = B) ≥ 1/2. The key steps therefore\ninvolve ﬁnding an appropriate δ-packing of the target set and an upper-bound on the mutual information.\nThe following lemma shows that there exists a 1 2 -packing of the target set with log M ≥ k 2 log p−k k/2 :\nLemma 2. There exists a 1 2 -packing of B 0 (k) ∩ B 2 (1) in 2 - norm with log M ≥ k 2 log p−k k/2 . In particular, if δ < 1 2 , there exists a 2δ-packing {β 1 , . . . , β M } of the same set such that\nThe proof is based on a modiﬁcation of a result due to Raskutti et al. [6]. We now derive an explicit expression for P β , which we will use to compute the KL divergences appearing in inequality (7). By independence, P β is a product distribution of y i |z i , over all i. We claim that for each i,\nIndeed, (y i , z i ) is clearly jointly Gaussian with mean 0, and by computing covariances,\nso equation (8) follows immediately by standard results on conditional Gaussians. We now derive the following lemma: Lemma 3. For any β, β ∈ B 0 (k), we have\nProof: Assume σ and σ w are not both 0; otherwise, the theorem is trivially true. By equation (8), we can write\nP β (y) = E P β n 2 log σ\nn 2\nwhere σ 2 β = β T (Σ x − Σ x Σ −1 z Σ x )β + σ 2 , and σ 2 β is deﬁned analogously.\nwhere the last inequality uses Lemma A.1 in the Appendix. Expanding σ 2 in inequality (10) then yields the desired result.\nIn particular, for β j , β in the δ-packing, Lemmas 2 and 3 together imply that\nNote that for p/k ≥ 2 and k ≥ 8, we have log 2 ≤ k 8 log p−k k/2 , so inequality (11) implies that\n+ 1 4\nand using inequality (5), we conclude that M ≥ δ 2 4 with probability at least 1/2. Finally, note that when p/k ≥ 2, we have p−k k/2 = 2 p k − 1 ≥ p k , so we may replace the quotient\nAgain following Loh and Wainwright [5], we use the estimators Γ = Z T Z n − α diag Z T Z n , γ = Z T y n , where Z = Z 1−α is a rescaled version of the missing data matrix. Let u ∈ B 0 (k) ∩ B 2 (1). Using the fact that y = Xβ ∗ + and expanding, we have the bound\nNote that Z is sub-Gaussian with parameter σ 2 x (1−α) 2 , so we can bound the second term by σ x σ 1−α t with probability at least 1 − 2 exp(−cnt 2 ). Similarly, we may bound the third term by\n1 n\nwhere z i is the observed vector with 0\u2019s in missing positions. Conditioned on the missing positions, u T Z T (Z−X) n \t β ∗ is sub- exponential with parameter ασ 2 x (1−α) 2 . Since a mixture of sub- exponentials is sub-exponential with the same parameter, we\nwith t = (1 − α) k log(p/k) n \t yields the bound. D. Proof of Theorem 4\nNote that when σ = 0, the theorem is trivially true; hence, we assume σ > 0. We use the same δ-packing obtained in Lemma 2. To compute the KL divergences, we ﬁrst derive the distribution of y | Z for a ﬁxed β, which is a product distribution of y i | z i over all i. Furthermore, we may write\ny i = x i,obs , β obs + x i,mis , β mis + i , \t (12) where obs denotes the indices of the the observed coordinates and mis denotes the indices of the missing coordinates. Note that β obs and β mis vary with i. From equation (12), we have\n1 n\n1 2\n+ 1 2\nBy a Taylor expansion, log x ≤ (x − 1) + c(x − 1) 2 for x close to 1. Taking x = σ 2 i,β σ 2\n1 n\nc 2\nwhere u i ∈ R p is a binary vector with 1\u2019s corresponding to missing values in z i , and β 2 and β 2 are obtained by componentwise squaring of β and β . The matrix U with rows u T i is i.i.d. Bernoulli, hence sub-Gaussian with parameter 1. Applying Lemma A.1 to U and Z with covariances α(1 − α)I and (1 − α)σ 2 x I, taking t = (1 − α)σ 2 k log(p/k) n , we obtain\nby Cauchy-Schwarz and the triangle inequality. When {β 1 , . . . , β M } is a δ-packing of B 0 (k) ∩ B 2 (1), we have\nfor all j = , with probability at least 1 − exp(−ck log(p/k)). Choosing δ 2 = cσ 2 σ 2\nWe have derived upper and lower bounds for the 2 error for estimating an unknown sparse regression vector in a high-dimensional setting, and shown that the bounds match in the additive noise setting when the signal-to-noise ratio for the observed covariates is bounded below by a positive constant. Our bounds illustrate the interplay between the signal of the true unobserved covariates and the sources of corruption present in the covariate and response variables.\nAcknowledgments: PL acknowledges support from a Hertz Foundation Fellowship and an NDSEG Fellowship; MJW and PL were also partially supported by grants NSF-DMS- 0907632 and AFOSR-09NL184.\nLemma A.1. Suppose X ∈ R n×p is a sub-Gaussian matrix with parameter σ 2 x . For t ≤ σ 2 x , we have\n1 n\nProof: See Lemma 15 in Loh and Wainwright [5]. The only modiﬁcation is to use the tighter bound p 2k ≤ (p/k) k .\n[1] P. J. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of Statis- tics , 37(4):1705\u20131732, 2009.\n[2] L. Birg´e. Approximation dans les espaces metriques et theorie de l\u2019estimation. Z. Wahrsch. verw. Gebiete , 65:181\u2013327, 1983.\n[3] E. Candes and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than n. Annals of Statistics , 35(6):2313\u20132351, 2007.\n[4] R. Little and D. B. Rubin. Statistical analysis with missing data . Wiley, New York, 1987.\n[5] P. Loh and M.J. Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. Annals of Statistics, To appear. Available at http://arxiv.org/abs/1109.3714.\n[6] G. Raskutti, M.J. Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional linear regression over q -balls. IEEE Transactions on Information Theory, 57(10):6976\u20136994, 2011.\n[7] Y. Yang and A. Barron. Information-theoretic determina- tion of minimax rates of convergence. Annals of Statistics, 27(5):1564\u20131599, 1999.\n[8] B. Yu. Assouad, Fano, and Le Cam. In Research Papers in Probability and Statistics: Festschrift in Honor of Lucien Le Cam , pages 423\u2013435. 1996."},"refs":[]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564703.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S14.T8.4","endtime":"18:00","authors":"Po-Ling Loh, Martin J. Wainwright","date":"1341510000000","papertitle":"Corrupted and missing predictors: Minimax bounds for high-dimensional linear regression","starttime":"17:40","session":"S14.T8: High-Dimensional Inference","room":"Stratton (491)","paperid":"1569564703"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
