{"id":"1569564961","paper":{"title":{"text":"Selecting Two-Bit Bit Flipping Algorithms for Collective Error Correction"},"authors":[{"name":"Dung Viet Nguyen"},{"name":"Bane Vasi´ c"},{"name":"Michael W. Marcellin"}],"abstr":{"text":"Abstract\u2014A class of two-bit bit ﬂipping algorithms for decod- ing low-density parity-check codes over the binary symmetric channel was proposed in [1]. Initial results showed that decoders which employ a group of these algorithms operating in parallel can offer low error ﬂoor decoding for high-speed applications. As the number of two-bit bit ﬂipping algorithms is large, designing such a decoder is not a trivial task. In this paper, we describe a procedure to select collections of algorithms that work well together. This procedure relies on a recursive process which enumerates error conﬁgurations that are uncorrectable by a given algorithm. The error conﬁgurations uncorrectable by a given algorithm form its trapping set proﬁle. Based on their trapping set proﬁles, algorithms are selected so that in parallel, they can correct a ﬁxed number of errors with high probability."},"body":{"text":"With the introduction of high speed applications such as ﬂash memory, ﬁber and free-space optical communications comes the need for fast and low-complexity error control coding. Message passing algorithms for decoding low-density parity-check (LDPC) codes such as the sum-product algorithm (SPA) offer very attractive error performance, especially for codes with column-weight d c ≥ 4. However, the complexity of these algorithms is still high and the decoding speed is limited, mostly due the fact that the operations at variable and check nodes must be carried out for every edge in the Tanner graph. For regular column-weight-three LDPC codes, which allow lower complexity implementation, message passing algorithms (as well as other classes of decoding algorithms) usually suffer from high error ﬂoor. This weakness of message passing al- gorithms in regular column-weight-three LDPC codes justiﬁes the search for alternatives which offer better trade-offs between complexity, decoding speed and error performance.\nAmong existing decoding algorithms for LDPC codes on the binary symmetric channel (BSC), bit ﬂipping algorithms are the fastest and least complex. The check node operations of these algorithms are modulo-two additions while the vari- able node operations are simple comparisons. The simplicity of these algorithms also makes them amenable to analysis. Many important and interesting results on the error correction capability of the serial and parallel bit ﬂipping algorithms have been derived (see [1] for a list of references). Unfortunately, their error performance is typically inferior. As a result, bit- ﬂipping-oriented algorithms have been largely considered to be impractical, even after the introduction of some improved versions, such as the one in [2].\nIn [1], a class of bit ﬂipping algorithms that employ two bits for decoding LDPC codes over the BSC was proposed. Com- pared to serial and parallel bit ﬂipping, a two-bit bit ﬂipping (TBF) algorithm employs one additional bit at a variable node and one at a check node. The additional bits introduce memory in the decoding process, which slows down the decoding when necessary. Initial results showed that decoders which employ a group of these algorithms operating in parallel lower the error ﬂoor while maintaining low complexity. However, in [1] we have not given a complete failure analysis of these algorithms, nor have we established the methodology to derive good algorithms and/or a collection of mutually good algorithms.\nIn this paper, we provide complete failure analysis for TBF algorithms. More importantly, we give a rigorous procedure to select groups of algorithms based on their complementariness in correcting different error patterns. Decoders that employ algorithms selected using this procedure have provably good error performance and, by the nature of bit ﬂipping, high speed.\nAs one can expect, a TBF algorithm (like other sub-optimal graph-decoding algorithms) fails on some low-weight error patterns due to the presence of certain small subgraphs in the Tanner graph. In this paper, we characterize a special class of these subgraphs and refer to them with the common term \u201ctrapping sets.\u201d Our deﬁnition of a trapping set for a given algorithm readily gives a sufﬁcient condition for successful decoding. The set of all possible trapping sets of a given decoding algorithm constitutes the algorithm\u2019s trapping set proﬁle. A unique property of trapping sets for TBF algo- rithms is that a trapping set proﬁle may be obtained by a recursive procedure. The diversity among trapping set proﬁles of different algorithms allows us to select groups of algorithms such that they can collectively correct error patterns that are uncorrectable by individual algorithms.\nThe rest of the paper is organized as follows. Section II gives the necessary background. Section III gives motivation. In Section IV, we deﬁne trapping sets, trapping set proﬁles and describe the recursive procedure for constructing a trap- ping set proﬁle. Section V discusses the process of selecting algorithms. Numerical results are given in Section VI.\nLet C denote an (n, k) binary LDPC code. C is deﬁned by the null space of H, an m × n parity-check matrix of C. H is\nthe bi-adjacency matrix of G, a Tanner graph representation of C. G is a bipartite graph with two sets of nodes: n variable (bit) nodes V (G) = {1, 2, . . . , n} and m check nodes C(G) = {1, 2, . . . , m}; and a set of edges E(G). A (d v , d c )- regular LDPC code has a Tanner graph G in which all variable nodes have degree d v and all check nodes have degree d c . In this paper, we only consider (d v , d c )-regular LDPC codes. A subgraph of a bipartite graph G is a bipartite graph U such that V (U ) ⊂ V (G), C(U ) ⊂ C(G) and E(U ) ⊂ E(G).\nG is said to contain U . Furthermore, if Y is a graph which is isomorphic to U then G is also said to contain Y . In a bipartite graph G, the induced subgraph on a set of variable nodes V s ⊂ V (G) is a bipartite graph U with V (U ) = V s , C(U ) = {c ∈ C(G) : ∃v ∈ V s such that (v, c) ∈ E(G)} and E(U ) = {(v, c) ∈ E(G) : v ∈ V s }.\nA vector x = (x 1 , x 2 , . . . , x n ) is a codeword if and only if xH T = 0, where H T is the transpose of H. Assume the trans- mission of the all-zero codeword over the BSC. Denote by y the channel output vector and denote by ˆ x l = (ˆ x l 1 , ˆ x l 2 , . . . , ˆ x l n ) the decision vector after the lth iteration of the iterative algorithm, where l is a positive integer. At the end of the lth iteration, a variable node v is said to be corrupt if ˆ x l v = 1, otherwise it is correct. For the sake of convenience, we let ˆ x 0 = y. A variable node v with ˆ x 0 v = 1 is initially corrupt, otherwise it is initially correct. Let s l = (s l 1 , s l 2 , . . . , s l m ) denote the syndrome vector of the decision vector after the lth iteration, i.e., s l = ˆ x l H T . A check node c is said to be satisﬁed at the beginning of the lth iteration if s l−1 c = 0, otherwise it is unsatisﬁed. TBF algorithms are deﬁned as follows.\nDeﬁnition 1: The class F of TBF algorithms is given in Algorithm 1, where z l = (z l 1 , z l 2 , . . . , z l m ) gives the states of the check nodes at the beginning of the lth iteration while w l = (w l 1 , w l 2 , . . . , w l n ) gives the states of the variable nodes at the end of the lth iteration. A variable node v takes its state from the set A v = {0 s , 0 w , 1 w , 1 s }, i.e., it can be strong zero, weak zero, weak one or strong one. A check node takes its state from the set A c = {0 p , 0 n , 1 p , 1 n }, i.e., it can be previously satisﬁed, newly satisﬁed, previously unsatisﬁed or newly unsatisﬁed. The state w 0 v of a variable node v is initial- ized to ∆ v (0) ∈ {0 s , 0 w } if y v = 0 and to ∆ v (1) ∈ {1 s , 1 w } if y v = 1. The state z 1 c of a check node c is initialized to ∆ c (0) ∈ {0 p , 0 n } if s 0 c = 0 and to ∆ c (1) ∈ {1 p , 1 n } otherwise. A TBF algorithm F = (f, l m F , ∆ v , ∆ c ) iteratively updates z l and w l until all check nodes are satisﬁed or until a maximum number of iteration l m F is reached. The check node update function Φ : {0, 1} 2 → A c is deﬁned as follows: Φ(0, 0) = 0 p , Φ(0, 1) = 1 n , Φ(1, 0) = 0 n and Φ(1, 1) = 1 p . The variable node update is speciﬁed by a function f : A v × Ξ d v → A v , where Ξ d v is the set of all ordered 4-tuples ξ = (ξ 1 , ξ 2 , ξ 3 , ξ 4 ) such that ξ i ∈ N and i ξ i = d v . χ l 0\n(v) and χ l 1 n (v) give the number of check nodes with states z l c = 0 p , 0 n , 1 p and 1 n , respectively, that are connected to v. The function f must be symmetric with respect to 0 and 1 and must allow every state of a variable node to be reachable from any other state.\n∀v : w 0 v ← ∆ v (y v ), ∀c : z 1 c ← ∆ c (s 0 c ), l ← 1 while s l = 0 and l < l m F do\n∀v : w l v ← f (w l−1 v , χ l 0 p (v), χ l 0 n (v), χ l 1 p (v), χ l 1 n (v)); ∀c : z l+1 c ← Φ(s l−1 c , s l c ); l ← l + 1;\nhas \u201cstrength\u201d and a check node\u2019s reliability is evaluated based on its state in the previous iteration.\nConsider a collection A of iterative decoding algorithms for LDPC codes. Let us assume for a moment that the set of all uncorrectable error patterns for each and every algorithm in A is known. More precisely, in the context of LDPC codes, we assume that the induced subgraphs on such error patterns can be enumerated for each decoding algorithm. This naturally suggests the use of a decoder D which employs multiple algorithms drawn from A . The basis for this use of multiple algorithms is rather simple: If different algorithms are capable of correcting different error patterns, then a decoder employing a set of properly selected algorithms can achieve provably better error performance than any single-algorithm decoder. Disappointingly, the above hypothetical assumption is not valid for most iterative algorithms. For message passing algorithms such as the SPA, there is no simple criterion to verify weather or not an arbitrary error pattern is correctable, much less an explicit methodology to design a decoder which employs multiple algorithms in a collaborative manner.\nInterestingly, for TBF algorithms, we are able to establish a framework to analyze and enumerate all uncorrectable error patterns, and this is the main contribution of this paper. In particular, we characterize the decoding failures of TBF algorithms by redeﬁning trapping sets and introducing the deﬁ- nition of trapping set proﬁles. It is an important property of the newly deﬁned trapping sets that enable us to enumerate them using a recursive procedure. We remark that the enumeration of trapping sets is code independent. More importantly, the concept and explicit construction of trapping set proﬁles allow rigorous selections of multiple algorithms which can collec- tively correct a ﬁxed number of errors with high probability. Given that the selection of multiple algorithms would become straightforward once the trapping sets/trapping set proﬁles have been deﬁned and constructed, we devote a considerable portion of the paper to introducing these two objects. We also focus on giving criteria for selecting algorithms rather than explicitly describing the selection process.\nAlthough the term trapping set was originally deﬁned as a set of variable nodes that are not eventually correctable by an iterative decoding algorithm [3], in the literature it has been\nused more frequently to refer to a combinatorially deﬁned subgraph that may be harmful to decoding. The justiﬁcation for this less rigorous use of terminology is that the variable node set of a so-called trapping set (a subgraph) would be an actual set of non-eventually-correctable variable nodes if the parallel bit ﬂipping algorithm were used (see [4] for details). Examples of such trapping sets are ﬁxed sets [4] and absorbing sets [5]. For TBF algorithms, failure analysis can no longer solely rely on these combinatorial objects. For certain TBF algorithms, the smallest subgraphs that cause decoding failures are neither absorbing sets nor ﬁxed sets. We therefore (re)deﬁne the notion of a trapping set for TBF algorithms, as we now explain. We ﬁrst introduce the following deﬁnition on failures of a TBF algorithm.\nDeﬁnition 2: Consider a TBF algorithm F and a Tanner graph G. Let V e denote the set of variable nodes that are initially corrupt and let I denote the induced subgraph on V e . If the algorithm F does not converge on G after l m F iterations, then we say that F fails on the subgraph I of G.\nIt can be seen that the decoding failure of F is deﬁned with the knowledge of the induced subgraph on the set of initially corrupt variable nodes. To characterize failures of F , a collection of all induced subgraphs I must be enumerated. While this is difﬁcult in general, for practically important cases of small numbers of initial errors (less than 8) and small column-weight codes (d v = 3 or 4), the enumeration of such induced subgraphs is tractable.\nConsider a given Tanner graph I. Let E I (F ) denote a set of Tanner graphs containing a subgraph J isomorphic to I such that F fails on J . Since E I (F ) is undeniably too general to be useful, we focus our attention on a subset E r I (F ) of E I (F ), described as follows.\nDeﬁnition 3: Consider a Tanner graph S 1 ∈ E I (F ) such that F fails on the subgraph J 1 of S 1 . Then, S 1 ∈ E r I (F ) if there does not exist S 2 ∈ E I (F ) such that:\n2) there is an isomorphism between S 2 and a proper subgraph of S 1 under which the variable node set V (J 2 ) is mapped into the variable node set V (J 1 ).\nNow we are ready to deﬁne trapping sets and trapping set proﬁles of a TBF algorithm.\nDeﬁnition 4: If S ∈ E r I (F ) then S is a trapping set of F . I is called an inducing set of S. E r I (F ) is called the trapping set proﬁle with inducing set I of F .\nThe following proposition states an important property of a trapping set.\nProposition 1: Let S be a trapping set of F with inducing set I. Then, there exists at least one induced subgraph J of S which satisﬁes the following properties:\n3) Consider the decoding of F on S with V (J ) being the set of initially corrupt variable nodes. Then, for any variable node v ∈ V (S), there exist an integer 0 ≤ l ≤ l m F such that w l v ∈ {1 s , 1 w }.\nFrom Proposition 1, one can see that the trapping set proﬁle E r I (F ) of F contains the graphs that are most \u201ccompact.\u201d We consider these graphs most compact because for at least one J isomorphic to I, the decoding of F on such a graph with V (J ) being the set of initially corrupt variable nodes could be made successful by removing any variable node of the graph. This special property of trapping sets is the basis for an explicit recursive procedure to obtain all trapping sets up to a certain size, which compensates for the lack of a fully combinatorial characterization of trapping sets. We remark that for certain reasonably good algorithms, the necessary condition for a Tanner graph to be a trapping set can be easily derived. Before describing the recursive procedure for constructing trapping set proﬁles, we state the following proposition, which gives a sufﬁcient condition for the convergence of an algorithm F on a Tanner graph G.\nProposition 2: Consider decoding with an algorithm F on a Tanner graph G. Let V e be the set of initially corrupt variable nodes and I be the induced subgraph on V e . Then, algorithm F will converge after at most l m F decoding iterations if there does not exist a subset V s of V (G) such that V s ⊃ V e and the induced subgraph on V s is isomorphic to a graph in E r I (F ).\nRemark: Proposition 2 only gives a sufﬁcient condition because the existence of V s ⊂ V (G) which satisﬁes the above-mentioned conditions does not necessarily indicate that G ∈ E I (F ).\nThe recursive procedure for constructing a trapping set proﬁle E r I (F ) relies on Proposition 1. Let us assume that we are only interested in trapping sets with at most n max variable nodes. Consider the decoding of F on a Tanner graph I with V (I) being the set of initially corrupt variable nodes. Let n I = |V (I)|. If F fails on the subgraph I of I then E r I (F ) = {I} and we have found the trapping set proﬁle. If F does not fail on the subgraph I of I, then we expand I by recursively adding variable nodes to I until a trapping set is found. During this process, we only add variable nodes that become corrupt at the end of a certain iteration.\nConsider all possible bipartite graphs obtained by adding one variable node, namely v n I +1 , to the graph I such that when the decoding is performed on these graphs with V (I) being the set of initially corrupt variable nodes, the newly added variable node is a corrupt variable node at the end of the ﬁrst iteration, i.e., w 1 v nI +1 ∈ {1 w , 1 s }. Let O I denote the set of such graphs. Take one graph in O I and denote it by U . Then, there can be two different scenarios in this step. First, F does not fail on the subgraph I of U . In this case, U is certainly not a trapping set and we put U in a set of Tanner graphs denoted by E 1 I . Second, F fails on the subgraph I of U . In this case, U can be a trapping set and a test is carried out to determine if U is indeed one. If U is not a trapping set then it is discarded. We complete the formation of E 1 I by repeating the above step for all other graphs in O I .\nLet us now consider a graph U ∈ E 1 I . Again, we denote by O U the set of Tanner graphs obtained by adding one variable node, namely v n I +2 , to the graph U such that when the decoding is performed on these graphs with V (I) being the set of initially corrupt variable nodes, the newly added variable node is a corrupt variable node at the end of the ﬁrst iteration, i.e., w 1 v\n∈ {1 w , 1 s }. It is important to note that the addition of variable node v n I +2 , which is initially correct, cannot change the fact that variable node v n I +1 is also corrupt at the end of the ﬁrst iteration. This is because the addition of correct variable nodes to a graph does not change the states of the existing check nodes and the decoding dynamic until the newly added variable nodes get corrupted. Similar to what have been discussed before, we now take a graph in O U and determine if it is a trapping set, or it is to be discarded, or it is a member of the set of Tanner graph E 2 I . By repeating this step for all other graphs in E 1 I , all graphs in E 2 I can be enumerated. In a similar fashion, we obtain E 3 I , E 4 I , . . . , E (n max −n I ) I \t . For the sake of convenience, we also let E 0 I = {I}.\nAt this stage, we have considered one decoding iteration on I. It can be seen that if S is a trapping set with at most n max variable nodes then either S has been found, or S must contain a graph in (n max −n I −1) i=0 \t E i I . Therefore, we proceed by expanding graphs in E I = (n max −n I −1) i=0 \t E i I .\nLet K denote a Tanner graph in E I = (n max −n I −1) i=0 \t E i I . We now repeat the above graph expanding process with K being the input. Speciﬁcally, we ﬁrst obtain O K , which is deﬁned as the set of all Tanner graphs obtained by adding one variable node v n K +1 to the graph K such that when decoding is performed on these graphs with V (I) being the set of initially corrupt variable nodes, the newly added variable node is a corrupt variable node at the end of the second iteration, but not a corrupt variable node at the end of the ﬁrst iteration, i.e., w 1 v nK +1 ∈ {0 w , 0 s } and w 2 v nK +1 ∈ {1 w , 1 s }. Graphs in O K that are not trapping sets are either discarded or to form the set E 1 K . By recursively adding variable nodes, graphs in E 2 K , E 3 K , . . . , E n max −n I K \t are enumerated.\nOne can see that there are two recursive algorithms. The ﬁrst algorithm enumerates graphs in E K = (n max −n I ) i=0 \t E i K for a given graph K by recursively adding variable nodes. The second algorithm recursively calls the ﬁrst algorithm to enumerates graphs in E K = (n max −n I ) i=0 \t E i K for each graph K in E I = (n max −n I −1) i=0 \t E i I . Each recursion of the second algorithm corresponds to a decoding algorithm. As a result, the trapping set proﬁle is obtained after l m F recursions of the second algorithm.\nDue to page limits, we only summarize the most important criteria for selecting TBF algorithms. Let us ﬁrst brieﬂy discuss the number of possible algorithms.\nLet Q be the set of all functions from A v × Ξ d v → A v that satisfy the symmetry and the irreducibility condition. Due the symmetry condition, |Q| ≤ 4 2×|Ξ dv | . There are two possible\nvalues of ∆ v , and two possible values of ∆ c . However, with a given ∆ c , the two sets of algorithms F that correspond to two possible ∆ v are identical (as 0 s and 0 w , 1 s and 1 w can be interchanged). Consequently, if we disregard the maximum number of iterations, then | F | = 2|Q| ≤ 2 (4|Ξ dv |+1) . One can easily show that |Ξ d v | = d v +3 3 . Therefore, an upper-bound on the number of TBF algorithms is:\nFor example, this upper-bound is 2 81 when d v = 3, and is 2 141 when d v = 4.\nDue to the huge number of possible algorithms, it is necessary to focus on a small subset of algorithms. This subset of algorithms may be obtained by imposing certain constraints on the function f . One example of such a constraint is as follows: if f (0 s , ξ) ∈ {1 w , 1 s } then f (0 w , ξ) ∈ {1 w , 1 s }. This constraint requires that when a strong zero variable node is ﬂipped with a given combination of check nodes, a weak variable node is also ﬂipped with the same check node combination. Other constraints on f are derived by analyzing possible transitions of variable nodes and check nodes for a small number of iterations.\nWe ﬁrst discuss the main criterion to select one algorithm among all possible algorithms. Let n min I,F be the smallest num- ber of variable nodes of Tanner graphs in E r I (F ). We would like to select an algorithm F such that n min I,F is maximized. The justiﬁcation for this selection criterion relies on the following proposition, whose proof is omitted due to page limits.\nProposition 3: Given three random Tanner graph G, S 1 , S 2 with 0 < |V (S 1 )| < |V (S 2 )| < |V (G)|, the probability that G contains S 2 is less than the probability that G contains S 1 .\nFrom Proposition 3, one can see that the larger the number |V (S)| of a given Tanner graph S is, the easier it would be (if at all possible) to construct a Tanner graph G that does not contain S. Therefore, a larger n min I,F means that the sufﬁcient condition for the convergence of F can be met with higher probability. In this sense, an algorithm F with a larger n min I,F is more favorable.\n, then one can derive other comparison criteria based on E r I (F 1 ) and E r I (F 2 ), and/or compare F 1 and F 2 with a different assumption of I. For example, the probability of a graph G containing a trapping set S can be also be evaluated based on |C(S)|.\nWe now consider the problem of selecting of multiple algorithms. The basis for this selection is that one should select good individual algorithms with diverse trapping set proﬁles. In this paper, we only consider decoder D with algorithms F 1 , F 2 , . . . , F p operating in parallel, i.e., the received vector of the channel is the input vector for all algorithms. Note that one can also use trapping set proﬁles to select algorithms that operate in serial, i.e., the output from one algorithm is\nthe input to another. For a decoder D that employs parallel algorithms, the concept of trapping sets and trapping set proﬁles can be deﬁned in the same manner as trapping sets and trapping set proﬁles for a single TBF algorithm. One can easily modify the recursive procedures given in Section IV-B to generate trapping set proﬁles of the decoder D. Then, D can be designed with the same criterion discussed in the previous subsection.\nRemark: Knowledge on the Tanner graph of a code C can be used in the selection of algorithms. For example, if it is known that the Tanner graph of C does not contain a certain subgraph Y , then all graphs containing Y must be removed from a trapping set proﬁle.\nAs an example, we describe a selection of TBF algorithms for regular column-weight-three LDPC codes with girth g = 8. For simplicity, we let ∆ v = (0 s , 1 s ), ∆ c = (0 p , 1 p ) and l m F = 30 for all algorithms. By imposing certain constraints on the functions f , we obtain a set of 21, 962, 496 TBF algorithms. Out of these, there are 360, 162 algorithms which can correct any weight-three error pattern. Such an algorithm is capable of correcting any weight-three error pattern because its trapping set proﬁle E r I (F ) with any inducing set I con- taining three variable nodes is empty. Since all weight-three error patterns can be corrected with a single algorithm, our next step is to select a collection of algorithms which can collectively correct weight-four and -ﬁve error patterns with high probability. To achieve this goal, we construct all trapping set proﬁles with inducing sets containing four and ﬁve variable nodes for each algorithm. Note that there are 10 possible inducing sets (Tanner graphs with girth g = 8) containing four variable nodes and 24 possible inducing sets containing ﬁve variable nodes. Hence, for each algorithm, we construct a total of 34 trapping set proﬁles. From the trapping set proﬁles of all algorithms, we select a collection of 35 algorithms based on the criterion mentioned in the previous section. Then, we simulate the performance of a decoder D which employs these algorithms in parallel. The maximum total number of iterations of D is 35 × 30 = 1050.\nFigure 1 shows the frame error rate (FER) performance of D on the (155, 64) Tanner code. This code has d v = 3, d c = 5 and minimum distance d min = 20. For comparison, the FER performance of the SPA with a maximum of 100 iterations is also included. It can be seen that the FER performance of D approach (and might surpasses) that of the SPA in the error ﬂoor region. It is also important to note that if we eliminate all trapping sets containing subgraphs that are not present in the Tanner graph of this code, then all the obtained trapping set proﬁles are empty. This indicates that D can correct any error pattern up to weight 5 in the Tanner code.\nFigure 1 also shows the FER performance of D on a quasi- cyclic code C 732 of length n = 732, rate R = 0.75 and minimum distance d min = 12. The FER performance of the SPA is also included for comparison. It can be seen that the slope of the FER curve of D in the error ﬂoor region is higher\nthan that of the SPA. Finally, we remark that the slope of the FER curve of D in the error ﬂoor region is between 5 and 6, which indicates that D can correct error patterns of weight 4 and 5 with high probability. This also agrees with the fact that in our simulation, no weight-four error pattern that leads to decoding failure of D was observed.\nWe remark that the implementation of TBF algorithms operating in parallel can be done with a relatively small number of common logic gates. For example, if a decoder D employs both the TBFA1 and the TBFA2 given in [1], then the implementation of the variable node updates require less than 800 AND-gate inputs and 100 OR-gate inputs. In comparison, the implementation of a 6-bit adder requires 2196 AND-gate inputs and 355 OR-gate inputs while that of a 6-bit comparator requires 1536 AND-gate inputs and 190 OR-gate inputs. One can also expect that the complexity introduced by an additional algorithm would decrease as the number of algorithms increases, because many min-terms in the variable node update logic functions would be already available. More details will be provided in the journal version of this paper.\nThis work is funded by NSF under the grants CCF-0963726, CCF-0830245."},"refs":[{"authors":[{"name":"D. V. Nguyen"},{"name":"M. W. Marcellin"},{"name":"B. Vasic"}],"title":{"text":"Two-bit bit ﬂipping decoding of LDPC codes"}},{"authors":[{"name":"N. Miladinovic"},{"name":"M. Fossorier"}],"title":{"text":"Improved bit-ﬂipping decoding of low- density parity-check codes"}},{"authors":[{"name":"T. J. Richardson"}],"title":{"text":"Error ﬂoors of LDPC codes"}},{"authors":[{"name":"D. V. Nguyen"},{"name":"S. K. Chilappagari"},{"name":"B. Vasic"},{"name":"M. W. Marcellin"}],"title":{"text":"On the construction of structured LDPC codes free of small trapping sets"}},{"authors":[{"name":"L. Dolecek"},{"name":"Z. Zhang"},{"name":"V. Anantharam"},{"name":"M. J. Wainwright"},{"name":"B. Nikolic"}],"title":{"text":"Analysis of absorbing sets and fully absorbing sets of array-based LDPC codes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564961.pdf"},"links":[{"id":"1569565883","weight":6},{"id":"1569566981","weight":6},{"id":"1569566605","weight":6},{"id":"1569566683","weight":6},{"id":"1569566761","weight":6},{"id":"1569566591","weight":6},{"id":"1569552245","weight":6},{"id":"1569564469","weight":6},{"id":"1569565837","weight":6},{"id":"1569565317","weight":6},{"id":"1569564249","weight":6},{"id":"1569565809","weight":6},{"id":"1569558483","weight":6},{"id":"1569566795","weight":6},{"id":"1569564613","weight":6},{"id":"1569566865","weight":6},{"id":"1569564311","weight":6},{"id":"1569566167","weight":6},{"id":"1569563981","weight":6},{"id":"1569566311","weight":6},{"id":"1569555999","weight":6},{"id":"1569566423","weight":6},{"id":"1569559805","weight":6},{"id":"1569565735","weight":6},{"id":"1569565559","weight":13},{"id":"1569566913","weight":6},{"id":"1569566809","weight":53},{"id":"1569566223","weight":6},{"id":"1569565029","weight":6},{"id":"1569565357","weight":6},{"id":"1569566695","weight":6},{"id":"1569566673","weight":6},{"id":"1569566297","weight":13},{"id":"1569565439","weight":13},{"id":"1569565885","weight":6},{"id":"1569565493","weight":13},{"id":"1569566805","weight":6},{"id":"1569565665","weight":6},{"id":"1569566983","weight":6},{"id":"1569565661","weight":6},{"id":"1569566887","weight":6},{"id":"1569566917","weight":6},{"id":"1569566595","weight":6},{"id":"1569566819","weight":6},{"id":"1569565541","weight":6},{"id":"1569566533","weight":20},{"id":"1569564787","weight":6},{"id":"1569561185","weight":6},{"id":"1569566075","weight":6},{"id":"1569566397","weight":6},{"id":"1569558779","weight":6},{"id":"1569566817","weight":6},{"id":"1569564923","weight":6},{"id":"1569565039","weight":6},{"id":"1569565805","weight":6},{"id":"1569565861","weight":6},{"id":"1569565537","weight":6},{"id":"1569565035","weight":6},{"id":"1569564253","weight":6},{"id":"1569564505","weight":6},{"id":"1569565635","weight":6},{"id":"1569566375","weight":6},{"id":"1569564807","weight":6},{"id":"1569566443","weight":6},{"id":"1569565315","weight":6},{"id":"1569560581","weight":6}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T5.5","endtime":"13:10","authors":"Dung Nguyen, Bane Vasić, Michael W. Marcellin","date":"1341579000000","papertitle":"Selecting Two-Bit Bit Flipping Algorithms for Collective Error Correction","starttime":"12:50","session":"S16.T5: Decoding Techniques for LDPC Codes","room":"Kresge Little Theatre (035)","paperid":"1569564961"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
