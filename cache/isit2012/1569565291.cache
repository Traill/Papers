{"id":"1569565291","paper":{"title":{"text":"Multi-stage Coding for Channels With a Rewrite Option and Reversible Input"},"authors":[{"name":"Kittipong Kittichokechai"},{"name":"Tobias J. Oechtering"},{"name":"Mikael Skoglund"}],"abstr":{"text":"Abstract\u2014We consider a problem of constrained multi-stage coding for channels with a rewrite option. It is a natural extension of Weissman\u2019s channels with action-dependent states to the multi- stage coding case where an encoder in each stage observes its own message as well as all previous-stage messages, inputs, and outputs. In addition to decoding all messages at the ﬁnal stage, the new reconstruction constraint introduced in Sumszyk and Steinberg\u2019s information embedding with reversible stegotext is imposed on the problem such that the decoder is required to be able to reconstruct all channel input sequences reliably. The complete characterization of the channel capacity region is given for the two-stage case, while the inner and outer bounds to the capacity regions for the cases of three or more stages are provided. For the two-stage case, a discussion regarding the rate constraint of the message in the second stage is also given in which we can draw a connection to the two-stage coding condition which appears in our previous study on channel with action-dependent state and reversible input."},"body":{"text":"The problem of coding for channels with random states has received considerable attention due to its broad set of applications, e.g., in a computer memory with defective cells, digital watermarking, etc. In [1] Gel\u2019fand and Pinsker solved the problem of coding for channels with states where the states are known noncausally at the encoder. Several extensions are then considered such as the cases with causal, two-sided, and partial state information [2].\nIn most of the previous works on coding for channels with random states, the channel states are usually assumed to be generated by nature, i.i.d. according to some distribution. Recently, Weissman in [3] considered a problem in which the channel state is allowed to depend on a message-dependent action sequence. This novel action-dependent coding frame- work introduces new interesting features to the general system model involving cost-constrained communication and interac- tion among nodes, and is therefore highly relevant to many applications including sensor networking and control, and multistage coding for memories. It is also closely related to the problem of a MAC with common message and states at one transmitter as mentioned in [3], and it has some connection to the relay with unlimited lookahead channel [4]. Additional work on coding with action includes [5], [6], [7], and [8].\nAnother interesting extension of the Gel\u2019fand-Pinsker prob- lem is studied by Sumszyk and Steinberg [9] in the context of information embedding where apart from decoding the embedded message, the decoder has an additional constraint on reconstructing the channel input signal (stegotext) reliably\n(reversible stegotext). As with action-dependent coding, also the framework of additional reconstruction requirements pro- vides new useful features of simultaneous signal transmission in the general system model.\nIn our recent works [10], [11], we considered the combined setting of Weissman [3] and Sumszyk and Steinberg [9] where the channel state is allowed to depend on the action sequence and the decoder is interested in decoding both message and channel input signal reliably. In [11] we provided a complete characterization of the channel capacity in which we have shown that the two-stage coding condition plays a role in restricting the set of capacity achieving input distributions. In fact, this condition arises from the causal structure of the system in which we require to reconstruct the signal generated in the second stage. We also showed by an example that the condition can be active in computing the capacity.\nTo gain more insight into the structure of the problem as well as the role of the two-stage coding condition, in this work we study a natural extension of the previously considered problem in which we add more encoding stages (K stages) and in each stage a channel encoder observes its own message and all previous-stage messages. The channel output of each transmission stage will play a role of channel state in the next-stage transmission and it is assumed to be available noncausally to the next-stage encoder as a channel state information. The encoder in each stage is assumed to have memory, i.e., all previous channel inputs and outputs are known. At the ﬁnal stage, the decoder is interested in decoding all messages as well as all channel input sequences. See Fig. 1 for the case of K = 3. This setup can be seen as a multi-stage coding for channels with a \u201crewrite option\u201d based on the noise- free feedback, i.e., the writing output is given noncausally to the next-stage encoder as a channel state information, and the encoder has an option to make another pass where it may rewrite at whichever locations it chooses [3]. Our setting is\nmotivated by scenarios in multiple writing on a memory cell with a rewrite option where the decoder is interested in both decoding the embedded messages and in tracing what has been written in the previous stages (reversible input).\nWe give a single-letter characterization of the capacity region for the case of two-stage coding (K = 2). Interest- ingly but perhaps not surprisingly, this helps to establish a connection to the result in our previously studied problem on channel with action-dependent state and reversible input. That is, the two-stage coding condition noted in [10], [11] can simply be seen as a degenerate rate constraint derived from the underlying rate constraint in this multi-stage coding setup. For K ≥ 3, we provide inner and outer bounds to the capacity region. The inner bound can be achieved by a straightforward extension of the coding scheme for the case of K = 2. As for proving the tightness of bounds, we ﬁnd it challenging due to the structure of our problem formulation which prevents us from having the desired Markov chains to account for the dependency of the channel states on the channel outputs.\nThe rest of the paper is organized as follows. In Section II we formulate the problem and present the capacity region for the two-stage case. The discussion on how the result is related to another variation of the problem is then given. Section III discusses the connection between a rate constraint in the second stage transmission and the two-stage coding condition noted in [10], [11]. Lastly, inner and outer bounds to the capacity region for K ≥ 3 is given in Section IV.\nWe ﬁrst provide the problem formulation for the general K-stage setting. Then we will focus on the case of K = 2 where we are able to derive the capacity result. The discussion on how the result is related to other problems is also given.\nLet n denote the block length and X i , Y i , i = 1, . . . , K be ﬁnite sets. The setting consists of K encoders and one decoder. Message M i chosen uniformly from the set M (n) i = {1, 2, . . . , |M (n) i |} is given to the encoders i, . . . , K, for all i = 1, . . . , K. The encoder in each stage is assumed to observe as well all previous-stage inputs and outputs. A set of channels is described by a set of tuples {(X i , Y i −1 , P Y i |X i ,Y i−1 , Y i )}, for i = 1, . . . , K, where X i , Y i , and P Y i |X i ,Y i−1 are the input alphabet, the output alphabet, and the transition probability from X i × Y i −1 to Y i in stage i, respectively. The decoder, which might be considered as two separate decoders, i.e., message decoder and channel input decoder, decodes all the\nmessages and channel inputs based on the ﬁnal-stage channel output Y n K . The channels in all stages i = 1, . . . , K are assumed to be memoryless and used without feedback with transition probabilities,\nDeﬁnition 1: An (|M (n) 1 |, . . . , |M (n) K |, n) code for the channels {P Y i |X i ,Y i−1 } i =1,...,K consists of the following func- tions: encoders\nf (n) i : M (n) 1 × . . . × M (n) i × X n 1 × . . . × X n i −1 × Y n 1 × . . . × Y n i −1 → X n i , i = 1, . . . , K\nDeﬁnition 2: A rate tuple (R 1 , . . . , R K ) is said to be achievable if for any δ > 0 there exists for all sufﬁciently large n an (|M (n) 1 |, . . . , |M (n) K |, n)-code such that 1 n log |M (n) i | ≥ R i − δ, for i = 1, . . . , K, P (n) m,e ≤ δ, and P (n) x,e ≤ δ, where P (n) m,e and P (n) x,e are the average error probabilities that (M 1 , M 2 , . . . , M K ) = g (n) m (Y n K ) and (X n 1 , X n 2 , . . . , X n K ) = g (n) x (Y n K ), respectively. The capacity region C (K) is the closure of the set of all achievable rate tuples.\nWe ﬁrst consider the case of K = 2 for which we can derive the capacity region. In this case, the message M 1 is given to both encoder 1 and 2, and M 2 is given only to the encoder 2. The ﬁrst-stage channel input sequence X n 1 is chosen based on the message M 1 and is the input to the ﬁrst-stage channel which is described by a triple (X 1 , P Y 1 |X 1 , Y 1 ). The output Y n 1\nthen becomes the channel state of the second-stage channel which is described by a quadruple (X 2 , Y 1 , P Y 2 |X 2 ,Y 1 , Y 2 ), and is also given to the encoder 2 noncausally. The decoder decodes the messages (M 1 , M 2 ) and the input (X n 1 , X n 2 ) based on the ﬁnal-stage channel output Y n 2 .\nTheorem 1: The capacity region C (2) in the two-stage setup is given by the set of all (R 1 , R 2 ) satisfying\nR 1 + R 2 ≤ I(X 1 , X 2 ; Y 2 ) − I(X 2 ; Y 1 |X 1 ), \t (1) R 2 ≤ I(X 2 ; Y 2 |X 1 ) − I(X 2 ; Y 1 |X 1 ), \t (2)\nProof: The achievability proof is based on a standard random coding argument using Gel\u2019fand-Pinsker coding, and is a straightforward extension of that in [10]. The sketch of the proof for a general K-stage problem is given in the appendix. For the converse proof we refer readers to the one given for the K-stage problem, K ≥ 3, in Section IV in which we have to apply with some modiﬁcations at the end of the proof.\nSo far we have characterized the capacity region of the two- stage problem formulated with the reversible input require- ment. It is also natural to consider a new and slightly different communication problem where the decoder is interested in decoding the messages (M 1 , M 2 ) and the \u201cchannel state\u201d Y n 1 instead. Due to the deterministic encoding functions, the channel input (X n 1 , X n 2 ) can be retrieved based on the decoded messages and state information. We note that this communica- tion problem has a more demanding reconstruction constraint than our previous problem since it essentially requires that the decoder can decode the message, the state, and the channel input signal, all reliably. We show that if the objective is to decode only the message and the channel input, then decoding the message and the state ﬁrst, and then re-encoding the channel input is suboptimal.\nProposition 1: Consider the new two-stage communication problem in which the decoder is interested in decoding the messages (M 1 , M 2 ) and the \u201cchannel state\u201d Y n 1 reliably. The capacity region C (2) Y 1 of such a channel is given by the set of all (R 1 , R 2 ) satisfying\nR 1 + R 2 ≤ I(X 1 , Y 1 , X 2 ; Y 2 ) − H(Y 1 |X 1 ), \t (3) R 2 ≤ I(Y 1 , X 2 ; Y 2 |X 1 ) − H(Y 1 |X 1 ), \t (4)\nProof: Since decoding (M 1 , M 2 ) and Y n 1 implies that (X n 1 , X n 2 ) is also decoded from the deterministic encoding functions, one can substitute (Y 1 , X 2 ) in place of X 2 in Theorem 1 and obtain the new capacity region. More speciﬁ- cally, the achievable scheme in this case is different from the previous case of decoding (M 1 , M 2 ) and (X n 1 , X n 2 ) in that the state information codebook is introduced and it should \u201ccover\u201d all possible generated Y n 1 losslessly . Due to space constraint, the detailed achievability and converse proofs are omitted.\nRemark 1: We know that the channel input sequence can be retrieved based on the decoded message, the state information, and a known deterministic encoding function. Therefore, it is natural to compare the capacity region C (2) in Theorem 1 with C (2) Y 1 in Proposition 1. For a given channel P Y 1 |X 1 , P Y 2 |X 2 ,Y 1 , we have that C (2) ⊇ C (2) Y 1 .\nProof: One can show that both terms on the right hand side of (1) and (2) are greater than or equal to those of (3) and (4) for all joint distributions factorized as in (5), and thus conclude that C (2) ⊇ C (2) Y 1 .\nIn this section we will relate our main result for the case of K = 2 to our previous result on capacity of channels with action-dependent state and reversible input considered in [10], [11]. It is obvious from the problem formulation that if the rate of the message M 2 is zero, i.e., no new message to be transmitted at the encoder 2, then our setting simply\nreduces to the problem of channel with action-dependent state and reversible input. In [11], the channel capacity is given in the form with a constraint on the set of input distributions I (X 2 ; Y 2 |X 1 ) − I(X 2 ; Y 1 |X 1 ) ≥ 0 which is called the \u201ctwo- stage coding condition.\u201d This condition arises essentially from the two-stage coding structure of the problem and the extra requirement that the decoder should be able to reconstruct the signal generated in later stages. In addition to the rate constraint on the message, the two-stage coding condition can be interpreted as a necessary and sufﬁcient condition for reliable transmission of the channel input signal over the channel in second stage transmission.\nBy studying the multi-stage coding problem in this paper, we can straightforwardly connect the two-stage coding condi- tion to the result for K = 2 in this paper. That is, the two-stage coding condition can be seen as a degenerate rate constraint derived from the underlying rate constraint of the message M 2 in the multi-stage coding setting. It is therefore not surprising that this condition is present in the capacity expression in [11], and in fact it can be active when computing the capacity as shown in an example in [12].\nWe now consider an extension to the multi-stage setting in which we characterize the inner and outer bounds to the capac- ity region. The inner bound can be derived as a straightforward extension of the two-stage setting, while we ﬁnd it challenging to generalize the converse proof for K ≥ 3. Based on our setting, the main difﬁculty in deriving tight bounds lies in the structure of the problem which prevents us from having the desired Markov chains to account for the dependency between the channel outputs and the channel states.\nTheorem 2 (Inner Bound to the Capacity Region): The rate tuple (R 1 , . . . , R K ) is achievable if\nP Y i |X i ,Y i−1 (y i |x i , y i −1 ) \t (6) Proof: The proof is a straightforward extension of the\nachievability proof for the two-stage case and its sketch is given in the appendix.\nany achievable rate tuple (R 1 , . . . , R K ), K ≥ 3, it follows that\nProof: For any achievable rate tuple (R 1 , . . . , R K ), we have that for all j = 1, . . . , K,\n= H(M K j , Y Y Y K −1 j −1 |W j ) − H(Y Y Y K −1 j −1 |M K j , W j ) − H(M K j , X X X K j |Y Y Y K , W j )\nY Y K −1 j −1 , X X X K j |W j ) − H(Y Y Y K −1 j −1 |M K j , W j ) − H(M K j , X X X K j |Y Y Y K , W j )\nwhere (a) follows from the independency M K j ⊥W j , where W j \t (M j −1 1 , X X X j −1 1 , Y Y Y j −2 1 ), and from Fano\u2019s inequality, i.e., H (M K j , X X X K j |Y Y Y K , W j ) ≤ nǫ (j) n , lim n →∞ ǫ (j) n = 0, (b) follows from the deterministic encoding function X X X i = f (n) i (M i 1 , X X X i −1 1 , Y Y Y i −1 1 ), i = j, . . . , K, (c) holds since X X X i = f (n) i (M i 1 , X X X i −1 1 , Y Y Y i −1 1 ), and Y Y Y i −1 −W i −M K i forms a Markov chain with W i = (M i −1 1 , X X X i −1 1 , Y Y Y i −2 1 ), and conditioning reduces entropy. Note that the term W i is introduced for the sake of readability and it essentially captures the previous variables known before the encoding stage i.\nwhere (d) follows from Csisz´ar\u2019s sum identity [13], i.e., for i = j, . . . , K, n l =1 I (Y n i −1,l+1 ; Y K,l |W i , M i , X X X i , Y l −1 K, 1 ) −\nNext, we replace W i by (M i −1 1 , X X X i −1 1 , Y Y Y i −2 1 ) again and continue the chain of inequalities.\nwhere (e) holds using a telescoping series to bound the sum from i = j to i = K of the ﬁrst mutual informa- tion term, (f ) follows since conditioning reduces entropy, and from the Markov chain Y i −1,l − ((X i −1 1 ) l , (Y i −2 1 ) l ) −\nwhere (♯) holds since conditioning reduces entropy and we have the Markov chains Y K,l − ((X K 1 ) l , Y K −1,l ) − ((Y K −2 1 ) l , Z K ). As for j = K, we have\nwhere (∗) follows from the Markov chain Y K,l − ((X K 1 ) l , (Y K −1 1 ) l ) − Z K . Note that when K = 2, the above steps for j = 1 will not result in the desired expression. To ﬁx this, we can use similar steps as in the case j = K for both j = 1, 2 instead.\nTo this end, we introduce a random variable Q distributed uniformly over {1, . . . , n} and independent of all others. We have P Y i,Q |X i,Q ,Y i−1,Q = P Y i |X i ,Y i−1 for all Q, and i = 1, . . . , K, and we can identify X i,Q X i and Y i,Q Y i . Since Y i −(X i , Y i −1 )−(X i −1 1 , Y i −2 1 , Q ) forms a Markov chain for all i = 1, . . . , K, we have\nRemark 2: In general we do not know how good the inner and outer bounds are for the case of K ≥ 3. However, for a degenerate case where in each stage the channel output is conditionally independent of the state given the input, the bounds will coincide and provide the result of the point-to- point case as expected from inspection, i.e., the optimal input distribution turns out to be the one which is independent of all previous states and inputs.\nIn this work we studied an extension of the problem of coding with action-dependent state and reversible input to the multistage setting. The capacity region for the two-stage case is found, and inner and outer bounds to the capacity region are given in the case of three stages or more. We were not able to establish bounds that match in the case K ≥ 3 due to the structure of our problem which prevents us from having the desired Markov chains to account for the dependency of the channel states on the channel outputs. For the two- stage problem, it turned out that the capacity result helps us to establish a connection to our previous result on the two- stage coding condition. It shows that the two-stage coding condition is simply the degenerate rate constraint derived from the underlying rate constraint on the message in the second stage. Our inner and outer bounds also suggest the presence of similar two-stage coding conditions in the multi-stage setting, i.e., the rate constraint on the message in later stages implicitly includes the condition for reliable transmission of the channel input signals over the channels in later stages.\ntion consists of randomly generating 2 nR 1 codewords X n 1 (m 1 ) each i.i.d. according to n l =1 P X 1 (x 1,l (m 1 )), and for each X n 1 (m 1 ), generating 2 n (R 2 +R \u2032 2 ) codewords X n 2 (m 1 , m 2 , m \u2032 2 )\neach i.i.d. according to \t n l =1 P X 2 |X 1 (x 2,l |x 1,l (m 1 )). Next, for each pair of (X n 1 , X n 2 ), generate 2 n (R 3 +R \u2032 3 ) codewords X n 3 (m 1 , m 2 , m \u2032 2 , m 3 , m \u2032 3 ) each i.i.d. according to \t n l =1 P X 3 |X 1 ,X 2 (x 3,l |x 1,l , x 2,l ). We continue this process until we have generated 2 n (R K +R \u2032 K ) codewords X n K (m 1 , m 2 , m \u2032 2 , . . . , m K , m \u2032 K ) for all (X n 1 , . . . , X n K −1 ).\nGiven the messages (m 1 , . . . , m i ) and the previous inputs and outputs (X n 1 , . . . , X n i −1 ), (Y n 1 , . . . , Y n i −1 ), i = 1, . . . , K, the encoder in stage i looks for and transmits the codeword X n i that is jointly typical with (X n 1 , . . . , X n i −1 , Y n 1 , . . . , Y n i −1 ). By joint typicality property (covering lemma) [4], we ensure the vanishing probability of encoding error as n → ∞ if R \u2032 i > I (X i ; Y 1 , . . . , Y i −1 |X 1 , . . . , X i −1 ) for all i = 2, . . . , K.\nGiven the ﬁnal-stage output Y n K , the decoder looks for (X n 1 , . . . , X n K ) that is jointly typical with it. By joint typi- cality property (packing lemma) [4], we ensure the vanishing probability of decoding error as n → ∞ if R 1 + R 2 + R \u2032 2 + . . . + R K + R \u2032 K < I (X 1 , . . . , X K ; Y K ), and K i =j R i + R \u2032 i <\nBy these results together with random coding argument, we obtain that the rate tuple (R 1 , . . . , R K ) is achievable if\nThe authors wish to thank the anonymous reviewers and Hieu Do for helpful comments and discussions. This work was supported in part by the Swedish Research Council and the Swedish Foundation for Strategic Research."},"refs":[{"authors":[{"name":"S. I. Gel\u2019fand"},{"name":"M. S. Pinsker"}],"title":{"text":"Coding for channel with random parameters"}},{"authors":[{"name":"G. Keshet"},{"name":"Y. Steinberg"},{"name":"N. Merhav"}],"title":{"text":"Channel coding in the presence of side information"}},{"authors":[{"name":"T. Weissman"}],"title":{"text":"Capacity of channels with action-dependent states"}},{"authors":[{"name":"A. E. Gama"},{"name":"Y. H. Ki"}],"title":{"text":"Network Information Theory"}},{"authors":[{"name":"H. Permuter"},{"name":"T. Weissman"}],"title":{"text":"Source coding with a side information \u201cVending Machine\u201d"}},{"authors":[{"name":"H. Asnani"},{"name":"H. Permuter"},{"name":"T. Weissman"}],"title":{"text":"Probing capacity"}},{"authors":[{"name":"Y.-K. Chia"},{"name":"H. Asnani"},{"name":"T. Weissman"}],"title":{"text":"Multi-terminal source coding with action dependent side information"}},{"authors":[{"name":"B. Ahmadi"},{"name":"O. Simeone"}],"title":{"text":"Robust coding for lossy computing with receiver-side observation costs"}},{"authors":[{"name":"O. Sumszyk"},{"name":"Y. Steinberg"}],"title":{"text":"Information embedding with reversible stegotext"}},{"authors":[{"name":"K. Kittichokechai"},{"name":"T. J. Oechtering"},{"name":"M. Skoglund"}],"title":{"text":"On the capacity of a channel with action-dependent state and reversible input"}},{"authors":[{"name":"K. Kittichokechai"},{"name":"T. J. Oechtering"},{"name":"M. Skoglund"}],"title":{"text":"Capacity of the channel with action-dependent state and reversible input"}},{"authors":[{"name":"K. Kittichokechai"},{"name":"T. J. Oechtering"},{"name":"M. Skoglund"}],"title":{"text":"Coding with action-dependent side information and additional reconstruction require- ments"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K ¨orner"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems "}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565291.pdf"},"links":[{"id":"1569566381","weight":4},{"id":"1569566527","weight":9},{"id":"1569565383","weight":4},{"id":"1569566725","weight":9},{"id":"1569565377","weight":4},{"id":"1569564635","weight":4},{"id":"1569566605","weight":4},{"id":"1569566683","weight":9},{"id":"1569566943","weight":4},{"id":"1569566571","weight":4},{"id":"1569552245","weight":9},{"id":"1569564481","weight":4},{"id":"1569565931","weight":9},{"id":"1569551535","weight":4},{"id":"1569565461","weight":4},{"id":"1569564731","weight":4},{"id":"1569566671","weight":4},{"id":"1569564233","weight":4},{"id":"1569564203","weight":13},{"id":"1569566751","weight":4},{"id":"1569566467","weight":9},{"id":"1569565771","weight":4},{"id":"1569566999","weight":4},{"id":"1569566579","weight":9},{"id":"1569565455","weight":9},{"id":"1569566709","weight":13},{"id":"1569564989","weight":4},{"id":"1569564189","weight":4},{"id":"1569566095","weight":4},{"id":"1569565907","weight":18},{"id":"1569566753","weight":4},{"id":"1569558681","weight":9},{"id":"1569559995","weight":4},{"id":"1569565213","weight":9},{"id":"1569566531","weight":4},{"id":"1569567665","weight":9},{"id":"1569561143","weight":4},{"id":"1569565833","weight":4},{"id":"1569553909","weight":22},{"id":"1569553537","weight":18},{"id":"1569552251","weight":4},{"id":"1569553519","weight":9},{"id":"1569566513","weight":4},{"id":"1569554881","weight":9},{"id":"1569554971","weight":4},{"id":"1569566209","weight":4},{"id":"1569565655","weight":4},{"id":"1569565151","weight":4},{"id":"1569558985","weight":4},{"id":"1569566473","weight":4},{"id":"1569565033","weight":18},{"id":"1569565887","weight":4},{"id":"1569565633","weight":4},{"id":"1569555879","weight":4},{"id":"1569565219","weight":9},{"id":"1569566223","weight":4},{"id":"1569564969","weight":9},{"id":"1569565029","weight":4},{"id":"1569561245","weight":4},{"id":"1569566505","weight":4},{"id":"1569566603","weight":4},{"id":"1569565467","weight":4},{"id":"1569566667","weight":4},{"id":"1569564097","weight":9},{"id":"1569560997","weight":4},{"id":"1569566501","weight":4},{"id":"1569560503","weight":9},{"id":"1569565439","weight":4},{"id":"1569562551","weight":9},{"id":"1569551347","weight":4},{"id":"1569565415","weight":9},{"id":"1569555367","weight":4},{"id":"1569566383","weight":9},{"id":"1569565571","weight":4},{"id":"1569566929","weight":4},{"id":"1569565765","weight":4},{"id":"1569557275","weight":4},{"id":"1569565919","weight":9},{"id":"1569565181","weight":4},{"id":"1569566267","weight":9},{"id":"1569565421","weight":4},{"id":"1569566651","weight":4},{"id":"1569565013","weight":4},{"id":"1569566641","weight":13},{"id":"1569551905","weight":9},{"id":"1569556759","weight":18},{"id":"1569566619","weight":4},{"id":"1569561185","weight":4},{"id":"1569566301","weight":4},{"id":"1569558779","weight":4},{"id":"1569565669","weight":4},{"id":"1569566435","weight":4},{"id":"1569566577","weight":4},{"id":"1569557851","weight":4},{"id":"1569565389","weight":4},{"id":"1569562367","weight":4},{"id":"1569555891","weight":4},{"id":"1569550425","weight":40},{"id":"1569565889","weight":4},{"id":"1569565635","weight":4},{"id":"1569565113","weight":4},{"id":"1569566375","weight":4},{"id":"1569564257","weight":9},{"id":"1569566555","weight":4},{"id":"1569564931","weight":9},{"id":"1569564141","weight":13},{"id":"1569564509","weight":9},{"id":"1569551541","weight":4},{"id":"1569566113","weight":9}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S17.T4.4","endtime":"16:20","authors":"Kittipong Kittichokechai, Tobias J. Oechtering, Mikael Skoglund","date":"1341590400000","papertitle":"Multi-stage Coding for Channels With a Rewrite Option and Reversible Input","starttime":"16:00","session":"S17.T4: Communication Models","room":"Stratton 20 Chimneys (306)","paperid":"1569565291"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
