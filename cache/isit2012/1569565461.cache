{"id":"1569565461","paper":{"title":{"text":"Gaussian Rate-Distortion via Sparse Linear Regression over Compact Dictionaries"},"authors":[{"name":"Ramji Venkataramanan"},{"name":"Antony Joseph"},{"name":"Sekhar Tatikonda"}],"abstr":{"text":"Abstract\u2014We study a class of codes for compressing memory- less Gaussian sources, designed using the statistical framework of high-dimensional linear regression. Codewords are linear combi- nations of subsets of columns of a design matrix. With minimum- distance encoding we show that such a codebook can attain the rate-distortion function with the optimal error-exponent, for all distortions below a speciﬁed value. The structure of the codebook is motivated by an analogous construction proposed recently by Barron and Joseph for communication over an AWGN channel."},"body":{"text":"One of the important outstanding problems in informa- tion theory is the development of practical codes for lossy compression of general sources at rates approaching Shan- non\u2019s rate-distortion bound. In this paper, we study the compression of memoryless Gaussian sources using a class of codes constructed based on the statistical framework of high-dimensional linear regression. The codebook consists of codewords that are sparse linear combinations of columns of an n × N design matrix or \u2018dictionary\u2019, where n is the block- length and N is a low-order polynomial in n. Dubbed Sparse Superposition Codes or Sparse Regression Codes (SPARC), these codes are motivated by an analogous construction pro- posed recently by Barron and Joseph for communication over an AWGN channel [1], [2]. The structure of the codebook enables the design of computationally efﬁcient encoders based on the rich theory on sparse linear regression and sparse approximation. Here, the performance of these codes under minimum-distance encoding is studied. The design of compu- tationally feasible encoders will be discussed in future work.\nSparse regression codes for compressing Gaussian sources were ﬁrst considered in [3] where some preliminary results were presented. In this paper, we analyze the performance of these codes under optimal (minimum-distance) encoding and show that they can achieve the distortion-rate bound with the optimal error exponent for all rates above a speciﬁed value (approximately 1.15 bits/sample). The proof uses Suen\u2019s inequality [4], a bound on the tail probability of a sum of dependent indicator random variables. This technique may be of independent interest and useful in other problems in information theory.\nWe lay down some notation before proceeding further. Upper-case letters are used to denote random variables, lower- case for their realizations, and bold-face letters to denote\nrandom vectors and matrices. All vectors have length n. The source sequence is denoted by S (S 1 , . . . , S n ), and the re- construction sequence by ˆ S ( ˆ S 1 , . . . , ˆ S n ). X denotes the\nversion. We use natural logarithms, so entropy is measured in nats. f (x) = o(g(x)) means lim x→∞ f (x)/g(x) = 0; f (x) = Θ(g(x)) means f (x)/g(x) asymptotically lies in an interval [k 1 , k 2 ] for some constants k 1 , k 2 > 0.\nConsider an i.i.d Gaussian source S with mean 0 and variance σ 2 . A rate-distortion codebook with rate R and block length n is a set of e nR length-n codewords, denoted {ˆ S(1), . . . , ˆ S(e nR )}. The quality of reconstruction is measured through a mean-squared error distortion criterion\nwhere ˆ S is the codeword chosen to represent the source sequence S. For this distortion criterion, an optimal encoder maps each source sequence to the codeword nearest to it in Euclidean distance. The rate-distortion function R ∗ (D), the minimum rate for which the distortion can be bounded by D with high-probability, is given by [5]\n(1) This rate can be achieved through Shannon-style random\ncodebook selection: pick each codeword independently as an i.i.d Gaussian random vector distributed as Normal(0, σ 2 −D). Such a code has encoding complexity which grows exponen- tially with block length. Lattice-based codes for Gaussian vec- tor quantization have been widely studied, e.g [6], [7]. There are computationally efﬁcient quantizers for certain classes of lattice codes, but the high-dimensional lattices needed to approach the rate-distortion bound have exponential encoding complexity [7]. We also note that for sources with ﬁnite alpha- bet, various coding techniques have been proposed recently to approach the rate-distortion bound with computationally feasible encoding and decoding [8]\u2013[11].\nA sparse regression code (SPARC) is deﬁned in terms of a design matrix A of dimension n × M L whose entries are zero\nmean i.i.d Gaussian random variables with variance a 2 −D L , where the constant a 2 will be speciﬁed in the sequel. Here n is the block length and M and L are integers whose values will be speciﬁed shortly in terms of n and the rate R. As shown in Figure 1, one can think of the matrix A as composed of L sections with M columns each. Each codeword is the sum of L columns, with one column from each section. More formally, a codeword can be expressed as Aβ, where β is a binary-valued M L × 1 vector (β 1 , . . . , β M L ) with the following property: there is exactly one non-zero β i for 1 ≤ i ≤ M , one non-zero β i for M + 1 ≤ i ≤ 2M , and so forth. Denote the set of all β\u2019s that satisfy this property by B M,L .\nMinimum-distance Encoder : This is deﬁned by a mapping g : R n → B M,L . Given the source sequence S n , the encoder determines the β that produces the codeword closest in Euclidean distance, i.e.,\nDecoder : This is a mapping h : B M,L → R n . On receiving β ∈ B M,L from the encoder, the decoder produces reconstruc- tion h(β) = Aβ.\nSince there are M columns in each of the L sections, the total number of codewords is M L . To obtain a compression rate of R nats/sample, we therefore need\nThere are several choices for the pair (M, L) which satisfy this. For example, L = 1 and M = e nR recovers the Shannon- style random codebook in which the number of columns in the dictionary A is e nR , i.e., exponential in n. For our constructions, we choose M = L b for some b > 1 so that (2) implies\nin the dictionary A is now Θ n log n b+1 , a polynomial in n. This reduction in dictionary complexity can be harnessed to develop computationally efﬁcient encoders for the sparse regression code. We note that the code structure automatically yields low decoding complexity.\nSince each codeword in a SPARC is a sum of L columns of A (one from each section), codewords sharing one or more common columns in the sum will be dependent. Also, SPARCs are not linear codes since the sum of two codewords does not equal another codeword in general.\nThe probability of error at distortion-level D of a rate- distortion code C n with block length n and encoder and decoder mappings g, h is\nDeﬁnition 1: The error exponent at distortion-level D of a sequence of rate R codes {C n } n=1,2,... is given by\nn log P e (C n , D). \t (4) The optimal error exponent for a rate-distortion pair (R, D) is the supremum of the error exponents over all sequences of codes with rate R, at distortion-level D.\nThe error-exponent describes the asymptotic behavior of the probability of error; bounds on the probability of error for ﬁnite block lengths were obtained in [12], [13]. The optimal error exponent was obtained by Marton [14] for discrete memoryless sources and was extended to Gaussian sources by Ihara and Kubo [15].\nFact 1: [15] For an i.i.d Gaussian source distributed as Normal(0, σ 2 ) and mean-squared error distortion criterion, the optimal error exponent at rate R and distortion-level D is\n− 1 − log ρ 2 σ 2 R > R ∗ (D) 0 \t R ≤ R ∗ (D)\nFor R > R ∗ (D), the exponent in (5) is the Kullback-Leibler divergence between two zero-mean Gaussian distributions, the ﬁrst with variance ρ 2 and the second with variance σ 2 . [15] shows that at rate R, we can compress all sequences which have empirical variance less than ρ 2 to within distortion D with double-exponentially decaying probability of error. Consequently, the dominant error event is obtaining a source sequence with empirical variance greater than ρ 2 , which has exponent given by (5).\nTheorem 1: Fix a rate R and target distortion D such that σ 2 /D > x ∗ , where x ∗ ≈ 4.913 is the solution of the equation\nFix b > \t 3.5R R−(1−D/ρ 2 ) , where ρ 2 is determined by (6). For every positive integer n, let M n = L b n where L n is determined by (3). Then there exists a sequence C = {C n } n=1,2,... of rate R sparse regression codes - with code C n deﬁned by an n ×\nM n L n design matrix - that attains the optimal error exponent for distortion-level D given by (5).\nRemark : The minimum value of b speciﬁed by the the- orem enables us to construct SPARCs with the optimal error exponent. The proof also shows that we can con- struct SPARCs which achieve the rate-distortion function for b > \t 2.5R R−(1−D/ρ 2 ) , with probability of error that decays sub- exponentially in n when b is less than 3.5R/(R−(1−D/ρ 2 )).\nDue to space constraints, we omit some details in the proof which will be included in a longer version of this paper. Given rate R > R ∗ (D), let ρ 2 be determined by (6). For each a 2 < ρ 2 , we will show that there exists a family of SPARCs that achieves the error exponent 1 2 a 2 σ 2 − 1 − log a 2 σ 2 , thereby proving the theorem.\nCode Construction : For each block length n, pick L as speciﬁed by (3) and M = L b . Construct an n × M L design matrix A with entries drawn i.i.d Normal(0, a 2 −D L ). The codebook consists of all the vectors Aβ, where β ∈ B M,L .\nEncoding and Decoding : If the source sequence S is such that |S| 2 ≥ a 2 , then the encoder declares error. Else, it ﬁnds\nError Analysis : Denoting the probability of error for this random code by P e,n , we have\nwhere E (S) is the event that the minimum of |S − Aβ| 2 over β ∈ B M,L is greater than D, and ν(|S| 2 ) is the distribution of the random variable |S| 2 . The asymptotic behavior of the ﬁrst term above is straightforward to analyze and is given by the following lemma, obtained through a direct application of Cram´er\u2019s large-deviation theorem [16].\nThe rest of the proof is devoted to bounding the second term in (7). Recall that\nwhere ˆ S(i) is the ith codeword in the sparse regression codebook. We now deﬁne indicator random variables U i (S) for i = 1, . . . , e nR as follows:\n(10) For a ﬁxed S, the U i (S)\u2019s are dependent. Suppose that the codewords ˆ S(i), ˆ S(j) respectively correspond to the binary vectors ˆ β(i), ˆ β(j) ∈ B M,L . Recall that each vector in B M,L is uniquely deﬁned by the position of the 1 in each of the L sections. If ˆ β(i) and ˆ β(j) overlap in r of their \u20181 positions\u2019, then the column sums forming codewords ˆ S(i) and ˆ S(j) will share r common terms.\nFor each codeword ˆ S(i), there are L r (M − 1) L−r other codewords which share exactly r common terms with ˆ S(i), for 0 ≤ r ≤ L − 1. In particular, there are (M − 1) L codewords that are pairwise independent of ˆ S(i). We now obtain an upper bound for the probability in (10) using Suen\u2019s correlation inequality [4]. First, some deﬁnitions.\nDeﬁnition 2 (Dependency Graphs [4]): Let {U i } i∈I be a family of random variables (deﬁned on a common probability space). A dependency graph for {U i } is any graph Γ with vertex set V (Γ) = I whose set of edges satisﬁes the following property: if A and B are two disjoint subsets of I such that there are no edges with one vertex in A and the other in B, then the families {U i } i∈A and {U i } i∈B are independent.\nFact 2: [4, Example 1.5, p.11] Suppose {Y α } α∈A is a family of independent random variables, and each U i , i ∈ I is a function of the variables {Y α } α∈A i for some subset A i ⊆ A. Then the graph with vertex set I and edge set {ij : A i ∩ A j = ∅} is a dependency graph for {U i } i∈I .\nRemark 1: The graph Γ with vertex set V (Γ) = {1, . . . , e nR } and edge set e(Γ) given by\nis a dependency graph for the family {U i (S)} e nR i=1 , for each ﬁxed S. This follows from Fact 2 by recognizing that each U i is a function of a subset of the columns of the matrix A and the columns of A are picked independently in the code construction.\nFact 3 (Suen\u2019s Inequality [4]): Let U i ∼ Bern(p i ), i ∈ I, be a ﬁnite family of Bernoulli random variables having a dependency graph Γ. Write i ∼ j if ij is an edge in Γ. Deﬁne\n1 2\nU i = 0 ≤ exp − min λ 2\n, λ 6δ\nWe now apply this inequality with the dependency graph speciﬁed in Remark 1 to compute an upper bound for (10).\nFirst term λ/2: Since each codeword is the sum of L columns of A whose entries are i.i.d Normal(0, a 2 − D),\nE(U i (S)) does not depend on i. For any ﬁxed S with |S| 2 = z 2 , we have\nSecond term λ/δ: Due to the symmetry of the code con- struction,\n(12) Using this together with the expression for λ in (11), we have λ\nwhere we have used M = L b . Since (1 − L −b ) L b → e −1 , using a Taylor expansion we can show that for L sufﬁciently large\nλ δ\nThird Term λ 2 /∆: We lower bound λ 2 /∆ by obtaining a lower bound for λ 2 using Lemma 2, and an upper bound for the denominator ∆ as follows. 1\nwhere E ij (r) is the event that ˆ S(i), ˆ S(j) have exactly r common terms. We have\nwhere the third equality is due to the fact that (ˆ S(i), ˆ S(j)) has the same joint distribution as (Oˆ S(i), Oˆ S(j)) for any orthogonal (rotation) matrix O. The ( ˆ S k (i), ˆ S k (j)) pairs are i.i.d across k, and each is jointly Gaussian with zero-mean vector and covariance matrix\nwhen ˆ S(i), ˆ S(j) share r common terms. Using a two- dimensional Chernoff bound, we have ∀t < 0 and sufﬁciently large n\nUsing (17) in (15) and then in (14), and using Lemma 2 to bound λ 2 , we obtain\nHere and in the sequel, κ denotes a generic positive constant whose exact value is not needed. Using A L to denote the set {1/L, 2/L, . . . , (L − 1)/L}, α to denote r L , and noting that M L = e nR , we have\nDividing through by L log L and using the relation (3) as well as the deﬁnition (20) for γ n , we get log(λ 2 /∆)\nbα + b R\n(C α (t) − log(a 2 /D)) − 1 L\nWe need the right side of the above to be positive since we want λ 2 /∆ to grow with L. For this, we need:\nUsing t = − 1 2D(1+α) for C α (t), we can show that (24) implies the following simpliﬁed condition for sufﬁciently large L:\nWhen b > b min , the right side of (23) will be strictly positive for large enough L.\nMinimum Rate Condition : For (25) to be valid, we need the the denominator on the right side to be positive. Since a 2 is any number less than ρ 2 where R = 1 2 log ρ 2 D , the condition for the denominator to be positive is\na 2 . \t (26) This is satisﬁed whenever a 2 /D > x ∗ as required by the theorem.\nThus for large enough L, (23) becomes log(λ 2 /∆)\nCombining the bounds obtained above for each of the three terms, we have for sufﬁciently large n,\nwhere T 0 = 1 2 ( a 2 σ 2 − 1 − log a 2 σ 2 ) from Lemma 1. Since R = 1 2 log(ρ 2 /D), T 1 grows exponentially in n for all a 2 < ρ 2 n −2/n When b > 2, T 2 = L b−1 grows faster than n = bL log L/R. For\nT 3 also grows faster than n. This corresponds to the minimum value of b speciﬁed in the statement of the theorem. Therefore, under this condition, the probability of error for large n is dominated by the ﬁrst term in (31). This completes the proof.\nWe have studied a new ensemble of codes for Gaussian source coding where the codewords are structured linear combinations of elements of a dictionary. The size of the dictionary is a low-order polynomial in the block length. We showed that with minimum-distance encoding, this ensemble achieves the rate-distortion function with the optimal error exponent for all distortions below σ 2 4.91 , or equivalently for rates higher than 1.15 bits per source sample. This value may be an artifact of some looseness in our bounding techniques, especially in analyzing the λ 2 /∆ term of Suen\u2019s inequality.\nFor distortions between σ 2 4.91 and σ 2 , we can show that SPARCs achieve rates which are slightly larger than the optimal rate-distortion function. This will be discussed in an extended version of the paper. We note that for i.i.d Gaussian sources with mean-squared distortion, SPARCs are universal codes in the following sense: A SPARC designed to compress a Gaussian source of variance σ 2 with distortion D can com- press all Gaussian sources of variance less than σ 2 to within distortion D. The ﬁnal goal is to develop computationally feasible encoding algorithms that rapidly approach the rate- distortion bound with growing block length."},"refs":[{"authors":[{"name":"A. Barron"},{"name":"A. Joseph"}],"title":{"text":"Least squares superposition codes of moder- ate dictionary size are reliable at rates up to capacity"}},{"authors":[{"name":"A. Barron"},{"name":"A. Joseph"}],"title":{"text":"Toward fast reliable communication at rates near capacity with Gaussian noise"}},{"authors":[{"name":"I. Kontoyiannis"},{"name":"K. Rad"},{"name":"S. Gitzenis"}],"title":{"text":"Sparse superposition codes for Gaussian vector quantization"}},{"authors":[{"name":"S. Janso"}],"title":{"text":"Random Graphs"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory"}},{"authors":[{"name":"M. Eyuboglu"},{"name":"J. Forney"}],"title":{"text":"Lattice and trellis quantization with lattice- and trellis-bounded codebooks-high-rate theory for memoryless sources"}},{"authors":[{"name":"R. Zamir"},{"name":"S. Shamai"},{"name":"U. Erez"}],"title":{"text":"Nested linear/lattice codes for structured multiterminal binning"}},{"authors":[{"name":"A. Gupta"},{"name":"S. Verdu"},{"name":"T. Weissman"}],"title":{"text":"Rate-distortion in near-linear time"}},{"authors":[{"name":"I. Kontoyiannis"},{"name":"C. Gioran"}],"title":{"text":"Efﬁcient random codebooks and databases for lossy compression in near-linear time"}},{"authors":[{"name":"S. Jalali"},{"name":"T. Weissman"}],"title":{"text":"Rate-distortion via Markov Chain Monte Carlo"}},{"authors":[{"name":"S. Korada"},{"name":"R. Urbanke"}],"title":{"text":"Polar codes are optimal for lossy source coding"}},{"authors":[{"name":"D. Sakrison"}],"title":{"text":"A geometric treatment of the source encoding of a Gaussian random variable"}},{"authors":[{"name":"V. Kostina"},{"name":"S. Verd´u"}],"title":{"text":"Fixed-length lossy compression in the ﬁnite blocklength regime: Gaussian source"}},{"authors":[{"name":"K. Marton"}],"title":{"text":"Error exponent for source coding with a ﬁdelity criterion"}},{"authors":[{"name":"S. Ihara"},{"name":"M. Kubo"}],"title":{"text":"Error exponent for coding of memoryless Gaussian sources with a ﬁdelity criterion"}},{"authors":[{"name":"A. Demb"},{"name":"O. Zeitoun"}],"title":{"text":"Large Deviations Techniques and Applica- tions "}},{"authors":[{"name":"R. R. Bahadur"},{"name":"R. R. Rao"}],"title":{"text":"On deviations of the sample mean"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565461.pdf"},"links":[{"id":"1569566381","weight":9},{"id":"1569566527","weight":3},{"id":"1569565383","weight":6},{"id":"1569565883","weight":3},{"id":"1569565223","weight":3},{"id":"1569566725","weight":3},{"id":"1569565663","weight":6},{"id":"1569565377","weight":6},{"id":"1569566385","weight":6},{"id":"1569564635","weight":6},{"id":"1569565867","weight":3},{"id":"1569565067","weight":6},{"id":"1569566875","weight":3},{"id":"1569559617","weight":3},{"id":"1569566605","weight":3},{"id":"1569566683","weight":18},{"id":"1569566227","weight":6},{"id":"1569566697","weight":6},{"id":"1569565551","weight":3},{"id":"1569566943","weight":12},{"id":"1569552245","weight":30},{"id":"1569564481","weight":3},{"id":"1569566415","weight":3},{"id":"1569566469","weight":6},{"id":"1569566081","weight":9},{"id":"1569565355","weight":12},{"id":"1569565931","weight":3},{"id":"1569551535","weight":3},{"id":"1569566765","weight":6},{"id":"1569564245","weight":3},{"id":"1569564731","weight":3},{"id":"1569566207","weight":3},{"id":"1569564227","weight":9},{"id":"1569566671","weight":3},{"id":"1569566303","weight":6},{"id":"1569566119","weight":3},{"id":"1569564233","weight":6},{"id":"1569566319","weight":3},{"id":"1569565123","weight":9},{"id":"1569558459","weight":3},{"id":"1569565291","weight":3},{"id":"1569566821","weight":3},{"id":"1569566751","weight":3},{"id":"1569566467","weight":3},{"id":"1569565771","weight":9},{"id":"1569566157","weight":3},{"id":"1569560613","weight":6},{"id":"1569566903","weight":3},{"id":"1569566999","weight":15},{"id":"1569565859","weight":3},{"id":"1569566579","weight":3},{"id":"1569558483","weight":6},{"id":"1569565347","weight":3},{"id":"1569565455","weight":3},{"id":"1569566497","weight":3},{"id":"1569566963","weight":9},{"id":"1569566709","weight":9},{"id":"1569564989","weight":6},{"id":"1569565897","weight":3},{"id":"1569564189","weight":6},{"id":"1569566985","weight":3},{"id":"1569564613","weight":3},{"id":"1569566095","weight":3},{"id":"1569565907","weight":3},{"id":"1569563981","weight":3},{"id":"1569566419","weight":3},{"id":"1569566617","weight":3},{"id":"1569566905","weight":9},{"id":"1569558681","weight":6},{"id":"1569555999","weight":3},{"id":"1569566657","weight":3},{"id":"1569565213","weight":3},{"id":"1569565841","weight":3},{"id":"1569567665","weight":3},{"id":"1569561143","weight":6},{"id":"1569565833","weight":3},{"id":"1569564611","weight":6},{"id":"1569567015","weight":3},{"id":"1569566437","weight":6},{"id":"1569553909","weight":3},{"id":"1569559111","weight":3},{"id":"1569566687","weight":6},{"id":"1569562285","weight":3},{"id":"1569566939","weight":3},{"id":"1569553537","weight":3},{"id":"1569565427","weight":3},{"id":"1569552251","weight":6},{"id":"1569553519","weight":9},{"id":"1569566885","weight":3},{"id":"1569554881","weight":3},{"id":"1569554971","weight":9},{"id":"1569566209","weight":3},{"id":"1569562821","weight":3},{"id":"1569566909","weight":9},{"id":"1569558985","weight":6},{"id":"1569566473","weight":3},{"id":"1569566629","weight":3},{"id":"1569566721","weight":6},{"id":"1569565633","weight":3},{"id":"1569555879","weight":6},{"id":"1569565219","weight":9},{"id":"1569558509","weight":3},{"id":"1569556671","weight":6},{"id":"1569566037","weight":3},{"id":"1569565095","weight":9},{"id":"1569566223","weight":9},{"id":"1569564969","weight":3},{"id":"1569565357","weight":3},{"id":"1569561245","weight":3},{"id":"1569566505","weight":3},{"id":"1569565393","weight":3},{"id":"1569562207","weight":3},{"id":"1569566191","weight":15},{"id":"1569567033","weight":3},{"id":"1569565527","weight":3},{"id":"1569566051","weight":3},{"id":"1569565467","weight":3},{"id":"1569566655","weight":3},{"id":"1569566233","weight":3},{"id":"1569566667","weight":3},{"id":"1569566317","weight":3},{"id":"1569564097","weight":3},{"id":"1569566407","weight":3},{"id":"1569560503","weight":3},{"id":"1569565463","weight":3},{"id":"1569565439","weight":6},{"id":"1569566229","weight":3},{"id":"1569566133","weight":6},{"id":"1569562551","weight":6},{"id":"1569563395","weight":6},{"id":"1569566901","weight":6},{"id":"1569551347","weight":15},{"id":"1569565415","weight":3},{"id":"1569566383","weight":3},{"id":"1569565885","weight":3},{"id":"1569566805","weight":6},{"id":"1569566929","weight":3},{"id":"1569565549","weight":6},{"id":"1569565611","weight":6},{"id":"1569566983","weight":6},{"id":"1569566097","weight":3},{"id":"1569566479","weight":3},{"id":"1569565397","weight":9},{"id":"1569566873","weight":3},{"id":"1569565765","weight":12},{"id":"1569565435","weight":9},{"id":"1569566129","weight":9},{"id":"1569565093","weight":6},{"id":"1569566711","weight":9},{"id":"1569565661","weight":6},{"id":"1569566267","weight":6},{"id":"1569564131","weight":3},{"id":"1569565511","weight":3},{"id":"1569566737","weight":3},{"id":"1569561221","weight":3},{"id":"1569566253","weight":6},{"id":"1569564305","weight":6},{"id":"1569566547","weight":3},{"id":"1569566651","weight":3},{"id":"1569566823","weight":3},{"id":"1569565013","weight":3},{"id":"1569566237","weight":6},{"id":"1569565375","weight":3},{"id":"1569566755","weight":3},{"id":"1569566771","weight":9},{"id":"1569566641","weight":9},{"id":"1569559035","weight":6},{"id":"1569564247","weight":3},{"id":"1569551905","weight":12},{"id":"1569566487","weight":6},{"id":"1569565529","weight":6},{"id":"1569556759","weight":9},{"id":"1569566619","weight":12},{"id":"1569565271","weight":9},{"id":"1569561185","weight":3},{"id":"1569558779","weight":6},{"id":"1569565669","weight":6},{"id":"1569563721","weight":6},{"id":"1569566389","weight":3},{"id":"1569566435","weight":9},{"id":"1569564923","weight":6},{"id":"1569566299","weight":3},{"id":"1569564281","weight":6},{"id":"1569564769","weight":6},{"id":"1569565769","weight":3},{"id":"1569566933","weight":6},{"id":"1569557851","weight":3},{"id":"1569565389","weight":6},{"id":"1569559919","weight":3},{"id":"1569565861","weight":3},{"id":"1569566147","weight":9},{"id":"1569565537","weight":3},{"id":"1569562367","weight":3},{"id":"1569555891","weight":3},{"id":"1569566847","weight":3},{"id":"1569565997","weight":3},{"id":"1569559597","weight":3},{"id":"1569565337","weight":3},{"id":"1569560459","weight":9},{"id":"1569565853","weight":6},{"id":"1569550425","weight":3},{"id":"1569566341","weight":3},{"id":"1569565889","weight":3},{"id":"1569564505","weight":6},{"id":"1569565165","weight":6},{"id":"1569566413","weight":3},{"id":"1569565707","weight":3},{"id":"1569565113","weight":3},{"id":"1569566375","weight":12},{"id":"1569565143","weight":3},{"id":"1569564257","weight":3},{"id":"1569564931","weight":3},{"id":"1569564141","weight":12},{"id":"1569561579","weight":6},{"id":"1569566987","weight":3},{"id":"1569565031","weight":6},{"id":"1569564509","weight":6},{"id":"1569551751","weight":3},{"id":"1569564419","weight":3},{"id":"1569566067","weight":3},{"id":"1569566825","weight":3},{"id":"1569564807","weight":3},{"id":"1569566609","weight":3},{"id":"1569566113","weight":3},{"id":"1569566443","weight":3},{"id":"1569566727","weight":3},{"id":"1569565315","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S3.T1.4","endtime":"16:00","authors":"Ramji Venkataramanan, Antony Joseph, Sekhar Tatikonda","date":"1341243600000","papertitle":"Gaussian Rate-Distortion via Sparse Linear Regression over Compact Dictionaries","starttime":"15:40","session":"S3.T1: Lossy Source Coding","room":"Kresge Rehearsal B (030)","paperid":"1569565461"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
