{"id":"1569567009","paper":{"title":{"text":"Multi-Level Error-Resilient Neural Networks"},"authors":[{"name":"Amir Hesam Salavati"},{"name":"Amin Karbasi"}],"abstr":{"text":"Abstract\u2014The problem of neural network association is to retrieve a previously memorized pattern from its noisy version using a network of neurons. An ideal neural network should include three components simultaneously: a learning algorithm, a large pattern retrieval capacity and resilience against noise. Prior works in this area usually improve one or two aspects at the cost of the third.\nOur work takes a step forward in closing this gap. More speciﬁcally, we show that by forcing natural constraints on the set of learning patterns, we can drastically improve the retrieval capacity of our neural network. Moreover, we devise a learning algorithm whose role is to learn those patterns satisfying the above mentioned constraints. Finally we show that our neural network can cope with a fair amount of noise."},"body":{"text":"Neural networks are famous for their ability to learn and reliably perform a required task. An important example is the case of (associative) memory where we are asked to memorize (learn) a set of given patterns. Later, corrupted versions of the memorized patterns will be shown to us and we have to return the correct memorized patterns. In essence, this problem is very similar to the one faced in communication systems where the goal is to reliably transmit and efﬁciently decode a set of patterns (so called codewords) over a noisy channel.\nAs one would naturally expect, reliability is certainly a very important issue both in the neural associative memories and in communication systems. Indeed, the last three decades witnessed many reliable artiﬁcial associative neural networks. See for instance [5], [14], [6], [9], [13], [4].\nHowever, despite common techniques and methods de- ployed in both ﬁelds (e.g., graphical models, iterative algo- rithms, etc), there has been a quantitative difference in terms of another important criterion: the efﬁciency. In communication systems, by using modern coding techniques it has become clear that the number of reliably transmitted codewords over a noisy channel can be made exponential in n, the length of the codewords. However, using current neural networks of size n to memorize a set of randomly chosen patterns, the maximum number of patterns that can be reliably memorized scales linearly in n [8], [14].\nIn search for the reasons beyond the inefﬁciency of the storage capacity of neural networks, we found out that a large body of past work (e.g., [5], [14], [6], [9]) followed a common assumption that a neural network should be able to memorize any subset of patterns drawn randomly from the set of all possible vectors of length n. Although this assumption gives the network a sense of generality, it reduces its storage capacity to a great extent.\nAn interesting question which arises in this context is whether one can increase the storage capacity of neural networks beyond the current linear scaling and achieve results similar to coding theory. To this end, Kumar et al. [7] suggested a new formulation of the problem where only a suitable set of patterns was considered for storing. This way they could show that the performance of neural networks in terms of storage capacity increases signiﬁcantly. Following the same philosophy, we will focus on memorizing a random subset of patterns of length n such that the dimension of the training set is k < n. In other words, we are interested in memorizing a set of patterns that have a certain degree of structure and redundancy. We exploit this structure both to increase the number of patterns that can be memorized (from linear to exponential) and to increase the number of errors that can be corrected when the network is faced with corrupted inputs.\nThe success of [7] is mainly due to forming a bipartite network/graph (as opposed to a complete graph) whose role is to enforce the suitable constraints on the patterns, very similar to the role played by Tanner graphs in coding. More speciﬁcally, one layer is used to feed the patterns to the network (so called variable nodes in coding) and the other takes into account the inherent structure of the input patterns (so called check nodes in coding). A natural way to enforce structures on inputs is to assume that the connectivity matrix of the bipartite graph is orthogonal to all of the input patterns. However, the authors in [7] heavily rely on the fact that the bipartite graph is fully known and given, and satisﬁes some sparsity and expansion properties. The expansion assumption is made to ensure that the resulting set of patterns are resilient against fair amount of noise. Unfortunately, no algorithm for ﬁnding such a bipartite graph was proposed.\nOur main contribution in this paper is to relax the above as- sumptions while achieving better error correction performance. More speciﬁcally, we ﬁrst propose an iterative algorithm that can ﬁnd a sparse bipartite graph that satisﬁes the desired set of constraints. We also provide an upper bound on the block error rate of the method that deploys this learning strategy. We then proceed to devise a multi-layer network whose performance in terms of error tolerance improves signiﬁcantly upon [7] and no longer needs to be an expander.\nIn contrast to the mainstream work in neural associative memories, we focus on non-binary neurons, i.e., neurons that can assume a ﬁnite set of integer values S = {0, 1, . . . , S − 1}\nfor their states (where S > 2). A natural way to interpret the multi-level states is to think of the short-term (normalized) ﬁring rate of a neuron as its output. Neurons can only perform simple operations. In particular, we restrict the operations at each neuron to a linear summation over the inputs, and a possibly non-linear thresholding operation. In particular, a neuron x updates its state based on the states of its neighbors {s i } n i=1 as follows:\n1) It computes the weighted sum h = n i=1 w i s i , where w i denotes the weight of the input link from s i .\n2) It updates its state as x = f (h), where f : R → S is a possibly non-linear function from the ﬁeld of real numbers R to S.\nNeural associative memory aims to memorize C patterns of length n by determining the weighted connectivity matrix of the neural network (learning phase) such that the given patterns are stable states of the network. Furthermore, the network should be able to tolerate a fair amount of noise so that it can return the correct memorized pattern in response to a corrupted query (recall phase). Among the networks with these two abilities, the one with largest C is the most desirable.\nWe ﬁrst focus on learning the connectivity matrix of a neural graph which memorizes a set of patterns having some inherent redundancy. More speciﬁcally, we assume to have C vectors of length n with non-negative integer entries, where these patterns form a subspace of dimension k < n. We would like to memorize these patterns by ﬁnding a set of non-zero vectors w 1 , . . . , w m ∈ R n that are orthogonal to the set of given patterns. Furthermore, we are interested in rather sparse vectors. Putting the training patterns in a matrix X C×n and focusing on one such vector w, we can formulate the problem as:\nw 0 ≤ q and w 2 2 ≥ \t (1b) where q ∈ N determines the degree of sparsity and ∈ R + prevents the all-zero solution. A solution to the above problem yields a sparse bipartite graph which corresponds to the basis vectors of the null space speciﬁed by the patterns in the training set. It can therefore be described by Figure 1 with a connectivity matrix W ∈ R m×n such that X W T = 0.\nIn the recall phase, the neural network is fed with noisy in- puts. A possibly noisy version of an input pattern is initialized as the states of the pattern neurons x 1 , x 2 , . . . , x n . Here, we assume that the noise is integer valued and additive (modulu S). In formula, we have y = W (x µ + z) = W z where z is the noise added to pattern x µ . Hence, we are looking for an algorithm that can use this information to provably eliminate the effect of noise and return the correct pattern.\nRemark 1. A solution in the learning/recall phase is accept- able only if it can be found by simple operations at neurons.\nBefore presenting our solution, we brieﬂy overview the relation between the previous works and the one presented in this paper.\n... ...\nDesigning artiﬁcial associative memories has been an active topic of research for the past three decades. Inspired by the Hebbian learning rule, Hopﬁeld in his seminal work [5] introduced the Hopﬁeld network: an auto-associative neural mechanism of size n with binary state neurons in which patterns are assumed to be binary vectors of length n. The capacity of a Hopﬁeld network under vanishing block error probability was later shown to be O(n/ log(n)) in [8].\nDue to the low capacity of Hopﬁeld networks, extension of associative memories to non-binary neural models has also been explored in the past. For instance, in [6] the authors investigated a multi-state complex-valued neural associative memories for which the estimated capacity is C < 0.15n. Under the same model but using a different learning method, Muezzinoglu et al. [9] showed that the capacity can be increased to C = n. However the complexity of the weight computation mechanism is prohibitive.\nA line of recent work has made considerable efforts to exploit the inherent structure of the patterns in order to increase both capacity and error correction capabilities. Such methods either make use of higher order correlations of patterns or focus merely on those patterns that have some sort of redundancy. As a result, they differ from previous methods for which every possible random set of patterns was considered. Pioneering this prospect, Berrou and Gripon [4] achieved considerable improvements in the pattern retrieval capacity of Hopﬁeld networks, by utilizing Walsh-Hadamard sequences. Using low correlation sequences has also been considered in [13], which results in increasing the storage capacity of Hopﬁeld networks to n without requiring any separate decoding stage.\nIn contrast to the pairwise correlation of the Hopﬁeld model [5], Peretto et al. [11] deployed higher order neural models: the state of the neurons not only depends on the state of their neighbors, but also on the correlation among them. Under this model, they showed that the storage capacity of a higher-order Hopﬁeld network can be improved to C = O(n p−2 ), where p is the degree of correlation considered. The main drawback of this model was again the huge computational complexity required in the learning phase. To address this difﬁculty while being able to capture higher-order correlations, a bipartite\ngraph inspired from iterative coding theory was introduced in [7]. Under the assumptions that the bipartite graph is known, sparse, and expander, the proposed algorithm increased the pattern retrieval capacity to C = O(a n ), for some a > 1. The main drawbacks in the proposed approach is the lack of a learning algorithm as well as the assumption that the weight matrix should be an expander. The sparsity criterion on the other hand, as it was noted by the authors, is necessary in the recall phase and biologically more meaningful.\nIn this paper, we focus on solving the above two problems in [7]. We start by proposing an iterative learning algorithm that identiﬁes a sparse weight matrix W . The weight matrix W should satisfy a set of linear constraints X W T = 0. We then propose a novel network architecture which eliminates the need for the expansion criteria while achieving better performance than the error correction algorithm proposed in [7].\nTo learn a sparse neural graph, we follow ideas borrowed from iterative neural learning [15], [10] and compressive sensing [2], [3]. Constructing a factor-graph model for neural associative memory has been also addressed in [1] where a message-passing algorithm is proposed to memorize any set of random patterns. However, in this paper we focus on memorizing patterns belonging to subspaces with sparsity in mind as well. The difference would again be apparent in the pattern retrieval capacity (linear vs. exponential in network size).\nWe are interested in an iterative algorithm that is simple enough to be implemented by a network of neurons. Therefore, we ﬁrst relax (1) as follows:\nIn the above problem, we have approximated the constraint w 0 ≤ q with g(w) ≤ q since . 0 is not a well-behaved\nfunction. The function g(w) is chosen such that it favors sparsity. For instance one can pick g(w) to be . 1 , which leads to 1 -norm minimizations. In this paper, we consider the function\nwhere σ is chosen appropriately. By calculating the derivative of the objective function and primal-dual optimization tech- niques we obtain the following iterative algorithm for (2) (the details are tedious and left to our technical report [12]):\nInput: pattern matrix X , stopping point p. Output: w\n. Update λ t+1 = λ t + γ( − w 2 2 ) . t ← t + 1.\nwhere t denotes the iteration number, X is the transpose of matrix X , δ and α t are small step sizes and [·] + denotes max(·, 0).\nFor our choice of g(w), the i th entry of the function f (w) = g(w), denoted by f i (w) reduces to 2σw i (1 − tanh(σw 2 i ) 2 ).\nFor very small values of w i , f i (w) w i and for large values of w i , f i (w) 0. Therefore, by looking at (3b) we see that the last term is pushing small values in w(t + 1) towards zero while leaving the larger values intact. Therefore, we remove the last term completely and enforce small entries to zero in each update which in turn enforces sparsity. The ﬁnal iterative learning procedure is shown in Algorithm 1.\nHere, θ t is a positive threshold at iteration t and η(.) θ t is the point-wise soft-thresholding function given below:\nRemark 2. the above choice of soft-theresholding function is very similar to the one selected by Donoho et al. in [3] in order to recover a sparse signal from a set of measurements. The authors prove that their choice of soft-threshold function results in optimal sparsity-undersampling trade-off.\nThe next theorem derives the necessary conditions on α t , λ t and θ t such that Algorithm 1 converges to a sparse solution.\nTheorem 1. If θ t → 0 as t → ∞ and if λ t < a min /(a max − a min ), then there is a proper choice of α t in every iteration t that ensures constant decrease in the objective function\nX .w(t) max . Here a min = min µ x µ 2 / X 2 and a max = max µ x µ 2 / X 2 . For λ t = 0, i.e. w(t) 2 ≥ , picking 0 < α t < 1 ensures gradual convergence.\nSketch of the proof: Let E(t) = y(t) max . We would like to show that E(t + 1) < E(t) for all iterations t. We can write w(t + 1) = w (t) − χ(w (t); θ t ) where w (t) = (1 + 2λ t )w(t) − 2α t X y X\nX \t we will obtain E(t + 1) ≤ D t max E(t) + θ t where D(t) = (1 + 2λ t )I C×C − 2α t X X T X 2\n. Hence we need θ t → 0 as t → ∞ and ensuring D t max < 1. The latter condition is satisﬁed if λ t ≤ a min /(a max − a min ). For the detailed proof, we refer the interested readers to our technical report [12].\nIt must be noted that the above algorithm gives one possible solution to the learning problem (2), as there are multiple local\n2: \t Forward iteration: Calculate the weighted input sum h i = n j=1 W ij x j , for each neuron y i and set:\n \n4: \t Update the state of each pattern neuron j according to x j = x j + sgn(g j ) only if |g j | > ϕ.\nminimums for this problem corresponding to different null- bases of the subspace deﬁned by the patterns in the training set. Any of these vectors are acceptable for the proposed algorithm.\nIn order to have error correction capabilities we propose a new network structure (see Figure 2). To make the description clear and simple we only concentrate on a two-level neural network. However, the generalization of this idea is trivial and left to the reader.\nThe idea behind this new architecture is that we divide the input pattern of size n into L sub-patterns of length n/L. Now we feed each sub-pattern to a neural network which enforces m constraints 1 on the sub-pattern in order to correct the input errors. Such model might be specially useful in cases where the input is modular, similar to the case of memorizing different words of a sentence and enforcing global grammatical constraints on the sentence as a whole, or the case where local sub-patterns have few dominant principle components.\nThe local networks in the ﬁrst level and the global network in the second level use Algorithm 2, which is a variant of the \u201dbit-ﬂipping\u201d method proposed in [7], to correct the errors. Note that if the states of the pattern neurons x i correspond to a pattern from X (i.e., the noise-free case), then for all i = 1, . . . , m we have y i = 0. The quantity g j can be interpreted\nas feedback to pattern neuron x j from the constraint neurons. Hence, the sign of g j provides an indication of the sign of the noise that affects x j , and |g j | indicates the conﬁdence level in the decision regarding the sign of the noise.\nTheorem 2. Let ¯ d and d min be the average and minimum degree of pattern neurons, respectively. Then, Algorithm 2 can correct a single error in the input pattern with probability at least 1 − ¯ d m\nFor the proof, we refer the interested readers to our technical report [12].\nGiven that each local network is able to correct one pattern with high probability, L such networks can correct L input errors if they are separated such that only one error appears in the input of each local network. Otherwise, there would be a probability that the network could not handle the errors. In that case, we feed the overall pattern of length n to the second layer with the connectivity matrix W g , which enforces m g global constraints. And since the probability of correcting two erroneous nodes increases with the input size, we expect to have a better error correction probability in the second layer. Therefore, using this simple scheme we expect to gain a lot in correcting errors in the patterns. In the next section, we provide simulation results which conﬁrm our expectations and show that the block error rate can be improved by a factor of 100 in some cases.\nRemark 3. The number of constraints for the second layer af- fects the gain one obtains in the error performance. Intuitively, if the number of global constraints is large, we are enforcing more constraints so we expect a better error performance. We can think of determining the number even adaptively, i.e. if the error performance that we are getting is unacceptable, we can look deeper in patterns to identify their internal structure by searching for more constraints. This would be a subject of our future research.\nThe following theorem shows that the proposed neural architecture is capable of memorizing an exponential number of patterns in n.\nTheorem 3. Let X be the C × n matrix, formed by C vectors of length n with non-negative integer entries between 0 and S − 1. Furthermore, let k g = rn for some 0 < r < 1. Then, there exists a data set X with C = a rn , a > 1, and rank(X ) = k g < n, such that they can be memorized by the proposed multi-level neural network shown in ﬁgure 2.\nThe proof of the theorem can be found in the technical report [12].\nWe have simulated the proposed learning algorithm in the multi-level architecture to investigate the block error rate of the suggested approach and the gain we obtain in error rates by adding a second level. We constructed 4 local networks, each with n/4 pattern and m constraint nodes.\nWe generated a sample data set of C = 10000 patterns of length n where each block of n/4 belonged to a subspace of dimension k < n/4. Note that C can be an exponential number in n. However, we selected C = 10000 as an example to show the performance of the algorithm because even for small values of k, and exponential number in k will become too large to handle numerically. The result of the learning algorithm is four different local connectivity matrices W 1 , . . . , W 4 as well as a global weight matrix W g . The number of local constraints was m = n/4 − k and the number of global constraints was m g = n − k g , where k g is dimension of the subspace for overall pattern.\nTable VI-A shows the average number of iterations executed before convergence is reached for different local and global constraints. It also gives the average sparsity ratio for the columns of weight matrices. The sparsity ratio is deﬁned as ρ = κ/n, where κ is the number of non-zero elements.\nFor the recall phase, in each trial we pick a pattern randomly from the training set, corrupt a given number of its symbols with ±1 noise and use the suggested algorithm to correct the errors. As mentioned earlier, the errors are corrected ﬁrst at the local and the at the global level. When ﬁnished, we compare the output of the ﬁrst and the second level with the original (uncorrupted) pattern x. An error is declared if the output does not match at each stage.\nFigure 3 illustrates the pattern error rates n = 400 with two different values of k g = 100 and k g = 200. The results are also compared to that of the bit-ﬂipping algorithm in [7] to show the improved performance of the proposed algorithm. As one can see, having a larger number of constraints at the global level, i.e. having a smaller k g , will result in better pattern error rates at the end of the second stage.\nTable VI-B shows the gain we obtain by adding an addi- tional second level to the network architecture. The gain is calculated as the ratio between the pattern error rate at the output of the ﬁrst and the second level.\nThe authors would like to thank Prof. Amin Shokrollahi for helpful comments and discussions. This work was supported by Grant 228021-ECCSciEng of the European Research Coun- cil."},"refs":[{"authors":[{"name":"A. Braunstei"},{"name":"R. Zecchin"}],"title":{"text":"Learning by message-passing in networks of discrete synapses , Phys"}},{"authors":[{"name":"E. Cand"},{"name":"T. Ta"}],"title":{"text":"Near optimal signal recovery from random projections: Universal encoding strategies?"}},{"authors":[{"name":"D. L. Donoh"},{"name":"A. Malek"},{"name":"A. Montanar"}],"title":{"text":"Message passing algorithms for compressed sensing , Proc"}},{"authors":[{"name":"V. Gripo"},{"name":"C. Berro"}],"title":{"text":"Sparse neural networks with large learning diversity , IEEE Trans"}},{"authors":[{"name":"J. J. Hopﬁel"}],"title":{"text":"Neural networks and physical systems with emergent collective computational abilities , Proc"}},{"authors":[{"name":"S. Jankowsk"},{"name":"A. Lozowsk"}],"title":{"text":"J"}},{"authors":[{"name":"R. Kuma"},{"name":"H. Salavat"},{"name":"A. Shokrollah"}],"title":{"text":"K"}},{"authors":[{"name":"R. McEliec"},{"name":"E. Posne"},{"name":"E. Rodemic"},{"name":"S. Venkates"}],"title":{"text":"The capacity of the Hopﬁeld associative memory , IEEE Trans"}},{"authors":[{"name":"M. K. Muezzinogl"},{"name":"C. Guzeli"},{"name":"J. M. Zurad"}],"title":{"text":"A new design method for the complex-valued multistate Hopﬁeld associative memory , IEEE Trans"}},{"authors":[{"name":"E. Oj"},{"name":"T. Kohone"}],"title":{"text":"The subspace learning algorithm as a formalism for pattern recognition and neural networks , Neural Networks, Vol"}},{"authors":[{"name":"P. Perett"},{"name":"J. J. Nie"}],"title":{"text":"Long term memory storage capacity of multicon- nected neural networks , Biological Cybernetics, Vol"}},{"authors":[{"name":"A. H. Salavat"},{"name":"A. Karbas"}],"title":{"text":"Multi-Level Error-Resilient Neural Networks with Learning, arXiv:1202"}},{"authors":[{"name":"A. H. Salavat"},{"name":"K. R. Kuma"},{"name":"W. Gerstne"},{"name":"A. Shokrollah"}],"title":{"text":"Neural Pre-coding Increases the Pattern Retrieval Capacity of Hopﬁeld and Bidirectional Associative Memories , IEEE Intl"}},{"authors":[{"name":"S. S. Venkates"},{"name":"D. Psalti"}],"title":{"text":"Linear and logarithmic capacities in associative neural networks , IEEE Trans"}},{"authors":[{"name":"L. X"},{"name":"A. Krzyza"},{"name":"E. Oj"},{"name":"J. Neur"}],"title":{"text":"Neural nets for dual subspace pattern recognition method, Int"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569567009.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S6.T8.4","endtime":"12:50","authors":"Amin Karbasi, Amir Hesam Salavati","date":"1341318600000","papertitle":"Multi-Level Error-Resilient Neural Networks","starttime":"12:30","session":"S6.T8: Probability and Estimation","room":"Stratton (491)","paperid":"1569567009"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
