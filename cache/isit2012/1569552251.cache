{"id":"1569552251","paper":{"title":{"text":"The Dispersion of Slepian-Wolf Coding"},"authors":[{"name":"Vincent Y. F. Tan ∗\u2020"},{"name":"Oliver Kosut \u2021"}],"abstr":{"text":"Abstract\u2014We characterize second-order coding rates (or dis- persions) for distributed lossless source coding (the Slepian- Wolf problem). We introduce a fundamental quantity known as the entropy dispersion matrix, which is analogous to scalar dispersion quantities. We show that if this matrix is positive- deﬁnite, the optimal rate region under the constraint of a ﬁxed blocklength and non-zero error probability has a curved boundary compared to being polyhedral for the Slepian-Wolf case. In addition, the entropy dispersion matrix governs the rate of convergence of the non-asymptotic region to the asymptotic one. As a by-product of our analyses, we develop a general universal achievability procedure for dispersion analysis of some other network information theory problems such as the multiple- access channel. Numerical examples show how the region given by Gaussian approximations compares to the Slepian-Wolf region."},"body":{"text":"Distributed lossless source coding consists in separately encoding two (or more) correlated sources (X n 1 , X n 2 ) ∼\np X 1 ,X 2 (x 1k , x 2k ) into a pair of rate-limited messages (M 1 , M 2 ). Subsequently, given these compressed versions of the sources, a decoder seeks to reconstruct (X n 1 , X n 2 ). One of the most remarkable results in information theory, proved by Slepian and Wolf [1], states that the set of achievable rate pairs (R 1 , R 2 ) is equal to that when each of the encoders is given knowledge of the other source, i.e., encoder 1 knows X n 2 and vice versa. The optimal rate region R ∗ is the polyhedron\nR 1 ≥ H(X 1 |X 2 ) R 2 ≥ H(X 2 |X 1 )\nAs with most other statements in information theory, this result is asymptotic in nature. In this paper, we take a step towards non-asymptotic results by analyzing the second-order coding rates of the Slepian-Wolf (SW) problem.\nAn two-sender SW code is characterized by four parame- ters; the blocklength n, the rates of the ﬁrst and second sources (R 1 , R 2 ) and the probability of error deﬁned as\nwhere ˆ X n 1 and ˆ X n 2 are the reconstructed versions of X n 1 and X n 2 respectively. Traditionally, we require P (n) e → 0 as n → ∞. In this paper, we ﬁx n and require the code to be such that P (n) e ≤ . We then ask what the set of achievable pairs of\nrates as a function of (n, ) is. The main tool that we use is a multidimensional version of the Berry-Ess`een theorem [2].\nThis paper characterizes the (n, )-optimal rate region for the SW problem R ∗ (n, ) up to an O( log n n ) factor. In the course of doing so, we introduce a fundamental quantity called the entropy dispersion matrix of p X 1 ,X 2 and show that if this matrix is non-singular, the boundary of R ∗ (n, ) is, unlike that of SW, a smooth curve. We also demonstrate numerically how our region compares to the SW region and to the problem of ﬁnite blocklength source coding with side information also at the encoder. While the SW problem is the focus of this paper, our achievability technique is general enough to be applicable to multi-terminal channel coding problems such as the multiple-access, broadcast and interference channels. The results for these other problems are not included this paper. The interested reader may refer to [3] for more details.\nThe redundancy of SW coding was discussed in [4]\u2013[6]. However, the authors considered a single source X 1 to be compressed and side information X 2 available only at the decoder. Thus, X 2 is neither coded nor estimated. They showed that a scalar dispersion quantity governs the second- order coding rate. He et al. [5] also analyzed a variable-length variant of the SW problem and showed that the dispersion is smaller than in the ﬁxed-length setting. This dispersion is similar to that for channel coding. Sarvotham et al. [7] considered the SW problem with two sources to be compressed but limited their setting to the case the sources are symmetric. This work generalizes their setting in that we consider all discrete sources. This paper is a network information theory analogue of the works on second-order coding rates [8], [9] and ﬁnite blocklength analysis [10]\u2013[13]. We employ the information spectrum method [14] in our converse proof. This was also done in [9].\nRandom variables and the values they take on will be denoted by upper case (e.g., X) and lower case (e.g., x) respectively. Types (empirical distributions) will be denoted by upper case (e.g., P ) and distributions by lower case (e.g., p). For a sequence x n ∈ X n , the type is denoted as P x n and conditional types are denoted similarly. The entropy\nand conditional entropy are denoted as H(X 1 ) = H(p X 1 ) and H(X 2 |X 1 ) = H(p X 2 |X 1 |p X 1 ) respectively. For a pair of sequences x n 1 , x n 2 , the notations ˆ H(x n 1 ) := H(P x n\n) and ˆ H(x n 2 |x n 1 ) := H(P x n\n|x n 1 |P x n 1 ) denote, respectively, the empir- ical marginal and conditional entropies. For two vectors u, v ∈ R d , the notation u ≤ v means u t ≤ v t for all t = 1, . . . , d. We also use the notation [2 nR ] := {1, . . . , 2 nR }.\nLet (X 1 , X 2 , p X 1 ,X 2 (x 1 , x 2 )) be a discrete memoryless multiple source (DMMS). This means that (X n 1 , X n 2 ) ∼\nDeﬁnition 1. An (n, 2 nR 1 , 2 nR 2 , )-SW code consists of two encoders f j,n : X n j → M j := [2 nR j ], j = 1, 2, and a decoder ϕ n : M 1 ×M 2 → X n 1 ×X n 2 such that the the error probability in (2) with ( ˆ X n 1 , ˆ X n 2 ) := ϕ n (f 1,n (X n 1 ), f 2,n (X n 2 )) does not exceed . The rates are deﬁned as R j := 1 n log |M j |.\nDeﬁnition 2. A rate pair (R 1 , R 2 ) is (n, )-achievable if there exists an (n, 2 nR 1 , 2 nR 2 , )-SW code for the DMMS p X 1 ,X 2 (x 1 , x 2 ). The (n, )-optimal rate region R ∗ (n, ) ⊂ R 2 is the set of all (n, )-achievable rate pairs.\nFor a positive-semideﬁnite symmetric matrix V 0, let the random vector Z ∼ N (0, V). Deﬁne the set\nNote that S (V, ) ⊂ R 3 and is analogous to the cumulative distribution function of a zero-mean Gaussian with covariance matrix V. If ≤ 1 2 , S (V, ) is a convex, unbounded set in the positive orthant. The boundary of S (V, ) is a differentiable manifold if V is positive-deﬁnite (V 0).\n− log p X 1 |X 2 (X 1 |X 2 ) − log p X 2 |X 1 (X 2 |X 1 ) − log p X 1 ,X 2 (X 1 , X 2 )\nThe mean of the entropy density vector is E[h(X 1 , X 2 )] = H(p X 1 ,X 2 ) := [H(X 1 |X 2 ), H(X 2 |X 1 ), H(X 1 , X 2 )] T .\nDeﬁnition 4. The entropy dispersion matrix V(p X 1 ,X 2 ) is the covariance of the random vector h(X 1 , X 2 ).\nWe abbreviate the deterministic quantities H(p X 1 ,X 2 ) and V(p X 1 ,X 2 ) as H and V respectively. Observe that V is an analogue of the scalar dispersion quantities that have gained attention in recent years [10]\u2013[13]. We will ﬁnd it convenient to deﬁne the rate vector R := [R 1 , R 2 , R 1 + R 2 ] T ∈ R 3 .\nDeﬁnition 5. Deﬁne the region R in (n, ) ⊂ R 2 to be the set of rate pairs (R 1 , R 2 ) that satisfy\nwhere ν := |X 1 ||X 2 | + 1 and 1 := (1, 1, 1) T . Also let R out (n, ) ⊂ R 2 be the set of rate pairs (R 1 , R 2 ) that satisfy\nAn illustration is provided in Fig. 1. Henceforth, ∈ (0, 1). C. Main Result and Interpretation\nTheorem 1. The (n, )-optimal rate region R ∗ (n, ) satisﬁes R in (n, ) ⊂ R ∗ (n, ) ⊂ R out (n, ). \t (7)\nThis theorem is proved for V 0 in Section III. Sources for which V is singular include those which are (i) independent, i.e., I(X 1 ; X 2 ) = 0, (ii) either X 1 or X 2 is uniform over their alphabets. The authors in [7] dealt with the speciﬁc case where X 1 , X 2 ∈ F 2 , X 1 = Bern( 1 2 ), X 2 = X 1 ⊕ N with N = Bern(q), q ∈ (0, 1 2 ), i.e., a discrete symmetric binary source (DSBS). In Section IV, we comment on how the proof can be adapted to derive R ∗ (n, ) for a DSBS and all V 0.\nThe direct part of Theorem 1 is proved using the usual random binning argument together with a multidimensional Berry-Ess`een theorem [2]. The converse is proved using an information spectrum technique by Han [14]. Theorem 1 extends to the case where there are more than two senders.\nBy examining R in (n, ) and R out (n, ), it can be seen that we have characterized the (n, )-rate region up to an O( log n n ) factor. This residual is a consequence of (i) universal decoding for the direct part and (ii) approximations resulting from using the multidimensional Berry-Ess`een theorem [2]. Observe that as n → ∞, the (n, )-rate region approaches the SW region [1] at a rate of O( 1 √ n ). This follows from the multidimensional central limit theorem. However, somewhat unexpectedly, if V 0, the (n, )-rate region is not-polyhedral [cf. (1)]. Its boundary is a smooth curve in R 2 . This curvature, given by V, is due to the fact that the three empirical entropies\n|X n 2 ), ˆ H(X n 2 |X n 1 ) and ˆ H(X n 1 , X n 2 ) have to be jointly smaller than some rate vector. By Taylor\u2019s theorem, we see that the empirical entropy vector behaves like a multivariate Gaussian with mean H and covariance V.\nProof: Let (R 1 , R 2 ) be a rate pair such that R belongs to the inner bound R in (n, ), deﬁned in (5).\nCodebook Generation : For j = 1, 2, randomly and indepen- dently assign an index f 1,n (x n j ) ∈ [2 nR j ] to each sequence x n j ∈ X n j according to a uniform pmf. The sequences of the same index form a bin, i.e., B j (m j ) := {x n j ∈ X n j : f 1,n (x n j ) = m j }. Note that B j (m j ), m j ∈ [2 nR j ] are random sets. The bin assignments are revealed to all parties. In particular, the decoder knows the bin rates R j .\nEncoding : Given x n j ∈ X n j , encoder j transmits the bin index f j,n (x n j ). Hence, for length-n sequence, the rates of m 1 and m 2 are R 1 and R 2 respectively.\nDecoding : The decoder, upon receipt of the bin indices (m 1 , m 2 ) ﬁnds the unique sequence pair (ˆ x n 1 , ˆ x n 2 ) ∈ B 1 (m 1 )× B 2 (m 2 ) such that the empirical entropy vector\nˆ H(ˆ x n 1 |ˆ x n 2 ) ˆ H(ˆ x n\n|ˆ x n 1 ) ˆ H(ˆ x n 1 , ˆ x n 2 )\nwhere δ n := (|X 1 ||X 2 | + 1 2 ) log(n+1) n . Deﬁne the empirical entropy typical set T (R, δ n ) := {z ∈ R 3 : z ≤ R − δ n 1}. Then, (8) is equivalent to ˆ H(ˆ x n 1 , ˆ x n 2 ) ∈ T (R, δ n ). If there is more than one pair or no such pair in B 1 (m 1 ) × B 2 (m 2 ), declare a decoding error. Note that our decoding scheme is universal [15], i.e., the decoder does not depend on knowledge of the true distribution p X 1 ,X 2 .\nAnalysis of error probability : Let the sequences sent by the two users be (X n 1 , X n 2 ) and let their corresponding bin indices be (M 1 , M 2 ). We bound the probability of error averaged over the random code construction. Clearly, the ensemble probability of error is bounded above by the sum of the probabilities of the following four events:\nE 2 := {∃ ˜ x n 1 ∈ B 1 (M 1 ) \\ {X n 1 } : ˆ H(˜ x n 1 , X n 2 ) ∈ T (R, δ n )} E 3 := {∃ ˜ x n 2 ∈ B 2 (M 2 ) \\ {X n 2 } : ˆ H(X n 1 , ˜ x n 2 ) ∈ T (R, δ n )} E 4 := {∃ ˜ x n 1 ∈ B 1 (M 1 ) \\ {X n 1 }, ˜ x n 2 ∈ B 2 (M 2 ) \\ {X n 2 } :\nwhere we made the dependence of the empirical entropy vector on the type explicit. We now bound the probability in (10). Let vec(p X 1 ,X 2 ) ∈ R |X 1 ||X 2 | + \t be a vectorized version of the joint distribution p X 1 ,X 2 . Consider the Taylor series expansion:\n(11) where the Jacobian J ∈ R 3×(|X 1 ||X 2 |) is deﬁned entry-wise as\nand g 3 (p X 1 ,X 2 ) := H(X 1 , X 2 ). Because the g t \u2019s are twice continuously differentiable, each entry of the second order correction term ∆ ∈ R 3 in (11) is bounded above by C vec(P X n\n2 for some constant C > 0. Let [J] t be the t-th row of the matrix J. Now, note that\n= 1 n\n,X n 2 places a probability mass 1/n on each sample (X 1k , X 2k ). Deﬁne the random vector\nwhere (a) follows from the deﬁnition T (R, δ n ) and (b) follows from the probability relation\nAs is shown in [3], P( ∆ ∞ ≥ c n ) ≤ 1/n 2 if c n = O(1/n). With this choice of c n ,\nbecause R − H = z √ n + ν log n n 1 for some z such that P(Z ≤ z) ≥ 1− for Z ∼ N (0, V) [cf. deﬁnition of S (V, )]. Since ν > |X 1 ||X 2 | + 1/2 (the coefﬁcient of δ n ), we have\n1 n\nwhere ψ n = O( log n n ). Now note that the summands above are i.i.d. random vectors. These random vectors have zero mean, covariance matrix V \t 0 and ﬁnite third moment ξ := E h(X 1 , X 2 ) 3 2 because X 1 , X 2 are ﬁnite sets. Since the set integrated over in (16) is convex, by the multidimensional Berry-Ess`een theorem [2] (dimension d = 3),\nwhere (a) follows from Taylor\u2019s approximation theorem. Be- cause ψ n = O( log n √ n ) dominates the O( 1 √ n ) term resulting from the Berry-Ess`een approximation, we conclude that\nFor the second event, by symmetry and uniformity, P(E 2 ) = P(E 2 |X n 1 ∈ B 1 (1)). Now consider the chain of inequalities:\nwhere (a) follows because for ˜ x n 1 = x n 1 , the events {x n 1 ∈ B 1 (1)}, {˜ x n 1 ∈ B 1 (1)} and {(X n 1 , X n 2 ) = (x n 1 , x n 2 )} are mutually independent, (b) follows by the union bound and (c) follows from {˜ x n 1 : ˆ H(˜ x n 1 , x n 2 ) ∈ T (R, δ n )} ⊂ {˜ x n 1 :\nˆ H(˜ x n 1 |x n 2 ) ≤ R 1 − δ n }. Equality (d) follows from the uniformity in the random binning. In (e), we partitioned the sum over (x n 1 , x n 2 ) into type classes indexed by Q = Q ¯ X 1 , ¯ X 2 and ˜ x n 1 ∈ X n 1 into sums over stochastic matrices V : X 2 → X 1 for which the V -shell of a sequence of type Q ¯ X 2 in X n 2 is not empty (denoted as V ∈ V (Q ¯ X 2 )). In (f ) we upper bounded the cardinality of the V -shell as |T V (x n 2 )| ≤ 2 nH(V |P xn 2 ) [15, Lem. 1.2.5]. In (g), we used the Type Counting Lemma [15, Eq. (2.5.1)]. By using the deﬁnition of δ n , (19) gives P(E 2 ) ≤ n −1/2 . Similarly P(E 3 ) ≤ n −1/2 and P(E 4 ) ≤ n −1/2 .\nCombining this with (18), the error probability averaged over the random binning is P(E ) ≤ . Hence, there is a deterministic code whose error probability is no greater than\nProof: For the outer bound, [14, Lemma 7.2.2] asserts that every (n, 2 nR 1 , 2 nR 2 , P (n) e )-SW code must satisfy\nP (n) e ≥ 1 − P 1 n\nfor all n and for any γ > 0. Recall that h(X n 1 , X n 2 ) is the entropy density vector in (4) evaluated at (X n 1 , X n 2 ). Suppose that, to the contrary, there exists a rate pair (R 1 , R 2 ) such that R / ∈ R out (n, ) but (R 1 , R 2 ) is (n, )-achievable. Then, by (6), z :=\n1) / ∈ S (V, ). By the deﬁnition of S (V, ) in (3), z ∈ R 3 is such that P(Z ≤ z) < 1 − . Now consider the probability in (20), denoted as s n :\nwhere (a) follows from the deﬁnition of z, (b) follows from the multidimensional Berry-Ess`een theorem [2] and (c) follows by taking γ := log n 2n and using Taylor\u2019s approximation theorem. Uniting (20) and (21) yields P (n) e > , contradicting the (n, )- achievability of (R 1 , R 2 ) for all n sufﬁciently large.\nInstead of the universal decoder in (8), one could use a non-universal one by comparing the entropy density vector\nwith the rate vector. This is likened to maximum-likelihood decoding. Taylor expansion in (11) would not be required. Under this decoding strategy, there is symmetry between the error probabilities in the direct and converse parts. Also see [14, Lem. 7.2.1-2]. The rate penalty of using a universal decoder is of the order O( log n n ). This is insigniﬁcant compared to the dispersion term which is of the order O( 1 √ n ).\nWhen V is rank-deﬁcient, consider the set S (V, ). Sup- pose for the moment that rank(V) = 1. This is the case considered in [7] where the source is a DSBS(q). For such a DSBS, V = v1 3×3 for v = Var(− log p X 1 |X 2 (X 1 |X 2 )) = Var(− log p X 2 |X 1 (X 2 |X 1 )) = Var(− log p X 1 ,X 2 (X 1 , X 2 )). As such, all the probability mass of the degenerate Gaussian N (0, V) lies in a subspace of dimension one. Therefore, the set S (V, ) = {z ∈ R 3 : z ≥\nThe quantity v n Q −1 ( ) is the rate redundancy [4]\u2013[7] for ﬁxed-length SW coding in the ﬁnite blocklength regime for a DMMS for which rank(V) = 1. In this case, the bounds in (5) and (6) (up to O( log n n ) factors) degenerates to\nwhere the scalar dispersion v := q(1−q)[log((1−q)/q)] 2 . This reduces to results in previous works [4]\u2013[7]. Our analysis, of course, applies to all sources. Furthermore, we improve on the residual term, which is now of the order O( log n n ). The case where rank(V) = 2 follows analogously. All the proba- bility mass of N (0, V) is concentrated on a two-dimensional subspace in R 3 and the boundary of the set S (V, ) are not differentiable. As such only one of the \u201ccorners\u201d of S (V, ) will be curved and this will be reﬂected in a result similar to (22). This argument can be formalized and is done in the extended version of this work [3].\nIn this section, we present examples to illustrate R ∗ (n, ). We neglect the O( log n n ) terms throughout; thus we are just\nconcerned about Gaussian approximations. The source is taken to be p X 1 ,X 2 = [1−3a, a; a, a] where a = 0.1. This source has a positive-deﬁnite dispersion. In Fig. 1, we plot the boundaries of the SW region [1] and the boundary of R ∗ (n, ) for = 0.01. We also plot the boundary of the (n, )-region for coding with side information at encoders and decoder (SI-ED). This region R ∗ SI−ED (n, ) ⊂ R 2 is the set of (R 1 , R 2 ) satisfying\nFrom Fig. 1, we see that R ∗ (n, ) has a curved boundary, reﬂecting the correlations among the entropy densities. Also, it approaches the SW boundary as n grows. The boundaries of R ∗ (n, ) and R ∗ SI−ED (n, ) coincide if R 2 meets the condition in (23) with equality and R 1 is large (and vice versa).\nThere are two interesting \u201cslices\u201d of the plots in Fig. 1. These are the equal rate slice (along the 45 ◦ line) and the slice passing through the origin and a corner point (R ∗ 1,n , R ∗ 2,n ) of R ∗ SI−ED (n, ), deﬁned as follows:\nThese two slices are indicated by the markers (×, ) in Fig. 1. The sum rates along both slices are plotted as functions of n in Figs. 2 and 3 respectively. We observe from Fig. 2 that the two sum rates on the 45 ◦ equal rate line approach each other as n grows. Moreover, empirically we observe (and can prove) that their difference decays as exp(−Θ(n)), which is subsumed by the O( log n n ) term, i.e., the dispersions are the same. Thus, when n ≥ 10 3 , there is essentially no loss in performing SW coding versus cooperative encoding if we wish to optimize the sum rate. On the other hand, from Fig. 3, we see that the corresponding difference in corner points decays at a much slower rate of Θ(n −1/2 ). Thus, the corner rate dispersions are different and if we wish to operate at this point, SW loses second-order coding rate relative to the cooperative scenario. See [3] for further analysis of this point.\nIn this paper, we quantiﬁed the second-order coding rates of the Slepian-Wolf problem. We showed that these rates are governed by a so-called entropy dispersion matrix. Admittedly, our results cannot be described as being ﬁnite blocklength. We seek to work towards such results in the future and to compare the accuracy of the Gaussian approximation in Theorem 1 to upper and lower bounds on the blocklength required to achieve a target error probability."},"refs":[{"authors":[{"name":"D. Slepian"},{"name":"J. K. Wolf"}],"title":{"text":"Noiseless coding of correlated information sources"}},{"authors":[{"name":"V. Bentkus"}],"title":{"text":"On the dependence of the Berry-Esseen bound on dimen- sion"}},{"authors":[{"name":"V. Y. F. Tan"},{"name":"O. Kosut"}],"title":{"text":"On the dispersions of three network information theory problems"}},{"authors":[{"name":"D. Baron"},{"name":"M. A. Khojastepour"},{"name":"R. G. Baraniuk"}],"title":{"text":"Redundancy rates of Slepian-Wolf coding"}},{"authors":[{"name":"D.-K. He"},{"name":"L. A. Lastras-Monta˜no"},{"name":"E.-H. Yang"},{"name":"A. Jagmohan"},{"name":"J. Chen"}],"title":{"text":"On the redundancy of Slepian-Wolf coding"}},{"authors":[{"name":"S. Watanabe"},{"name":"R. Matsumoto"},{"name":"T. Uyematsu"}],"title":{"text":"Strongly secure privacy ampliﬁcation cannot be obtained by encoder of Slepian-Wolf code"}},{"authors":[{"name":"S. Sarvotham"},{"name":"D. Baron"},{"name":"R. G. Baraniuk"}],"title":{"text":"Non-asymptotic perfor- mance of symmetric Slepian-Wolf coding"}},{"authors":[{"name":"V. Strassen"}],"title":{"text":"Asymptotische Absch¨atzungen in Shannons Informations- theorie"}},{"authors":[{"name":"M. Hayashi"}],"title":{"text":"Second-order asymptotics in ﬁxed-length source coding and intrinsic randomness"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"H. V. Poor"},{"name":"S. Verd´u"}],"title":{"text":"Channel coding in the ﬁnite blocklength regime"}},{"authors":[{"name":"V. Kostina"},{"name":"S. Verd´u"}],"title":{"text":"Fixed-length lossy compression in the ﬁnite blocklength regime: Discrete memoryless sources"}},{"authors":[{"name":"A. Ingber"},{"name":"Y. Kochman"}],"title":{"text":"The dispersion of lossy source coding"}},{"authors":[{"name":"D. Wang"},{"name":"A. Ingber"},{"name":"Y. Kochman"}],"title":{"text":"The dispersion of joint source- channel coding"}},{"authors":[{"name":"T. S. Ha"}],"title":{"text":"Information-Spectrum Methods in Information Theory"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K¨orne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems "}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569552251.pdf"},"links":[{"id":"1569566381","weight":3},{"id":"1569565383","weight":9},{"id":"1569565223","weight":3},{"id":"1569566725","weight":3},{"id":"1569566385","weight":3},{"id":"1569564635","weight":12},{"id":"1569559665","weight":3},{"id":"1569559617","weight":9},{"id":"1569566321","weight":3},{"id":"1569566683","weight":3},{"id":"1569566227","weight":3},{"id":"1569566597","weight":6},{"id":"1569566571","weight":9},{"id":"1569552245","weight":48},{"id":"1569564481","weight":6},{"id":"1569560833","weight":3},{"id":"1569566415","weight":3},{"id":"1569566469","weight":3},{"id":"1569565355","weight":3},{"id":"1569565931","weight":9},{"id":"1569566373","weight":3},{"id":"1569565461","weight":6},{"id":"1569564245","weight":3},{"id":"1569564227","weight":6},{"id":"1569563411","weight":3},{"id":"1569559541","weight":3},{"id":"1569565123","weight":3},{"id":"1569565291","weight":3},{"id":"1569566821","weight":3},{"id":"1569566467","weight":6},{"id":"1569565771","weight":3},{"id":"1569566903","weight":3},{"id":"1569566999","weight":6},{"id":"1569566843","weight":6},{"id":"1569565455","weight":6},{"id":"1569566497","weight":6},{"id":"1569566963","weight":3},{"id":"1569564989","weight":3},{"id":"1569566523","weight":3},{"id":"1569565897","weight":3},{"id":"1569551763","weight":3},{"id":"1569564613","weight":3},{"id":"1569566095","weight":6},{"id":"1569566167","weight":3},{"id":"1569563981","weight":9},{"id":"1569561085","weight":3},{"id":"1569566419","weight":3},{"id":"1569566905","weight":19},{"id":"1569566753","weight":3},{"id":"1569566311","weight":3},{"id":"1569558681","weight":3},{"id":"1569558859","weight":3},{"id":"1569565841","weight":3},{"id":"1569566531","weight":6},{"id":"1569567665","weight":9},{"id":"1569561143","weight":32},{"id":"1569561795","weight":3},{"id":"1569566325","weight":3},{"id":"1569566423","weight":3},{"id":"1569566437","weight":22},{"id":"1569558901","weight":3},{"id":"1569553909","weight":3},{"id":"1569559111","weight":3},{"id":"1569565427","weight":3},{"id":"1569553519","weight":3},{"id":"1569554971","weight":3},{"id":"1569566209","weight":3},{"id":"1569562821","weight":3},{"id":"1569565655","weight":6},{"id":"1569566909","weight":9},{"id":"1569565151","weight":6},{"id":"1569564333","weight":3},{"id":"1569566809","weight":3},{"id":"1569566629","weight":22},{"id":"1569563897","weight":6},{"id":"1569565887","weight":6},{"id":"1569565633","weight":3},{"id":"1569555879","weight":3},{"id":"1569565219","weight":3},{"id":"1569558509","weight":3},{"id":"1569564851","weight":3},{"id":"1569561245","weight":3},{"id":"1569566505","weight":3},{"id":"1569565393","weight":3},{"id":"1569562207","weight":3},{"id":"1569567033","weight":16},{"id":"1569566603","weight":6},{"id":"1569565467","weight":3},{"id":"1569566233","weight":3},{"id":"1569566317","weight":3},{"id":"1569560997","weight":9},{"id":"1569565463","weight":9},{"id":"1569565439","weight":6},{"id":"1569562551","weight":12},{"id":"1569563395","weight":3},{"id":"1569566901","weight":3},{"id":"1569551347","weight":25},{"id":"1569565415","weight":3},{"id":"1569555367","weight":3},{"id":"1569566383","weight":3},{"id":"1569565571","weight":6},{"id":"1569565885","weight":3},{"id":"1569565665","weight":3},{"id":"1569566983","weight":6},{"id":"1569566779","weight":3},{"id":"1569566479","weight":3},{"id":"1569565397","weight":3},{"id":"1569566873","weight":3},{"id":"1569565765","weight":3},{"id":"1569565263","weight":3},{"id":"1569565093","weight":3},{"id":"1569565919","weight":12},{"id":"1569565181","weight":3},{"id":"1569565661","weight":6},{"id":"1569566887","weight":3},{"id":"1569566267","weight":16},{"id":"1569564595","weight":3},{"id":"1569566917","weight":3},{"id":"1569565353","weight":6},{"id":"1569566651","weight":3},{"id":"1569566595","weight":3},{"id":"1569552025","weight":3},{"id":"1569566137","weight":3},{"id":"1569565013","weight":3},{"id":"1569565829","weight":3},{"id":"1569565375","weight":3},{"id":"1569566755","weight":3},{"id":"1569565541","weight":3},{"id":"1569565597","weight":3},{"id":"1569565293","weight":3},{"id":"1569566641","weight":3},{"id":"1569564247","weight":3},{"id":"1569551905","weight":6},{"id":"1569565457","weight":6},{"id":"1569565529","weight":3},{"id":"1569556759","weight":6},{"id":"1569566619","weight":3},{"id":"1569561185","weight":6},{"id":"1569566075","weight":3},{"id":"1569566817","weight":6},{"id":"1569567483","weight":3},{"id":"1569564923","weight":6},{"id":"1569566299","weight":3},{"id":"1569564281","weight":3},{"id":"1569565805","weight":3},{"id":"1569561713","weight":3},{"id":"1569566933","weight":3},{"id":"1569563919","weight":3},{"id":"1569566577","weight":9},{"id":"1569557851","weight":6},{"id":"1569565389","weight":3},{"id":"1569566147","weight":3},{"id":"1569559597","weight":3},{"id":"1569559251","weight":3},{"id":"1569565337","weight":3},{"id":"1569566341","weight":3},{"id":"1569565889","weight":16},{"id":"1569565113","weight":6},{"id":"1569566375","weight":22},{"id":"1569564257","weight":3},{"id":"1569566555","weight":6},{"id":"1569564141","weight":16},{"id":"1569565031","weight":3},{"id":"1569565139","weight":3},{"id":"1569564419","weight":3},{"id":"1569566067","weight":19},{"id":"1569566825","weight":3},{"id":"1569564807","weight":3},{"id":"1569563007","weight":3},{"id":"1569566113","weight":6},{"id":"1569566443","weight":6}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S6.T1.2","endtime":"12:10","authors":"Vincent Tan, Oliver Kosut","date":"1341316200000","papertitle":"The Dispersion of Slepian-Wolf Coding","starttime":"11:50","session":"S6.T1: Finite Blocklength Analysis for Source Coding","room":"Kresge Rehearsal B (030)","paperid":"1569552251"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
