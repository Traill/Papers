{"id":"1569566505","paper":{"title":{"text":"Learning Minimal Latent Directed Information Trees"},"authors":[{"name":"Jalal Etesami"},{"name":"Negar Kiyavash"},{"name":"Todd P. Coleman"}],"abstr":{"text":"PAPER AWARD\u2013 We propose a framework for learning the structure of a minimal latent tree with an associated discrepancy measure. Speciﬁcally, we apply this algorithm to recover the minimal latent directed information tree on a mixture of set of ob- served and unobserved random processes. Directed information trees are a new type of probabilistic graphical model based on directed information that represent the casual dynamics among random processes in a stochastic systems. To the best of our knowledge, this is the ﬁrst approach that recovers these type of latent graphical models where samples are available only from a subset of processes."},"body":{"text":"Latent graphical models refer to a class of probabilistic graphical models that relate a set of observed variables to a set of hidden variables. Introducing latent variables can greatly improve the ﬂexibility of probabilistic modeling, allowing it to address a diverse range of problems with hidden factors, for instance in speech recognition and bio-informatics to name a few.\nGraphical models simplify analysis of multivariate prob- abilistic complex systems. These graphs can be directed, undirected, or a mix. Traditional graphical models explain re- lationships between groups of random variables without a time axis. Physical systems evolve dynamically with time [9], [10]. To understand causal dynamics, we have recently developed a class of directed graphical models that correspond to random processes and reﬂect its inherent generative model, analogous to such graphs for coupled differential equations [8]. As such, these directed graphical models - termed directed information graphs - have an advantage over undirected graphical models in understanding causation. In this type of graphical models, nodes represent random processes and edges are selected using a criterion that evaluates the directed information [1].\nInference on latent graphical models pertains to when certain random variables are observed, and others are not. In structure learning, it is of interest to discover the topological structure of the graphical model itself (i.e. where edges are present and when there are not), given observations of a subset of the random variables. Some quartet 1 -based distance\nreconstruction methods have been suggested [2]to discover the structure of latent Markov graphical models [6]. In these type of graphical models, nodes represent random variables and existence or absence of edges represent conditional depen- dence or independence respectively. Recently, Anandkumar et al. applied this approach to learning linear multivariate tree models when only the leaves are observed [3]. In [3] the nodes are multivariate random vectors and it is assumed that the conditional expected value of each node given its parent is determined by its parent and a deterministic matrix. Recursive grouping (RG) [4] and Chow-Liu recursive grouping (CLRG) [5], [4] are two other distance-based learning algorithms that can recover latent Markov graphical models [6], where some of the observed nodes are internal nodes. Both RG and CLRG can only recover latent models on set of hidden and observed random which are jointly Gaussian or have a symmetric discrete joint distribution [4].\nIt is the purpose of this paper to develop a framework to per- form structure learning on directed information graphs, where we observe subsets of random processes. Speciﬁcally, we will consider the scenario of latent directed information trees, where the directed information graph representing observed and unobserved processes is a tree. In such trees, some of the nodes represent the observed processes while others represent the hidden ones. Therefore learning such trees requires both ﬁnding the number of hidden processes as well as recovering the connections among all hidden and observed nodes.\nIn the aforementioned graphical models literature, nodes are either random variable or multivariate random vectors; hence no notion of time dependency is encoded in this class of graphical models. Consequently, they are unable to capture causal dynamics in stochastic systems. On the contrary, we will recover a fundamentally different graphical model for which the causation among a set of random processes can be encoded. In graphical models of our interest, the nodes encode random processes and the observed nodes can be internal or leaves.\nThe contributions of this work can be summarized as follows. We propose an algorithm to recover the minimal latent directed information tree on a set of observed and unobserved nodes with an associated discrepancy measure. We\napply our proposed algorithm to ﬁnding the latent directed information trees, a new type of probabilistic graphical model based on directed information that may be used to succinctly represent the casual dynamics among random processes in a stochastic systems. In contrast to previous latent tree estima- tion approaches, (1) our approach is applicable to random processes with temporal dependence structure; (2) we allow the possibility that some observed nodes are internal and (3) we do not require jointly Gaussian or symmetry properties of the joint distribution of nodes.\nThe remainder of the paper is organized as follows. We formally introduce the minimal latent directed information trees in Section III. In Section IV, we show that given a so-called discrepancy measure on the observed nodes of any tree we can recover the entire latent structure. Speciﬁcally in Section V, we show that in case of latent directed information trees, this measure corresponds to the time delays between pairs of observed processes.\nConsider a stochastic dynamical system described by m random processes X = (X 1 , ..., X m ) with joint distribution P X such that each random process contains n random variables. We denote the jth random variable in the ith random process by X i,j and the random process X i from time 1 up to time j by X j i,1 . Using the chain rule over increasing time indices, the distribution can be factored as\nIn many causal systems, given the full past, the future of processes are conditionally independent. In such cases, given the full past of all processes the joint distribution can be simpliﬁed to\nFor example, a collection of random processes described by coupled stochastic differential equations satisﬁes the analogous statement in continuous time.\nUsing casual conditioning notation introduced by Kramer [7] 2 ,\nwhere [j] := {1, ..., m}\\{j}. In this notation the set of random processes X [j] inﬂuence the random process X j by one time delay. This notation may be generalized to the case for which\n##. .. .. ..\n##. .. .. ..\n00> > > > > > >\n##. .. .. ..\n66I I I I I I I I I\n00> > > > > > >\n99N N N N N N N N N N\n66I I I I I I I I I\n00> > > > > > >\n##. .. .. ..\nI I I I I I I I\n99N N N N N N N N N N\n66I I I I I I I I I\n00> > > > > > >\n##. .. .. ..\nFig. 1: Time dependencies between random processes X and Y in two different scenarios. Directed edges are used to show which variables of process X take part in generating which variable of process Y .\nthe dependency has a delay of d time steps. We denote the causal conditioned distribution with a delay of d ∈ N time steps as follows\n(5) In equation (5), X i −1 J,1 stands for (X i −1 j\n), and J = {j 1 , ..., j s } is a set of random processes which inﬂuence X 1 . Figure 1 illustrates the time dependency in equation (3) (a) and equation (5) when d = 3 (b). It is easy to check that for d = 1, equation (5) becomes Kramer\u2019s causal conditioned distribution (3).\nAssumption 1: For the reminder of this paper we only consider a collection of random processes for which (i) there exists a reference measure ϕ such that dP X dϕ > 0 and (ii) the joint distribution is given by (4). For simplicity, we will write P X ||Y instead of P X || 1 Y .\nConsider two random processes X i and X j and a set of indices I such that I ⊆ [i, j], then the conditional KL divergence, directed information, and conditioned directed information are given, respectively, by\n(7) I(X j → X i ) := D\nDeﬁnition 3.1: For a joint distribution a generative model is a function A : {1, ..m} → P({1, ..m}), (power set of {1, ..m}) such that for each process j ∈ {1, ..m}, j /∈ A(j) and\n kkXXXXXXXXXXXX X rr\n33B B B B B B B\nB B B B B\nvertices or nodes and a set of ordered pairs of vertices, called arrows or edges\nconnected if there is at least one path between any two nodes and if there is exactly one path between any pair of vertices, then it is called tree. A directed tree is a graph such that its undirected underlying graph obtained by replacing all arrows with undirected edges is a tree. In a directed tree\na node r without any incoming arrow is called root and all of its neighbors are considered to be its children.\nDeﬁnition 3.2: A generative model graph is a directed graph where each node corresponds to a random process, and there is an arrow from i to j, for i, j ∈ {1, ..m} if and only if i ∈ A(j). It is called minimal if for each i, A(i) has minimal cardinality. Under Assumption 1, there is a unique minimal generative model graph [8].\nDeﬁnition 3.3: A directed information graph is a directed graph over a set of random processes X where there is an arrow from i to j for i, j ∈ {1, ..., m} if and only if\nTheorem 3.4 ([8]): For any joint distribution P X satisfying Assumption 1, the corresponding minimal generative model graph and directed information graph are equivalent.\nIn the reminder of this paper we will refer to generative model graphs and directed information graphs interchangeably. Consider a set of random processes X for which the directed information graph is a tree T = (V,\nDenote O = (X 1 , ..., X m ) as the set of observable processes and their corresponding nodes in the DIT is denoted by O. Likewise, denote L = (Y 1 , ..., Y k ) as the set of latent processes and their corresponding nodes are denoted by L. Brieﬂy, X = (O, L) and V = O ∪ L.\nA probability distribution P O is called tree-decomposable if it is a marginal of a tree-structured graphical model P O,L . In this case, P O,L is said to be a tree-extension of P O ; it is said to have a redundant latent node h ∈ L if we could remove h and the marginal on the set of visible nodes O remains as P O . In other words, h ∈ L is redundant if the directed information graph corresponding to the joint distribution of observed and latent nodes excluding Y h , (P O,Y\n) remains a tree. Finally, a latent directed information tree is called minimal if it has no redundant hidden node [6].\n33B B B B B B B\nB B B B B\nAssumption 2: We assume that the joint distribution of the set of observed processes is tree-decomposable and its minimal tree has only one root, i.e., a node without any incoming edge.\nExample 2: Fig. 3 demonstrates a directed information tree which satisﬁes Assumption 2.\nLemma 3.5: Under Assumption 2, every vertex except the root in the minimal latent directed information tree (LDIT) has exactly one incoming edge. Moreover, all hidden nodes with one incoming and one outgoing edge are redundant hidden nodes. In other words, a minimal LDIT has no internal latent vertex with degree less than three.\nProof: The ﬁrst statement is immediate from Assump- tion 2. As for the second claim, we give a proof sketch. Suppose there exists a latent node (X 2 ) with one incoming and one outgoing arrow (X 1 → X 2 → X 3 ) in a minimal LDIT. From the deﬁnition of a generative model graph, it follows that\n(12) From (11), (12) and the fact that A(3) = 2, we obtain\nLemma 3.6: In a system of random processes X with a directed information tree T = (V,\npath from X i to X j and a path from X j to X k , i.e., · · · → X i → · · · → X j → · · · → X k → · · ·\nor equivalently, D\nProof: Without loss of generality, let X 1 to be the root then by the deﬁnition of generative model graph and Theorem 3.4, one can write the joint distribution of X as follows\nBy marginalizing over the descendants of X k and applying Lemma 3.5 several times for the intermediate nodes between X i , X j and X k , one can obtain\nWhere X C i is the set of all ancestors and siblings of X i in the DIT. The left-hand side of (16) can be expanded by the chain rule as follows\n, (14) is proven.\nLemma 3.7: Consider a system of random processes X with a directed information tree T = (V,\npath from j to i of length d, i.e., there is a sequence of nodes (i 1 , ..., i d −1 ) where j is the ancestor of i 1 , i k is the ancestor of i k+1 for (1 ≤ k ≤ d − 2), and i d −1 is the ancestor of i then\nProof: It is sufﬁces to prove the lemma for d = 2, as the case for larger d, may be proven by induction. Consider the case where d = 2 (X → Y → Z). First we prove that\nNote that if (19) holds, then by multiplying all terms for m = 1, ..., n, we obtain\nwhich proves our claim. Equation (19) can be proved by induction on n. The base case follows from the deﬁnition of causal conditioning and Lemma 3.6. Now suppose that (19) holds for m less than k. Note that from the chain rule for any m in particular m = k we have\n(20) This equation can be simpliﬁed by using induction hypothesis and Lemma 3.6 in particular equations (14) which imply\nSubstituting (21), (22), and (23) into the right-hand side of (20) and induction hypothesis prove our claim.\nLemma 3.7 implies that by walking along the path between two random process X i and X j , each time we pass a node, the time dependency between X i and X j decreases at least by one unit. In the next sections we will see that these time delays will help us to recover the structure of a minimal LDIT.\nA simple observation about the directed tree with one root is that each pair of nodes has a unique common ancestor and also we know by Lemma 3.5 that all leaves in a minimal LDIT are observable. Therefore, if we could ﬁnd that how far each pair of leaves are from their common ancestor then intuitively, we expect to be able to recover the entire tree. In\nthis section, we deﬁne a notion of distance on a tree in order to determine the distance from each pair of nodes to their common ancestor. Moreover, we will show that given these distances for a subset of nodes including leaves, the tree will be recoverable uniquely.\nDeﬁnition 4.1: Given a tree T = (V, E) with the root r, we deﬁne the discrepancy between two nodes as a function γ r (v 1 , v 2 ) : V × V → Z + , that assigns a positive integer to the path from v 1 to the common ancestor between v 1 and v 2 , such that\n3) If the path from v 1 to v 3 contains the common ancestor of v 1 and v 2 , then\nThe image of this function can be presented by the discrepancy matrix:\nNote that for a given tree, the discrepancy matrix is not unique. Any function that satisﬁes Deﬁnition 4.1\u2019s conditions is a valid discrepancy measure.\nExample 3: Consider the tree depicted in Fig. 4 with root v 5 and a discrepancy matrix given by\n     \n0 1 2 1 2 2 0 4 2 4 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0\n     \nFor instance, looking at the third row, this particular function assigns 1 to the path from v 3 to its common ancestor with v 1 which is v 5 . Similarly, the weight of the path from v 3 to its common ancestor with v 2 , v 5 , is one and so on.\nNote that the discrepancy matrix describes the topology of a tree T = (V, E) uniquely. Moreover, under some conditions a tree can be recovered just by knowing the discrepancy matrix of a proper subset of V .\nTheorem 4.2: Let T = (V, E) to be a tree with root r and S ⊆ V such that every node v ∈ V \\ (S ∪ {r}) has degree at least 3. Then, the discrepancy matrix of S (Γ S ), will determine V and E uniquely.\nProof: The proof is by induction on |S|. Suppose, a tree T = (V, E) can be recovered uniquely, by any S ⊆ V\nsuch that every node v ∈ V \\ (S ∪ {r}) has degree at least 3 and |S| ≤ k − 1. For the case that |S| = k, let B v := arg min u ∈S\\{v} γ r (v, u). If B v = S \\ {v} for all\nv then T is a star with one hidden node at the center. Otherwise, ﬁx a vertex v ∈ S such that B v ̸= S \\ {v}. If min u ∈S\\{v} γ r (v, u) = 0, then all the nodes in B v are the descendants of v. In this case by induction hypothesis, the subtree of T containing v and all its descendant, is recoverable by B v ∪ {v} as well as the rest of the tree by S \\ B v . The case of min u ∈S\\{v} γ r (v, u) > 0 is similar, however, in this case, B v and {v} have a common hidden ancestor which is also connected to v.\nIn this section, we establish the discrepancy measure for a minimal directed information tree. Lemma 3.7 states that the lag between random processes grow by walking along the directed paths in a minimal DIT. This allows us to have the following deﬁnition in a minimal DIT.\nDeﬁnition 5.1: For any pairs of random processes (X j , X k ) ∈ X, we deﬁne the directed measure from X j to X k denoted by γ(j, k) as follows\nTheorem 5.2: Let X be a collection of random processes with a graphical model which is a minimal directed informa- tion tree T = (V,\nProof: We show that for a given path X → Y → Z in a minimal LDIT, if γ(X, Y ) = l and γ(Y, Z) = d then γ(X, Z) > max {l, d}.\nSince γ(Y, Z) = d, i.e., D(P Z |Y || P Z || d Y ) = 0, by using the same argument as in the proof of Lemma 3.7, we obtain\nFinally, the claim follows by substituting (29) and (30) into the right-hand side of (28). The statement γ(X, Z) > l may be proven by showing P Z l+1 |Z l ,X = P Z l+1 |Z l+1 in a similar fashion.\nThis work develops a new approach for learning a latent tree when a certain discrepancy measure is available for the observed nodes. This procedure may be applied to learning latent directed information trees from the samples of observed processes. Our algorithm produces a matrix of integer values based on the samples and uses the elements of the matrix to discover the hidden nodes and the connections between the hidden and observed nodes. Note that Theorem 5.2 showed that a discrepancy measure may be obtained for minimal DITs. This in conjunction to Theorem 4.2 means that we can recover the structure of a minimal LDIT from the samples available at the observed nodes. Equation (25) leads us to extract the directed measures by estimating mutual information between the present of one observed process and past of other observed processes. Future work entails consistently estimate mutual information from data; this is feasible under appropriate statistical assumptions [11]-[15].\nThis work was supported in part to N. Kiyavash by AFOSR under grants FA 9550-11-1-0016, FA 9550-10-1-0573, and FA 9550-10-1-0345; and by NSF grant CCF 10-54937 CAR; and to T. Coleman by NSF Science & Technology Center grant CCF-0939370 and NSF grant CCF 10-65352."},"refs":[{"authors":[{"name":"J. Massey"}],"title":{"text":"Causality, feedback and directed information"}},{"authors":[{"name":"T. Jiang"},{"name":"P. E. Kearney"},{"name":"M. Li"}],"title":{"text":"A polynomial-time approximation scheme for inferring evolutionary trees from quartet topologies and its application"}},{"authors":[{"name":"A. Anandkumar"},{"name":"K. Chaudhuri"},{"name":"D. Hsu"},{"name":"S. M. Kakade"},{"name":"L. Song"},{"name":"T. Zhang"}],"title":{"text":"Spectral Methods for Learning Multivariate Latent Tree Structure"}},{"authors":[{"name":"J. Choi"},{"name":"V. Tan"},{"name":"A. Anandkumar"},{"name":"A. Willsky"}],"title":{"text":"Learning Latent Tree Graphical Models"}},{"authors":[{"name":"C. Chow"},{"name":"C. Liu"}],"title":{"text":"Approximating discrete probability distributions with dependence trees"}},{"authors":[{"name":"J. Pearl"}],"title":{"text":"Probabilistic reasoning in intelligent systems: networks of plausible inference"}},{"authors":[{"name":"G. Kramer"}],"title":{"text":"Directed information for channels with feedbacks"}},{"authors":[{"name":"C. Quinn"},{"name":"N. Kiyavash"},{"name":"T. Coleman"}],"title":{"text":"Directed Information Graphs"}},{"authors":[{"name":"D. Materassi"},{"name":"G. Innocenti"}],"title":{"text":"Topological identiﬁcation in networks of dynamical systems"}},{"authors":[{"name":"C. Shalizi"},{"name":"M. Camperi"},{"name":"K. Klinkner"}],"title":{"text":"Discovering functional com- munities in dynamical networks"}},{"authors":[{"name":"J. Jiao"},{"name":"H. Permuter"},{"name":"L. Zhao"},{"name":"Y. Kim"},{"name":"H. Weissman"}],"title":{"text":"Universal Estimation of Directed Information"}},{"authors":[{"name":"H. Cai"},{"name":"R. Kulkarni"},{"name":"S. Verd´u"}],"title":{"text":"Universal divergence estimation for ﬁnite-alphabet sources"}},{"authors":[{"name":"H. Cai"},{"name":"S. R. Kulkarni"},{"name":"S. Verd´u"}],"title":{"text":"Universal entropy estimation via block sorting"}},{"authors":[{"name":"F. Perez-Cruz"}],"title":{"text":"Estimation of information theoretic measures for con- tinuous random variables"}},{"authors":[{"name":"K. Sricharan"},{"name":"A. O. Hero"}],"title":{"text":"Ensemble estimators for multivariate entropy estimation"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566505.pdf"},"links":[{"id":"1569566381","weight":2},{"id":"1569566485","weight":2},{"id":"1569565383","weight":2},{"id":"1569565223","weight":2},{"id":"1569566725","weight":2},{"id":"1569566385","weight":2},{"id":"1569564635","weight":2},{"id":"1569566875","weight":2},{"id":"1569559617","weight":2},{"id":"1569566683","weight":2},{"id":"1569566227","weight":2},{"id":"1569566597","weight":2},{"id":"1569552245","weight":5},{"id":"1569565495","weight":2},{"id":"1569566469","weight":2},{"id":"1569565355","weight":2},{"id":"1569565931","weight":2},{"id":"1569565547","weight":2},{"id":"1569565461","weight":2},{"id":"1569564245","weight":5},{"id":"1569564227","weight":5},{"id":"1569565837","weight":2},{"id":"1569565123","weight":2},{"id":"1569566941","weight":2},{"id":"1569565291","weight":2},{"id":"1569564203","weight":2},{"id":"1569565771","weight":2},{"id":"1569566999","weight":2},{"id":"1569566843","weight":2},{"id":"1569565455","weight":2},{"id":"1569566497","weight":2},{"id":"1569566963","weight":2},{"id":"1569566709","weight":2},{"id":"1569564989","weight":2},{"id":"1569565897","weight":2},{"id":"1569565953","weight":2},{"id":"1569566343","weight":5},{"id":"1569561085","weight":2},{"id":"1569566311","weight":2},{"id":"1569566063","weight":2},{"id":"1569558681","weight":17},{"id":"1569555999","weight":2},{"id":"1569565213","weight":2},{"id":"1569565841","weight":2},{"id":"1569565833","weight":2},{"id":"1569564611","weight":2},{"id":"1569566325","weight":2},{"id":"1569566437","weight":2},{"id":"1569553909","weight":2},{"id":"1569553537","weight":2},{"id":"1569565427","weight":2},{"id":"1569552251","weight":2},{"id":"1569566231","weight":2},{"id":"1569554971","weight":2},{"id":"1569566209","weight":2},{"id":"1569562821","weight":2},{"id":"1569565559","weight":5},{"id":"1569564333","weight":2},{"id":"1569566913","weight":10},{"id":"1569566809","weight":2},{"id":"1569566629","weight":2},{"id":"1569565033","weight":5},{"id":"1569555879","weight":2},{"id":"1569556671","weight":2},{"id":"1569564969","weight":2},{"id":"1569566043","weight":2},{"id":"1569565029","weight":2},{"id":"1569565393","weight":5},{"id":"1569562207","weight":2},{"id":"1569567033","weight":2},{"id":"1569567029","weight":2},{"id":"1569566695","weight":2},{"id":"1569566673","weight":2},{"id":"1569565739","weight":2},{"id":"1569566233","weight":2},{"id":"1569566667","weight":2},{"id":"1569566317","weight":2},{"id":"1569565463","weight":2},{"id":"1569562551","weight":2},{"id":"1569563395","weight":2},{"id":"1569551347","weight":2},{"id":"1569565415","weight":5},{"id":"1569555367","weight":2},{"id":"1569565611","weight":2},{"id":"1569566983","weight":2},{"id":"1569565397","weight":5},{"id":"1569566873","weight":5},{"id":"1569565661","weight":2},{"id":"1569566267","weight":2},{"id":"1569564131","weight":2},{"id":"1569566917","weight":2},{"id":"1569566823","weight":2},{"id":"1569566595","weight":7},{"id":"1569565013","weight":2},{"id":"1569565375","weight":2},{"id":"1569566639","weight":2},{"id":"1569566713","weight":2},{"id":"1569565293","weight":2},{"id":"1569566641","weight":2},{"id":"1569551905","weight":2},{"id":"1569564787","weight":2},{"id":"1569566487","weight":2},{"id":"1569565529","weight":2},{"id":"1569566619","weight":2},{"id":"1569566817","weight":2},{"id":"1569566911","weight":2},{"id":"1569565769","weight":2},{"id":"1569566171","weight":2},{"id":"1569563919","weight":2},{"id":"1569566577","weight":2},{"id":"1569565631","weight":2},{"id":"1569559597","weight":2},{"id":"1569565337","weight":2},{"id":"1569565737","weight":2},{"id":"1569566807","weight":2},{"id":"1569550425","weight":2},{"id":"1569566341","weight":2},{"id":"1569565889","weight":2},{"id":"1569563725","weight":2},{"id":"1569565635","weight":2},{"id":"1569566375","weight":2},{"id":"1569564931","weight":2},{"id":"1569566067","weight":2},{"id":"1569566825","weight":2},{"id":"1569566443","weight":2},{"id":"1569565315","weight":2}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S15.T8.2","endtime":"10:30","authors":"Jalal Etesami, Negar Kiyavash, Todd P Coleman","date":"1341569400000","papertitle":"Learning Minimal Latent Directed Information Trees","starttime":"10:10","session":"S15.T8: Tree Learning","room":"Stratton (491)","paperid":"1569566505"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
