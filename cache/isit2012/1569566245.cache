{"id":"1569566245","paper":{"title":{"text":"Universality in Polytope Phase Transitions and Iterative Algorithms"},"authors":[{"name":"Mohsen Bayati"},{"name":"Marc Lelarge"},{"name":"Andrea Montanari"}],"abstr":{"text":"Abstract\u2014We consider a class of nonlinear mappings F A,N in R N indexed by symmetric random matrices A ∈ R N ×N with independent entries. Within spin glass theory, special cases of these mappings correspond to iterating the TAP equations and were studied by Erwin Bolthausen. Within information theory, they are known as \u2018approximate message passing\u2019 algorithms. We study the high-dimensional (large N ) behavior of the iterates of F for polynomial functions F, and prove that it is universal, i.e. it depends only on the ﬁrst two moments of the entries of A. As an application, we prove the universality of a certain phase transition arising in polytope geometry and compressed sensing. This solves a conjecture by David Donoho and Jared Tanner."},"body":{"text":"Let A ∈ R N ×N be a random Wigner matrix, i.e. a symmetric random matrix with i.i.d. entries (A ij ) i≤j satisfying E{A ij } = 0 and E{A 2 ij } = 1/N . A considerable effort has been devoted to studying the distribution of the eigenvalues of such a matrix [3]. The universality phenomenon is a striking recurring theme in these studies. Roughly speaking, many asymptotic properties of the joint eigenvalues distribution are independent of the entries distribution as long as it matches the ﬁrst two moments, and satisﬁes certain tail conditions. We re- fer to [3] and references therein for a selection of such results. Universality is extremely useful because it allows to compute asymptotics for one entries distribution (e.g. Gaussian) and then export the results to a broad class of distributions.\nIn this paper we are concerned with random matrix uni- versality, albeit we do not focus onto eigenvalues properties. Given A ∈ R N ×N , and an initial condition x 0 ∈ R N independent of A, we consider the sequence (x t ) t∈N deﬁned by letting, for t ≥ 0,\nb t ≡ 1 N\nHere, div denotes the divergence operator and, for each t ≥ 0, f ( · ; t) : R N → R N is a separable function, i.e. f (z; t) = (f 1 (z 1 ; t), . . . , f 2 (z N ; t)) with the f i ( · ; t) : R → R polynomial functions of bounded degree. In particular b t =\nThe present paper is concerned with the asymptotic distri- bution of x t as N → ∞ with t ﬁxed, and establishes the following results:\nUniversality. As N → ∞, the ﬁnite-dimensional marginals of the distribution of x t are asymptotically insensitive to the distribution of the entries of A ij .\nState evolution. The entries of x t are asymptotically Gaussian with zero mean, and variance that can be explicitly computed through a recursion, known as state evolution\nPhase transitions in polytope geometry. As an application, we use state evolution to prove universality of a phase transi- tion on polytope geometry, with connections to compressed sensing. This solves a conjecture put forward by David Donoho and Jared Tanner [6], [12].\nWe think that the ﬁrst two results provide a useful tool to establish a variety of universality in contexts in which the iteration (1) would not a priori seem relevant. The third of the above contributions demonstrates this. In fact, we shall consider a more general setting than the one just described, see Sections I-A and I-B.\nIterations of the form (1) emerge in a number of contexts. In particular, approximate message passing algorithms (AMP) for compressed sensing reconstruction [8], [4] can be recast in the form (1) for suitable choices of the matrix A and functions f . Generalizations of this algorithm were developed by several groups [17], [15], [14] Here, we will use the term AMP for the generalized recursion (1). We also notice that the universality of compressed sensing phase transitions can be conjectured from the results of the non-rigorous replica method [13], [16].\nIn the rest of this section we will state formally our results. Due to their length, we defer proofs to the journal version of this paper. We limit ourselves to present one important idea (namely the representation of message passing as sum over tree embeddings) in Section II.\nWe will consider here and below a setting that is somewhat more general than the one described so far. We will indeed generalize the iteration (1) to take place in the vector space V q,N ≡ (R q ) N R N ×q . Given a vector x ∈ V q,N , we shall most often regard it as an N -vector with entries in R q , namely x = (x 1 , . . . , x N ), with x i ∈ R q . Components of x i ∈ R q will be indicated as (x i (1), . . . , x i (q)) ≡ x i .\nGiven a matrix A ∈ R N ×N , we let it act on V q,N in the natural way, namely letting, for v ∈ V q,N , v = Av be given by v i = N j=1 A ij v j for all i ∈ [N ], i.e. v i (s) = N j=1 A ij v j (s) for s ∈ [q]. In other words we identify A with the Kronecker product A ⊗ I q×q .\nDeﬁnition I.1. An AMP instance is a triple (A, F , x 0 ) where: 1) A ∈ R N ×N is a symmetric matrix with A i,i = 0 for all\n2) F = {f k : k ∈ [N ]} is a collection of mappings f k : R q × N → R q , (x, t) → f k (x, t) that are Lipschitz in their ﬁrst argument;\nGiven F = {f k : k ∈ [N ]}, we deﬁne f ( · ; t) : V q,N → V q,N by letting v = f (v; t) be given by v i = f i (v i ; t) for i ∈ [N ]. Deﬁnition I.2. The approximate message passing orbit corre- sponding to the instance (A, F , x 0 ) is the sequence of vectors {x t } t≥0 , x t ∈ V q,N deﬁned as follows, for t ≥ 0,\nHere B t : V q,N → V q,N is the linear operator deﬁned by letting, for v = B t v,\nThe above deﬁnition can be summarized by the following expression for the evolution of a single coordinate under AMP\nRecall that a centered random variable X is subgaussian with scale σ 2 if, for all λ > 0, we have E e λX ≤ e σ2 λ2 2 .\nDeﬁnition I.3. Let {(A(N ), F N , x 0,N )} N ≥1 be a sequence of AMP instances indexed by the dimension N , with A(N ) a random matrix and x 0,N a random vector. We say that the sequence is (C, d)-regular (or, for short, regular) polynomial sequence if\n(1) For each N , the entries (A ij (N )) 1≤i<j≤N are independent centered random variables. Further they are subgaussian with common scale factor C/N .\n(2) For each N , the functions f i r ( · ; t) in F N (possibly random, as long as independent from A(N ), x 0,N ) are polynomials with maximum degree d and coefﬁcients bounded by C.\n(3) For each N , A(N ) and x 0,N are independent. Further, we have N i=1 exp{ x 0,N i 2 2 /C} ≤ N C almost surely.\nThe next theorem establishes that the behavior of the AMP sequence {x t } t≥0 is, in the high dimensional limit, insensitive to the distribution of the entries of the random matrix A.\nTheorem \t I.4. Let \t (A(N ), F N , x 0,N ) N ≥1 \t and ( ˜ A(N ), F N , x 0,N ) N ≥1 be any two (C, d)-regular polynomial sequences of instances, that differ only in the distribution of the random matrices A(N ) and ˜ A(N ).\nDenote by {x t } t≥0 , {˜ x t } t≥0 the corresponding AMP or- bits. Assume further that for all N and all i < j, E{A 2 ij } = E{ ˜ A 2 ij }. Then, for any set of Lipschitz functions {ψ N,i } N ≥0,1≤i≤N ψ N,i : R q → R, with Lip(ψ N,i ) +\n1 N\nIn order to characterize the high-dimensional limit, we need to make some assumptions on the collection of functions F N . We consider a setting in which the instance (A(N ), F N , x 0 ) has a block structure: this generality is necessary for applica- tions e.g. to polytope phase transitions.\nDeﬁnition I.5. We say that the sequence of AMP instances {(A(N ), F N , x 0,N )} N ≥0 is polynomial converging (or simply converging) if it is (C, d)-regular and there exists: (i) An integer k; (ii) A symmetric matrix W ∈ R k×k with non- negative entries; (iii) A function g : R q × R ˜ q × [k] × N → R q ; (iv) k probability measures P 1 , . . . , P k on R ˜ q , with P a a ﬁnite mixture of (possibly degenerate) Gaussians for each a ∈ [k]; (v) For each N , a ﬁnite partition C N 1 ∪C N 2 ∪· · · ∪C N k = [N ]; (vi) k positive semideﬁnite matrices Σ 0 1 ,. . . Σ 0 k ∈ R q×q , such that the following happens.\n(1) For each a ∈ [k], we have lim N →∞ |C N a |/N = c a ∈ (0, 1).\n(1) For each N ≥ 0, each a ∈ [k] and each i ∈ C N a , we have f i (x, t) = g(x, Y (i), a, t) where Y (1), . . . , Y (N ) are independent random variables with Y (i) ∼ P a whenever i ∈ C N a for some a ∈ [k].\n(2) For each N , the entries {A ij (N )} 1≤i<j≤N are indepen- dent subgaussian random variables with scale factor C/N , EA ij = 0, and, for i ∈ C N a and j ∈ C N b , E{A 2 ij } = W ab /N .\nNotice that, for each r ∈ [q], a ∈ [k], t ∈ N, the functions g r ( · , a, t) must be polynomials with maximum degree d and coefﬁcients bounded by C.\nOur next result establishes that the low-dimensional marginals of {x t } are asymptotically Gaussian. State evolution characterizes the covariance of these marginals. For each t ≥ 1, state evolution deﬁnes a set of k positive semideﬁnite matrices Σ t = (Σ t 1 , Σ t 2 , . . . , Σ t k ), with Σ t a ∈ R q×q . These are obtained by letting, for each t ≥ 1\nTheorem I.6. Let (A(N ), F N , x 0 ) N ≥0 be a converging se- quence of AMP instances, and denote by {x t } t≥0 the corre- sponding AMP sequence. Then for each t ≥ 1, each a ∈ [k], and each locally Lipschitz function ψ : R q × R → R such that |ψ(x, y)| ≤ K(1 + y 2 2 + x 2 2 ) K , we have, in probability,\nA polytope Q is said to be centrosymmetric if x ∈ Q implies −x ∈ Q. Following [6], [7] we say that such a polytope is k-neighborly if every subset of k vertices of Q which does not contain an antipodal pair, spans a (k − 1) dimensional face. The neighborliness of Q is the largest value of k for which this condition holds. The prototype of neighborly polytope is the 1 ball C n ≡ {x ∈ R n : x 1 ≤ 1}, whose neighborliness is indeed equal to n.\nIt was shown in a series of papers [6], [7], [10], [9], [11] that polytope neighborliness has tight connections with the geometric properties of random point clouds, and with sparsity-seeking methods to solve underdetermined systems of linear equations. The latter are in turn central in a number of applied domains, including model selection for data analysis and compressed sensing.\nIntuitive images of low-dimensional polytopes suggest that \u2018typical\u2019 polytopes are not neighborly: already selecting k = 2 vertices, does lead to a segment that connects them and passes through the interior of Q. This conclusion is spectacularly wrong in high dimension. Natural random constructions lead to polytopes whose neighborliness scales linearly in the di- mension. Motivated by the above applications, and following [6], [7], [10], [9], we focus here on a weaker notion of neighborliness. Roughly speaking, this corresponds to the largest k such that most subsets of k vertices of Q span a (k − 1)-dimensional face. In order to formalize this notion, let F (Q; ) be the number of -dimensional faces of Q.\nDeﬁnition I.7. Let Q = {Q n } n≥0 be a sequence of cen- trosymmetric polytopes indexed by the number of vertices 2n, and denote by m = m(n) their dimension: Q n ⊆ R m . We say that Q has weak neighborliness ρ ∈ (0, 1) if for any ξ > 0,\nIf the sequence Q is random, we say that Q has weak neighborliness ρ (in probability) if the above limits hold in probability.\nIn other words, a sequence of polytopes Q n has weak neighborliness ρ, if Q n has close to the maximum possible number of k faces, for all k < mρ(1 − ξ).\nThe existence of such sequences is clear when m(n) = n (since in this case we can take Q n = C n , with ρ = 1), but it is highly non-trivial when m is only a fraction of n. It comes indeed as a surprise that this is instead a generic situation as demonstrated by the following construction. For a matrix 1 A ∈ R m×n , and S ⊆ R n , let AS ≡ {Ax ∈ R m : x ∈ S}. In particular, AC n is the centrosymmetric m-dimensional polytope obtained by projecting the n-dimensional 1 ball to m dimensions. The following result was proved in [6].\nTheorem I.8 (Donoho, 2005). There exists a function ρ ∗ : (0, 1) → (0, 1) such that the following holds. Fix δ ∈ (0, 1). For each n ∈ N, let m(n) = nδ and deﬁne A(n) ∈ R m(n)×n to be a random matrix with i.i.d. Gaussian entries. Then, the sequence of polytopes {A(n)C n } n≥0 has weak neighborliness ρ ∗ (δ) almost surely.\nA characterization of the curve δ → ρ ∗ (δ) was provided in [6], but a more explicit expression will be given below.\nThe proof of Theorem I.8 is based on exact expressions for the number of faces F(A(n)C n ; ). These are in turn derived from earlier work in polytope geometry by Affentranger and Schneider [2] and by Vershik and Sporyshev [18]. This approach relies in a fundamental way on the invariance of the Gaussian distribution of A(n) under rotations.\nMotivated by applications to data analysis and signal pro- cessing, Donoho and Tanner [12] carried out extensive numer- ical simulations for random polytopes of the form A(n)C n for several choices of the distribution of A(n). They formulated a universality hypothesis according to which the conclusion of Theorem I.8 holds for a far broader class of random matrices.\nHere we establish the ﬁrst rigorous result indicating uni- versality of polytope neighborliness for a broad class of random matrices. Deﬁne the curve (δ, ρ ∗ (δ)), δ ∈ (0, 1), parametrically by letting, for α ∈ (0, ∞):\n, ρ = 1 − αΦ(−α) φ(α)\n2π is the Gaussian density and Φ(x) ≡ x −∞ φ(z) dz. Explicitly, if the above functions on the right-hand sides in Eqs. (7) are denoted by f δ (τ ), f ρ (τ ), then ρ ∗ (δ) ≡ f ρ (f −1 τ (δ)).\nHere we extend the scope of Theorem I.8 from Gaussian matrices, to matrices with independent subgaussian entries (not necessarily identically distributed).\nTheorem I.9. Fix δ ∈ (0, 1). For each n ∈ N, let m(n) = nδ and deﬁne A(n) ∈ R m(n)×n to be a random matrix with\nindependent subgaussian entries, with mean 0, unit variance, and common scale factor s independent of n. Then the sequence of polytopes {A(n)C n } n≥0 has weak neighborliness ρ ∗ (δ) almost surely.\nBy comparison, the most closely related result towards universality is by Adamczak, Litvak, Pajor, and Tomczak- Jaegermann [1]. For a class of matrices A(n) with i.i.d. columns, these authors prove that A(n)C n has neighborliness scaling linearly with n. This however does not suggest that a limit weak neighborliness exists, and is universal, as estab- lished instead in Theorem I.9.\nOur main results are theorems I.4, I.6, I.9. Let us brieﬂy outline the basic steps in our proof:\n(i) Show that AMP (1) is asymptotically equivalent to a message passing algorithm with N 2 messages.\n(ii) Show that messages in the latter algorithm can be repre- sented as sums over embedding of trees with bounded degree into the complete graphs with weights A ij .\n(iii) Apply the moments method to these sums over trees to show that only the ﬁrst two moments of A ij matter to the asymptotic distribution of the messages, and hence prove Theorem I.4.\n(v) Use the same approach (tree representation, and moments method) to compare the iteration (1) to an analogous iteration in which the matrix A changes and is i.i.d. across iterations. Analyze the latter to establish state evolution, Theorem I.6.\n(vi) Use the correspondence between polytope neighborliness and compressed sensing [7] to show that the weak neighborli- ness ρ ∗ (δ) is actually a weak threshold for compressed sensing reconstruction under 1 minimization.\n(vii) Show that a suitable AMP algorithm (introduced in [8]) asymptotically solves the 1 minimization problem. The rect- angular m × n sensing matrix is embedded into a symmetric block-structured matrix A ∈ R N ×N with N = m + n. Use state evolution to characterize the asymptotic behavior of the minimizer, cf. [5]. This proves Theorem I.9.\nIn the rest of this section we provide further details regard- ing steps (i), (ii), (iii). We introduce a message passing al- gorithm that is asymptotically equivalent to the AMP iteration (1), and describe its tree embeddings representation.\nIn this section, we deﬁne two message passing sequences corresponding to the instance (A, F , x 0 ). For each i ∈ [N ] we use the notation [N ] \\ i to denote the set [N ] \\ {i}. We now deﬁne the sequence of vectors (z t i→j ) t∈N , where for each i = j ∈ [N ], z t i→j is a vector in R q or equivalently for each t ∈ N, we can see (z t i→j ) as an N × N matrix with entries in R q (diagonal elements are never used). The initial condition is denoted by z 0 i→j ∈ R q for any i, j ∈ [N ] and is independent of j, such that z 0 i→j = x 0 i for all j = i. The r-th entry of the vector z t+1 i→j is deﬁned by the following recursion for t ≥ 0,\nThe key step in proving our universality Theorem I.4 is the following result. This establishes statistical insensitivity of the message passing algorithm moments to the entries distribution.\nProposition \t II.1. Let (A(N ), F N , x 0,N ) N ≥1 and ( ˜ A(N ), F N , x 0,N ) N ≥1 be any two (C, d)-regular polynomial sequences of instances, that differ only in the distribution of the random matrices A(N ) and ˜ A(N ). Assume that for all N and all i < j, E{A 2 ij } = E{ ˜ A 2 ij }. Denote by z t i (resp. ˜ z t i ) deﬁned by (9) while iterating (8) with matrix A (resp. ˜ A).\nThen for any t ≥ 1 and m(1), . . . , m(q) ≥ 0, we have for any i ∈ [N ],\nOur second message passing sequence is deﬁned as follows: for a (C, d)-regular sequence of instances (A(N ), F N , x 0,N ) N ≥1 , we deﬁne for each N , an i.i.d. sequence of N × N random matrices {A t } t∈N such that A 0 = A(N ). Then we deﬁne (y t i→j ) by y 0 i→j = x 0 i and for t ≥ 0\nThe proof of Theorem I.6 is based on the following result that shows that the distributions of z t i and y t i are asymptotically identical. This evolution of y t i is much easier to analyze thanks to the i.i.d. property of the matrices {A t } t∈N .\nProposition II.2. Let (A(N ), F N , x 0,N ) N ≥1 be a (C, d)- regular polynomial sequence of instances. Let z t i and y t i be the sequences of vectors obtained by iterating (8)-(9) and (10)- (11) respectively. Then for any t ≥ 1 and m(1), . . . , m(q) ≥ 0, we have for any i ∈ [N ],\nThe proof of Proposition II.1 and II.2 relies on the tree representation described in the next section\n(r, t) ∈ R and |c i 1 ,...,i q (r, t)| ≤ c max (uniformly in ∈ [N ] and t ∈ N).\nWe now introduce families of ﬁnite rooted labeled trees that will allow us to get a simple expression for the z t i→j (r)\u2019s and z t i (r), see Lemma II.3 below. For a vertex v in a rooted tree T different from the root, we denote by π(v) the parent of v in T . We denote the root of T by ◦. We consider that the edges of T are directed towards the root and write (u → v) ∈ E(T ) if π(u) = v. The unlabeled trees that we consider are such that the root and the leaves have degree one; each other vertex has degree less than d + 1, i.e. has less than d children. We now describe the possible labels on such trees. The label of the root is in [N ], the label of a leaf is in [N ] × [q] × N q and all other vertices have a label in [N ] × [q]. For a vertex v different from the root or a leaf, we denote its label by ( (v), r(v)) and call (v) its type and r(v) its mark. The label (or type) of the root is also denoted by (◦); the label of a leaf v is denoted by ( (v), r(v), v[1], . . . v[q]).\nFor a vertex u ∈ T , we denote |u| its generation in the tree, i.e. its graph-distance from the root. Also for a vertex u ∈ T (which is not a leaf), we denote by u[r] the number of children of u with mark r ∈ [q] (with the convention u[0] = 0). The children of such a node are ordered with respect to their mark: the labels of the children of u are then ( 1 , 1), . . . , ( u[1] , 1), ( u[1]+1 , 2), . . . , ( u[1]+···+u[q] , q), where each ( u[0]+···+u[i] , . . . , u[0]+···+u[i+1]−1 ) is a u[i + 1]-tuple with coordinates in [N ]. We denote by L(T ) the set of leaves of a tree T , i.e. the set of vertices of T with no children. For v ∈ L(T ), its label is such that for all i ∈ [q], v[i] ∈ N and v[1] + · · · + v[q] ≤ d. We will distinguish between two types of leaves: those with maximal depth t = max{|v|, v ∈ L(T )} and the remaining ones. If v ∈ L(T ) and |v| ≤ t − 1, then we impose v[1] = · · · = v[q] = 0. This case corresponds to \u2018natural\u2019 leaves and since they have no children, the notation is consistent with the notation introduced for other nodes of the tree. For all other leaves, we do no make this assumption so that v[1] + · · · + v[q] can take any value in [d]. These leaves are \u2018artiﬁcial\u2019 and can be thought of as leaves resulting from cutting a larger tree after generation t so that the vector of the v[r]\u2019s keeps the information on the number of children with mark r in the original tree.\nWe deﬁne T t to be the set of such labeled trees T with t generations, that satisfy the following conditions:\n1) If v 1 = ◦, v 2 , . . . , v k is a path starting from the root (i.e. with π(v i+1 ) = v i for i ≥ 1), then the corresponding sequence of types (v i ) is non-backtracking,i.e. for any 1 ≤ i ≤ k−2, the three labels (v i ), (v i+1 ) and (v i+2 ) are distinct.\n2) If u ∈ L(T ) and |u| ≤ t − 1 (i.e. u is a \u2018natural\u2019 leaf), then we have u[1] + · · · + u[q] = 0.\n3) If u ∈ L(T ) and |u| = t (i.e. u is an \u2018artiﬁcial\u2019 leaf) then we have u[1] + · · · + u[q] ≤ d.\nFor a labeled tree T ∈ T t and a set of coefﬁcients c = (c i 1 ,...,i q (r, t)), we deﬁne three weights:\n(a) T t i→j (r) ⊂ T t the family of trees such that: (i) The root has type i; (ii) The root has only one child, call it v; (iii) The type of v is (v) / ∈ {i, j} and its mark is r(v) = r.\n(b) T t i (r) ⊂ T t the family of trees such that: (i) The root has type i; (ii) The root has only one child, call it v; (iii) The type of v is (v) = i and its mark is r(v) = r.\nOur ﬁnal Lemma establishes that indeed the above rep- resentation corresponds to the message passing algorithm introduced in the previous subsection.\nLemma II.3. Under the same assumptions as in Proposition II.1, we have\nPartially supported by NSF CAREER award CCF- 0743978 and AFOSR grant FA9550-10-1-0360."},"refs":[{"authors":[{"name":"R. Adamcza"},{"name":"A. Litva"},{"name":"A. P. N. Tomczak-Jaegermann"}],"title":{"text":"Restricted isometry property of matrices with independent columns and neighborly polytopes by random sampling"}},{"authors":[{"name":"R. Affentrange"},{"name":"R. Schneider"}],"title":{"text":"Random projections of regular simplices"}},{"authors":[{"name":"G. W. Anderso"},{"name":"A. Guionne"},{"name":"O. Zeitouni"}],"title":{"text":"An introduction to random matrices "}},{"authors":[{"name":"M. Bayat"},{"name":"A. Montanari"}],"title":{"text":"The dynamics of message passing on dense graphs, with applications to compressed sensing"}},{"authors":[{"name":"M. Bayat"},{"name":"A. Montanari"}],"title":{"text":"The LASSO risk for gaussian matrices"}},{"authors":[{"name":"D. L. Donoho"}],"title":{"text":"High-dimensional centrally symmetric polytopes with neighborliness proportional to dimension"}},{"authors":[{"name":"D. L. Donoho"}],"title":{"text":"Neighborly polytopes and sparse solution of under- determined linear equations"}},{"authors":[{"name":"D. L. Donoh"},{"name":"A. Malek"},{"name":"A. Montanari"}],"title":{"text":"Message Passing Algo- rithms for Compressed Sensing"}},{"authors":[{"name":"D. L. Donoh"},{"name":"J. Tanner"}],"title":{"text":"Neighborliness of randomly-projected simplices in high dimensions"}},{"authors":[{"name":"D. L. Donoh"},{"name":"J. Tanner"}],"title":{"text":"Sparse nonnegative solution of underde- termined linear equations by linear programming"}},{"authors":[{"name":"D. L. Donoh"},{"name":"J. Tanner"}],"title":{"text":"Counting faces of randomly projected polytopes when the projection radically lowers dimension"}},{"authors":[{"name":"D. L. Donoh"},{"name":"J. Tanner"},{"name":"R. Soc"}],"title":{"text":"Observed universality of phase transitions in high-dimensional geometry, with implications for modern data analysis and signal processing"}},{"authors":[{"name":"Y. Kabashim"},{"name":"T. Wadayam"},{"name":"T. Tanaka"}],"title":{"text":"A typical reconstruction limit for compressed sensing based on lp-norm minimization"}},{"authors":[{"name":"A. Malek"},{"name":"L. Anitor"},{"name":"A. Yan"},{"name":"R. Baraniuk"}],"title":{"text":"Asymptotic Analysis of Complex LASSO via Complex Approximate Message Passing (CAMP)"}},{"authors":[{"name":"S. Rangan"}],"title":{"text":"Generalized Approximate Message Passing for Estimation with Random Linear Mixing"}},{"authors":[{"name":"S. Ranga"},{"name":"A. K. Fletche"},{"name":"V. K. Goyal"}],"title":{"text":"Asymptotic analysis of map estimation via the replica method and applications to compressed sensing"}},{"authors":[{"name":"P. Schniter"}],"title":{"text":"Turbo Reconstruction of Structured Sparse Signals"}},{"authors":[{"name":"A. M. Vershi"},{"name":"P. V. Sporyshev"}],"title":{"text":"Asymptotic behavior of the number of faces of random polyhedra and the neighborliness problem"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566245.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T9.2","endtime":"10:30","authors":"Mohsen Bayati, Marc Lelarge, Andrea Montanari","date":"1341396600000","papertitle":"Universality in Polytope Phase Transitions and Iterative Algorithms","starttime":"10:10","session":"S9.T9: Compressive Sensing and Phase Transitions","room":"Stratton West Lounge (201)","paperid":"1569566245"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
