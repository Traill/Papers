{"id":"1569566887","paper":{"title":{"text":"Locally Repairable Codes"},"authors":[{"name":"Dimitris S. Papailiopoulos"},{"name":"Alexandros G. Dimakis"}],"abstr":{"text":"Abstract\u2014One main challenge in the design of distributed storage codes is the Exact Repair Problem: if a node storing encoded information fails, to maintain the same level of reliability, we need to exactly regenerate what was lost in a new node. A major open problem in this area has been the design of codes that i) admit exact and low cost repair of nodes and ii) have arbitrarily high data rates.\nIn this paper, we are interested in the metric of repair locality , which corresponds to the the number of disk accesses required during a node repair. Under this metric we characterize an information theoretic trade-off that binds together locality, code distance, and storage cost per node. We introduce Locally repairable codes (LRCs) which are shown to achieve this tradeoff. The achievability proof uses a \u201clocality aware\u201d ﬂow graph gadget which leads to a randomized code construction. We then present the ﬁrst explicit construction of LRCs that can achieve arbitrarily high data-rates."},"body":{"text":"Distributed and cloud storage systems have reached such a massive scale that recovery from failures is now part of regular operation rather than a rare exception. These large scale storage systems have to allow for high data availability and be able to tolerate multiple physical node failures to prevent data loss. These systems can achieve the targeted data availability and reliability requirements by introducing redundancy among the stored bits. Erasure coded storage systems achieve high reliability without requiring the increased storage cost that is associated with data replication [5]. Three application contexts where erasure coding techniques are being currently deployed or under investigation are Cloud storage systems like Facebook\u2019s Hadoop cluster, archival storage, and peer-to-peer storage systems like Cleversafe and Wuala (see e.g. [1], [3])\nA central issue that arises in coded storage is the Repair Problem : how to maintain the encoded representation when failures (node erasures) occur. To maintain the same redun- dancy when a storage node leaves the system, a newcomer node has to join the array, access some existing nodes, and exactly reproduce the lost contents. During this repair process, there are several metrics that can be optimized: the total information read from existing disks, the total number of bits communicated in the network [7]\u2013[12] (called repair bandwidth [2]), or the total number of disks required for each repair [4], [6]. Currently, the most well-understood metric is repair bandwidth that was characterized in [2]. A great\nvariety of asymptotic and explicit repair bandwidth optimal code constructions were introduced in [1], [3], [7]\u2013[12].\nIt seems that for cloud storage applications, the main repair performance bottleneck is the disk I/O overhead. The disk I/O is proportional to the number of nodes r involved in the repair process of a failed node. This number r deﬁnes our metric of interest: repair locality.\nRepair locality was identiﬁed as a good metric for repair cost independently by Gopalan et al. [14], Oggier et al. [6], and Papailiopoulos et al. [15]. Codes that have good locality properties where studied in [4], [13]\u2013[15], [18]. In [14], a trade-off between locality and code distance, i.e., reliability, was deﬁned for scalar linear codes. However, up to now there do not exist explicit and high rate codes optimized for locality and there is no universal approach (for both linear and nonlinear codes) that characterizes the information theoretic limits between repair locality r, code distance d, and storage per node α. In this paper we address this open problem.\nOur Contribution: Let a ﬁle of size M that is cut in k pieces which are encoded in n > k elements of size α = (1 + ) M k . We establish an information theoretic tradeoff between the repair locality r, the code distance d, and the amount of storage spent per node α, for storage codes of length n. We derive our bounds using a characterization of the code distance d in terms of entropy. A new information ﬂow graph is fundamental to our derivations. Using random linear network coding (RLNC) arguments on this ﬂow graph [16], we show that linear vector codes sufﬁce to achieve the trade-off. We call these optimal codes locally repairable codes.\nThen, we focus on the operational point where any k coded elements can recover the ﬁle, i.e., d = n − k + 1. We construct the ﬁrst explicit family of LRCs that have locality r at the cost of an excess storage overhead of = 1 r . This cost can be made asymptotically (in n, k) negligible when r is any sub-linear function of k such as r = log(k), or r =\nk. Our designs are vector linear, work for any n, k, r and require ﬁnite ﬁeld of order n. A general LRC construction for any feasible point of the tradeoff is left as an interesting open problem.\nIn the following, we see how we can use entropy on the coded elements of a storage code to make arguments on the code distance. This way, we aim to establish a universal information theoretic tradeoff of (linear or nonlinear) codes\nthat binds together the metric of locality, the code distance, and the storage capacity spent for each coded element or node. We would like to note that in many points in this work, we use the phrase \u201ccoded element\u201d instead of \u201cnode\u201d.\nwhere X i s can be viewed as k source elements over some ﬁnite ﬁeld F that are i.i.d. random variables each having entropy H(X i ) = M k , for all i ∈ [k], where [N ] denotes the set of integers {1, . . . , N }. Moreover, let an encoding (generator) function G : F 1×k → F 1×n that takes as input the k elements and outputs n coded elements\nwhere each encoded element (which can be also seen as a random variable) has entropy\nfor all i ∈ [n]. The generator function G deﬁnes a code C. The rate of the code is the ratio of the aggregate useful information to the aggregate stored information, i.e., the entropy of the source elements to the sum of the entropy of each encoded element\nDeﬁnition 1 (Minimum Code Distance): The \t minimum distance d of the code C is equal to the minimum number of erasures of elements in y after which the entropy of the non-erased variables is strictly less than M , that is,\nsuch that H ({Y 1 , . . . , Y n }\\E) < M and E ∈ 2 {Y 1 ,...,Y n } , where 2 {Y 1 ,...,Y n } is the power set of the elements in {Y 1 , . . . , Y n }.\nIn other words, a code has minimum distance d, when there is \u201cenough\u201d entropy after any d − 1 coded element erasures to reconstruct the ﬁle. The above deﬁnition can be restated in its \u201cdual\u201d form: the minimum distance d of the code C is equal to n minus the maximum number of non-erased coded elements in y that cannot reconstruct the ﬁle, that is, d = n − max H(S)<M |S|, where S ∈ 2 {Y 1 ,...,Y n } .\nRemark 1: Observe that the above distance deﬁnition is universal in the sense that it applies to linear or nonlinear codes and is oblivious to any type of element subpacketization.\nDeﬁnition 2 (Repair Locality): A coded element Y i , i ∈ [n], has repair locality r, if it is a function of r other coded variables Y i = f i (Y R(i) ). The set R(i) indexes the smallest set of r coded elements that can reconstruct Y i and f i is some function on these r coded elements.\nIn [14] Gopalan et al. show that for length n scalar linear codes , where G is a linear function on x, each coded element\nY i , i ∈ [n], has entropy α = M k , and locality r, then the minimum code distance is bounded as\nObserve that according to the above bound, low-locality r << k is penalizing minimum distance by a component of k r . This distance, or reliability, penalty cannot be avoided for scalar codes. On the other hand, maximum reliability, i.e., d = n − k + 1, costs in locality. Indeed an (n, k)-Maximum- Distance Separable (MDS) code has both the maximum pos- sible distance n − k + 1 and the worst possible locality r = k. However, for our purposes we would like locality to be low: either a constant, or a sub-linear function of k.\nIn the following, we derive an information theoretic tradeoff between locality r, distance d, and storage per node α. We see how the third parameter α can be used to deﬁne operational points of high distance and low locality. We will refer to codes that achieve this tradeoff as (n, k, r, d, α)-LRCs.\nWe will eventually present explicit LRCs for any n, k, r that have the \u201c(n, k) erasure property\u201d, i.e., that any set of k coded elements has entropy at least M , which is equivalent to requiring distance d = n − k + 1. This operational point will require storage α = M k + 1 r M k .\nIn the following we determine the information theoretic minimum distance of a code, where each coded element has entropy α and repair locality r. We do that by an algorithmic proof in the same manner as [14] that bounds the distance for any possible code C. We give a lower bound over all codes, of the largest set S of coded elements whose entropy is less than M . To simplify calculations, we denote the storage of each node as α = (1 + ) M k , where ≥ 0.\nThe only structural property of a code that we use in our proof, is the fact that there exist (r + 1) sized repair groups. For a code of length n, locality r, and for each of its coded elements, say Y i , there exist at most other r coded elements Y R(i) that can reconstruct Y i , for i ∈ [n]. Then, the coded elements indexed by Γ(i) = {i, R(i)} from an (r + 1)-group, that has the property\nfor all i ∈ [n], due to the functional dependencies induced by the locality property. To determine the upper bound on minimum distance of a C(n, r, d, α) code, we construct the maximum set of nodes, or coded elements, S that have entropy less than the M .\nTheorem 1: For a code C(n, r, d, α), the minimum distance is bounded as\nProof: The proof can be found in the Appendix of the full-version of the manuscript [17].\nRemark 2: We would like to note that the main difference of our proving technique compared to the one in [14], is that it involves counting arguments on information ﬂows, or entropies, instead of ranks of matrices.\nCorollary 1: In terms of the code distance, non-overlapping (r + 1)-groups are optimal.\nObserve that if we set = 0 in the above bound, we get the same bound as [14]. This means that the bound derived in [14] applies to nonlinear codes as well.\nIn the following, for simplicity we will assume that (r+1)|n and then prove that the above bound is achievable using infor- mation ﬂows and random linear network coding techniques.\nIn this section, we show that the bound of Theorem 1 is achievable using a random linear network coding (RLNC) scheme [16]. In our proof, we use a variant of the information ﬂow graph of [2] and show that when the (n, k, r, d, α) parameters of an LRC agree with the bound in Theorem 1, then the min-cut of this ﬂow graph is large enough for a speciﬁc multicast session to be feasible. The feasibility of this multicasting problem is shown to be equivalent to the existence of (n, k, r, d, α)-LRCs. More precisely we have the following theorem which we prove in this section.\nTheorem 2: Let (r + 1)|n. Then, there exist vector linear codes (n, k, r, d, α)-LRCs over F 2 n , that have minimum dis- tance d = n − M a − M ra − 1.\nIn the same manner as [2], the information ﬂow graph is a directed network, where the k input elements correspond to k sources, the n coded elements are represented as intermediate nodes, and the sinks of the network are what we call Data Collectors (DC), each of which requires to decode all k source elements. The speciﬁcations of this network, such as the number and degree of nodes, the edge-capacities, and the cut- set bounds, are determined by the (n, k, r, d, α) parameters. Here, in contrast to the work in [2], we need to account for the locality properties of the code. By incorporating a subgraph that accounts for the dependencies among the coded elements of a repair group, we obtain our \u201clocality aware\u201d ﬂow graph.\nIn Fig. 1, we show the general ﬂow graph that we use in our proof. The network that is deﬁned by the ﬂow-graph has k sources and N = \t n n−d+1 sinks (DCs). We refer to this directed graph as G(n, k, r, d, α) with vertex set\nThe directed edge set is implied by the following edge capacity function\nThe vertices {X i ; i ∈ [k]} correspond to the k ﬁle elements and Y out j ; j ∈ [n] correspond to the coded elements. The\nedge capacity between the in- and out- Y i vertices corresponds to the entropy of a single coded block. The fundamental difference with the ﬂow graph of [2] is the additional ﬂow constraints invoked by repair locality assumptions: coded elements (nodes) in an (r + 1)-group have joint ﬂow (entropy) at most rα, instead of (r + 1)α. To enforce this constraint, we bottleneck the in-ﬂow of each group by a node that restricts it to be at most rα. In Fig. 2, we show the part of the ﬂow graph that enforces the \u201cbottleneck\u201d induced by a repair group, where we consider the ﬁrst group, without loss of generality. For a group Γ(i), i ∈ n r+1 , we add node Γ in i that receives\nﬂow from the sources and is connected with an edge of capacity rα to a new node Γ out i . The latter connects to the r+1 elements of the i-th group. We should note that when we are considering a speciﬁc group, it is implied that any block within that group can be repaired from the remaining r elements. When a block is lost, the functional dependence among the elements of that group allows a newcomer block to compute a function on the remaining r elements and reconstruct what was lost.\nLinear combinations of the ﬁle elements travel along the edges of this graph towards the sinks, which we call Data\nCollectors (DCs). A DC needs to connect to as many coded elements as such that it can reconstruct the ﬁle. This is equivalent to requiring source-to-sink (s − t) cuts between the ﬁle elements and the DCs that are at least equal to M , i.e., the ﬁle size. An s − t cut in G(n, k, r, d, α) determines the amount of ﬂow, or entropy, that can travel from the source elements to the destinations. When d is consistent with the bound of Theorem 1, then the minimum of all the cuts is at least as much as the ﬁle size M .\nLemma 1: The minimum source-DC cut in G(n, k, r, d, α) is larger than or equal to M , when d is consistent Theorem 1.\nProof: The proof can be found in the Appendix of the full-version of the manuscript [17].\nThen, a successful multicast session on G(n, k, r, d, α) can be interpreted as, and is equivalent to, all DCs decoding the ﬁle, i.e., all k source elements can be reconstructed at each sink using the received linear combinations. Interestingly, the linear combinations of the elements along the edges between the n node couples (Y in i , Y out i ), are exactly the coding coefﬁcients that need to be used by the n coded elements to achieve distance d. Hence, we will use the following lemma to prove the existence of codes.\nLemma 2 (RLNC): For a network with k sources and N destinations where η links transmit linear combination of inputs, the probability of success is at least 1 − N q\nWe can now combine the above with the fact that there exists a RLNC that succeeds when q > N , i.e., when q > n d =\n, to obtain Theorem 2. For simplicity we used the upper bound 2 n for the binomial coefﬁcient n r .\nIn this section, we study the operational point of d = n−k+ 1. We calculate the minimum storage overhead that allows the \u201cany k property\u201d and we construct explicit LRCs. Our codes are based on existing MDS codes, like Reed-Solomon (RS) codes, and the ﬁnite ﬁeld order that we require is q ≥ n.\nWe ﬁrst solve for the storage overhead that is required to have distance n − k + 1. This overhead is the minimum one that satisﬁes the equation d = n− k 1+ − k r(1+ ) +2, where d = n − k + 1. Therefore, the minimum storage overhead for erasure distance can be found through the following optimization\nk 1 +\nDue to the ceiling function in the above constraint we do not obtain a closed form expression of the minimizer min . However, we identify a potential range of values that satisfy the equation\nLemma 3: An LRC with node repair locality r and distance n − k + 1 requires an additional storage overhead that is equal to min = 1 r − δ k , where δ k ∈ 0, r+1 r 1 k+1 .\nProof: The proof can be found in the Appendix of the full-version of the manuscript [17].\n− r+1 r 1 k+1 , 1 r to ﬁnd the minimum that satisﬁes the erasure distance.\nIn the following we present LRCs with repair locality r for = 1 r . In the proof of Lemma 3, we show that when = 1 r ,\nthen d = n − k + 1 is the maximum possible distance when (r+1) k. When (r+1)|k the optimal distance is d = n−k+2.\nThe codes that follow are optimal, i.e., LRC, for all n, k, r, when (r + 1) k and (r + 1)|n.\nLet a ﬁle x, of size M = rk, that is subpacketized in r parts, x = x (1) . . . x (r) , with each x (i) , i ∈ [r], having size k. We encode each of the r ﬁle parts independently, into coded vectors y (i) of length n, where (r + 1)|n, using an outer (n, k) MDS code y (1) = x (1) G, . . . , y (r) = x (r) G, where G is the n × k MDS generator matrix. As MDS pre-codes, we use (n, k)-RS codes that require F q , with q ≥ n. We generate a single parity sum vector from all the coded vectors s = r i=1 y (i) . This precoding process yields a total of rn coded blocks in the y (i) vectors and n parity blocks in s, i.e., an aggregate of (r + 1)n blocks available to place in n nodes. In our code, each node expends α = M k + 1 r M k = r + 1 (coded blocks) in storage capacity.\nBelow we state the circular placement of elements in nodes of the ﬁrst (r + 1)-group\nThere are three key properties that are obeyed by our placement:\ni) each node contains r coded blocks coming from different y (l) coded vectors and 1 additional parity element,\nii) The blocks in the r + 1 nodes of the i-th (r + 1)-group, have indices that appear only in that speciﬁc repair group, i ∈ n r+1 , and\niii) the blocks of each \u201crow\u201d have indices that obey a circular pattern, i.e., the ﬁrst row of elements has indices {1, 2, . . . , r + 1} the second {2, 3, . . . , r + 1, 1}, and so on.\nWe follow the same placement for all n r+1 groups. In Fig. 3, we show an LRC of the above construction with n = 6 and k = 4 that has locality 2.\nWe will concentrate on the repair of lost nodes in the ﬁrst repair group of r nodes. This is sufﬁcient since the\nblock placement is identical across repair groups. The key observation is that each node within a repair group stores r +1 blocks of distinct indices: the r +1 blocks of a particular index are stored in r + 1 distinct nodes within the repair group. Hence, when for example the ﬁrst node fails, then element y (1) 1 is regenerated by downloading s 1 from the second node in the group, y (r+1) 1 \t from the third, . . . , and y (2) 1 from the last node in the group. A simple XOR of the above blocks sufﬁces to reconstruct the lost block. Hence, for every lost block of a failed node, the r remaining blocks of the same index that are stored in the r remaining nodes of the repair group have to be XORed to regenerate what was lost.\nHence, a single block repair has locality r. Interestingly, the same applies to the repair locality of an entire node. For each lost block, r other coded blocks are used for regeneration, and all originate from the r remaining nodes. In Fig. 4, we show how repair is performed for the code construction presented in Fig. 3.\nThe effective coding rate of the codes presented in this section is\nThat is, the rate of the code is a fraction r r+1 of the coding rate of an (n, k) MDS code, hence is always upper bounded by r r+1 . This is due to the extra storage overhead required to store the parity blocks s i , i ∈ [n].\nRemark 4: Observe that if we set the repair locality to r = f (k) and f is a sub-linear function of k (i.e., log(k) or\nk), then we obtain non-trivially low locality r << k, while the excess storage cost = 1 r is vanishing when n, k grow.\nThe distance of the presented code is (at least) d = n−k +1 due to the MDS precodes that are used in its design: any k nodes in the system contain rk distinct ﬁle pieces, k from each ﬁle piece. Hence, performing erasure decoding on these r k-tuples of blocks we can generate the r pieces of the ﬁle."},"refs":[{"authors":[],"title":{"text":"The \t Coding \t for \t Distributed \t Storage \t wiki http://tinyurl"}},{"authors":[{"name":"A. G. Dimakis"},{"name":"P. G. Godfrey"},{"name":"Y. Wu"},{"name":"M. J. Wainwright"},{"name":"K. Ram- chandran"}],"title":{"text":"Network coding for distributed storage systems"}},{"authors":[{"name":"A. G. Dimakis"},{"name":"K. Ramchandran"},{"name":"Y. Wu"},{"name":"C. Suh"}],"title":{"text":"A survey on network codes for distributed storage"}},{"authors":[{"name":"O. Khan"},{"name":"R. Burns"},{"name":"J. Plank"},{"name":"C. Huang"}],"title":{"text":"In search of I/O-optimal recovery from disk failures"}},{"authors":[{"name":"H. Weatherspoon"},{"name":"J. D. Kubiatowicz"}],"title":{"text":"Erasure coding vs. replication: a quantitative comparison"}},{"authors":[{"name":"F. Oggier"},{"name":"A. Datta"}],"title":{"text":"Self-repairing homomorphic codes for dis- tributed storage systems"}},{"authors":[{"name":"V. Rashmi"},{"name":"N. B. Shah"},{"name":"P. V. Kumar"}],"title":{"text":"Explicit construction of optimal exact regenerating codes for distributed storage"}},{"authors":[{"name":"C. Suh"},{"name":"K. Ramchandran"}],"title":{"text":"Exact regeneration codes for distributed storage repair using interference alignment"}},{"authors":[{"name":"K. Rashmi"},{"name":"N. B. Shah"},{"name":"P. V. Kumar"}],"title":{"text":"Optimal exact-regenerating codes for distributed storage at the MSR and MBR points via a product- matrix construction"}},{"authors":[{"name":"I. Tamo"},{"name":"Z. Wang"}],"title":{"text":"MDS Array Codes with Optimal Rebuilding"}},{"authors":[{"name":"V. R. Cadambe"},{"name":"C. Huang"},{"name":"S. A. Jafar"},{"name":"J. Li"}],"title":{"text":"Optimal repair of MDS codes in distributed storage via subspace interference alignment"}},{"authors":[{"name":"D. S. Papailiopoulos"},{"name":"A. G. Dimakis"},{"name":"V. R. Cadambe"}],"title":{"text":"Repair optimal erasure codes through Hadamard designs"}},{"authors":[{"name":"C. Huang"},{"name":"M. Chen"},{"name":"J. Li"}],"title":{"text":"Pyramid codes: ﬂexible schemes to trade space for access efﬁciency in reliable data storage systems"}},{"authors":[{"name":"P. Gopalan"},{"name":"C. Huang"},{"name":"H. Simitci"},{"name":"S. Yekhanin"}],"title":{"text":"On the locality of codeword elements"}},{"authors":[{"name":"D. S. Papailiopoulos"},{"name":"G. Dimakis"},{"name":"C. Huang"},{"name":"J. Li"}],"title":{"text":"Simple regenerating codes: network coding for cloud stor- age"}},{"authors":[{"name":"T. H"},{"name":"R. Koette"},{"name":"M. M´edar"},{"name":"M. Effro"},{"name":"J. Sh"},{"name":"D. Karge"}],"title":{"text":"A ran- dom linear network coding approach to multicast,\u201d IEEE Transactions on Information Theory , vol"}},{"authors":[{"name":"D. S. Papailiopoulos"},{"name":"A. G. Dimakis"}],"title":{"text":"Locally repairable codes"}},{"authors":[{"name":"J. Han"},{"name":"L. A. Lastras-Monta˜no"}],"title":{"text":"Reliable memories with subline accesses"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566887.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T1.3","endtime":"12:30","authors":"Dimitris Papailiopoulos, Alex Dimakis","date":"1341576600000","papertitle":"Locally Repairable Codes","starttime":"12:10","session":"S16.T1: Coded Storage and Caching","room":"Kresge Rehearsal B (030)","paperid":"1569566887"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
