{"id":"1569566817","paper":{"title":{"text":"Combinatorial Message Sharing and Random Binning for Multiple Description Coding"},"authors":[{"name":"Emrah Akyol"},{"name":"Kumar Viswanatha"},{"name":"Kenneth Rose"}],"abstr":{"text":"Abstract\u2014This paper proposes a new multiple description (MD) coding method and an associated achievable rate-distortion region for L ≥ 2 channels. The proposed scheme randomly bins codebooks chosen from the codebook structure, similar to that of the recently proposed combinatorial message sharing (CMS) scheme designed for conditional codebook encoding. The proposed scheme effectively performs multilayer random binning for each subset of the description, which hence enables to utilize the symmetry of a \u201csubset\u201d of the description rates wherever it exists. The new scheme specializes in to the conventional multilayer random binning as an extreme special case.\nIndex Terms\u2014Multiple description coding, source coding, rate- distortion theory"},"body":{"text":"The multiple description (MD) coding problem is a long standing open problem in source coding [1]\u2013[3]. For the two description problem, the rate-distortion region has been completely characterized for the quadratic Gaussian problem [1], and the encoding scheme in [2] was shown to be optimal. While several achievable regions exist [2], [3] for general sources, a complete characterization remains an open problem.\nRecent focus has been more on the general L-description problem ( L > 2). Venkataramani, Kramer and Goyal (VKG) [4] extended the conditional codebook approach of [3] to L descriptions using one common base layer that is broadcasted to all the descriptions. Recently, we extended this scheme by realizing the need for a combinatorial number of base descriptions, namely \u201ccombinatorial message sharing\u201d (CMS). This extension allows ﬂexible adaptation to non-symmetric rates and distortions [5]. In [6], we showed that the CMS achievable region strictly subsumes the VKG region.\nThe above prior work deals with the MD problem within its most general setting, using a conditional codebook approach: base layer codebooks (or the reﬁnement layer codewords) are generated conditioned the previous layer codeword. Such a scheme is best suited for the general setting of non-symmetric rates and distortions. We next consider a practically important special case of symmetric MD, where all descriptions are encoded at the same rate and the distortion only depends on the number of descriptions received, and not the speciﬁc subset of descriptions. An achievable rate-distortion region under these assumptions is derived by Puri, Pradhan and Ramchandran (PPR) [7], [8], by utilizing a random binning method often\nused in distributed source coding. In [7], a coding scheme was proposed, namely the source-channel erasure codes (SCEC), which have similar structure to the (L, k) maximum distance separable codes (MDS). In [8], these codes were layered from (L, L) to (L, 1) resulting in a scheme that we will call the multilayer PPR scheme. This coding scheme achieves several cross sections of the outer bound for this fully symmetric setting, as shown in series of papers by Wang and Wisvanath [9], [10] and close to the outer bound in general [11]. Notably, in [12], this scheme is enhanced by utilizing ideas from quantization splitting and channel (or network) codes.\nHere, we propose a new encoding scheme that leverages the ﬂexibility of our recent CMS framework while incorporating in it random binning techniques. Speciﬁcally, we perform random binning within codebooks structured according to the CMS principle at the base layer. CMS enables this scheme to adapt to non-symmetric rates and distortions, while utilizing the efﬁciency of random binning whenever there is symmetry, including the partially at the subset level. More speciﬁcally, we have a PPR-like multilayer random binning scheme for each subset of the combinatorially organized base layer code- books, hence, we efﬁciently utilize the symmetry with random binning, while the scheme can adapt to the overall asymmetry in rates and distortions.\nAs an example setting to provide intuition for the potential beneﬁts of the proposed scheme, consider a scenario where symmetry is limited to a subset of the description rates, say, R 1 > R 2 = R 3 = R 4 = R. Then, a PPR-like scheme has to bin the codebooks at the same rate of R, due to its highly symmetric structure. However, an excess rate of R 1 − R bits will be unused. On the other extreme, a conditional codebook based scheme will use all descriptions at their full rates, but will not utilize the symmetry of R 2 , R 3 and R 4 . The proposed paradigm, applying multilayer random binning for each subset, will efﬁciently utilize available partial symmetry. At the same time, it adapts to the asymmetry of the overall rates, and hence avoids unused excess rates.\nThis paper is organized as follows: In Section II, we present the notation and an overview of prior L-channel MD schemes. In Section III, we describe the new approach along with the new achievable region for this scheme. Section IV provides conclusions.\nLet { X(i)} i=1,2,... be a zero-mean memoryless and sta- tionary source taking values in alphabet X and let ˆ X be the reconstruction alphabet. The vector [X(1), X(2), ..., X(n)] is compactly denoted by x n . The distortion measure d : X × ˆ X → R + is bounded and additive. We use I L to denote the set {1, 2, .., L}. For parsimony of notation, we employ H(X) to denote the entropy of a discrete random variable X, or differential entropy if X is continuous. For an arbitrary set A, we use 2 A m to denote the set of all subsets of A with cardinality greater or equal to m, i.e.,\nVKG scheme is a generalization to L ≥ 2, of the conditional codebook schemes originally designed for L = 2. Let us present an overview of the encoding scheme. The order of codebook generation of the auxiliary random variables is shown in Figure 1. Observe that the codebook generation follows the order: shared layer → base layer → reﬁnement layer. First, the codebook associated with V I L is generated with the marginal p V IL . Conditioned on each codeword from V I L , the codebook for U l where l ∈ I L , is generated according to their respective conditional densities. Next, for each U A , A ∈ 2 I 2 a single codeword is generated, conditioned on the codeword tuples from the previous layers.\nOn observing a typical sequence x n , the encoder tries to ﬁnd a jointly typical codeword tuple one from each codebook. The codeword index of U l is sent in the description l ∈ I L . Along with the private messages, each descriptions also carries a shared message, which is the codeword index of V I L . Note that V I L is the only shared random variable. U l where l ∈ I L form the base layer random variables and all U A , A ∈ 2 I 2 form the reﬁnement layers.\nIn [5], we generalized VKG by including a common code- book for each subset of the base descriptions. The order of codebook generation of the auxiliary random variables is\nshown in Figure 2 which also speciﬁes the conditioning of the combinatorially shared codebooks. This scheme clearly subsumes VKG as a special case and in fact strictly subsumes its achievable region as was shown in [6].\nHere, we focus on the symmetric MD coding problem, where all descriptions are encoded at the same rate and distortion is determined by how many rather than which descriptions were received. Thus, the achievable rate distortion region is a subset of the L + 1 dimensional space of vectors (R, D 1 , D 2 , ..., D L ) where D k is the distortion incurred if k descriptions are received, and R is the description rate.\nSuppose for now that we know that at least k descriptions are received. Then one may, as a ﬁrst approach, employ the best rate kR source code and encode it for the L channels using a (L, k) maximum distance separable (MDS) code. Thus, if any k descriptions are received we will have the optimal distortion for this rate. However, if more than k descriptions are received the decoder does not gain any new information. Such an encoding scheme is equivalent to gener- ating one codebook at rate kR and having it random binned independently for each of the descriptions. We will refer to such codes as MDS codes.\nAlternatively, one could generate L independent codebooks in order to gain new information for each received description, and randomly bin the codewords so that the decoder can ﬁnd only one sequence jointly typical with the source. However, it is not obvious at ﬁrst that if we can still achieve the optimal distortion for a rate kR code. The key result of [7] shows that with the reception of k descriptions one may recover the source coded at rate kR and monotonically decrease the distortion with reception of additional descriptions. We refer to such codes as \u20181\u201csource channel erasure codes, SCEC. The achievable region RD P P R 1 is formalized below.\nPPR-1 (SCEC) Region: Let Y 1 , ..., Y L be a collection of random variables that are jointly and symmetrically distributed with an arbitrary source random variable X. Then, if at least k descriptions are received and\nRandom Code Generation: Construct a random codebook with p Y i independently for each Y i , i ∈ I L by selecting 2 nR codewords uniformly.\nRandom Binning: To each codebook associate 2 nR bins, each containing approximately 2 n(R −R) codewords.\nEncoding: Given a sourceword x n , ﬁnd the indices i 1 , i 2 , ..., i L such that the corresponding codewords and source x n , y 1 (i 1 ), y 2 (i 2 ), ..., y L (i L ) are jointly typical. Transmit the bin index i l over the channel l, ∀l ∈ L.\nDecoding: For a set of received bin indices indexed by K the decoder ﬁnds for k i ∈ K the y K (i K ) that are jointly typical and each are contained in the bin corresponding to the received index. If more than one such set exists, it declares an error. Otherwise it decodes using g K (y K ). It follows from strong typicality and counting arguments [13], [14] that error event probability vanishes asymptotically for large n.\nIn [8], a multilayer random binning scheme is proposed, which can be roughly described as the concatenation of (L, k) SCEC\u2019s for k = 1, 2, ..L. More speciﬁcally, there are a total of L layers in this coding scheme, and the encoding and decoding can be performed from lower layers to higher layers sequentially. At the l th layer and for any description j, a codebook of size 2 nR l,j is generated using the marginal distribution of Y l,j . The codewords in a codebook are then randomly and independently assigned a total of 2 nR l,j bin indices, for all l and j.\nThe rates R l,j s should be sufﬁciently large such that for any typical source sequence, with high probability there exist code words in these codebooks of a certain layer such that they are jointly typical with the source sequence and the codewords found in previous layers. Using the property of symmetric distribution (see [8]), it can be shown that this can be done with\nAt the decoder, with any l descriptions such that l ∈ I L −1 , the lower l layers are decoded sequentially from the lower to the higher layers in l steps. More precisely, the decoder receives descriptions in the set |A| = l. Taking an induction approach, we assume the ﬁrst l − 1 layers of codewords are correctly decoded, and only consider the decoding of the l th layer. If there exists a unique set of codewords in the bins of the corresponding codebooks, speciﬁed by the descriptions in the set , that are jointly typical with each other, and at the same time they are jointly typical with the correctly decoded lower layer codewords, then the decoder reconstructs using the single letter decoding function g A ; otherwise a decoding failure occurs. For successful decoding, it can be shown that the rates need to satisfy\nR l,j . Let us call the achievable region obtained by this scheme RD P P R 2 .\nHere, we integrate the random binning approach within the CMS coding structure. Basically, we use the PPR approach of layering (L, k) MDS codes for each subset of the base descriptions and continue to use the PPR enhancement de- scriptions, i.e., layered SCEC. Note that at the base layer, we do not have the luxury of using independent codebooks (hence, we have MDS codes instead of SCEC at the base layer), but we can bin the base layer codewords, independently and at different rates for each description. We begin with a four description example, that illustrates the operation and the use of the proposed scheme.\nThe base and reﬁnement codebook generation is shown in Figure 3. Base and reﬁnement layer codebooks are denoted C and C respectively. The rates of the codebooks (denoted r) are determined by the ability to encode successfully by ﬁnding a set of codewords jointly typical with the source and the binning rates (denoted ρ) are determined by the requirement of successfully decoding by ﬁnding a unique jointly typical set of codewords in the received bins. For the base descriptions broadcasted to all channels { 1, 2, 3, 4}, there are 4 layers of MDS codes, a (4, l) code that uses the codebook C 1234,l for l = 1, 2, 3, 4. Hence, there are 4 codebooks for that level. The codeword from C 1234,l can be decoded if l descriptions are received. Note that there is identical coding rates for each of the descriptions, but we bin each codebook, independently 4 times at different binning rates denoted as ρ 1234,l,i , for i = 1, 2, 3, 4 for the codebook C 1234,l . In each bin there are approximately 2 n(r 1234,1 −ρ 1234,1,i codewords that can be decoded with the help of side information, hence only the bin rates contribute to the description rates. The codeword from the codebook C 1234,l can be decoded if any l descriptions are received. For the base descriptions, broadcasted to the subset of three channels A ∈ { 123, 234, 124, ...} there are three layers MDS codes: (3, l) MDS code that uses C A,l for l = 1, 2, 3. Next, we have the layer with |A| = 2 and |A| = 1 in a similar fashion.\nFor the reﬁnement descriptions, there are three layers of SCEC: (4, l) SCEC for l = 1, 2, 3. Note that a (4, 4) SCEC code is not necessary because there is already an (4, 4) MDS code in the base description as the codebook C 1234,4 that is transmitted to all the decoders. Any reﬁnement information can be placed there, since it is decodable only if all of the descriptions are received. Note that a (4, 4) MDS code is identical to a (4, 4) SCEC.\nFor decoding, lets us consider the decoder { 12} which receives the descriptions 1 and 2. This decoder uses C 1234,1 ,\nC 1,2 , C 2,1 , C 2,2 are the enhancement layer codebooks used to reconstruct the source through the single letter function g 1,2 .\nNote that the proposed scheme can utilize the subset sym- metry in our running example of R 1 > R 2 = R 3 = R 4 = R albeit using MDS codes instead of SCEC, for the subset {2, 3, 4}, by setting the encoding and binning rates accord- ingly., i.e., by setting ρ 2,3,4,k,j = ρ j 1 for all k = 1, 2, 3 and similarly, ρ 2,3,k,j = ρ 3,4,k,j = ρ 2,4,k,j = ρ j 2 for k = 1, 2 for the remaining base and reﬁnement layers, we have a scheme that is very similar to PPR-2 for descriptions 2, 3 and 4. The trivial combination of CMS base layer with PPR reﬁnement layer will not be able to utilize this symmetry because the symmetry lies within the subset. Similarly, CMS scheme will not be able to use this symmetry in the subsets, since it is designed for the general setting.\nFor each subset, A of the channels that a base description broadcasts to, we layer (|A|, k) MDS codes for k = 1, 2, .., |A| over the channels in A. For the reﬁnement descriptions, we layer (L, k) SCEC\u2019s for k = 1, 2, ..., L − 1. Decoder A will decode the reﬁnement codewords generated from the codebooks with k ≤ |A| and subset K base descriptions of all codebooks with k ≤ |A ∩ K|. It then reconstructs the source jointly using all received descriptions.\nThe following theorem present the achievable region for the proposed MDC scheme. We call this region, RD CM S −RB .\nTheorem 1. An achievable rate-distortion region with the proposed MDC scheme is the convex closure of all rate- distortion tuples satisfying\n1 \t (3) R l =\nfor some joint distribution, decoding functions and rates satisfying the following\nProof: Consider a joint distribution that satisﬁes the rate and distortion bounds. Then, an encoding scheme with random codebooks and binning can be constructed as follows:\nBase layer codebook generation: For every A ∈ 2 I L 2 , we generate a set of |A| MDS codes, where the k th code is a (|A|, k) MDS code, which is basically a codebook generated with the distribution P V A , and the codeword from this codebook is binned |A| times independently. For each\nindex, q A,k ∈ 2 nr A,k we generate a length n codeword by in- dependently choosing its symbols from a codebook generated with the marginal P U A,k .\nReﬁnement layer codebook generation: We generate a set of L − 1 codes where the k th code is an (L, k) SCEC, which itself consists of a set of L codebooks, one for each channel. The k th codebook for channel l denoted as C k,l is generated as follows. For each index q k,l ∈ 2 nr k,l generate a length n codeword choosing its symbols from a codebook generated with the marginal P U k,l .\nBinning: For each base codebook C A,k , we randomly bin the codewords L independent times for each description, with the binning rate ρ A,k,l for all A ∈ 2 I L 1 , k ∈ I |A| , l ∈ I L .\nFor the reﬁnement codebooks, C k,l , we randomly bin (once), with the binning rate ρ k,l , for all k ∈ I L −1 , l ∈ I L .\nEncoding: Given a length n source sequence x n , the encoder ﬁnds, if possible, two sets of indices q A∈2 IL\nand q I L −1 , I L such that the codewords associated with these indices are jointly typical. If more than one is found, we choose arbitrarily. If none is found, we declare an encoding error ε E by assigning each index to an arbitrary number not used in the encoding.\nDecoding: Decoder A, where A ∈ 2 I L 1 , receives the descriptions associated with A. If these indices indicate an encoding failure ε E , the decoder estimates the source using only the statistics and achieves a distortion bounded by a ﬁnite constant d max . Otherwise, it ﬁnds a unique set of quantization indices such that the corresponding codewords are jointly typical.\nSuccessful encoding guarantees the existence of at least one set of indices. If more than one is found, the decoder declares a decoding failure ε D and reconstructs the source using only the statistics knowledge, achieves a distortion upper bounded by d max . Otherwise, the decoder recovers the quantization indices perfectly and reconstruct the source using g A and achieves an overall distortion of no more than D A + + P{ε E ∪ ε D }d max as in (3) for any and large enough n.\nNext, we will show P{ε E ∪ ε D } → 0 as n → ∞. First, we break up the encoding and decoding error probabilities as follows by the union bound:\nwhere ε D (A) denotes the decoding error event for the decoder A. It follows from the well known property of typical sets and the counting arguments used in [7] that if (5) holds, P{ε E } vanishes. We are left with upper bounding P{ε D (A)|ε c E }. Let the error exponent be Φ lim n →∞ 1 n log P{ε D (A)|ε c E }. Then, using the same error bounding approach in [7] (omitted due to space constraints), we get\nProof: Since R P P R 1 ⊆ R P P R 2 by construction, we will only show R P P R 2 ⊆ R CM S −RB . Setting V A,k,l = Φ in RD CM S −RB for A ∈ 2 I L , k ∈ I |A| , l ∈ I L , where Φ is\ndeterministic, and setting codebook and binning rates at the reﬁnement layer to identical value for each description, i.e., r k,l = r k , ρ k,l = ρ k , ∀l ∈ I L yield R P P R 2 by construction.\nIn this paper, we proposed a novel MD coding scheme based on random binning within the CMS coding structure. The proposed scheme leverages the CMS structure to achieve efﬁcient adaptation to asymmetric rate and distortion while utilizing partial symmetry wherever it exists, with layered random binning. An example is given to provide intuition for potential beneﬁts over prior schemes. Future work includes studying the beneﬁts of the proposed scheme over CMS and other MD schemes based on random binning."},"refs":[{"authors":[{"name":"L. Ozarow"}],"title":{"text":"On a source-coding problem with two channels and three receivers"}},{"authors":[{"name":"E. Gamal"},{"name":"T. Cover"}],"title":{"text":"Achievable rates for multiple descriptions"}},{"authors":[{"name":"Z. Zhang"},{"name":"T. Berger"}],"title":{"text":"New results in binary multiple descriptions"}},{"authors":[{"name":"R. Venkataramani"},{"name":"G. Kramer"},{"name":"K. Goyal"}],"title":{"text":"Multiple description coding with many channels"}},{"authors":[{"name":"K. Viswanatha"},{"name":"E. Akyol"},{"name":"K. Rose"}],"title":{"text":"Combinatorial message sharing for a reﬁned multiple descriptions achievable region"}},{"authors":[{"name":"K. Viswanatha"},{"name":"E. Akyol"},{"name":"K. Rose"}],"title":{"text":"A strictly improved achievable region for multiple descriptions using combinatorial message sharing"}},{"authors":[{"name":"S. Pradhan"},{"name":"R. Puri"},{"name":"K. Ramchandran"}],"title":{"text":"n-channel symmetric multiple descriptions-part i:(n, k) source-channel erasure codes"}},{"authors":[{"name":"R. Puri"},{"name":"S. Pradhan"},{"name":"K. Ramchandran"}],"title":{"text":"n-channel symmetric multiple descriptions-part ii: An achievable rate-distortion region"}},{"authors":[{"name":"H. Wang"},{"name":"P. Viswanath"}],"title":{"text":"Vector gaussian multiple description with individual and central receivers"}},{"authors":[{"name":"H. Wang"},{"name":"P. Viswanath"}],"title":{"text":"Vector gaussian multiple description with two levels of receivers"}},{"authors":[{"name":"C. Tian"},{"name":"S. Mohajer"},{"name":"N. Diggavi"}],"title":{"text":"Approximating the gaussian multiple description rate region under symmetric distortion constraints"}},{"authors":[{"name":"C. Tian"},{"name":"J. Chen"}],"title":{"text":"New coding schemes for the symmetric k- description problem"}},{"authors":[{"name":"T. Berger"}],"title":{"text":"Multiterminal source coding"}},{"authors":[{"name":"I. Csisza"},{"name":"J. K¨orne"}],"title":{"text":"Information theory: coding theorems for discrete memoryless systems , Cambridge Univ Pr, 2011"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566817.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S8.T4.3","endtime":"17:40","authors":"Emrah Akyol, Kumar Viswanatha, Kenneth Rose","date":"1341336000000","papertitle":"Combinatorial Message Sharing and Random Binning for Multiple Description Coding","starttime":"17:20","session":"S8.T4: Multiple Description Coding","room":"Stratton 20 Chimneys (306)","paperid":"1569566817"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
