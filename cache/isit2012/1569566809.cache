{"id":"1569566809","paper":{"title":{"text":"Quantized Min-Sum Decoders with Low Error Floor for LDPC Codes"},"authors":[{"name":"Xiaojie Zhang"},{"name":"Paul H. Siegel"}],"abstr":{"text":"Abstract\u2014The error ﬂoor phenomenon observed with LDPC codes and their graph-based, iterative, message-passing (MP) decoders is commonly attributed to the existence of error-prone substructures in a Tanner graph representation of the code. Many approaches have been proposed to lower the error ﬂoor by designing new LDPC codes with fewer such substructures or by modifying the decoding algorithm. In this paper, we show that one source of the error ﬂoors observed in the literature may be the message quantization rule used in the iterative decoder implementation. We then propose a new quantization method to overcome the limitations of standard quantization rules. Performance simulation results for two LDPC codes commonly found to have high error ﬂoors when used with the ﬁxed-point min-sum decoder and its variants demonstrate the validity of our ﬁndings and the effectiveness of the proposed quantization algorithm."},"body":{"text":"Low-density parity-check (LDPC) codes have been the focus of much research over the past decade as a conse- quence of their near Shannon-limit performance under iterative message-passing (MP) decoding [1]. However, the error ﬂoor phenomenon has hindered the adoption of LDPC codes and iterative decoders in some applications requiring very low error rates. Roughly speaking, an error ﬂoor is an abrupt change in the slope of the error-rate performance curve of an MP decoder in the high SNR region. Since many important applications, such as data storage and high-speed digital communication, often require extremely low error rates, the study of error ﬂoors in LDPC codes remains of considerable practical, as well as theoretical, interest.\nThe most common way to improve the error ﬂoor perfor- mance of LDPC codes has been to redesign the codes to have Tanner graphs with large girth and without small error-prone substructures (EPSs), such as near-codewords [2], trapping sets [3], or absorbing sets [4]. Another approach has been to modify the standard iterative decoding algorithms. In [5], a post-processing decoder was proposed to improve performance by matching the conﬁguration of unsatisﬁed check nodes (CNs) to precomputed trapping sets. The post-processing ap- proaches proposed in [6], [7] increase or decrease the reliabil- ity of messages from certain nodes. A bi-mode erasure decoder to reduce error ﬂoors due to small size EPSs was introduced in [8]. All these modiﬁed decoders either change the message update rules at check nodes or require extra information from an auxiliary code. Adding post-processing stages to the MP\ndecoder also increases the decoding complexity relative to the original decoding algorithms.\nIn ﬁxed-point implementation of iterative MP decoding, efforts were also made to improve the the error-rate perfor- mance in the waterfall region and/or error-ﬂoor region by optimizing parameters of uniform quantization [9]\u2013[12]. Zhao et al. studied the effect of the message clipping and uniform quantization on the performance of the min-sum decoder in waterfall region, and heuristically optimized the number of quantization bits and the quantization step size for selected LDPC codes. In [10], a dual mode adaptive uniform quanti- zation scheme was proposed to better approximate the log- tanh function used in sum-product algorithm (SPA) decoding. Speciﬁcally, for magnitudes less than 1, all quantization bits were used to represent the fractional part; for magnitudes greater than or equal to 1, all bits were dedicated to the representation of the integer part. In [11], [12], Zhang et al. proposed a conceptually similar idea to increase precision in the quantization of the log-tanh function. Uniform quantization was applied to messages generated by both variable nodes and check nodes, but the quantization step sizes used in the two cases were separately optimized. We note, however, that none of these modiﬁed quantization schemes were primarily intended to signiﬁcantly increase the saturation level, or range, of quantized messages, and in their reported simulation results, error ﬂoors can still be clearly observed.\nIn this work, we investigate the cause of error ﬂoors in binary LDPC codes from the perspective of the MP decoder implementation, with special attention to limitations that de- crease the numerical accuracy of messages passed during decoding. Based upon an analysis of the decoding process in the vicinity of an EPS, we propose a novel quantization method, (q + 1)-bit quasi-uniform quantization, that does not require a modiﬁcation of either the decoding update rules or the graphical code representation upon which the iterative MP decoder operates. The proposed quantization method has an extremely large saturation level, a property that, to the best of our knowledge, distinguishes if from other quantization techniques for iterative MP decoding that have appeared in the literature. We present simulation results for min-sum de- coding and some of its variants that demonstrate a signiﬁcant reduction in the error ﬂoors of two representative LDPC codes, with no increase in the decoding complexity. Similar results, not included in this paper due to space constraints, verify\nthe applicability of the new quantization method to other MP decoding algorithms, such as the sum-product algorithm (SPA) often used in belief-propagation (BP) decoding.\nThe remainder of the paper is organized as follows. In Section II, we investigate the impact that message quanti- zation can have on MP decoder performance and the error ﬂoor phenomenon. In Section III, we propose an enhanced quantization method intended to overcome the limitations imposed by traditional quantization rules. In Section IV, we incorporate the new quantizer into various versions of min- sum decoding and, through computer simulation of several LDPC codes known for their high error ﬂoors, demonstrate the signiﬁcant improvement in error-rate performance that this affords. Section V concludes the paper.\nThe term trapping set proposed by Richardson [3] is op- erationally deﬁned as a subset of variable nodes (VNs) that is susceptible to errors under a certain iterative MP decoder over an MBIOS channel. Hence, this concept depends on both the channel and the decoding algorithm. To facilitate our discussion, we deﬁne the term absolute trapping set from a graph-theoretic perspective, independent of the channel and the decoder. Let G = (V ∪ C, E) denote the Tanner graph of a binary LDPC code with the set of VNs V = {v 1 , . . . , v n }, the set of CNs C = {c 1 , . . . , c m }, and the set of edges E.\nDeﬁnition 1 (absolute trapping set): A subset of V ∪ C is an (a, b) absolute trapping set if there are b odd-degree check nodes in the subgraph induced by a variable nodes, the subgraph is connected, and it has at least one check node of degree one.\nIt is worth noting that the deﬁnition of absolute trapping set is slightly different from the conventional generalized deﬁnition of trapping set [5] of which a stopping set is a special case. By requiring at least one check node of degree one, we exclude stopping sets from our deﬁnition of absolute trapping set. As we will discuss later in this section, these degree-one check nodes are essential because they are able to pass correct extrinsic messages into the trapping set. In the literature, almost all trapping sets of interest have degree-one check nodes, and therefore, are absolute trapping sets. Hence, unless indicated, all trapping sets referred to in this paper are absolute trapping sets as well.\nBefore introducing the main results, we ﬁrst present some important notations and deﬁnitions. Let S be the induced subgraph of an (a, b) trapping set contained in G with VN set V S ⊆ V and CN set C S ⊆ C. Let C 1 ⊆ C S be the set of degree-one CNs in the subgraph S, and let V 1 ⊆ V S be the set of neighboring VNs of CNs in C 1 . We refer to a message of an edge adjacent to variable node v as a correct message if its sign reﬂects the correct value of v, and as an incorrect message otherwise.\nIn analogy to the deﬁnition of computation tree in [13], we deﬁne a k-iteration computation tree as follows.\nDeﬁnition 2 (k-iteration computation tree): A k-iteration computation tree T k (v) for an iterative decoder in the Tanner\ngraph G is a tree graph constructed by choosing variable node v ∈ V as its root and then recursively adding edges and leaf nodes to the tree that participate in the iterative message-passing decoding during k iterations. To each vertex that is created in T k (v), we associate the corresponding node update function in G.\nLet D(u) be the set of all descendants of the vertex u in a given computation tree.\nDeﬁnition 3 (separation assumption): Given a Tanner graph G and a subgraph S induced by a trapping set, a variable node v ∈ V 1 is said to be k-separated if, for at least one neighboring degree-one check node c ∈ C 1 of v in S, no variable node v ∈ V S belongs to D(c) ⊂ T k (v). If every v ∈ V 1 is k-separated, the subgraph S is said to satisfy the k-separation assumption.\nWith the separation assumption, the descendants of c ∈ C 1 are separated from all the nodes in the trapping set, meaning that messages originating from the trapping set would not cycle back through check node c within k iterations. We note that the separation assumption is much weaker than the isolation assumption in [14] \u2013 the separation assumption applies only to VNs v ∈ V 1 and their neighboring CNs in C 1 .\nTo get further insight into the connection between trapping sets and decoding failures of iterative MP decoders, we con- sider the min-sum decoder, whose VN and CN update rules we now brieﬂy recall. A VN v i receives input message L ch i from the channel, which can be the log-likelihood ratio (LLR) of the corresponding channel output. Denote by L i→j and L j→i the messages sent from v i to c j and from c j to v i , respectively, and denote by N (k) the set of neighboring nodes of VN v k (or CN c k ). Then, the message sent from v i to c j in min-sum decoding is given by\nIt can been seen from (1) and (2) that the min-sum decoding algorithm is linear, meaning that linearly scaling all input messages from the channel would not affect the decoding performance.\nTheorem 1: Let G be the Tanner graph of a variable-regular LDPC code that contains a subgraph S induced by a trapping set. When S satisﬁes the k-separation assumption and when the messages from the BSC to all VNs outside S are correct, the min-sum decoder can successfully correct all erroneous VNs in S, provided k is large enough.\nProof: Assume VN v r ∈ V 1 in S is k-separated and the corresponding k-iteration computation tree is T k (v r ). Let c r ∈ C 1 be the neighboring degree-one CN of v r in S. By as- sumption, all descendants of c r in T k (v r ) receive correct initial messages from the BSC. Denote the subtree starting with CN\nc r as T (c r ). All VN nodes in T (c r ) receive correct channel messages and these messages have the same magnitude.\nNow, with the VN/CN update rules of the min-sum decoder, we analyze the messages sent from the descendants of c r in T (c r ). First, according to the CN update rule described in (2), all messages received by a VN from its children CNs in T (c r ) must have the same sign as the message received from the channel by this VN, because all the messages passed in T (c r ) are correct. Moreover, since the LDPC code is variable-regular and all the channel messages from the BSC have the same magnitude, it can be shown that, for the min-sum decoder, all incoming messages received by a VN from its children CNs in T (c r ) must have the same magnitude as well. Let |L l | be the magnitude of the messages sent by the VNs whose shortest path to a leaf VN contains l CNs in T (c r ). Hence, |L 0 | is the magnitude of messages sent by leaf VNs, as well as the magnitude of channel inputs. Then, we have\n> (d v − 1) l |L 0 | \t (3) where d v is the variable node degree. Hence, it can be seen that the magnitudes of messages sent towards the root CN c r of the computation tree T (c r ) grow exponentially, with d v − 1 as the base, in every upper VN level. Therefore, the magnitude of the message sent from c r to its parent node v r , the k- separated root VN of T k (v r ), in the l-th iteration is greater than (d v − 1) l |L 0 | for l ≤ k.\nNow, let us consider the subtree, denoted by T (c ), formed by branches in T k (v r ) that start from a neighboring CN c ∈ C S \\ C 1 . It is not hard to see that there exists an integer t such that any t-level subtree starting from a VN v ∈ S in T (c ), i.e., a subtree with t levels of VNs, must have at least one k-separated VN as its descendant. It is obvious that t ≤ a and the value t depends on the structure of the trapping set. Note that the leaf VNs of these t-level subtrees are not necessarily the leaf VNs of T k (v r ). Suppose the message received by v r from its child c ∈ C S after l iterations, denoted by L l , has a different sign than the message received from c r ∈ C 1 ; otherwise, v r would already be corrected. By considering each such t-level subtree as a \u201csupernode\u201d with (d v − 1) t children, we get the following upper bound\n|L l | < |L 0 | (d v − 1) t − 1 l/t . \t (4) Therefore, we can see that, if l ≤ k is large enough and there is no limitation imposed on the magnitude of messages, the correct messages coming from outside of the trapping set to VNs in V 1 through their neighboring CNs in C 1 will eventually have greater magnitude than the sum of incorrect messages from other neighboring CNs, and the decoder will ultimately correct all VNs in the trapping set.\nCorollary 2: Let G be the Tanner graph of a variable- regular LDPC code that contains a subgraph S induced by a trapping set. When S satisﬁes the k-separation assumption and the channel messages from the AWGNC to all VNs outside S are correct, the min-sum decoder can successfully correct all erroneous VNs in S, provided k is large enough.\nProof: Consider the minimum magnitude of all input LLRs from the AWGNC as |L 0 |, and follow the proof of Theorem 1.\nTheorem 1 and Corollary 2 can be easily extended to several variations on min-sum decoding, such as attenuated min-sum (AMS) decoding and offset min-sum (OMS) decoding [15], as long as the attenuation factor and the offset factor are ﬁxed constants.\nDeﬁnition 4 (unsaturated decoder): An iterative MP de- coder that does not impose any limitation on the magnitudes of messages is called an unsaturated MP decoder.\nFor most LDPC codes, the trapping sets typically satisfy the k-separation assumption only for small values of k. Neverthe- less, as described more fully in Section IV, in computer sim- ulations of unsaturated min-sum decoding applied to several LDPC codes traditionally associated with high error ﬂoors, we have not observed, in tens of billions of channel realizations of both the BSC and the AWGNC, any decoding failure in which the error patterns correspond to the support of a small trapping set. Similar results were reported in [16], where no error ﬂoors were observed when unsaturated BP decoding was applied to selected LDPC codes on the AWGN channel.\nAs reported in the literature, most hardware implementa- tions and their computer-based simulations use some form of uniform quantization. We will refer to uniform quantizers with quantization step ∆ and q-bit representation of quantization levels, with one of the q bits denoting the sign. The quantized values are l∆ for −N ≤ l ≤ N , where N = 2 q−1 − 1.\nAs shown in the proof of Theorem 1 and Corollary 2, when a trapping set satisﬁes the k-separation assumption for a large value of k, the magnitudes of correct messages outside the trapping set grow exponentially in the number of iterations. Therefore, it would be desirable for the message quantizer to capture, at least to some extent, the exponential increase of these message magnitudes while retaining precision in the representation of messages with smaller magnitudes. To this end, we propose a new (q + 1)-bit quasi-uniform quantization method that adds an additional bit to q-bit uniform quantiza- tion to indicate a change of step size in the representation of large message magnitudes. Hence, the messages after quanti- zation will belong to an alphabet of size 2 q+1 −1. Speciﬁcally, the (q + 1)-bit quasi-uniform quantization rule is given by\n            \n           \nwhere N = 2 q−1 − 1, −N + 1 ≤ l ≤ N − 1, 1 ≤ r ≤ N , and d is a quantization parameter within the range (1, d v − 1]. Generally, the values represented by the (q + 1)-bit quasi- uniform quantization message (0, l) are l∆, and the values of message (1, ±r) are ±d r N ∆ respectively. For messages within the range of [−N ∆, N ∆], the new quasi-uniform quantizer provides the same precision as a q-bit uniform quantizer with quantization step ∆. For messages outside that range, non-uniform quantization with increasing step sizes of the form d r N ∆ is used to allow reliable messages to be more accurately represented.\nSince the range of uniformly quantized messages in MP decoders is small in practice, the correct messages outside a trapping set could reach the saturation level within a few iterations. As a result, even though correct, these messages may not be large enough to offset the contribution of incorrect incoming messages for problematic VNs. Hence, even after optimization of the step and size of a uniform quantizer, the decoder may not produce the same error ﬂoor performance as an unsaturated min-sum decoder [9]. In contrast, the saturation levels of the proposed (q+1)-bit quasi-uniform quantizer are greatly extended, allowing the correct messages outside a trapping set to grow large enough to overcome all incorrect messages reaching the problematic VNs from other VNs within the trapping set.\nAlthough the motivation for the proposed quasi-uniform quantization method came from an analysis of min-sum de- coder behavior on variable-regular LDPC codes, the technique can also be adapted to decoding of irregular LDPC codes by suitably adjusting the parameter d. We have also found that the proposed quasi-uniform quantization method works well with most iterative message-passing decoding algorithms, including the usual variants on min-sum decoding and various approximations to the SPA. These results will be reported elsewhere.\nTo demonstrate the improved performance offered by our proposed quasi-uniform quantization method, we compare its error-rate performance to that of uniform quantization with min-sum decoding applied to two known LDPC codes on the BSC and the AWGNC. The two LDPC codes we evaluated are a rate-0.3 (640,192) quasi-cyclic (QC) LDPC code [8] and the rate-0.5 (2640,1320) Margulis LDPC code [2]. The frame error rate (FER) curves are based on Monte Carlo simulations that generated at least 200 error frames for each point in the plots, and the maximum number of decoding iterations was set to 200.\nThe (640,192) QC-LDPC code, designed by Han and Ryan [8], is a variable-regular code with variable degree 5 and check degrees ranging from 5 to 9. It has 64 isomorphic (5,5) trapping sets and 64 isomorphic (5,7) trapping sets. We applied our exhaustive trapping set search algorithm [17] to this code, and these are the only two types of (a, b) trapping set for a ≤ 15 and b ≤ 7. The error ﬂoor starts relatively high\nfor saturated decoders, so it is quite easy to reach the error ﬂoor with Monte Carlo simulation.\nFigs. 1\u20134 show the simulation results for various types of quantized min-sum decoders and unsaturated decoders. For the BSC, we scaled the magnitudes of decoder input messages from the channel to 1, since for linear decoders, such as Gallager-B and the min-sum decoder, the scaling of channel input messages does not affect the decoding performance. For attenuated and offset min-sum decoding, we can compensate for the scaling by adjusting the attenuation and the offset factor, respectively. The step size ∆ of the uniform quantizer and of the uniformly quantized range of the quasi-uniform quantizer, is set to 1 in Fig. 1 and 0.5 in the rest. So, for exam- ple, when ∆ = 1, the 3-bit uniform quantizer produces values {±3, ±2, ±1, 0}, and the (3+1)-bit quasi-uniform quantizer yields values in {0, ±1, ±2, ±3, ±9, ±27, ±81, ±243} when d = 3. In the simulation, the parameter d was heuristically chosen, and when q is large, a small d would be enough to represent a large range of magnitudes.\nIn Fig. 1, we see that the slope of the error ﬂoors resulting from uniform quantization is close to that of the Gallager-B decoder. This is because, when most messages saturate at the same magnitude, min-sum decoding essentially degenerates\nto Gallager-B decoding, relying solely upon the signs of messages. In comparison to the uniform quantizer with the same number of bits, the proposed quasi-uniform quantization method signiﬁcantly reduces the error ﬂoor and provides error- rate performance very close to that of an unsaturated decoder.\nIn all of the decoding failures observed when using the quasi-uniform quantizer, no error pattern corresponded to the support of a small trapping set. With uniform quantization, on the other hand, almost all of the decoding failures cor- responded to small trapping set supports when the crossover probability of the BSC was small or the SNR of the AWGNC was high. We also compared decoder performance on se- quences in which every VN in a single (5,5) or (5,7) trapping set of the (640,192) code was incorrect, with all other VNs set to correct values. In all cases, the unsaturated min-sum decoder and the min-sum decoder with the proposed quantization method decoded successfully, while decoders with the uniform quantizer failed. The same results were also obtained for the (12,4) and (14,4) trapping sets in the Margulis code.\nIn this paper, we have shown that the use of uniform quantization in iterative message-passing decoding can be a\nsigniﬁcant factor contributing to the error ﬂoor phenomenon in LDPC code performance. To address this problem, we proposed a novel (q+1)-bit quasi-uniform quantization method that effectively extends the dynamic range of the quantizer. Without modifying the CN and VN update rules or adding extra stages to standard iterative decoding algorithms, the use of this quantizer was shown to signiﬁcantly lower the error ﬂoors of two well-studied LDPC codes when used with min- sum decoding and its variants on the BSC and AWGNC. Although not shown here, the results extend to other iterative message-passing decoding algorithms.\nThis work was supported in part by the Center for Magnetic Recoding Research at the University of California, San Diego and by the NSF under Grant CCF-0829865. The authors would like to thank Brian Butler for helpful discussions."},"refs":[{"authors":[{"name":"R. G. Gallager"}],"title":{"text":"Low-density parity-check codes"}},{"authors":[{"name":"D. MacKay"},{"name":"M. Postol"}],"title":{"text":"Weakness of Margulis and Ramanujan- Margulis low-density parity check codes"}},{"authors":[{"name":"T. Richardson"}],"title":{"text":"Error-ﬂoors of LDPC codes"}},{"authors":[{"name":"L. Dolecek"},{"name":"Z. Zhang"},{"name":"V. Anantharam"},{"name":"M. Wainwright"},{"name":"B. Nikolic"}],"title":{"text":"Analysis of absorbing sets and fully absorbing sets of array-based LDPC codes"}},{"authors":[{"name":"E. Cavus"},{"name":"B. Daneshrad"}],"title":{"text":"A performance improvement and error ﬂoor avoidance technique for belief propagation decoding of LDPC codes"}},{"authors":[{"name":"Z. Zhang"},{"name":"L. Dolecek"},{"name":"B. Nikoli´c"},{"name":"V. Anantharam"},{"name":"M. Wainwright"}],"title":{"text":"Lowering LDPC error ﬂoors by postprocessing"}},{"authors":[{"name":"N. Varnica"},{"name":"M. P. C. Fossorier"},{"name":"A. Kavcic"}],"title":{"text":"Augmented belief propagation decoding of low-density parity-check codes"}},{"authors":[{"name":"Y. Han"},{"name":"W. E. Ryan"}],"title":{"text":"Low-ﬂoor decoders for LDPC codes"}},{"authors":[{"name":"J. Zhao"},{"name":"F. Zarkeshvari"},{"name":"A. Banihashemi"}],"title":{"text":"On implementation of min- sum algorithm and its modiﬁcations for decoding LDPC codes"}},{"authors":[{"name":"T. Zhang"},{"name":"Z. Wang"},{"name":"K. Parhi"}],"title":{"text":"On ﬁnite precision implementation of LDPC codes decoder"}},{"authors":[{"name":"Z. Zhang"},{"name":"L. Dolecek"},{"name":"B. Nikoli´c"},{"name":"V. Anatharam"},{"name":"M. J. Wainwright"}],"title":{"text":"Design of LDPC decoders for improved low error rate performance: quantization and algorithm choices"}},{"authors":[{"name":"Z. Zhang"}],"title":{"text":"Design of LDPC decoders for improved low error rate performance"}},{"authors":[{"name":"B. Frey"},{"name":"R. Koetter"},{"name":"A. Vardy"}],"title":{"text":"Signal-space characterization of iterative decoding"}},{"authors":[{"name":"S. K. Planjery"},{"name":"D. Declercq"},{"name":"S. K. Chilappagari"},{"name":"B. Vasic"}],"title":{"text":"Multilevel decoders surpassing belief propagation on the binary symmetric channel"}},{"authors":[{"name":"J. Chen"},{"name":"A. Dholakia"},{"name":"E. Eleftheriou"},{"name":"M. Fossorier"},{"name":"X. Hu"}],"title":{"text":"Reduced-complexity decoding of LDPC codes"}},{"authors":[{"name":"B. Butler"},{"name":"P. Siegel"}],"title":{"text":"Error ﬂoor approximation for LDPC codes in the AWGN channel"}},{"authors":[{"name":"X. Zhang"},{"name":"P. H. Siegel"}],"title":{"text":"Efﬁcient algorithms to ﬁnd all small error- prone substructures in LDPC codes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566809.pdf"},"links":[{"id":"1569565883","weight":6},{"id":"1569565867","weight":2},{"id":"1569564605","weight":2},{"id":"1569566981","weight":2},{"id":"1569566605","weight":2},{"id":"1569566683","weight":2},{"id":"1569565551","weight":2},{"id":"1569566761","weight":4},{"id":"1569566591","weight":2},{"id":"1569552245","weight":4},{"id":"1569565613","weight":2},{"id":"1569565355","weight":2},{"id":"1569564469","weight":2},{"id":"1569564897","weight":11},{"id":"1569558325","weight":2},{"id":"1569565837","weight":4},{"id":"1569566303","weight":4},{"id":"1569563411","weight":2},{"id":"1569560427","weight":4},{"id":"1569565317","weight":4},{"id":"1569564249","weight":4},{"id":"1569565809","weight":4},{"id":"1569558483","weight":2},{"id":"1569566089","weight":2},{"id":"1569566795","weight":4},{"id":"1569561679","weight":2},{"id":"1569566787","weight":2},{"id":"1569566895","weight":9},{"id":"1569566865","weight":6},{"id":"1569565321","weight":2},{"id":"1569564311","weight":4},{"id":"1569566167","weight":2},{"id":"1569566679","weight":2},{"id":"1569563981","weight":4},{"id":"1569561085","weight":4},{"id":"1569566617","weight":2},{"id":"1569559565","weight":2},{"id":"1569566311","weight":2},{"id":"1569555999","weight":2},{"id":"1569558859","weight":2},{"id":"1569565199","weight":4},{"id":"1569566369","weight":2},{"id":"1569561143","weight":2},{"id":"1569565535","weight":2},{"id":"1569566423","weight":6},{"id":"1569565257","weight":2},{"id":"1569559805","weight":4},{"id":"1569558901","weight":9},{"id":"1569565735","weight":2},{"id":"1569559111","weight":2},{"id":"1569565839","weight":2},{"id":"1569565915","weight":2},{"id":"1569552251","weight":2},{"id":"1569566425","weight":2},{"id":"1569566445","weight":2},{"id":"1569565559","weight":9},{"id":"1569566909","weight":2},{"id":"1569564857","weight":4},{"id":"1569566913","weight":4},{"id":"1569566447","weight":4},{"id":"1569565817","weight":2},{"id":"1569565279","weight":2},{"id":"1569566003","weight":2},{"id":"1569565185","weight":4},{"id":"1569565469","weight":9},{"id":"1569565029","weight":2},{"id":"1569565357","weight":2},{"id":"1569561245","weight":2},{"id":"1569566505","weight":2},{"id":"1569566853","weight":2},{"id":"1569566695","weight":2},{"id":"1569566673","weight":2},{"id":"1569565739","weight":2},{"id":"1569566233","weight":2},{"id":"1569566297","weight":6},{"id":"1569566501","weight":2},{"id":"1569565439","weight":4},{"id":"1569551347","weight":2},{"id":"1569565415","weight":4},{"id":"1569565493","weight":11},{"id":"1569566805","weight":6},{"id":"1569566293","weight":2},{"id":"1569565665","weight":4},{"id":"1569566983","weight":2},{"id":"1569566779","weight":2},{"id":"1569565765","weight":4},{"id":"1569565925","weight":2},{"id":"1569565093","weight":2},{"id":"1569565241","weight":4},{"id":"1569565661","weight":2},{"id":"1569566887","weight":4},{"id":"1569564595","weight":4},{"id":"1569566917","weight":4},{"id":"1569565353","weight":2},{"id":"1569566547","weight":2},{"id":"1569565177","weight":6},{"id":"1569566595","weight":2},{"id":"1569566529","weight":2},{"id":"1569565375","weight":2},{"id":"1569566639","weight":4},{"id":"1569566819","weight":2},{"id":"1569566713","weight":2},{"id":"1569565541","weight":4},{"id":"1569564247","weight":2},{"id":"1569564437","weight":2},{"id":"1569566533","weight":11},{"id":"1569563975","weight":2},{"id":"1569564861","weight":4},{"id":"1569564787","weight":2},{"id":"1569565271","weight":2},{"id":"1569561185","weight":6},{"id":"1569566075","weight":6},{"id":"1569558779","weight":2},{"id":"1569566817","weight":4},{"id":"1569564923","weight":4},{"id":"1569566299","weight":2},{"id":"1569564281","weight":2},{"id":"1569565769","weight":9},{"id":"1569566601","weight":2},{"id":"1569565805","weight":2},{"id":"1569563919","weight":2},{"id":"1569557851","weight":2},{"id":"1569565861","weight":9},{"id":"1569566147","weight":2},{"id":"1569565537","weight":2},{"id":"1569565997","weight":2},{"id":"1569565035","weight":4},{"id":"1569564961","weight":18},{"id":"1569567013","weight":2},{"id":"1569564253","weight":2},{"id":"1569565853","weight":2},{"id":"1569563725","weight":2},{"id":"1569551539","weight":2},{"id":"1569564505","weight":2},{"id":"1569565635","weight":2},{"id":"1569561397","weight":6},{"id":"1569565731","weight":2},{"id":"1569566797","weight":4},{"id":"1569565707","weight":2},{"id":"1569566375","weight":4},{"id":"1569565143","weight":2},{"id":"1569566973","weight":2},{"id":"1569565031","weight":2},{"id":"1569565139","weight":2},{"id":"1569564807","weight":2},{"id":"1569566443","weight":2},{"id":"1569566727","weight":2},{"id":"1569565315","weight":4},{"id":"1569560581","weight":2}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T5.3","endtime":"12:30","authors":"Xiaojie Zhang, Paul H. Siegel","date":"1341576600000","papertitle":"Quantized Min-Sum Decoders with Low Error Floor for LDPC Codes","starttime":"12:10","session":"S16.T5: Decoding Techniques for LDPC Codes","room":"Kresge Little Theatre (035)","paperid":"1569566809"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
