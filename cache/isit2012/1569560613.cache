{"id":"1569560613","paper":{"title":{"text":"On the equivalence between Stein identity and de Bruijn identity"},"authors":[{"name":"Sangwoo Park"},{"name":"Erchin Serpedin"},{"name":"Khalid Qaraqe"}],"abstr":{"text":"Abstract\u2014This paper illustrates the equivalence between two fundamental results: Stein identity, originally proposed in the statistical estimation realm, and de Bruijn identity, considered for the ﬁrst time in the information theory ﬁeld. Two distinctive extensions of de Bruijn identity are presented as well. For arbitrary but ﬁxed input and noise distributions, the ﬁrst-order derivative of differential entropy is expressed by means of a function of the posterior mean, while the second-order derivative of differential entropy is manifested in terms of a function of Fisher information. Several applications exemplify the utility of the proposed results."},"body":{"text":"De Bruijn identity, which shows a link between differential entropy and Fisher information, was ﬁrst mentioned and exploited by Stam in the context of building the ﬁrst rigorous proof of the entropy power inequality (EPI), a fundamental result exploited by Shannon to prove a lower bound on the capacity of additive Gaussian noise channels. Recently, a renewed interest was manifested in the applications of de Bruijn identity in estimation and turbo (iterative) decoding schemes, and in relating the input-output mutual information with the minimum mean-square error (MMSE) of additive Gaussian and non-Gaussian noise channels [1], [2].\nA large number of approaches have been applied to prove de Bruijn identity. Amongst them, herein paper, we consider the most direct approach because it shares one common technique, viz. integration by parts, with another well-known result- referred to as Stein identity [3], [4]. Stein identity points out a relationship between the expectations of a function (whose variable is a Gaussian random variable (RV)) and its ﬁrst-order derivative, and has been also widely researched with tremendous success in many applications from diverse ﬁelds such as statistics, economics, probability theory, decision theory, and graph theory (see e.g., [5]).\nThe main theme of this paper is to investigate how de Bruijn identity is connected to Stein identity. To compare these identities, the following additive noise channel is considered throughout the paper:\nwhere a denotes a nonnegative deterministic parameter, and RVs X and W are assumed arbitrary and independent. When\nRV W is Gaussian, the equivalence between the generalized Stein and de Bruijn identities is proved. In particular, when RV Y is Gaussian, not only Stein and de Bruijn identities are equivalent, but also they are equivalent to the heat equation identity [5].\nThe second major goal of this paper is to generalize de Bruijn identity in two distinctive ways. Considering the ﬁxed noise distribution channel model (1), the ﬁrst-order derivative of differential entropy of output signal Y will be expressed as a function of the posterior mean E X |Y [ ·|·], while the second- order derivative of differential entropy of output signal Y\nwill be represented explicitly in terms of Fisher information. Even though some of these relationships do not include Fisher information, they still show fundamental relationships among basic concepts in information theory and statistical estimation theory, and these relationships hold for arbitrary noise channels.\nSeveral applications are brieﬂy mentioned to illustrate the usefulness of the proposed new results. Costa\u2019s entropy power inequality (EPI) is derived in a simpler and alternative way. Cram´er-Rao lower bound (CRLB) [6], Bayesian Cram´er-Rao lower bound (BCRLB) [7], and a new lower bound- tighter than the BCRLB- for the mean square error (MSE) in Bayesian estimation can be further derived by exploiting the proposed results. Even though some of the proposed applications have already been proved before, herein paper a series of novel rela- tionships and perspectives are presented on these applications.\nThe rest of this paper is organized as follows. Preliminary results and relationships between Stein and de Bruijn identities are provided in Section II. Two distinctive extensions of de Bruijn identity are presented in Section III. In Section IV, several potential applications of the aforementioned results are brieﬂy mentioned due to lack of space. Finally, conclusions are stated in Section V.\nTo interconnect Stein identity and de Bruijn identity, ﬁrst we will introduce three fundamental results.\nTheorem 1 (De Bruijn\u2019s Identity [8], [9]): Given the chan- nel model (1), let X be an arbitrary RV with a ﬁnite second- order moment, and W be a Gaussian RV with zero mean and unit variance. Independence between RVs X and W is also\nh(Y ) = 1 2\nwhere h( ·) stands for differential entropy, J(Y ) = E Y [S Y (Y ) 2 ], E Y [ ·] is the expectation with respect to Y , S Y (Y ) is deﬁned as d log f Y (y; a)/dy, and log denotes the natural logarithm.\nTheorem 2 (Generalized Stein Identity [10]): Let Y be an absolutely continuous RV. If the probability density function (pdf) f Y (y) satisﬁes lim y →±∞ k(y)f Y (y) = 0, and\nfor any function r(y) which satisﬁes E Y [ |r(Y )t(Y )|] < ∞, E Y [ r(Y ) 2 ] < ∞, and E Y [ k(Y ) d dy r(Y ) ] < ∞. In particu- lar, when Y is a Gaussian RV with mean µ y and variance σ 2 y , (2) is simpliﬁed to\nEquation (3) represents the well-known classic Stein identity. Proof: See [10].\nTheorem 3 (Heat Equation Identity [5]): Let Y be a Gaus- sian RV with mean µ and variance 1 + a. Assume g(y) is a twice continuously differentiable function, and that both g(y) and | d dy g(y) | are 1 O(e c |y| ) for some 0 ≤ c < ∞. Then,\nSince Stein identity (3) is valid only for Gaussian RVs, we have to consider either a special case of de Bruijn identity in Theorem 1 or use a generalized version of Stein identity (2) to compare it with de Bruijn identity. Therefore, given (1) with W a Gaussian RV, the equivalence between Stein identity and de Bruijn identity is next established.\nTheorem 4: Given the channel (1), let X be an arbitrary RV with a ﬁnite second-order moment, and W be a Gaussian RV with zero mean and unit variance. Independence between RVs X and W is also assumed. If r(y; a) is deﬁned as\nin (2), de Bruijn identity is equivalent to the generalized Stein identity. In particular, if X is also a Gaussian RV, and g(y; a) = − log f Y (y; a) in (4), then de Bruijn, Stein, and heat equation identities are all equivalent to one another.\nProof: Like the proof of Theorem 3 in [5], the equivalence is proved by showing that de Bruijn identity is derived from Stein identity, and vice versa. Before illustrating the details of the proof, Lemma 1 will be exploited.\nLemma 1: For RVs X and Y deﬁned in (1), the following relations hold:\n1 2\n= − 1 2\nf Y (Y ; a) f Y (Y ; a)\nUsing Stein identity, dominated convergence theorem (DCT), and Fubini\u2019s theorem, the equalities in (6) are established. Adopting the change of variables y = u +\nDue to (5) and normality of conditional pdf f Y |X (y |u; a), (7) is veriﬁed. Re-deﬁning w as (y −u)/\nf Y |X (y |u; a) d da log f Y (y; a)dy − d da\nSecond, the generalized Stein identity is derived from de Bruijn identity as follows. First, deﬁne the function\nand express its expectation as follows E Y [g(Y ; a)]\n( u − x √ a\nr(u; a)dudx −\n( u − x √ a\nwhere Φ Y |X ( ·) denotes the standard conditional normal cu- mulative density function. Then, differentiate both sides of (8) with respect to parameter a:\nr(u; a)du + (d/da)q(a), equations (B) and (C) are simpliﬁed as follows:\n( u − X √ a\n( u −X √ a\n= − 1 2\nf Y (u; a) f Y (u; a)\nand 1\n1 2\nfrom de Bruijn identity, we derive the generalized Stein identity as\n1 2\n= 1 2\nwhere t(y; a) = − d dy f Y (y; a)/f Y (y; a), and ⇐⇒ denotes equivalence between before and after the notation.\nThe main ingredient for establishing de Bruijn identity is that Gaussian density functions satisfy the heat equation. How- ever, general pdfs do not satisfy the heat equation. Therefore, to extend de Bruijn identity to additive non-Gaussian noise channels, a general relationship between differentials of a pdf with respect to y and a of the form:\nwill be exploited. Simple calculations show that (10) can be obtained directly from equation (1). The relationship (10) represents the key ingredient in establishing a connection between the derivative of differential entropy and posterior mean, as illustrated by the following theorem.\nTheorem 5: Consider the channel (1), where X and W are arbitrary RVs, independent of each other. Given the following assumptions:\n, lim\nthe ﬁrst-order derivative of differential entropy can be ex- pressed as\nProof: This theorem is proved using (10), integration by parts, and DCT. Since the proof is similar to the one in Theorem 6, the details of the proof are delegated to [11].\nFor additive non-Gaussian noise channels, differential en- tropy appears not to be expressible in terms of Fisher infor- mation. Instead, differential entropy is expressed by a function of the posterior mean as shown in Theorem 5. Fortunately, several noise distributions of interest in communication prob- lems satisfy the required assumptions in Theorem 5 (e.g., Gaussian, gamma, exponential, chi-square with restrictions on parameters, Rayleigh, etc). Therefore, Theorem 5 is quite\npowerful. In particular, if the posterior mean E X |Y [X |Y ] is expressed by a polynomial function of Y , e.g., X and W are independent Gaussian RVs in (1) or RVs belonging to the natural exponential family of distribution [12], then (11) can be expressed in simpler forms. We would like also to mention that a result similar in nature to Theorem 5 was reported before in [2] using a different approach. Similarly to [2], a series of specialized applications of Theorem 5 could be envisaged.\nNow, we consider the second-order derivative of differential entropy. One interesting property of the second-order deriva- tive of differential entropy is that it can always be expressed as a function of Fisher information.\nTheorem 6: Consider the channel (1), where X and W are two arbitrary RVs, independent of each other. Given the following assumptions:\ndy, ii) lim\n, lim\n−J a (Y ) − 1 2a d da h(Y ) − 1 4a 2 E Y\nwhere J a (Y ) = E Y [S Y a (Y ) 2 ], and S Y a = d log f Y (y; a)/da. Proof: Since the ﬁrst-order derivative of differential en-\ntropy with respect to a is expressed as d\nwe obtain the second-order derivative of the differential en- tropy with respect to a as follows:\nda 2 f Y (y; a)dy. (12) From (10), we derive an additional relationship between the\nd dy\nUpon substituting (d 2 /da 2 )f Y (y; a) from (13) into (12), the second term of equation (12) takes the expression:\n−J a (Y ) − 1 2a d da h(Y ) − 1 4a 2 E Y\nand the proof is completed. Additional details of the proof are delegated to [11].\nAlthough we have not enumerated all possible pdfs for The- orems 5 and 6, most of the pdfs that present an exponentially- decaying factor in their expression satisfy the assumptions re- quired in Theorems 5 and 6, since such a condition is sufﬁcient for the required interchange between limit and integral.\nIn information theory, entropy power inequality (EPI) is one of the most important inequalities since it helps to assess channel capacity results under different circumstances, e.g., the capacity of Gaussian MIMO broadcast channel and the secrecy capacity of Gaussian wire-tap channel. Therefore, various versions of EPI such as Costa\u2019s EPI [13] and the extremal entropy inequality [14] were proposed by different authors. Herein section, we will prove Costa\u2019s EPI, a stronger version of a classical EPI, using Theorems 1, 4, and 6.\nLemma 2 (Costa\u2019s EPI [13]): For an arbitrary but ﬁxed RV X with a ﬁnite second-order moment, and a Gaussian RV W with zero mean and unit variance,\nare independent of each other, and the entropy power N (Y ) is deﬁned as N (Y ) = (1/2πe) exp(2h(Y )).\nby Theorem 1 (de Bruijn identity), and N (Y ) ≥ 0, proving (16) is equivalent to proving the following inequality:\nSince W is a Gaussian RV, Fisher information inequality (FII) [9], which is proved using Theorems 4 and 6, is equivalently expressed as\nSince J (Y ) converges to J (X) as a approaches zero, l(0) = 0, and the following inequality holds for an arbitrary but ﬁxed random variable X and arbitrary small non-negative real- valued ϵ:\nSince the inequality in (20) holds for an arbitrary random variable X, we deﬁne X as ˜ X +\nbut ﬁxed random variable, ˜ W is a Gaussian random variable whose variance is identical to the variance of W , and ˜ X, ˜ W , and W are independent of one another. Then, the inequality in (20) is equivalent to the following inequalities:\nwhere ⇔ denotes the equivalence between before and after the notation.\nSince random variable ˜ X is arbitrary and a is an arbitrary non-negative real-valued number in equation (21), the proof is completed.\nDue to space limitations, we only detailed Costa\u2019s EPI as an application of the proposed results. However, the pro- posed results can be further exploited in a variety of other applications information theory, statistical signal processing, and wireless communications. First, both CRLB and BCRLB can be derived based on Theorems 1, 2, 4, 5, and 6 (for\ndetails see [11]). A new lower bound, tighter than BCRLB, for the mean square error for a Bayesian estimator can be also derived based on Theorems 1 and 6 [11]. Second, since Theorem 5 is equivalent to Theorem 1 in [2], Theorem 5 can be used for applications such as generalized EXIT charts and power allocation in systems with parallel non-Gaussian noise channels as mentioned in [2]. Finally, by Theorem 4, we showed the equivalence among Stein, de Bruijn, and heat equation identities. Therefore, a broad range of problems (in probability, decision theory, Bayesian statistics, economics and graph theory [5] that were traditionally established/analyzed via Stein identity) could be analyzed via de Bruijn identity (in the light of Theorems 4, 5 and 6).\nThis paper mainly revealed three important information- estimation theoretic results. First, we proved that Stein identity is equivalent to de Bruijn identity. Second, the ﬁrst-order and second-order derivatives of differential entropy with respect to the parameter a were expressed in terms of the posterior mean and Fisher information, respectively. Finally, several applications of the aforementioned results were provided. The proposed applications illustrate that the newly developed results are useful not only in information theory but also in the estimation theory ﬁeld and other ﬁelds."},"refs":[{"authors":[{"name":"D. Guo"},{"name":"S. Shamai (Shitz)"},{"name":"S. Verd´ u"}],"title":{"text":"Mutual information and minimum mean-square error in Gaussian channels"}},{"authors":[{"name":"D. Guo"},{"name":"S. Shamai (Shitz)"},{"name":"S. Verd´ u"}],"title":{"text":"Additive non-Gaussian noise channels: mutual information and conditional mean estimation"}},{"authors":[{"name":"C. Stein"}],"title":{"text":"Inadmissibility of the Usual Estimator for the Mean of a Multivariate Normal Distribution"}},{"authors":[{"name":"H. M. Hudson"}],"title":{"text":"A natural identity for exponential families with appli- cations in multiparameter estimation"}},{"authors":[{"name":"L. Brown"},{"name":"A. DasGupta"},{"name":"L. R. Haff"},{"name":"W. E. Strawderman"}],"title":{"text":"The heat equation and Stein\u2019s identity: Connections, applications"}},{"authors":[{"name":"S. M. Ka"}],"title":{"text":"Fundamentals of Statistical Signal Processing: Estimation Theory (Vol 1)"}},{"authors":[{"name":"H. L. Van Tree"},{"name":"I. New York: Wile"}],"title":{"text":"Detection, Estimation, and Modulation Theory: Part  2001"}},{"authors":[{"name":"A. J. Sta"}],"title":{"text":"Some inequalities satisﬁed by the quantities of information of Fisher and Shannon, Inf"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory"}},{"authors":[{"name":"S. K. Kattumannil"}],"title":{"text":"On Stein\u2019s identity and its applications"}},{"authors":[{"name":"S. Park"},{"name":"E. Serpedin"},{"name":"K. Qaraqe"}],"title":{"text":"On the equivalence between Stein and de Bruijn identities"}},{"authors":[{"name":"C. N. Morris"}],"title":{"text":"Natural exponential families with quadratic variance functions: statistical theory"}},{"authors":[{"name":"M. H. Costa"}],"title":{"text":"A new entropy power inequality"}},{"authors":[{"name":"T. Liu"},{"name":"P. Viswanath"}],"title":{"text":"An Extremal Inequality Motivated by Multi- terminal Information-Theoretic Problems"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569560613.pdf"},"links":[{"id":"1569566381","weight":7},{"id":"1569566485","weight":3},{"id":"1569566725","weight":3},{"id":"1569565663","weight":7},{"id":"1569565377","weight":7},{"id":"1569566981","weight":3},{"id":"1569556029","weight":3},{"id":"1569566469","weight":3},{"id":"1569566081","weight":3},{"id":"1569565355","weight":11},{"id":"1569565931","weight":3},{"id":"1569566765","weight":7},{"id":"1569565547","weight":3},{"id":"1569565461","weight":7},{"id":"1569564227","weight":7},{"id":"1569566303","weight":7},{"id":"1569559541","weight":3},{"id":"1569566941","weight":3},{"id":"1569558459","weight":3},{"id":"1569566821","weight":3},{"id":"1569565771","weight":3},{"id":"1569566999","weight":15},{"id":"1569558483","weight":7},{"id":"1569556091","weight":3},{"id":"1569566963","weight":7},{"id":"1569566709","weight":7},{"id":"1569566523","weight":3},{"id":"1569564189","weight":7},{"id":"1569566239","weight":3},{"id":"1569563981","weight":3},{"id":"1569566905","weight":7},{"id":"1569566733","weight":3},{"id":"1569566753","weight":7},{"id":"1569566759","weight":3},{"id":"1569559995","weight":3},{"id":"1569565841","weight":3},{"id":"1569561143","weight":7},{"id":"1569564611","weight":3},{"id":"1569567015","weight":3},{"id":"1569559805","weight":3},{"id":"1569566811","weight":3},{"id":"1569566687","weight":7},{"id":"1569554971","weight":3},{"id":"1569566209","weight":7},{"id":"1569566371","weight":3},{"id":"1569558985","weight":42},{"id":"1569566473","weight":3},{"id":"1569564333","weight":3},{"id":"1569565033","weight":3},{"id":"1569557083","weight":3},{"id":"1569565929","weight":3},{"id":"1569566721","weight":7},{"id":"1569555879","weight":7},{"id":"1569565219","weight":7},{"id":"1569556671","weight":7},{"id":"1569566223","weight":7},{"id":"1569565357","weight":7},{"id":"1569561245","weight":7},{"id":"1569566191","weight":11},{"id":"1569565527","weight":3},{"id":"1569565363","weight":3},{"id":"1569566695","weight":3},{"id":"1569566051","weight":3},{"id":"1569565909","weight":3},{"id":"1569565741","weight":3},{"id":"1569566229","weight":3},{"id":"1569566133","weight":7},{"id":"1569563395","weight":7},{"id":"1569551347","weight":7},{"id":"1569566383","weight":3},{"id":"1569565665","weight":3},{"id":"1569565549","weight":7},{"id":"1569565611","weight":7},{"id":"1569564175","weight":3},{"id":"1569566983","weight":3},{"id":"1569565397","weight":7},{"id":"1569565765","weight":7},{"id":"1569565435","weight":7},{"id":"1569566129","weight":3},{"id":"1569566253","weight":7},{"id":"1569566651","weight":3},{"id":"1569565013","weight":3},{"id":"1569566237","weight":11},{"id":"1569565375","weight":3},{"id":"1569566755","weight":3},{"id":"1569566713","weight":3},{"id":"1569566771","weight":7},{"id":"1569566641","weight":3},{"id":"1569564247","weight":3},{"id":"1569551905","weight":11},{"id":"1569564787","weight":3},{"id":"1569556759","weight":11},{"id":"1569566619","weight":3},{"id":"1569561185","weight":15},{"id":"1569558779","weight":3},{"id":"1569565669","weight":19},{"id":"1569563721","weight":7},{"id":"1569566817","weight":3},{"id":"1569564923","weight":11},{"id":"1569566299","weight":3},{"id":"1569564769","weight":7},{"id":"1569566933","weight":7},{"id":"1569563919","weight":3},{"id":"1569565389","weight":7},{"id":"1569559919","weight":3},{"id":"1569567013","weight":3},{"id":"1569560459","weight":7},{"id":"1569565853","weight":7},{"id":"1569563725","weight":3},{"id":"1569564505","weight":11},{"id":"1569565165","weight":7},{"id":"1569556327","weight":3},{"id":"1569564141","weight":7},{"id":"1569551541","weight":3},{"id":"1569566443","weight":7}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S1.T8.2","endtime":"10:30","authors":"Sangwoo Park, Erchin Serpedin, Khalid A. Qaraqe","date":"1341223800000","papertitle":"On the equivalence between Stein identity and de Bruijn identity","starttime":"10:10","session":"S1.T8: Information Theoretic Tools and Properties","room":"Stratton (491)","paperid":"1569560613"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
