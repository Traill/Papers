{"id":"1569560459","paper":{"title":{"text":"Fundamental limits on the power consumption of encoding and decoding"},"authors":[{"name":"Pulkit Grover \u2020"},{"name":"Andrea Goldsmith \u2020"},{"name":"Anant Sahai \u2021"}],"abstr":{"text":"Abstract\u2014 We provide fundamental information-theoretic bounds on the required circuit wiring complexity and power consumption for encoding and decoding of error-correcting codes. These bounds hold for all codes and all encoding and decoding algorithms implemented within the paradigm of our VLSI model. This model essentially views computation on a 2-D VLSI circuit as a computation on a network of connected nodes. The bounds are derived based on analyzing information ﬂow in the circuit. They are then used to show that there is a fundamental tradeoff between the transmit and encoding/decoding power, and that the total (transmit + encoding + decoding) power must diverge to inﬁnity at least as fast as cube-root of log 1 P\n, where P e is the average block-error probability. On the other hand, for bounded transmit-power schemes, the total power must diverge to inﬁnity at least as fast as square-root of log 1 P\ndue to the burden of encoding/decoding."},"body":{"text":"Information theory has been extremely successful in determining fundamental capacity limits to communication over a wide range of channels. However, these limits focus only on transmit power, ignoring the complexity and the associated power consumption for processing (e.g. encoding and decoding) the signals. At short distances, available empirical evidence suggests that transmit power does not necessarily dominate total power consumption [2], and hence the intuition from current theory can be misleading at short distances, at least in the context of coding. In fact, in many cases, uncoded transmission has been proposed [3] to do away with encoding and decoding altogether, even at the cost of signiﬁcantly larger transmit power! What, then, is the optimal approach? Our experimental work [4], [5] examines this issue using circuit simulations, where we empirically observe that the best choice of code, encoder and decoder becomes gradually complex as the communication distance increases. Thus, neither the traditional approach of operating close to capacity nor uncoded transmission is optimal at short distances. This naturally raises an important question: what is the choice of the code, encoder, and decoder for which the total power consumed is close to the minimum possible . Further, what is this minimum total power?\nWhile total power minimization has received little attention in information theory, a uniﬁed understanding of information theory and various notions of complexity has been a long-standing intellectual goal. The issue has been investigated for many code families, notions of complexity, and encoding/decoding algorithms (see [1] for a short survey). In comparison, Shannon\u2019s capacity results are fundamental: given the communication model, the\nchannel capacity is the limit on the achievable rate for any reliable communication strategy .\nWhat is the difﬁculty in obtaining fundamental limits on encod- ing/decoding power and complexity? Part of the challenge comes from the observation that even for a given code and implemen- tation technology (e.g. 45 nm CMOS), there are many possible encoding and decoding algorithms and (even for ﬁxed algorithms) many implementation architectures. Truly fundamental bounds therefore need to be oblivious not only to the code construction, but also to the chosen encoding and decoding algorithms and circuit architectures. The goal of this paper is to provide such bounds. In order to guide design of code-encoder-decoder triples for a given implementation technology, our bounds necessarily depend on the chosen technology. In order to do so, just as channel models take into account the limitations (e.g. noise, bandwidth, path-loss) imposed by communication channels, in Section III, we ﬁrst provide an implementation model that captures the limitations of information ﬂow in VLSI implementations.\nOur model is essentially an adaptation of Thompson\u2019s model for VLSI computation [6], [7]. In his thesis [7], Thompson provides fundamental wiring-complexity and power bounds in his VLSI model for two problems: computing the Fourier transform, and sorting. The bounds essentially build on fundamental net- work information-theoretic limits on the required communication complexity 1 of computing the desired function. To ascertain the information bottlenecks in this \u201cnetwork,\u201d a communication graph representing the circuit is bisected into two roughly equal pieces by cutting as few \u201cwires\u201d as possible. Knowing the number of bits that need to be communicated across this cut, a simple application of the cut-set bounding technique [9, Pg. 376] provides the minimum run-time (the number of \u201cclock-cycles,\u201d denoted here by τ ) for the computation.\nThe ﬁrst results on VLSI complexity of encoding/decoding were derived by El Gamal et al [10]. However, their novel technique does not extend to yield bounds on energy/power consumption (this aspect is discussed in detail in Section III). In order to establish results on energy and power consumption, in Section IV, we build on Thompson\u2019s original technique [7]. We analyze the computation process by repeatedly bisecting the circuit until the resulting sub-circuits are sufﬁciently small. We then use an error- exponent-style analysis that yields non-asymptotic bounds for any given error probability and blocklength. Our earlier tech- niques [11] limited our results to just the encoding complexity. In this paper, we are able to derive qualitatively stronger bounds\nWe introduce the system model in Section II, and the im- plementation model in Section III. In Section IV, we provide a lower bound on the energy and power consumption in en- coding and decoding. Assuming communication over a Gaus- sian channel with average transmit power P T , this lower bound shows that the energy consumed in encoding/decoding is at least 2 Ω k log 1 P blk\n/P T . Optimizing over transmit power, we conclude that the total (transmit + encoding + decoding) power must scale at least as fast as Ω 3 log 1 P blk\n. Further, if we use bounded transmit power even as P blk e is driven to zero, then the lower bound is larger, and scales at least as fast as Ω \t log 1 P blk\n. Thus, we conclude that the optimal strategy will increase transmit power as P blk e is driven to zero in order to reduce the complexity and power consumption of encoding and decoding. Section V includes further discussion and conclusions of our main results.\nOur power model here focuses on wires, ignoring the power consumed in the computational nodes. Our complementary results on power consumed in computational nodes appear in [12]\u2013[14]. In an order sense, while wire-power is dominant at low error probabilities and large gaps from capacity, node power dominates at transmissions closer to channel capacity.\nAlthough our results are non-asymptotic, we use asymptotic notation (i.e. the \u201cbig-Oh\u201d notation) to convey an intuitive un- derstanding. Vectors are denoted in bold (e.g. X n 1 is a vector of length n). For any set A, |A| denotes its cardinality.\nWe consider a point-to-point communication link. An informa- tion sequence of k fair coin ﬂips b k 1 is encoded into 2 nR binary- alphabet codewords X n 1 , hence the rate of the code is R = k n bits/channel use. The codeword X n 1 is modulated using BPSK modulation and sent through an Additive White Gaussian Noise (AWGN) channel of bandwidth W . The decoder estimates the input sequence b k 1 by ﬁrst performing a hard-decision on the received channel symbols before using these hard-decisions Y n 1 to decode the input sequence. The overall channel is therefore a Binary Symmetric Channel (BSC) with raw bit-error probability\n, where Q(x) = ∞ x 1 2 π e − x2 2 dx, ζ is the path- loss associated with the channel, P T is the transmit power of the BPSK-modulated signal, and σ 2 z is the variance of the Gaus- sian noise in the hard-decision estimation. The encoder-channel- decoder system operates at an average block-error probability P blk e\nAs mentioned earlier, our model is an adaptation of Thompson\u2019s model [7]. The model assumes that any VLSI circuit is a set of\ncomputational nodes that are connected to each other using ﬁnite- width wires. In each clock-cycle, the nodes communicate with all the other nodes that they are connected to. The nodes can perform simple computations (e.g. a three input NAND) on the inputs received by the nodes. The computation terminates at a predetermined τ number of clock-cycles.\nWe assume that the encoder E and the decoder D are imple- mented using the \u201cVLSI model of computation.\u201d The model, which is detailed below, captures essentially the communication limitations in a VLSI circuit, allowing a network-information- theoretic analysis to be performed on the circuit itself.\n\u2022 The circuit to compute the encoding/decoding function must be laid out on a grid of unit squares. It is composed of ﬁnite-memory computational nodes and interconnecting wires. Wires run along the edges and can cross at grid points. A computational node is a grid point that is either a logic element, a wire-connection, or an input/output pin.\n\u2022 The wires are assumed to be bi-directional. In each clock- cycle, each node sends one bit of information to each of the nodes that it is connected to over these wires.\n\u2022 The circuit is planar 3 , and each node is connected to at most four other nodes using the bi-directional wires.\n\u2022 The inputs of the computation (information bits for the encoder, and channel outputs for the decoder) are stored in separate source nodes, and the outputs of computation (codeword bits for the encoder, reconstructed information bits at the decoder) are stored in output nodes. The same node may act as a source-node and as an output node 4 . Also, each input value may enter the circuit at only the corresponding source node.\n\u2022 Each wire has a ﬁnite-width λ speciﬁed by the technology chosen to implement the circuit. Further, the area of each node is at least λ 2 .\n\u2022 The processing is done in \u201cbatches,\u201d i.e., a set of inputs is processed and outputs are released before the next set of inputs arrives into the source nodes.\nThe last assumption rules out \u201cpipelining\u201d [10] and sequential processing of inputs (e.g. by reading them from external storage while the computation is in process). An example implementation that lies outside our model is the decoding of a convolution code in a streaming manner, where bit estimates are outputted before the entire block is received.\nEnergy/power model : The energy consumed in computation is assumed to be given by E proc = ξ tech A wires τ , where ξ tech is the \u201cenergy parameter\u201d of the implementation that depends on the capacitance per-unit length of wiring in the circuit. Our model corresponds to a per-clock-cycle energy consumption of\nCV 2 [15], where C is the total wiring capacitance, and V is a ﬁxed chosen voltage (thus we disallow \u201cvoltage scaling\u201d [15]) .\nIn order to translate energy to power consumption, we assume that the decoding throughput (the number of information bits encoded/decoded per second) of the encoder/decoder is the same as the data rate R data (information bits/sec) across the channel. This assumption is necessitated by the requirement of avoiding buffer overﬂow at the receiver. Because the batch of k data bits are processed in parallel by the encoder/decoder, the amount of time available for the processing is T proc = k R\nseconds. The required power for encoding/decoding is therefore P proc =\n= E proc k R data , which is simply the energy-per-information- bit multiplied by the data rate.\nDeﬁnition 1 (Channel Model ( ζ, σ 2 z )): Channel Model ( ζ, σ 2 z ) denotes (as described in Section II) a BSC( p ch ) channel that is a result of hard-decision at the receiver across an AWGN channel of average transmit power P T , path loss ζ and noise variance σ 2 z .\nImplementation Model ( ξ tech , λ) denotes the implementation model (as described in Section III) having minimum wire-width λ, and energy parameter ξ tech .\nAs noted earlier, Thompson derived fundamental complexity and power bounds for the VLSI model for computing the Fourier transform and sorting [7]. The \u201ccut-set\u201d idea described in the introduction is used to obtain a lower bound on the minimum run- time (the number of \u201cclock-cycles\u201d, denoted here by τ ) for the computation. Although this minimum run-time can be reduced by increasing the \u201cmin-cut\u201d (referred to as the \u201cminimum bisection width\u201d in the VLSI theory literature [6]), this increase in the \u201cmin-cut\u201d comes at the cost of an increased wiring area. Thus there is a fundamental tradeoff between the number of clock- cycles τ , and the wiring area A wires . This is usually characterized by lower bounds on A wires τ 2 . As discussed above, the wire- energy consumption is proportional to a closely related quantity: the product A wires τ [7]. The results have been extended to many other computational problems, such as multiplication, computing boolean functions, etc. (see [1] and the references therein).\nEl Gamal et al. [10] use Thompson\u2019s VLSI model, and in a single analysis step, the authors break the entire circuit into multiple small sub-circuits. The authors show that the product A chip τ 2 is at least Ω nR 2 log n P blk\n, where A chip is the area of the smallest rectangle that encloses the circuit, n is the blocklength of the code,\nand R is the code rate. As noted earlier, we need lower bounds on A wires τ in order to obtain lower bounds on energy/power 5 . The proof technique in [10] therefore does not extend to provide bounds on A wires τ (because A wires < A chip ), which is needed to obtain bounds on energy/power of encoding/decoding.\nFor analyzing a computation process in the VLSI model, we deﬁne a communication graph for a circuit in the VLSI model.\nDeﬁnition 3 (Communication graph): A \t communication graph G corresponding to a circuit implemented in the model described in Section III, has vertices at the computational nodes, and edges along the interconnecting wires.\nThe set of vertices is denoted by V , and of edges by E. The com- putation can be viewed as being performed on the communication graph G. In order to analyze the bottlenecks of information ﬂow in G, we deﬁne bisections on G. A cut is said to \u201cbisect\u201d G (see Fig. 1) if roughly half of a speciﬁed subset of nodes lie on either side of the cut. More formally:\nDeﬁnition 4 (Bisection [6], [7]): Let S ⊆ V be a subset of the vertices, and E S ⊆ E be a subset of the edges in G. Then E S bisects S in G if removal of E S cuts V into sets V 1 and V 2 and S into sets S 1 ⊆ V 1 and S 2 ⊆ V 2 such that |S 1 | − |S 2 | ≤ 1.\nThe minimum bisection width (MBW) of S in G is deﬁned as min {|E S | s.t. E S bisects S in G }. The corresponding cut is called a minimum-bisection-width-cut; or an MBW cut.\nDeﬁnition 6 (Nested bisections): Let S ⊆ V be a subset of the vertices, and E S ⊆ E be a subset of the edges in G that bisects S such that removal of E S cuts G into G 1 , G 2 ; of V into V 1 and V 2 ; and of S into S 1 ⊆ V 1 and S 2 ⊆ V 2 . Let E S,i , i = 1, 2, be subsets of edges in G i that bisect S i . The resulting partitions of sets S, V each into four disjoint subsets is called a 2-step nested bisection of S in G.\nThe physical wires and computational nodes corresponding to the disjoint and mutually disconnected subsets that result from (nested) bisections constitute the sub-circuits. In our proof tech- niques, we will perform r-step nested bisections for some r ∈ Z + , conceptually breaking the original circuit into 2 r sub-circuits, indexed by i = 1, 2, . . . , 2 r . By assumption, bits are passed across an edge in each direction at every clock-cycle.\nDeﬁnition 7 (Bits communicated across a cut): Suppose the communication graph V of a circuit implemented in the Imple- mentation Model ( ξ tech , λ) is partitioned into two disconnected sets on removal of edges E cut . An ordered set of the bits passed along the edges in E cut in either direction during the computation process is called the vector of bits communicated across the cut. The length of this vector is called the number of bits communicated across the cut.\nIf w is the width of a cut, the number of bits communicated across the cut in τ processing cycles is simply 2wτ , where the factor of 2 is because of the bi-directional nature of the wires. In this paper,\nwe will mostly be interested in the bits communicated across an MBW-cut. In particular, we will be interested in the vector of communicated bits across all MBW-cuts in r-steps of nested bisections of a communication graph. This vector is simply the concatenation of vectors of bits communicated across the MBW- cuts in the r steps of nested bisections. The length of this vector, which is the total number of bits communicated across the MBW- cuts in the r bisection steps, is denoted by B [1: r] total .\nDeﬁnition 8 (Communication bits for a sub-circuit): An or- dered collection of bits communicated during the computation process along the wires that have exactly one computational node inside a given sub-circuit is called the vector of communica- tion bits for the i-th sub-circuit, denoted by b [1: r] comm,i . The bits could have been communicated in either direction along the bi- directional wires during s = 1, 2, . . . , r.\nThe length of b [1: r] comm,i , the vector of communication bits for the i- th sub-circuit, is denoted by B [1: r] subckt,i . Considering the sub-circuits at the end of r steps of successive bisections, because any wire that has exactly one computational node inside the sub-circuit must be connected to a exactly one node outside the sub-circuit, we have\nThis section presents a sequence of results that culminate in our lower bounds on total power. The results are based on r-step nested bisections of the communication graph of the respective circuit, each step bisecting the respective \u201cinformation nodes\u201d (input nodes for encoding; output nodes for decoding). For clarity of exposition, we assume that k is a power of 2. Thus, at the end of r steps of nested bisections, there are 2 r sub-circuits, each with\n\u201cinformation nodes.\u201d The results can be extended suitably to values of k that are not powers of 2.\nLemma 1 (Energy lower bounds): Under the Channel Model ( ζ, σ 2 z ) and Implementation Model ( ξ tech , λ), let k (a power of 2) be the number of information nodes at the encoder/decoder, and n be the nodes corresponding to channel input/output symbols. On performing r < log 2 (k/2) steps of nested bisections on this circuit, let the total number of bits that pass across the MBW-cuts of the r stages of nested bisections be denoted by B [1: r] total . Then,\nProof sketch: (see [1] for full proof) At each stage s = 1, 2, . . . , r, we ﬁrst obtain a lower bound on A wires τ 2 by lower bounding the sum 2 s i=1 A ( s) wires,i τ 2 for given number of bits B ( s) passed across MBW-cuts in the s-th stage, i.e., B ( s) = 2 s i=1 B ( s) i , where B ( s) i\nis the number of bits passed across the MBW-cut of the i-th sub- circuit in the s-th stage. This gives r different lower bounds on the product A wires τ 2 for the encoding/decoding process, one for each stage, each depending on B ( s) for the corresponding stage. Then, noting that r s=1 B ( s) = B [1: r] total , we obtain a lower bound on the product A wires τ 2 as a function of B [1: r] total . Using the simple lower bound on A wires , namely, A wires ≥ nλ 2 , we obtain a lower bound on A wires τ which is proportional to the energy consumed in our implementation model.\nObserve in Lemma 1 that while the total number of bits B [1: r] total that cross MBW-cuts in r stages increases with r, so does the\ndenominator 2 r . The following lemma (Lemma 2) uses the total number of communicated bits, B [1: r] total , to obtain a lower bound on error probability. Lemma 1 and Lemma 2 are used to obtain lower bounds on energy given the target P blk e . The value of r can then be chosen to obtain the best lower bound on energy.\nLemma 2 ( P blk e lower bounds): Under Channel Model ( ζ, σ 2 z ) and Implementation Model ( ξ tech , λ), on performing r < log 2 (k/2) steps of nested bisections on this circuit, let the total number of bits that pass across the MBW-cuts over the r stages be denoted by B [1: r] total,enc at the encoder and B [1: r] total,dec at the decoder.\n, where p ch = Q \t ζP T σ 2\nProof sketch for decoding computation: (see [1] for full proof) Working directly with a BSC model turns out to be complicated: the direct approach was adopted in [11]: the derived bounds there were only for encoding, and were looser than the bounds here. Instead, we take an indirect route: we ﬁrst ﬁnd bounds on a BEC and use them for a BSC by observing that a BSC( p ch ) can be interpreted as a physically degraded BEC with erasure probability 2p ch (see Fig. 2).\nConsider the i-th decoder sub-circuit at the r-th (ﬁnal) stage, i = 1, 2, . . . , 2 r , strengthened with the knowledge of the erasure- outputs (supplied for free). Denote the bits communicated across the boundary of the i-th decoder sub-circuit in either direction by b [1: r] comm,i (summed over all the r stages, and shared across MBW-cuts across different stages), and their number by B [1: r] subckt,i . As noted earlier, the total number of bits communicated across MBW-cuts (over all the r bisection stages) is 2 r i=1 B [1: r] subckt,i = 2B [1: r] total,dec . Deﬁne S := {i : B [1: r] subckt,i < k 2 r }. A simple averaging argument shows that |S| > 2 r−1 .\nThe i-th decoder sub-circuit can only use the communication bits b [1: r] comm,i and the n i channel outputs in order to decode the k 2 r information bits in the sub-circuit. Assuming optimistically that the communication bits themselves can deliver some of the k 2 r information bits in an error free manner, when B [1: r] subckt,i < k 2 r , the decoder still has to use the n i available channel outputs to decode the remaining k 2 r − B [1: r] subckt,i information bits. However, if the number of unerased channel outputs in the i-th sub-circuit is smaller than k 2 r − B [1: r] subckt,i , then this decoder sub-circuit likely makes an error (i.e. with probability > 1 2 , because it has to guess at least one bit). How many erasures should there be for this to happen? Clearly, the number of erasures # E,i must be at least n i − k 2 r + B [1: r] subckt,i + 1. Accounting for the event when at least the ﬁrst # E,i bits are erased, and denoting this event by W i , the probability of this event is\nAveraging this probability over i ∈ S, using the convex-∪ nature of the exponential function, and using the lower bound of 1 2 on the error probability under W i yields the lower bound.\nThe derivation for the bound on P blk e given the number of bits passed in encoding makes no assumptions on the decoding model (the decoder is assumed to be the optimal decoder). Because of space limitations, that derivation is relegated to [1].\nTheorem 1 (Energy in encoding/decoding): Under Channel Model ( ζ, σ 2 z ) and Implementation Model ( ξ tech , λ), let the energy consumed in encoding be denoted by E enc , and that in decoding be denoted by E dec . Then, for this model, for any r < log 2 (k/2), min {E enc , E dec } satisﬁes either\nChannel Model ( ζ, σ 2 z ) and Implementation Model ( ξ tech , λ), in the limit of small P blk e ,\nProof sketch: (see [1] for full proof) The proof follows by choice of r according to the following equation:\nUnder Channel Model ( ζ, σ 2 z ) and Implementation Model ( ξ tech , λ), across a channel of path-loss ζ, in the limit P blk e → 0, the total power is bounded as follows\n√ 1− R/3 σ 2 z ζ W Rλ 2 , and W is the channel bandwidth.\nProof sketch: (see [1] for full proof) In our hard-decision channel model, the term log 1 2 p\nscales proportionally to received power ζP T . Because we want to minimize the total power, and because E proc is normalized by k in order to obtain power,\n, to minimize the total power, x must satisfy x = 1 2 − 1 2 x. Thus, x = 1 3 , and we get that the total power, as well as the optimizing transmit power, must scale at least as fast as 3 log 1 P blk\n. It is also clear from (5) that if we use bounded P T , then the total power scales at least as fast as log 1 P blk\n. Plots for lower bounds on total power appear in [1].\nThis paper provides fundamental limits on complexity and power consumed in VLSI implementations of encoding/decoding\nof error-correcting codes. The limits are derived by analyzing the network of interconnected nodes in VLSI implementations using simple information-theoretic tools (i.e. cut-set bounds). The underlying intuition behind our results is simple: to utilize the beneﬁts offered by increased blocklengths, the circuit needs to exchange bits. Otherwise, the large code could effectively be split into codes of smaller blocklengths which would have worse error correction capability. Thus, in order to get the coding gains commensurate with large codes, there is more exchange of bits in the implementation, and hence more power consumption. This leads to a tradeoff between transmit power and computation power. Because of the simplicity of this intuition, we think that these results should extend to implementations that do not ﬁt directly into the model (e.g. 3-D circuits, soft decisions, etc.). For instance, an extension to multi-layered circuits (such as those commonly used today) can be easily obtained using tools developed by Thompson in [7].\nWe optimistically assume that the power ampliﬁer at the transmit- ter has 100 % efﬁciency. Incorporating RF and analog circuit power consumption at the transmitter would require an enhancement on our model where P T is replaced by a suitable function of P T that accounts for power consumed by these circuit elements."},"refs":[{"authors":[{"name":"P. Grover"},{"name":"A. J. Goldsmith"},{"name":"A. Sahai"}],"title":{"text":"Fundamental limits on transmission and computation power"}},{"authors":[],"title":{"text":"Energy Constrained Modulation Op- timization"}},{"authors":[{"name":"C. Marcu"},{"name":"D. Chowdhury"},{"name":"C. Thakkar"},{"name":"J.-D. Park"},{"name":"L.-K. Kong"},{"name":"M. Tabesh"},{"name":"Y. Wang"},{"name":"B. Afshar"},{"name":"A. Gupta"},{"name":"S. Gambini"},{"name":"R. Zamani"},{"name":"E. Alon"},{"name":"A. Niknejad"}],"title":{"text":"A 90 nm CMOS low-power 60 GHz transceiver with integrated baseband circuitry"}},{"authors":[{"name":"K. Ganesan"},{"name":"P. Grover"},{"name":"J. M. Rabaey"}],"title":{"text":"The power cost of overdesigning codes"}},{"authors":[{"name":"K. Ganesan"},{"name":"Y. Wen"},{"name":"P. Grover"},{"name":"J. M. Rabaey"},{"name":"A. J. Goldsmith"}],"title":{"text":"Optimizing code-decoder pairs for \u201cgreen\u201d code design"}},{"authors":[{"name":"C. D. Thompson"}],"title":{"text":"Area-time complexity for VLSI"}},{"authors":[],"title":{"text":"A complexity theory for VLSI"}},{"authors":[{"name":"A. C.-C. Yao"}],"title":{"text":"Some complexity questions related to distributive computing"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, 1st ed"}},{"authors":[{"name":"A. El Gamal"},{"name":"J. Greene"},{"name":"K. Pang"}],"title":{"text":"VLSI complexity of coding"}},{"authors":[{"name":"P. Grover"},{"name":"A. Goldsmith"},{"name":"A. Sahai"},{"name":"J. Rabaey"}],"title":{"text":"Information theory meets circuit design: Why capacity-approaching codes require more circuit area and power"}},{"authors":[{"name":"A. Sahai"},{"name":"P. Grover"}],"title":{"text":"A general lower bound on the VLSI decoding complexity"}},{"authors":[{"name":"P. Grover"}],"title":{"text":"Bounds on the tradeoff between rate and complexity for sparse- graph codes"}},{"authors":[{"name":"P. Grover"},{"name":"K. A. Woyach"},{"name":"A. Sahai"}],"title":{"text":"Towards a communication-theoretic understanding of system-level power consumption"}},{"authors":[{"name":"J. Rabae"},{"name":"A. Chandrakasa"},{"name":"B. Nikoli"}],"title":{"text":"Digital integrated circuits"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569560459.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S15.T6.3","endtime":"10:50","authors":"Pulkit Grover, Andrea Goldsmith, Anant  Sahai","date":"1341570600000","papertitle":"Fundamental limits on the power consumption of encoding and decoding","starttime":"10:30","session":"S15.T6: Fundamental Limits on Complexity","room":"Kresge Rehearsal A (033)","paperid":"1569560459"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
