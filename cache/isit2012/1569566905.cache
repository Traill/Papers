{"id":"1569566905","paper":{"title":{"text":"Finite Blocklength Coding for Multiple Access Channels"},"authors":[{"name":"Yen-Wei Huang"},{"name":"Pierre Moulin"}],"abstr":{"text":"Abstract\u2014This paper studies the maximum achievable rate region of multiple access channels (MAC) for a given blocklength n and a desired error probability . The inner region for the discrete memoryless MAC is approximated by a single- lettered expression I − 1 √\nQ inv (V, ) where I is associated with the capacity pentagon bounds by Ahlswede and Liao, V is the MAC dispersion matrix, and Q inv is the complementary multivariate Gaussian cumulative distribution region. For outer regions, we provide general converse bounds for both average error probability and maximum error probability criteria, and a single-lettered approximation for the discrete memoryless MAC."},"body":{"text":"Shannon\u2019s single-user channel coding problem and other classical channel problems give us an elegant formulation of capacities or capacity regions. The theorems state that the maximum rate of transmission approaches capacity or capacity region as the blocklength tends to inﬁnity. However such theorems do not provide convergence rates. This issue is addressed by asymptotic channel coding analysis conducted by Strassen [1] and others in the early 1960\u2019s and recently revisited by Polyanskiy et al. [2]. The past few years have witnessed several research efforts applying these techniques to other classical channel coding problems [3], [4].\nIn this work, we study the ﬁnite blocklength capacity region of the MAC. The characterization of the average error capacity region of the discrete memoryless MAC (DM-MAC) is given independently by Ahlswede [5] and Liao [6]. Unlike the single-user channel, the maximal error capacity region is generally smaller the average error capacity region [7] and its characterization remains an open problem. The strong converse results in [8], [9] further strengthened the notion of capacity region in the sense that no rate pairs outside the capacity region can be -achievable for any < 1. Independently of our work, Tan and Kosut [10] and MolavianJazi and Laneman [11] recently studied the ﬁnite blocklength capacity inner region for DM-MAC. Comparisons with their work are provided in Sec. IV.\nFormally, a two-user MAC 1 is characterized by a transition distribution P Y |X 1 X 2 where X 1 ∈ X n 1 , X 2 ∈ X n 2 , and Y ∈ Y n , and is called discrete memoryless if X 1 , X 2 , and\nY are ﬁnite and P Y |X 1 X 2 (y |x 1 , x 2 ) = n i=1 W (y i |x 1i , x 2i ). Hence, a DM-MAC is determined by the single-lettered con- ditional distribution W (y |x 1 , x 2 ). An (n, M 1 , M 2 ) MAC code C consists of two codebooks {x 1 (m 1 ) } and {x 2 (m 2 ) } where m j ∈ {1, . . . , M j } M j for j = 1, 2 and a (possibly ran- domized) decoder φ(y) = (m 1 , m 2 ). Code rates are deﬁned by R j n −1 log M j for j = 1, 2, and M M 1 M 2 .\nThe error probability for passing a message pair (m 1 , m 2 ) using MAC code C through channel P Y |X 1 X 2 is denoted by\nAn (n, M 1 , M 2 , ) MAC code C under average (resp. maxi- mum) error probability satisﬁes P e,avg (P Y |X 1 X 2 , C) ≤ (resp. P e,max (P Y |X 1 X 2 , C) ≤ ). A rate pair (R 1 , R 2 ) is (n, )- achievable under average (resp. maximum) error probability if there exists an (n, M 1 , M 2 , ) MAC code under average (resp. maximum) error probability.\nThe capacity region of a DM-MAC W is closely related to statistics of the information density vector i (U, X 1 , X 2 , W ) abbreviated by i (should there be no risk of confusion) and deﬁned by\n \ni(X 1 ; Y |X 2 , U ) i(X 2 ; Y |X 1 , U ) i(X 1 , X 2 ; Y |U)\n  \n  \nwhere U is an auxiliary \u201ctime sharing\u201d random variable, X 1 and X 2 are independent given U , and U ↔ (X 1 , X 2 ) ↔ Y forms a Markov chain, i.e.,\nThe marginal distributions P (Y |X 1 , U ), P (Y |X 2 , U ) and P (Y |U) are obtained by marginalizing out X 2 , X 1 , and (X 1 , X 2 ), respectively. The mutual information vector I (P U X 1 X 2 , W ) or I is deﬁned by the expectation of the information density vector under P U X 1 X 2 W :\n \nI(X 1 ; Y |X 2 , U ) I(X 2 ; Y |X 1 , U ) I(X 1 , X 2 ; Y |U)\nThe (n, )-capacity region R n, is deﬁned by the set of all (n, )-achievable rate pairs. For the DM-MAC, as \t 0 and n → ∞, this capacity region (average error probability) coincides with the MAC capacity region by Ahlswede [5] and Liao [6]:\nwhere R(P U P X 1 |U P X 2 |U ) is the set of all rate pairs [R 1 , R 2 ] satisfying\nThe rate vector is deﬁned by R = [R 1 , R 2 , R 1 + R 2 ] and the inequality denotes element-wise inequality. The strong converse by Dueck [8], later sharpened by Ahlswede [9], stated that every sequence of (n, M 1 , M 2 , ) MAC codes (average error probability) satisﬁes\nGiven a sequence of vectors a n , a sequence of regions F n , and a sequence of functions g n , the asymptotic notation a n ∈ F n + O(g n ) means there exists a constant c such that a n ∈ F n + cg n 1 for sufﬁciently large n. Here 1 denotes the vector comprised of all ones, and \u2018 +\u2019 denotes Minkowski set addition.\nIn this section, we characterize an asymptotic inner bound for the (n, )-capacity region of DM-MAC. The characteriza- tion utilizes the mutual information vector of (1), the second- order dispersion matrix of the information density vector, and the complementary multivariate Gaussian cumulative distribu- tion region (deﬁned below).\nUnder P U X 1 X 2 W , we deﬁne the dispersion matrix V (P U X 1 X 2 , W ) or V by the conditional covariance matrix\nLet Z be a d-dimensional Gaussian vector with mean 0 and covariance matrix Σ. For any ∈ (0, 1) let\nNote that for d = 1, we have Q inv (σ 2 , ) = {z ≥ σQ −1 ( ) } where Q −1 is the complementary Gaussian cumulative distri- bution function. An example of a two-dimensional Q inv region is shown in Fig. 1.\nFor any ﬁxed distribution P U X 1 X 2 = P U P X 1 |U P X 2 |U , we consider the following random codes:\nEncoder: Let {u i } n i=1 , n ≥ 1 be deterministic sequences such that for large enough n the empirical distribution ˆ P U (u)\nwhere · 1 denotes the L 1 norm. For each message pair (m 1 , m 2 ) ∈ M 1 × M 2 , draw X ji independently from P X j |U ( ·|u i ) for 1 ≤ i ≤ n, j = 1, 2.\nDecoder: Deﬁne the (variable-size) list L of all messages (m 1 , m 2 ) whose score exceeds a prescribed threshold vector\nL {(m 1 , m 2 ) ∈ M 1 × M 2 : Z n (m 1 , m 2 ) ≥ τ n } where\nThen for sufﬁciently large n any rate vector R ∈ R in n, (W ) − O log n n is (n, )-achievable (average error probability).\nSketch of the proof: Analysis of the threshold decoding scheme is based on a generalized multidimensional Berry- Esseen theorem, 2 which approximates the distribution of the sum of independent random vectors to a multivariate Gaussian distribution (details to be provided in the full version), and large deviations analysis.\nWithout loss of generality, we assume m 1 = m 2 = 1. The goal is to ﬁnd the threshold vector τ n such that\nPr( L = ∅) ≤ − α n \t (10) Pr( ∃ [(m 1 , m 2 ) = (1, 1)] ∈ L) ≤ α n \t (11)\nfor some α n = O(n −1/2 ). Now Z n of (7) is a sum of independent (but not identically distributed) random vectors. If we let\nthen by the generalized multidimensional Berry-Esseen theo- rem, there exists γ n = O(n −1/2 ) such that\nwhere Z ∼ N (0, Σ n ). Also by the choice of τ n we have Pr(Z n ≥ τ n ) ≥ 1 − + α n\nand therefore (10) is satisﬁed. On the other hand, using large deviations analysis we obtain\n      \nPr( ∃((m 1 = 1), 1) ∈ L) ≤ M 1 exp {−τ 1,n + O (1) } Pr( ∃(1, (m 2 = 1)) ∈ L) ≤ M 2 exp {−τ 2,n + O (1) } Pr( ∃((m 1 = 1), (m 2 = 2)) ∈ L)\n≤ M 1 M 2 exp {−τ 3,n + O (1) } . Therefore (11) is satisﬁed if\nFinally, by combining C n − I(P U X 1 X 2 , W ) 1 = O(1/n) and Σ n −V(P U X 1 X 2 , W ) 1 = O(1/n) (which can be shown using (6)) and the fact that P U X 1 X 2 and τ n are arbitrary, the theorem is proved.\nWe provide general outer regions for discrete multiple access channels (not necessarily memoryless), for both average and maximum error probability criteria. The results can be regarded as generalizations of the general converses for single- user channels in [2]. For DM-MAC, a single-lettered converse bound is also provided for the maximum error probability criterion.\nFix distributions P and {Q j } k j=1 ( k = 3 in Theorem 2 below) and consider the composite hypothesis problem:\nDeﬁnition 1: The set of achievable error exponent vectors is deﬁned by\nE P [δ(Y )] ≥ 1 − and − log E Q j [δ(Y )] ≥ E j , 1 ≤ j ≤ k . This is a generalization of the binary hypothesis testing\nperformance function β α of [2, (100)], which can also be written as:\nand a MAC code C (possibly randomized decoder), let\n, j = 1, 2, 3 P X 1 X 2 = P X 1 P X 2 = Q j X 1 X 2 = encoder output\ndistribution with indep. equiprobable codewords. Then we have \n− log(1 − 1 ) − log(1 − 2 ) − log(1 − 3 )\nProof: This is a generalization of [2, Theorem 26]. Consider the following test δ for deciding between P and\nQ j : denote the observed pair by (x 1 , x 2 , y); y is fed into the decoder and the test declares P if the message is decoded correctly. The probability that the test is correct if P is the actual distribution is E P [δ(X 1 , X 2 , Y)] = 1 − , and the prob- ability that the test is incorrect if Q j is the actual distribution is E Q j [δ(X 1 , X 2 , Y)] = 1 − j , j = 1, 2, 3. Therefore the error exponent vector [ − log(1 − 1 ), − log(1 − 2 ), − log(1 − 3 )]\nTheorem 3: Every (n, M 1 , M 2 , ) MAC code (average error probability) satisﬁes\nwhere Q 1 Y |X 2 , Q 2 Y |X 1 , and Q 3 Y range over all distributions on Y n .\nin Theorem 2 to be independent of X 1 , the error probability 1 is at least 1 − 1/M 1 . Similarly we have 2 ≥ 1 − 1/M 2 and\nTheorem 4: For any two channels P Y |X 1 X 2 and Q Y |X 1 X 2 , a MAC code C (possibly randomized decoder), and a subset Ω of the set of message pairs M 1 × M 2 , let\nProof: Let (m ∗ 1 , m ∗ 2 ) ∈ Ω be the message pair with P e (m ∗ 1 , m ∗ 2 , Q Y |X 1 X 2 ) = Ω . Consider the test δ for deciding between P Y |X 1 =x 1 (m ∗ 1 ),X 2 =x 2 (m ∗ 2 ) and Q Y |X 1 =x 1 (m ∗ 1 ),X 2 =x 2 (m ∗ 2 ) that declares P if the message is decoded correctly. The probability that the test is correct if P is the actual distribution is E P [δ(x 1 (m ∗ 1 ), x 2 (m ∗ 2 ), Y)] ≥ 1− Ω , and the probability that the test is incorrect if Q is the actual distribution is E Q [δ(x 1 (m ∗ 1 ), x 2 (m ∗ 2 ), Y)] = 1 − Ω . Therefore\nTheorem 5: Let C be an (n, M 1 , M 2 , ) MAC code (max- imum error probability) and let Ω be a subset of the set of message pairs M 1 × M 2 . Then\nProof: By particularizing Q Y |X 1 X 2 in Theorem 4 inde- pendent of X 1 and X 2 , the maximum error probability Ω is at least 1 − 1/|Ω|. Also Ω is trivially upper-bounded by . The claim follows.\nC. Single-Lettered Converse Bound: Maximum Error Proba- bility\nThe formulas of (12) and (13) are hard to evaluate in general. Here we provide a single-lettered converse bounds for DM-MAC W under the maximum error probability criterion.\nTheorem 6: Every sequence of (n, M 1 , M 2 , ) MAC codes (maximum error probability) satisﬁes\n−1 ( ) + |X 1 ||X 2 | log n + O(1) \t (14) log M 2 ≤ sup\nwhere V 11 and V 22 are the respective ﬁrst and second entries on the diagonal of the dispersion matrix V (P U X 1 X 2 , W ) of (5).\nSketch of the proof: We prove (14) and (15) follows in a similar manner. Let m 2 = 1 and divide the codeword x 2 (1) into |X 2 | subsets where within each subset x 2 (m 2 ) has the same symbol. Now within each subset, there are at most (n + 1) |X 1 |−1 types for x 1 , so there exists Ω ⊆ {(m 1 , 1) : 1 ≤ m 1 ≤ M 1 } such that each codeword pair (x 1 (m 1 ), x 2 (1)) where (m 1 , 1) ∈ Ω has the same joint type within each subset. If we let U = x 2 (1), then each codeword pair (x 1 , x 2 ) associated with message pair in Ω has the same conditional joint type T x 1 x 2 |U . Now we particularize Q Y in Theorem 5 to be P Y |U , then β 1− (W n ( ·|x 1 x 2 ), P Y |U ) is independent of x 1 and x 2 . On the other hand, Ω has size at least M 1 (n + 1) −|X 2 |(|X 1 |−1) . Therefore (14) follows from Theorem 5 and [2, Lemma 58].\nNote that Theorem 6 provides bounds only on M 1 and M 2 . Although a similar bound on M = M 1 M 2 is strongly desirable, we have not yet been able to generalize the result at this time. Since messages m 1 and m 2 are independent, it is difﬁcult to ﬁnd a large set Ω where each codeword pair has the same joint type. Also the bounds on M 1 and M 2 do not match the inner region in Theorem 1.\nIn this section we compare our inner and outer regions in Sec. II and III with two concurrent inner region results obtained by Tan and Kosut [10] and by MolavianJazi and Laneman [11]. Our inner region is the largest among the three, and numerical results are provided for two DM-MAC examples.\nOur inner region (denoted by R in ,H-M n, in the sequel) is given by\nIn [10, Theorem 2] and [11, Theorem 1], the inner regions are characterized in terms of the unconditional covariance matrix of the information density vector\nTan and Kosut\u2019s inner region has a form similar to (16) and (17) while MolavianJazi and Laneman\u2019s is a union of pentagons. When no time sharing is necessary, our inner region coincides with Tan and Kosut\u2019s, and MolavianJazi and Laneman\u2019s is slightly smaller. When time sharing is necessary, it can be shown (omitted here) by the law of total covariance that our inner region is strictly bigger than the other two. The three inner regions coincide only under some degenerate cases. 3\nExample 1 (Noisy Binary Erasure MAC): Let X 1 = X 2 = {0, 1} and a quaternary output Y = {0, 1, 2, e}. The channel W is deﬁned by\nwhere δ is the erasure error probability. The channel is a noisy version of the binary erasure MAC discussed in [13, Example 15.3.3]. Its capacity region is uniquely achieved by input distributions P X 1 = P X 2 = Bernoulli(1/2) (no time sharing necessary). The mutual information vector and the dispersion matrix are given respectively by\n2 (1 − δ)] and\n− δ) δ(1 − δ) \t 3 2 δ(1 − δ) δ(1 − δ) δ(1 − δ) \t 3 2 δ(1 − δ)\nThe capacity region C(W ) and the inner and outer regions are shown in Fig. 2. 4 The three inner regions are quite close because V = ˜ V.\nExample 2 (Noiseless XOR MAC): Let X 1 = X 2 = Y = {0, 1}. The channel W is deterministic and is deﬁned by\nLet U = 1 with probability p 1 and U = 2 with probability p 2 = 1 − p 1 . Let X j ≡ 0 if U = j and P X j = Bernoulli(1/2) if U = j for j = 1, 2. Now the mutual information vector equals\nand by ranging p 1 from 0 to 1 we obtain the capacity region as\n−p 1 p 2 0 −p 1 p 2 p 1 p 2 0\nThe capacity region C(W ), inner and outer regions are shown in Fig. 3. Our inner region is much larger than the other two."},"refs":[{"authors":[{"name":"V. Strassen"}],"title":{"text":"Asymptotische absch¨atzungen in Shannons informations- theorie"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"H. V. Poor"},{"name":"S. Verdu"}],"title":{"text":"Channel coding rate in the ﬁnite blocklength regime"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"H. Poor"},{"name":"S. Verdu"}],"title":{"text":"Dispersion of the Gilbert- Elliott channel"}},{"authors":[{"name":"A. Ingber"},{"name":"M. Feder"}],"title":{"text":"Finite blocklength coding for channels with side information at the receiver"}},{"authors":[{"name":"R. Ahlswede"}],"title":{"text":"Multi-way communication channels"}},{"authors":[{"name":"H. H. J. Liao"}],"title":{"text":"Multiple access channels"}},{"authors":[{"name":"G. Dueck"}],"title":{"text":"Maximal error capacity regions are smaller than average error capacity regions for multi-user channels"}},{"authors":[],"title":{"text":"The strong converse to the coding theorem for the multiple-access channel"}},{"authors":[{"name":"R. Ahlswede"}],"title":{"text":"An elementary proof of the strong converse theorem for the multiple-access channel"}},{"authors":[{"name":"V. Y. F. Ta"},{"name":"O. Kosut"}],"title":{"text":"Feb"}},{"authors":[{"name":"E. MolavianJazi"},{"name":"J. N. Laneman"}],"title":{"text":"Multiaccess communication in the ﬁnite blocklength regime"}},{"authors":[{"name":"V. Bentkus"}],"title":{"text":"On the dependence of the Berry-Esseen bound on dimen- sion"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, 2nd ed"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566905.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T4.4","endtime":"11:10","authors":"Yen-Wei Huang, Pierre Moulin","date":"1341312600000","papertitle":"Finite Blocklength Coding for Multiple Access Channels","starttime":"10:50","session":"S5.T4: Finite Blocklength Analysis","room":"Stratton 20 Chimneys (306)","paperid":"1569566905"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
