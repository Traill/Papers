{"id":"1569566051","paper":{"title":{"text":"Cut-Set Bound for Generalized Networks"},"authors":[{"name":"Silas L. Fong"},{"name":"Raymond W. Yeung"},{"name":"Gerhard Kramer"}],"abstr":{"text":"Abstract\u2014In a network, a node is said to incur a delay if its encoding of each transmitted symbol involves only its received symbols obtained before the time slot in which the transmitted symbol is sent (hence the transmitted symbol sent in a time slot cannot depend on the received symbol obtained in the same time slot). A node is said to incur no delay if its received symbol obtained in a time slot is available for encoding its transmitted symbol sent in the same time slot. In the classical discrete memoryless network (DMN), every node incurs a delay. A well- known result for the classical DMN is the cut-set outer bound. In this paper, we generalize the model of the DMN in such a way that some nodes may incur no delay, and we obtain the cut-set outer bound for the generalized DMN."},"body":{"text":"We consider a general network in which each node may send information to the other nodes. A node is said to incur a delay if its encoding of each transmitted symbol involves only its received symbols obtained before the time slot in which the transmitted symbol is sent. A node is said to incur no delay if its received symbol obtained in a time slot is available for encoding its transmitted symbol sent in the same time slot. In the classical model of the discrete memoryless network (DMN) [1], every node incurs a delay. A well-known result for the classical DMN is the cut-set outer bound [1,2]. However, the delay assumption makes the classical model not applicable to some simple networks, including the relay-without-delay relay network studied by El Gamal et al. [3]. Therefore, we are motivated to generalize the model of the DMN in such a way that some nodes may incur no delay, and to prove the cut-set outer bound for the generalized DMN.\nThis paper is organized as follows. Section II presents the notation of this paper. Section III presents a two-node network in which the classical cut-set outer bound cannot be applied. Section IV presents the formulation of the generalized DMN. Section V proves the cut-set outer bound for the generalized DMN. Section VI investigates a two-node generalized DMN where one node incurs no delay, and we prove the capacity region using our cut-set outer bound. Section VII concludes this paper.\nWe use P r {E} to represent the probability of an event E. We use a capital letter X to denote a random variable with alphabet X , and use the small letter x to denote the\nrealization of X. We use X n to denote a random column vector [X 1 X 2 . . . X n ] T , where the components X k have the same alphabet X . We let p X (x) denote the probability mass function of the discrete random variable X. We let p Y |X (y|x) denote the conditional probability P r {Y = y|X = x} for any discrete random variables X and Y . For simplicity, we drop the subscript of a notation if there is no ambiguity. For any N 2 -dimensional random tuple (W 1,1 , W 1,2 , . . . , W N,N ) ∈ W 1,1 × W 1,2 × . . . × W N,N and any set V ⊆ {1, 2, . . . , N } 2 , we let W V = (W i,j : (i, j) ∈ V ) be a subtuple of (W 1,1 , W 1,2 , . . . , W N,N ).\nWe now consider a two-node network that consists of a forward channel and a reverse channel, where the nodes are indexed by 1 and 2. Node 1 and node 2 transmit information to each other through the channels as follows. In each time slot, node 1 transmits symbol X 1 to node 2 through the forward channel characterized by a conditional probability distribution p 1 (y 2 |x 1 ), where X 1 and p 1 (y 2 |x 1 ) together deﬁne Y 2 , the output of the forward channel. In the same time slot, node 2 receives Y 2 and then transmits symbol X 2 to node 1 through the reverse channel characterized by a conditional proba- bility distribution p 2 (y 1 |x 1 , x 2 , y 2 ), where (X 1 , X 2 , Y 2 ) and p 2 (y 1 |x 1 , x 2 , y 2 ) together deﬁne Y 1 , the output of the reverse channel. Since node 2 receives Y 2 before transmitting X 2 , X 2 can depend on Y 2 . In other words, node 2 does not incur a delay and therefore the classical cut-set outer bound cannot be applied in this two-node network.\nTo facilitate discussion, we call this network the discrete memoryless channel (DMC) with noiseless reverse channel if\nNote that the DMC with noiseless reverse channel reduces to the DMC with feedback [1,4] if node 2 transmits in each time slot the symbol it receives in the same time slot.\nIn this paper, we consider a general network that consists of N nodes. Let I = {1, 2, . . . , N } be the index set of the nodes. The N terminals exchange information in n time slots as follows. Node i chooses message W i,j ∈ {1, 2, . . . , M i,j }\nand sends W i,j to node j for each (i, j) ∈ I × I. We assume that each message W i,j is uniformly distributed over {1, 2, . . . , M i,j } and all the messages are independent. For each k ∈ {1, 2, . . . , n} and each i ∈ I, node i transmits X i,k ∈ X i and receives Y i,k ∈ Y i in the k th time slot where X i and Y i are some alphabets that depend on i. After n time slots, node i declares ˆ W j,i to be the transmitted W j,i based on W {i}×I and Y n i for each (i, j) ∈ I × I. To simplify notation, let M I×I denote the N 2 -dimensional tuple (M 1,1 , M 1,2 , . . . , M N,N ).\nDeﬁnition 1: An α-dimensional tuple (S 1 , S 2 , . . . S α ), de- noted by S α , consisting of subsets of I is called an α-partition of I if ∪ α h =1 S h = I and S i ∩ S j = ∅ for all i = j.\nLet T ⊆ I be a set and S α be an α-partition of I. For any random tuple (X 1 , X 2 , . . . , X N ) ∈ X 1 × X 2 × . . . × X N , we let X T = (X i : i ∈ T ) be a subtuple of (X 1 , X 2 , . . . , X N ). In addition, we let X S h denote (X S 1 , X S 2 , . . . , X S h ) and let x S h be the realization of X S h for each h ∈ {1, 2, . . . , α}. Similarly, for any k ∈ {1, 2, . . . , n} and any random tuple (X 1,k , X 2,k , . . . , X N,k ) ∈ X 1 × X 2 × . . . × X N , we let X T,k =\n(X i,k : i ∈ T ) be a subtuple of (X 1,k , X 2,k , . . . , X N,k ). In ad- dition, we let X S h ,k denote (X S 1 ,k , X S 2 ,k , . . . , X S h ,k ) and let x S h ,k be the realization of X S h ,k for each h ∈ {1, 2, . . . , α}.\nDeﬁnition 2: A delay proﬁle is an N -dimensional tuple (b 1 , b 2 , . . . , b N ) where b i ∈ {0, 1, 2, . . .} for each i ∈ I.\nDeﬁnition 3: The discrete network consists of N ﬁnite in- put sets X 1 , X 2 , . . . , X N , N ﬁnite output sets Y 1 , Y 2 , . . . , Y N and α channels characterized by conditional distribu- tions p 1 (y G 1 |x S 1 ), p 2 (y G 2 |x S 2 , y G 1 ), . . . , p α (y G α |x S α , y G α−1 ), where S α and G α are two α-dimensional partitions of I. We call S α and G α the input partition and the output partition of the network respectively. The discrete network is denoted by (X I , Y I , S α , G α , p 1 , p 2 , . . . , p α ).\nWe deﬁne codes that use the network n times in the following two deﬁnitions.\nDeﬁnition 4: Let (X I , Y I , S α , G α , p 1 , p 2 , . . . , p α ) be a dis- crete network. For each i ∈ I, let h i and m i be the two unique integers such that i ∈ S h i and i ∈ G m i . Then, a delay proﬁle (b 1 , b 2 , . . . , b N ) is said to be feasible for the network if the following holds for each i ∈ I: If b i = 0, then h i > m i .\nDeﬁnition 5: Let B = (b 1 , b 2 , . . . , b N ) be a delay proﬁle feasible for (X I , Y I , S α , G α , p 1 , p 2 , . . . , p α ). A (B, n, M I×I )-code for n uses of the network consists of the following:\n1) A message set W i,j = {1, 2, . . . , M i,j } at node i for each (i, j) ∈ I × I.\n2) An encoding function f i,k : W {i}×I × Y k −b i i \t → X i for each i ∈ I and each k ∈ {1, 2, . . . , n}, where f i,k is the encoding function at node i in the k th time slot such\n3) A decoding function g i,j : W {j}×I × Y n j → W i,j for each (i, j) ∈ I × I, where g i,j is the decoding function for W i,j at node j such that g i,j (W {j}×I , Y n j ) = ˆ W i,j .\nGiven a (B, n, M I×I )-code, it follows from Deﬁnition 5 that for each i ∈ I, node i incurs a delay if b i > 0, where b i is the amount of delay incurred by node i. If b i = 0, node i incurs no delay, i.e., for each k ∈ {1, 2, . . . , n}, node i needs to receive Y i,k before encoding X i,k . The feasibility condition of B in Deﬁnition 4 ensures that the operations of any (B, n, M I×I )-code are well-deﬁned for the subsequently deﬁned discrete memoryless network; the associated coding scheme is described after the network is deﬁned.\nDeﬁnition 6: A discrete network (X I , Y I , S α , G α , p 1 , p 2 , . . . , p α ), when used multiple times, is called a discrete memoryless network (DMN) if the following holds for any (B, n, M I×I )-code. Let U k −1 = (W I×I , X k −1 I , Y k −1 I ) be\nthe collection of random variables that are generated before the k th time slot. Then, for each k ∈ {1, 2, . . . , n} and each h ∈ {1, 2, . . . , α},\n= P r{U k −1 = u, X S h ,k = x S h , Y G h−1 ,k = y G h−1 } p h (y G h |x S h , y G h−1 ).\nFollowing the notation in Deﬁnition 6, consider any (B, n, M I×I )-code on the DMN. In the k th time slot, X I,k and Y I,k are generated in the order\nby transmitting on the channels in this order p 1 , p 2 , . . . , p α using the (B, n, M I×I )-code (as prescribed in Deﬁnition 5). Speciﬁcally, X S h ,k , Y G h−1 ,k and channel p h together deﬁne Y G h ,k for each h ∈ {1, 2, . . . , α}. We will show in the following that the encoding of X S h ,k before the transmission on p h and the generation of Y G h ,k after the transmission on p h for each h ∈ {1, 2, . . . , α} are well-deﬁned. Fix any k ∈ {1, 2, . . . , n} and h ∈ {1, 2, . . . , α}. Consider the following two cases for encoding X i,k for each i ∈ S h :\nCase b i > 0: Since X i,k is a function of (W {i}×I , Y k −b i i \t ) and Y k −b i i \t has already been received by node i by the k th time slot, the encoding of X i,k at node i before the transmission on p h in the k th time slot is well-deﬁned.\nCase b i = 0 : Let m be the unique integer such that i ∈ G m . Since X i,k is a function of (W {i}×I , Y k i ) and Y i,k has already been received by node i after the transmission on p m in the k th time slot where m < h by feasibility of B, it follows that the encoding of X i,k at node i before the transmission on p h in the k th time slot is well-deﬁned.\nCombining the two cases, the encoding of X i,k before the transmission on p h in the k th time slot for each i ∈ S h is well-deﬁned, which implies that the encoding of X S h ,k before the transmission on p h in the k th time slot is well-deﬁned.\nIn addition, the transmission on p h in the k th time slot only depends on (X S h ,k , Y G h−1 ,k ). Since the transmissions on p 1 , p 2 , . . . , p h −1 and the encoding of X S h ,k occur before the transmission on p h in the k th time slot, it follows that Y G h−1 ,k and X S h ,k have already been generated before the generation of Y G h ,k according to (1), which implies that the generation\nExample 1: Consider a two-node DMN (X 1 , X 2 , Y 1 , Y 2 , ({1}, {2}), ({2}, {1}), p 1 , p 2 ) where all the alphabets are bi- nary,\nNote that p 1 (y 2 |x 1 ) is the conditional probability distribution for the binary symmetric channel (BSC). To facilitate dis- cussion, we call this network the BSC with correlated feed- back . For any ((1, 0), n, M {1,2}×{1,2} )-code on this network, X {1,2},k and Y {1,2},k are generated in the k th time slot in the order X 1,k , Y 2,k , X 2,k , Y 1,k for each k ∈ {1, 2, . . . , n}. Note that node 2 incurs no delay, and can use Y 2,k for encoding X 2,k because Y 2,k is generated before the generation of X 2,k .\nIn the classical model of the DMN, every node incurs a delay and the network is characterized by a single channel p 1 (y I |x I ). Therefore, the classical DMN can be viewed as a generalized DMN with a single channel p 1 (y I |x I ), and every code on the classical DMN can be viewed as some (B, n, M I×I )-code on the generalized DMN with B = (1, 1, . . . , 1) (cf. Deﬁnitions 3, 4, 5 and 6). The model studied in [3] is also a special case of the generalized DMN.\nDeﬁnition 7: For a (B, n, M I×I )-code on the DMN, the average probability of decoding error of W i,j is deﬁned as P n i,j = P r{g i,j (W {j}×I , Y n j ) = W i,j } for each (i, j) ∈ I ×I.\nDeﬁnition 8: A rate tuple (R 1,1 , R 1,2 , . . . , R N,N ), denoted by R I×I , is achievable for the DMN if there exists a sequence of (B, n, M I×I )-codes with lim\n≥ R i,j such that lim n →∞ P n i,j = 0 for each (i, j) ∈ I × I.\nWithout loss of generality, we assume that M i,i = 1 and R i,i = 0 for all i ∈ I in the rest of this paper.\nDeﬁnition 9: The capacity region R of the DMN is the closure of the set of all achievable rate tuples R I×I with R i,i = 0 for all i ∈ I.\nLemma 1: Let (X I , Y I , S α , G α , p 1 , p 2 , . . . , p α ) be a DMN. Fix any (B, n, M I×I )-code on the DMN. Then, for each k ∈ {1, 2, . . . , n} and each h ∈ {1, 2, . . . , α},\n(4) Proof: Let U k −1 = (W I×I , X k −1 I , Y k −1 I ) be the collec-\ntion of random variables that are generated before the k th time slot for the (B, n, M I×I )-code. It follows from Deﬁnition 6 that for each k ∈ {1, 2, . . . , n} and each h ∈ {1, 2, . . . , α},\nProposition 2: Fix any (B, n, M I×I )-code on the DMN (X I , Y I , S α , G α , p 1 , p 2 , . . . , p α ) and ﬁx an h ∈ {1, 2, . . . , α}. Then, for each i ∈ S h , X i,k is a function of (W {i}×I , Y k −1 i , Y {i}∩G h−1 ,k ) for each k ∈ {1, 2, . . . , n}.\nProof: Let B = (b 1 , b 2 , . . . , b N ). Fix an i ∈ S h . By Deﬁnition 5, X i,k is a function of (W {i}×I , Y k −b i i ) for each k ∈ {1, 2, . . . , n}. Consider the following two cases:\nCase b i = 0 : Let m be the unique integer such that i ∈ G m . Since B is feasible for the network (cf. Deﬁnition 4), h > m. Since i ∈ G m and X i,k is a function of (W {i}×I , Y k i ), X i,k is a function of (W {i}×I , Y k −1 i , Y {i}∩G m ,k ) and hence a function of (W {i}×I , Y k −1 i , Y {i}∩G h−1 ,k ) because h > m.\nThe following proposition is reproduced from Proposi- tion 2.5 in [4] to facilitate discussion.\nProposition 3: For any discrete random variables X, Y and Z, X → Y → Z forms a Markov Chain if and only if there exist two functions χ (x, y) and ϕ(y, z) such that p (x,y,z) = χ(x,y)ϕ(y,z) for all x, y and z whenever p(y) > 0.\nTheorem 1: Let (X I , Y I , S α , G α , p 1 , p 2 , . . . , p α ) be a DMN. Then for each achievable rate tuple R I×I , there exists a joint distribution for (X I , Y I ) satisfying\nProof: Suppose R I×I is in R. By Deﬁnitions 8 and 9, there exists a sequence of (B, n, M I×I )-codes such that\n(i, j) ∈ I × I. Fix n and the corresponding (B, n, M I×I )- code. It then follows from Deﬁnition 6 that for each k ∈ {1, 2, . . . , n} and each h ∈ {1, 2, . . . , α},\nforms a Markov Chain. Fix any T ⊆ I. Since the N 2 messages W 1,1 , W 1,2 , . . . , W N,N are independent, we have\nwhere the last inequality follows from Fano\u2019s inequality (cf. Deﬁnition 7). We now consider\n(7) Since ∪ α h =1 G h = I, it follows that\nwhere (a) follows from the Markov Chain in (5). Let Q n be a timesharing random variable distributed uniformly on {1, 2, . . . , n} such that Q n is independent of the collection of random variables {X I,k , Y I,k | k = 1, 2, . . . , n}. Then, for any A, B ⊆ I,\nwhere (a) follows from (10) and (b) follows from Lemma 1. Summing the expression in (8) over all k and dividing by n, we have\n1 n\nwhere (a) follows from (11). Similarly from (9), we have 1 n\nwhere (a) follows from (11), and (b) follows from the fact that Q n → (X S h ,Q n , Y G h−1 ,Q n ) → Y G h ,Q n forms a Markov Chain (cf. (12) and Proposition 3). Using (6), (7), (8), (9), (13) and (14), we obtain\nFor each (B, n, M I×I )-code in the sequence of (B, n, M I×I )-codes, let p X I,Qn ,Y I,Qn be the probability distribution induced by the (B, n, M I×I )-code. Consider each distribution for (X I , Y I ) as a point in an |X I ||Y I |- dimensional Euclidean space. Let {p X I,Qn\nbe a convergent subsequence of {p X I,Qn ,Y I,Qn } n =1,2,... with respect to the L 1 -distance, where the L 1 -distance between two distributions u (x) and v(x) on the same discrete alphabet X is deﬁned as x ∈X |u(x) − v(x)|. Since the set of all joint distributions {p X I,Qn ,Y I,Qn } is closed with respect to the L 1 -distance, there exists a joint distribution ¯ q (x I , y I ) such that\ndenote \t the \t conditional \t mutual \t information I (X T ∩S h , Y T ∩G h−1 ; Y T c ∩G h |X T c ∩S h , Y T c ∩G h−1 ) evaluated at the distribution ¯ q (x I , y I ) for each h ∈ {1, 2, . . . , α}. Since\nFor each h ∈ {1, 2, . . . , α}, letting q h,n (x S h , y G h−1 ) denote p X Sh ,Qn ,Y Gh−1 ,Qn (x S h , y G h−1 ), the marginal distribution\n(18) where (a) follows from (16) and (b) follows from (12), which implies that the marginal distribution\nIt then follows from (18) and (19) that the marginal distribution ¯ q (x S h , y G h )\nfor each h ∈ {1, 2, . . . , α} whenever ¯ q (x S h , y G h ) > 0, which implies by recursion that\n(20) whenever ¯ q (x I , y I ) > 0. Since ¯ q (x I , y I ) depends only on the sequence of (B, n, M I×I )-codes but not on T , the theorem follows from (17) and (20).\nThe classical DMN is characterized by a single channel p 1 (y I |x I ). Therefore, the cut-set outer bound in Theorem 1 reduces to i ∈T,j∈T c R i,j ≤ I(X T ; Y T c |X T c ) for the classical DMN where p (x I , y I ) = p(x I )p 1 (y I |x I ), which coincides with the classical cut-set outer bound.\nUsing Theorem 1, we obtain R 1,2 ≤ I(X 1 ; Y 2 ) for the DMC with noiseless reverse channel, which implies the well-known result that R 1,2 ≤ I(X 1 ; Y 2 ) for the DMC with feedback.\nLet R denote the capacity region of the BSC with correlated feedback in Example 1. Let H (ǫ) denote the entropy of a Bernoulli random variable X with P r {X = 0} = ǫ. It then follows from Theorem 1 that for each achievable R {1,2}×{1,2} ,\nWe now present a transmission scheme on this net- work that achieves (0, 1 − H(ǫ), 1, 0). Consider a capacity- achieving block code of length n for the BSC with crossover probability ǫ with rate (1 − H(ǫ)) − arbitrarily close to 1 − H(ǫ). Such a code encodes a message W 1,2 , where |W 1,2 | = ⌈2 n (1−H(ǫ)) − ⌉, into a codeword consisting of a sequence of bits {X \u2032 1,k } k =1,2,...,n . In the k th time slot, node 1 transmits X 1,k = X \u2032 1,k through channel p 1 . The message W 2,1 consists of a sequence of n uniform i.i.d. bits {X \u2032 2,k } k =1,2,...,n . In the k th time slot, upon receiving Y 2,k , node 2 transmits X 2,k = X \u2032 2,k + Y 2,k through channel p 2 , whose output bit Y 1,k is received by node 1. Since P r {Y 2,k = X \u2032 1,k } = 1 − ǫ by (2), node 2 can decode W 1,2 with vanishing probability of error as n goes to inﬁnity. In addition, since P r {Y 1,k = X 2,k + Y 2,k } = 1 by (3) and X 2,k = X \u2032 2,k + Y 2,k , it follows that with probability one,\nTherefore, node 1 receives the bit sequence {X \u2032 2,k } k =1,2,...,n without error. Consequently, (0, 1 − H(ǫ), 1, 0) is achievable, which implies from (23) that R = R ∗ .\nWe deﬁne the generalized DMN which contains the classical DMN as a special case. We also prove the cut-set outer bound for the generalized DMN. Our cut-set outer bound coincides with the classical cut-set outer bound for the classical DMN. Finally, we investigate the BSC with correlated feedback, a two-node generalized DMN where one node incurs no delay, and prove the capacity region of the two-node network using our cut-set outer bound.\nThe work of Raymond Yeung was partially supported by a grant from the University Grants Committee (Project No. AoE/E-02/08) of the Hong Kong Special Administrative Region, China. The work of Gerhard Kramer was supported by the German Ministry of Education and Research and the Alexander von Humboldt Foundation in the framework of the Alexander von Humboldt Professorship."},"refs":[{"authors":[{"name":"T. M. Cove"}],"title":{"text":"Elements of Information Theory, 2nd ed"}},{"authors":[{"name":"A. El Gamal"}],"title":{"text":"On information ﬂow in relay networks"}},{"authors":[{"name":"A. El Gamal"},{"name":"N. Hassanpour"},{"name":"J. Mammen"}],"title":{"text":"Relay networks with delays"}},{"authors":[{"name":"R. W. Yeun"}],"title":{"text":"Information Theory and Network Coding"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566051.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S6.T7.2","endtime":"12:10","authors":"Silas L. Fong, Raymond W. Yeung, Gerhard Kramer","date":"1341316200000","papertitle":"Cut-Set Bound for Generalized Networks","starttime":"11:50","session":"S6.T7: Tools for Bounding Capacity","room":"Stratton (407)","paperid":"1569566051"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
