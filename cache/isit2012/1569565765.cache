{"id":"1569565765","paper":{"title":{"text":"Polar write once memory codes"},"authors":[{"name":"David Burshtein"},{"name":"Alona Strugatski"}],"abstr":{"text":"Abstract\u2014A coding scheme for write once memory (WOM) using polar codes is presented. It is shown that the scheme achieves the capacity region of noiseless WOMs when an ar- bitrary number of multiple writes is permitted. The encoding and decoding complexities scale as O(N log N) where N is the blocklength. For N sufﬁciently large, the error probability decreases sub-exponentially in N. Some simulation results with ﬁnite length codes are presented."},"body":{"text":"C t = (R 1 , . . . , R t ) ∈ R t + | R l < α l−1 h(ǫ l ), l = 1, 2, . . . , t where 0 ≡ ǫ 0 ≤ ǫ 1 , ǫ 2 , . . . , ǫ t−1 ≤ ǫ t ≡ 1/2}\n(R t + denotes a t-dimensional vector with positive elements; h(x) = −x log 2 x − (1 − x) log 2 (1 − x) is the binary entropy function). We also deﬁne the maximum average rate, C t , as the maximum of ( t j=1 R j )/t over (R 1 , . . . , R t ) ∈ C t . The max- imum average rate was shown to be [3] C t = log 2 (t + 1)/t. This means that the total number of bits that can be stored on N WOM cells in t writes is N log 2 (t + 1) which is signiﬁcantly higher than N . WOM codes were proposed in the past by various authors, e.g. [1], [4], [2], [5], [6] and references therein. In this work we propose a new family of WOM codes based on polar codes [7]. The method relies on the fact that polar codes are asymptotically optimal for lossy source coding [8] and can be encoded and decoded efﬁ- ciently ( O(N log N ) operations where N is the blocklength). We show that our method achieves the capacity region of noiseless WOMs when an arbitrary number of multiple writes is permitted. The encoding and decoding complexities scale as O(N log N ). For N sufﬁciently large, the error probability is at most 2 −N β for any 0 < β < 1/2. We also design actual codes and present their performances.\nIn his seminal work [7], Arikan has introduced Polar codes for channel coding and showed that they can achieve the symmetric capacity (i.e. the capacity under uniform input distribution) of an arbitrary binary-input channel. In [9] it was shown that the results can be generalized to arbitrary discrete memory channels. We will follow the notation in [8]. Let G 2 = 1 0 1 1 and let its n-th Kronecker product be G ⊗n 2 . Also denote N = 2 n . Let u be an N -dimensional binary {0, 1} message vector, and let x = uG ⊗n 2 where the matrix multiplication is over GF(2). Suppose that we transmit x over a memoryless binary-input channel with transition probability W (y | x) and channel output vector y. If u is chosen at random with uniform probability then the resulting probability distribution P (u, x, y) is given by\n| u i = 0) W (i) N (y, ˆ u i−1 0 | u i = 1)\nIn fact, as noted in [8], the proof of (5) is not restricted to a BSS and extends to general sources, e.g. a binary erasure source [8].\nAlthough the result in [8] is concerned only with the average distortion, one may combine (5) with the strong converse result of the rate distortion theorem in [11, p. 127] to conclude that |d(X(Y), Y)/N − D| can be made arbitrarily small with probability that approaches 1 as n increases. We now extend this result. The following discussion is valid for an arbitrary discrete MBIOS, W (y | x), in (3). As in [8] we construct a source polar code with frozen set deﬁned by,\n(note that F depends on N , however for simplicity our notation does not show this dependence explicitly) and\nBy [8, Theorem 19 and Equation (22)] (see also [8, Equation (12)]),\nHence, for any ǫ > 0, if N is large enough then the rate R of the code satisﬁes,\nLet y be a source vector produced by a sequence of independent identically distributed (i.i.d.) realizations of Y . If u F is chosen at random with uniform probability then the vector u produced by the SC encoder (that utilizes (4)) has a conditional probability distribution given by [8]\n, y) if i ∈ F c (9) On the other hand, the conditional probability of u given y corresponding to (3) is,\nIn the sequel we employ standard strong typicality argu- ments. Similarly to the notation in [12, Section 10.6, pp. 325- 326], we deﬁne ǫ-strongly typical sequences x, y ∈ X N × Y N with respect to a distribution p(x, y) on X × Y, and denote it by A ∗(N ) ǫ (X, Y ) (or A ∗(N ) ǫ \t for short), as follows. Let C(a, b|x, y) denote the number of occurrences of the symbols a, b in x, y. Then x, y ∈ A ∗(N ) ǫ (X, Y ) if the following two conditions hold. First, for all a, b ∈ X × Y with p(a, b) > 0,\n|C(a, b | x, y)/N − p(a, b)| < ǫ. Second, for all a, b ∈ X × Y with p(a, b) = 0, C(a, b | x, y) = 0.\n   \n  \nα l−1 (1 − ǫ l ) if s = 0, b = 0 α l−1 ǫ l \t if s = 0, b = 1 (1 − α l−1 ) if s = 1, b = 0 0 \t if s = 1, b = 1\nand where α l is deﬁned in (2). This channel is also shown in Figure 1. It is easy to verify that the capacity of this channel is 1−α l−1 h(ǫ l ) and that the capacity achieving input distribution is symmetric, i.e., P (X = 0) = P (X = 1) = 1/2. For each\nchannel, l, we design a polar code with blocklength N and frozen set of sub-channels F l deﬁned by (6). The rate is\nwhere δ l > 0 is arbitrarily small for N sufﬁciently large. This code will be used as a source code. The relation between R l and R \u2032 l is\nNow we deﬁne E l (s, a) and D l (s) as follows. Encoding function, ˆs = E l (s, a):\n1) Let v = s ⊕ g where ⊕ denotes bitwise XOR and g is a sample from an N dimensional uniformly distributed random binary {0, 1} vector. The vector g is a common randomness source (dither), known both to the encoder and to the decoder.\n2) Let y j = (s j , v j ) and y = (y 1 , y 2 , . . . , y N ). Compress the vector y using the l-th polar code with u F l = a l . This results in a vector u and a vector x = uG ⊗n 2 .\nDecoding function, ˆ a = D l (ˆs): 1) Let x = ˆs ⊕ g.\nwhere (z) F l denotes the elements of the vector z in the set F l .\nNote that the information is embedded within the set F l . Hence, when considered as a WOM code, our code has rate R l = |F l |/N = (N − |F c l |)/N = 1 − R \u2032 l .\nFor the sake of the proof we slightly modify the coding scheme as follows:\n   \n  \n(1 − α l−1 )/2 if s = 1, v = 0 α l−1 /2 \t if s = 0, v = 0 α l−1 /2 \t if s = 0, v = 1\nThe proof follows from Theorem 1 that asserts, for N (i.e., n) large enough, that\nw.p. at least 1 − 2 −N β . The details are omitted due to space limitations.\nWe proceed to the proof of Theorem 2. We denote by S l , S, V, G, X and Γ l the random variables corresponding to s l , s, v, g, x and γ l .\nProof of Theorem 2: Note that we only need to prove successful encoding since the WOM is noiseless.\nRecall that Γ l = w H (S l ). Suppose that Γ l−1 ∼ B(N, 1 − α l−1 ). Our ﬁrst claim is that under this assumption, for ξ > 0 sufﬁciently small and N sufﬁciently large, w.p. at least 1 − 2 −N β , the encoding will be successful and Γ l /N < 1−α l −ξ. Considering step 1 of the encoding we see that (S, V) can be considered as i.i.d. sampling of the source (S, V ) deﬁned in (14) (since G is a BSS and using (M3) above). Hence, by Lemma 1 (with δ/2 instead of δ) and (M1), the compression of this vector in step 2 satisﬁes the following for any δ > 0 and N sufﬁciently large w.p. at least 1 − 2 −N β .\n2) For at most [(ǫ l − ζ)α l−1 + δ/2] N components k we have S k = 0 and X k = V k ⊕ 1 = S k ⊕ G k ⊕ 1 = G k ⊕ 1.\nHence in step 3 of the encoding, if S k = 1 then ˆ S k = X k ⊕ G k = 1 (i.e. the WOM constraints are satisﬁed). In addition there are at most [(ǫ l − ζ)α l−1 + δ/2]N components k for which S k = 0 and ˆ S k = 1. Therefore, w.p. at least 1 − 2 −N β , the vectors S and ˆ S satisfy the WOM constraints and,\n= [1 − α l − ζα l−1 + δ]N \t (15) (in the ﬁrst inequality we have used the fact that for n sufﬁciently large, Γ l−1 < (1 − α l−1 + δ/2)N w.p. at least 1 − e −N ǫ for some ǫ > 0 independent of N ). Setting ξ = ζα l−1 − δ yields our ﬁrst claim.\nFrom (15) we know that k in (M4) will indeed satisfy the required condition w.p. at least 1 − 2 −N β . The proof of the theorem now follows by using induction on l to conclude that (w.p. at least 1 − 2 −N β ) the l-th encoding is successful and Γ l ∼ B(N, 1 − α l ). The complexity claim is due to the results in [7].\nWe note the following. The test channel in the ﬁrst write is actually a BSC (since α l−1 = 1 in Figure 1). Similarly, it is easy to verify that in the last ( t) write we can merge together the source symbols (0, 0) and (0, 1) thus obtaining a test channel which is a binary erasure channel (BEC).\nIn practice (e.g., in ﬂash memory), the dither g can be determined from the address of the data word (i.e., the address is used as a seed to a random number generator).\na polar code with n = 16 were R 1 = .7913, R 2 = .6687 and R 3 = .34. For M = 1000 read/write experiments all information triples were encoded (and decoded) successfully.\nOne possible generalization of our work is to the case of a noisy WOM. In this case one might wish to consider communications over a Gelfand-Pinsker (GP) channel and use the results in [8]. However, these results may not be suitable for WOM codes, as they require a two-stage writing process where the second write does not satisfy the power constraint.\nOther codes and decoding methods may be considered in our WOM scheme, for example low-density generating-matrix (LDGM) codes that were shown useful in the past for lossy compression. Since iterative decoding usually yields better results compared to SC decoding of polar codes [7] [13], it may be possible to improve the performance of our SC encoder by using iterative encoding combined with decimation.\nThis research was supported by the Israel Science Founda- tion, grant no. 772/09."},"refs":[{"authors":[{"name":"R. Rivest"},{"name":"A. Shamir"}],"title":{"text":"How to reuse a write-once memory"}},{"authors":[{"name":"E. Yaakobi"},{"name":"S. Kayser"},{"name":"P. Siegel"},{"name":"A. Vardy"},{"name":"J. Wolf"}],"title":{"text":"Efﬁcient two- write WOM-codes"}},{"authors":[{"name":"C. Heegard"}],"title":{"text":"On the capacity of permanent memory"}},{"authors":[{"name":"G. Cohen"},{"name":"P. Godlewski"},{"name":"F. Merkx"}],"title":{"text":"Linear binary code for write- once memories"}},{"authors":[{"name":"E. Yaakobi"},{"name":"P. Siegel"},{"name":"A. Vardy"},{"name":"J. Wolf"}],"title":{"text":"Multiple error-correcting WOM-codes"}},{"authors":[{"name":"A. Shpilka"}],"title":{"text":"New constructions of WOM codes using the Wozencraft ensemble"}},{"authors":[{"name":"E. Arikan"}],"title":{"text":"Channel polarization: A method for constructing capacity- achieving codes for symmetric binary-input memoryless channels"}},{"authors":[{"name":"S. Korada"},{"name":"R. Urbanke"}],"title":{"text":"Polar codes are optimal for lossy source coding"}},{"authors":[{"name":"E. Sasoglu"},{"name":"E. Telatar"},{"name":"E. Arikan"}],"title":{"text":"Polarization for arbitrary discrete memoryless channels"}},{"authors":[{"name":"E. Arikan"},{"name":"E. Telatar"}],"title":{"text":"On the rate of channel polarization"}},{"authors":[{"name":"I. Csisz´a"},{"name":"J. K ¨orne"}],"title":{"text":"Information Theory: Coding Theorems for Discrete Memoryless Systems , 2nd ed"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, 2nd ed"}},{"authors":[{"name":"S. Korada"}],"title":{"text":"Polar codes for channel and source coding"}},{"authors":[{"name":"R. Mori"},{"name":"T. Tanaka"}],"title":{"text":"Performance and construction of polar codes on symmetric binary-input memoryless channels "}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565765.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S11.T5.4","endtime":"11:10","authors":"David Burshtein, Alona Strugatski","date":"1341485400000","papertitle":"Polar write once memory codes","starttime":"10:50","session":"S11.T5: Polar Codes:  Theory and Practice","room":"Kresge Little Theatre (035)","paperid":"1569565765"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
