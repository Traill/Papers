{"id":"1569565033","paper":{"title":{"text":"Wyner-Ziv Type Versus Noisy Network Coding For a State-Dependent MAC"},"authors":[{"name":"Abdellatif Zaidi \u2020"},{"name":"Pablo Piantanida"},{"name":"Shlomo Shamai (Shitz) \u2021"}],"abstr":{"text":"Abstract\u2014 We consider a two-user state-dependent mul- tiaccess channel in which the states of the channel are known non-causally to one of the encoders and only strictly causally to the other encoder. Both encoders transmit a common message and, in addition, the encoder that knows the states non-causally transmits an individual message. We ﬁnd explicit characterizations of the capacity region of this communication model. The analysis also reveals optimal ways of exploiting the knowledge of the state only strictly causally at the encoder that sends only the common message when such a knowledge is beneﬁcial. The encoders collaborate to convey to the decoder a lossy version of the state, in addition to transmitting the information messages through a generalized Gel\u2019fand-Pinsker binning. Particularly important in this problem are the questions of 1) optimal ways of performing the state compression and 2) whether or not the compression indices should be decoded uniquely. We show that both compression `a-la noisy network coding, i.e., with no binning, and compression using Wyner-Ziv bin- ning are optimal. The scheme that uses Wyner-Ziv binning shares elements with Cover and El Gamal original compress- and-forward, but di ﬀers from it mainly in that backward decoding is employed instead of forward decoding and the compression indices are not decoded uniquely. Finally, by exploring the properties of our outer bound, we show that, although not required in general, the compression indices can in fact be decoded uniquely essentially without altering the capacity region, but at the expense of larger alphabets sizes for the auxiliary random variables."},"body":{"text":"Advances in the study of the e ﬀect of strictly causal states in multiuser channels are rather very recent and concern mainly multiple access scenarios. In [1], Lapidoth and Steinberg study a two-encoder multiple access channel with independent messages and states known causally at the encoders. They show that the strictly causal state sequence can be beneﬁcial, in the sense that it increases the capacity for this model. This result is reminiscent of Dueck\u2019s proof [2] that feedback can increase the capacity region of some broadcast channels. In accordance with [2], the main idea of the achievability result in [1] is a block Markov coding scheme in which the two users collaborate to describe the state to the decoder by sending cooperatively a compressed version of it. As noticed in [1], although some non-zero rate that otherwise could be used to transmit pure information is spent in describing the state to the decoder, the net e ﬀect can be an increase in the capacity.\nIn [3], they show that strictly causal state information is beneﬁcial even if the channel is controlled by two independent states each known to one encoder strictly causally. In this case, each encoder can help the other encoder transmit at a higher rate by sending a compressed version of its state to the decoder. In [4], Li, Simeone and Yener improve the results of [1], [3] and extend them to the case of multiple encoders. The achievability results in [4] are inspired by the noisy network coding scheme of [5] and, unlike [1], [3], do not use Wyner- Ziv binning [6] for the compression of the state. In a very recent contribution [7], Lapidoth and Steinberg derive a new inner bound on the capacity region for the case of a single state governing the multiaccess channel. They also prove that the inner bound of [4] for the case of two independent states each known strictly causally to one encoder can indeed be strictly better than previous bounds in [1], [3] \u2013 a result which is conjectured previously by Li, Simeone and Yener in [4].\nIn this paper, which generalizes our former work [8], we study a two-user state-dependent multiple access channel with the channel states known non-causally at one encoder and only strictly causally at the other encoder. The decoder is not aware of the channel states. As shown in Figure 1, both encoders transmit a common message and, in addition, the encoder that knows the states non-causally transmits an individual message. More precisely, let W c and W 1 denote the common message and the individual message to be transmitted in, say, n uses of the channel; and S n = (S 1 , . . . , S n ) denote the state sequence a ﬀecting the channel during this time. At time i, Encoder 1 knows the complete sequence S n = (S 1 , . . . , S i−1 , S i , . . . , S n ) and sends X 1i = φ 1 (W c , W 1 , S n ),\nWe consider a stationary memoryless state-dependent MAC W Y|X 1 ,X 2 ,S whose output Y ∈ Y is controlled by the channel inputs X 1 ∈ X 1 and X 2 ∈ X 2 from the encoders and a channel state S ∈ S drawn according to a memoryless probability law Q S . We assume that the channel state S n is known non-causally at Encoder 1, i.e., beforehand, at the beginning of the transmission block. Encoder 2 knows the channel states only strictly-causally; that is, at time i, it knows the states only up to time i − 1, S i−1 = (S 1 , . . . , S i−1 ).\nBoth encoders transmit a common message W c and, in addition, Encoder 1 transmits also an individual message W 1 . We assume that W c and W 1 are independent random variables drawn uniformly from the sets W c = {1, · · · , M c } and W 1 = {1, · · · , M 1 }, respectively. The sequences X n 1 and X n 2 from the encoders are sent across a state-dependent multiaccess channel modeled as a memoryless conditional probability distribution W Y|X 1 ,X 2 ,S . The joint probability mass function on W c × W 1 × S n × X n 1 × X n 2 × Y n is given by\n,X 2 ,S (y i | x 1 ,i , x 2 ,i , s i ) . (1) The receiver estimates the pair ( ˆ W c , ˆ W 1 ) from the output Y n .\nDeﬁnition 1: For positive integers n, M c and M 1 , an (M c , M 1 , n, ) code for the multiple access channel with states known noncausally at one encoder and only strictly causally at the other encoder consists of a mapping\nP n e = E S Pr ψ(Y n ) (W c , W 1 )|S n = s n ≤ . \t (5) The rate of the common message and the rate of the individual message are deﬁned as\nA rate pair (R c , R 1 ) is said to be achievable if for every > 0 there exists an (2 nR c , 2 nR 1 , n, ) code for the channel W Y|X 1 ,X 2 ,S . The capacity region of the considered state-dependent MAC is deﬁned as the closure of the set of achievable rate pairs.\nDue to space limitation, the results of this paper are either outlined only or mentioned without proofs. Detailed proofs, converse proofs, the characterization of the capacity region in the Gaussian case as well as other results and discussions can be found in [11].\nIn this section, it is assumed that S, X 1 , X 2 are ﬁnite. A. Capacity Region\nLet P stand for the collection of all random variables (S , U, V, X 1 , X 2 , Y) such that U, V, X 1 and X 2 take values in ﬁnite alphabets U, V, X 1 and X 2 , respectively, and\n= P S ,U,V,X 1 X 2 (s , u, v, x 1 , x 2 )W Y|X 1 ,X 2 ,S (y|x 1 , x 2 , s) \t (7a) P S ,U,V,X 1 ,X 2 (s , u, v, x 1 , x 2 )\nThe relations in (7) imply that (U , V) ↔ (S, X 1 , X 2 ) ↔ Y is a Markov chain, and X 2 is independent of S.\nDeﬁne C to be the set of all rate pairs (R c , R 1 ) such that R 1 ≤ I(U; Y|V , X 2 ) − I(U; S|V , X 2 )\nfor some (S , U, V, X 1 , X 2 , Y) ∈ P. (8) The following proposition states some properties of C.\nAs stated in the following theorem, the set C characterizes the capacity region of the state-dependent discrete memoryless MAC model that we study.\nTheorem 1: The capacity region of the multiple access channel with states known only strictly causally at the encoder that sends the common message and non-causally at the encoder that sends both messages is given by C.\nRemark 1: The proof of achievability of Theorem 1 is based on a block-Markov coding scheme in which a lossy version of the state is conveyed to the decoder, in the spirit of [1], [3], [7], in addition to a generalized Gel\u2019fand-Pinsker binning for the transmission of the information messages [12]. However, unlike [1], [3] and [7] where Wyner-Ziv compression is utilized for the transmission of the lossy version of the state, here, inspired by the noisy network coding scheme of [5], at each block the compression index of the state of the previous block is sent using standard rate distortion, not Wyner- Ziv binning [6]. Also, unlike [1], [3] and [7] where every\ninformation message is divided into blocks and di ﬀerent submessages are sent over these blocks and then decoded one at a time using the same codebook as in the original compress-and-forward scheme by Cover and El Gamal [13], here the entire common message and the entire individual message are transmitted over all blocks using codebooks that are generated independently, one for each block, and the decoding is performed simultaneously using all blocks as in [5]. At the end of the transmission, the receiver uses the outputs of all blocks to perform simultaneous decoding of the information common and individual messages, without uniquely decoding the compression indices.\nThe transmission takes place in B blocks. The common message W c and the individual message W 1 are sent over all blocks. We thus have B W c = nBR c , B W 1 = nBR 1 , N = nB, R W c = B W c /N = R c and R W 1 = B W 1 /N = R 1 , where B W c is the number of common message bits, B W 1 is the number of individual message bits, N is the number of channel uses and R W c and R W 1 are the overall rates of the common and individual messages, respectively.\nCodebook Generation: Fix a measure P S ,U,V,X 1 ,X 2 ,Y ∈ P. Fix > 0, η c > 0, η 1 > 0, ˆη > 0, δ > 1 and denote M c = 2 nB[R c − η c ] ,\nWe randomly and independently generate a codebook for each block.\n1) For each block i, i = 1, . . . , B, we generate M c ˆ M inde- pendent and identically distributed (i.i.d.) codewords x 2 ,i (w c , t i ) indexed by w c = 1, . . . , R c , t i = 1, . . . , ˆ M, each with i.i.d. components drawn according to P X 2 .\n2) For each block i, for each codeword x 2 ,i (w c , t i ), we generate ˆ M i.i.d. codewords v i (w c , t i , t i ) indexed by t i = 1 , . . . , ˆ M, each with i.i.d. components drawn according to\n3) For each block i, for each codeword x 2 ,i (w c , t i ), for each codeword v i (w c , t i , t i ), we generate a collection of JM 1 i.i.d. codewords {u i (w c , t i , t i , w 1 , j i )} indexed by w 1 = 1 , . . . , M 1 , j i = 1, . . . , J, each with i.i.d. components draw according to P U|V ,X 2 .\nEncoding: Suppose that a common message W c = w c and an individual message W 1 = w 1 are to be transmitted. As we mentioned previously, w c and w 1 will be sent over all blocks. We denote by s[i] the state a ﬀecting the channel in block i, i = 1, . . . , B. For convenience, we let s[0] = ∅ and t −1 = t 0 = 1 (a default value). The encoding at the beginning of block i, i = 1, . . . , B, is as follows.\nEncoder 2, which has learned the state sequence s[i−1], knows t i−2 and looks for a compression index t i−1 ∈ [1 : ˆ M] such that v i−1 (w c , t i−2 , t i−1 ) is strongly jointly typical with s[i − 1] and x 2 ,i−1 (w c , t i−2 ). If there is no such index or the observed state s [i − 1] is not typical, t i−1 is set to 1 and an error is declared. If there is more than one such index t i−1 , choose the smallest. Encoder 2 then transmits the vector x 2 ,i (w c , t i−1 ).\nEncoder 1 obtains x 2 ,i (w c , t i−1 ) similarly. It then ﬁnds the smallest compression index t i ∈ [1 : ˆ M] such that v i (w c , t i−i , t i ) is strongly jointly typical with s[i] and x 2 ,i (w c , t i−1 ). Again, if there is no such index or the observed state s[i] is not typical, t i is set to 1 and an error is declared. Next, Encoder 1 looks for the smallest j i such that u i (w c , t i−1 , t i , w 1 , j i ) is jointly typical\nwith s[i] given (x 2 ,i (w c , t i−1 ) , v i (w c , t i−1 , t i )). Denote this j i by j i = j(s[i], w c , t i−1 , t i , w 1 ). If such j i is not found, an error is declared and j(s[i] , w c , t i−1 , t i , w 1 ) is set to j i = J. Encoder 1 then transmits a vector x 1 [i] which is drawn i.i.d. conditionally given u i (w c , t i−1 , t i , w 1 , j i ), s[i], v i (w c , t i−1 , t i ) and x 2 ,i (w c , t i−1 ) (using the conditional measure P X 1 | U ,S,V,X 2 induced by (7)).\nDecoding: At the end of the transmission, the decoder has collected all the blocks of channel outputs y[1] , . . . , y[B].\nStep (a): The decoder estimates message w c using all blocks i = 1 , . . . , B, i.e., simultaneous decoding. It declares that ˆw c is sent if there exist t B = (t 1 , . . . , t B ) ∈ [1 : ˆ M] B , w 1 ∈ [1 : M 1 ] and j B = ( j 1 , . . . , j B ) ∈ [1 : J] B such that x 2 ,i ( ˆ w c , t i−1 ), u i ( ˆ w c , t i−1 , t i , w 1 , j i ), v i ( ˆ w c , t i−1 , t i ) and y[i] are jointly typical for all i = 1, . . . , B. One can show that the decoder obtains the correct w c as long as n and B are large and\nStep (b): Next, the decoder estimates message w 1 using again all blocks i = 1, . . . , B, i.e., simultaneous decoding. It declares that ˆ w 1 is sent if there exist t B = (t 1 , . . . , t B ) ∈ [1 : ˆ M] B , j B = ( j 1 , . . . , j B ) ∈ [1 : J] B such that x 2 ,i ( ˆ w c , t i−1 ), u i ( ˆ w c , t i−1 , t i , ˆw 1 , j i ), v i ( ˆ w c , t i−1 , t i ) and y[i] are jointly typical for all i = 1, . . . , B. One can show that the decoder obtains the correct w 1 as long as n and B are large and\nR 1 ≤ I(U; Y|V , X 2 ) − I(U; S|V , X 2 ) \t (11a) R 1 ≤ I(U , V, X 2 ; Y) − I(U , V, X 2 ; S) . \t (11b)\nB. Wyner-Ziv Binning With Non-unique Decoding is Opti- mal\nIn the coding scheme of Theorem 1, the state compression is standard, i.e., uses no Wyner-Ziv binning, the same message is sent in every block, and the decoding of the sent message is performed jointly using all blocks. Although of no beneﬁt in the case of one relay, the combination of these three features was shown to be essential in achieving rates that are strictly larger than those o ﬀered by schemes based on Cover and El Gamal classic compress-and-forward scheme [13] for certain networks with multiple relays in [5]. That is, the coding scheme of [5] outperforms Cover and El Gamal classic compress-and-forward for some multi-relay networks in [5]. One can wonder whether the same holds for our model, i.e., whether schemes based on Cover and El Gamal classic compress-and-forward, i.e., block Markov encoding combined with Wyner-Ziv binning, fall short of achieving optimality for our model. In this paper, we show that the capacity region C as given by (8) can be achieved alternatively with a coding scheme that we obtain by building upon and modifying Cover and El Gamal original compress-and- forward scheme. The modiﬁcation consists essentially in 1) decoding block-by-block backwardly instead of block-by- block forwardly and 2) non-unique decoding of the com- pression indices. (In fact, by investigating more closely the converse proof of Theorem 1, we will show later that 2) can be relaxed essentially without altering the capacity region). The following theorem states the result.\nTheorem 2: For the state-dependent multiaccess channel model that we study, there exists an optimal coding scheme\nthat uses Wyner-Ziv binning for the state compression. That is, the capacity region C given by (8) can also be achieved using a coding scheme in which the state compression is performed using Wyner-Ziv binning.\nProof: The achievability proof of Theorem 2 is based on a block-Markov coding scheme that combines carefully Gel\u2019fand-Pinsker binning and Wyner-Ziv binning, and uti- lizes backward decoding with non-unique decoding of the compression indices.\nThe transmission takes place in B blocks. The common message W c is divided into B blocks w c ,1 , . . . , w c ,B of nR c bits each, and the individual messages W 1 are divided into B blocks w 1 ,1 , . . . , w 1 ,B of nR 1 bits each. For convenience, we let w c ,B = w 1 ,B = 1 (a default value). We thus have B W c = n(B − 1)R c , B W 1 = n(B − 1)R 1 , N = nB, R W c = B W c /N = R c ·(B − 1) /B and R W 1 = B W 1 /N = R 1 ·(B − 1) /B, where B W c\nis the number of common message bits, B W 1 is the number of individual message bits, N is the number of channel uses and R W c and R W 1 are the overall rates of the common and individual messages, respectively. For ﬁxed n, the average rate pair (R W c , R W 1 ) over B blocks can be made as close to (R c , R 1 ) as desired by making B large.\nCodebook Generation: Fix a measure P S ,U,V,X 1 ,X 2 ,Y ∈ P. Fix > 0 and denote M c = 2 n[R c − η c ] , M 1 = 2 n[R 1 − η 1 ] , M 0 = 2 n[R 0 +η 0 ] ,\n1) We generate M c M 0 independent and identically dis- tributed (i.i.d.) codewords x 2 (w c , s) indexed by w c = 1 , . . . , R c , s = 1, . . . , M 0 , each with i.i.d. components drawn according to P X 2 .\n2) For each codeword x 2 (w c , s), we generate ˆ M independent and identically distributed (i.i.d.) codewords v(w c , s, z) indexed by z = 1, . . . , ˆ M, each with i.i.d. components drawn according to P V|X 2 .\n3) For each codeword x 2 (w c , s), for each codeword v(w c , s, z), we generate a collection of JM 1 i.i.d. codewords {u(w c , s, z, w 1 , j)} indexed by w 1 = 1, . . . , M 1 , j = 1, . . . , J, each with i.i.d. components draw according to P U|V ,X 2 .\n4) Randomly partition the set {1 , . . . , ˆ M} into M 0 cells C s , s ∈ [1 , M 0 ].\nEncoding: Suppose that a common message W c = w c and an individual message W 1 = w 1 are to be transmitted. As we mentioned previously, message w c is divided into B blocks w c ,1 , . . . , w c ,B and message w 1 is divided into B blocks w 1 ,1 , . . . , w 1 ,B , with (w c ,i , w 1 ,i ) the pair messages sent in block i. We denote by s[i] the channel state in block i, i = 1, . . . , B. For convenience, we let s[0] = φ and z 0 = 1 (a default value), and s 0 the index of the cell containing z 0 , i.e., z 0 ∈ C s 0 . The encoding at the beginning of the block i, i = 1, . . . , B, is as follows.\nEncoder 2, which has learned the state sequence s[i−1], knows s i−2 and looks for a compression index z i−1 ∈ [1 , ˆ M] such that v (w c ,i−1 , s i−2 , z i−1 ) is strongly jointly typical with s[i − 1] and x 2 (w c ,i−1 , s i−2 ). If there is no such index or the observed state s [i − 1] is not typical, z i−1 is set to 1 and an error is declared. If there is more than one such index z i−1 , choose the smallest. One can show that the probability of error of this event is arbitrarily small provided that n is large and\nEncoder 2 then transmits the vector x 2 (w c ,i , s i−1 ), where s i−1 is such that z i−1 ∈ C s i−1 .\nEncoder 1 obtains x 2 (w c ,i , s i−1 ) similarly. It then ﬁnds the smallest compression index z i ∈ [1 , ˆ M] such that v(w c ,i , s i−1 , z i ) is strongly jointly typical with s[i] and x 2 (w c ,i , s i−1 ). Again, if there is no such index or the observed state s[i] is not typical, z i is set to 1 and an error is declared. Let s i ∈ [1 , M 0 ] such that z i ∈ C s i . Next, Encoder 1 looks for the smallest j i such that u(w c ,i , s i−1 , z i , w 1 ,i , j i ) is jointly typical with s[i], x 2 (w c ,i , s i−1 ) and v(w c ,i , s i−1 , z i ). Denote this j i by j i = j(s[i], w c ,i , s i−1 , z i , w 1 ,i ). If such j i is not found, an error is declared and j(s[i] , w c ,i , s i−1 , z i , w 1 ,i ) is set to j i = J. Encoder 1 then transmits a vector x 1 [i] which is drawn i.i.d. conditionally given s[i], u(w c ,i , s i−1 , z i , w 1 ,i , j i ), v(w c ,i , s i−1 , z i ) and x 2 (w c ,i , s i−1 ) (using the conditional measure P X 1 | S ,U,V,X 2 induced by P S ,U,V,X 1 ,X 2 ,Y ∈ P).\nThe decoding of the pair (w c ,B−1 , w 1 ,B−1 ) is performed in four steps, as follows.\nStep (a): The decoder knows w c ,B = 1 and looks for the unique cell index ˆs B−1 such that the vector x 2 (w c ,B , ˆs B−1 ) is jointly typical with y[B]. The decoding operation in this step incurs small probability of error as long as n is su ﬃciently large and\nStep (b): The decoder now knows ˆs B−1 (i.e., the in- dex of the cell in which the compression index z B−1 lies). It then decodes message w c ,B−1 by looking for the unique ˆ w c ,B−1 such that x 2 ( ˆ w c ,B−1 , s B−2 ), v( ˆ w c ,B−1 , s B−2 , z B−1 ), u ( ˆ w c ,B−1 , s B−2 , z B−1 , w 1 ,B−1 , j B−1 ) and y[B − 1] are jointly typical for some s B−2 ∈ [1 , M 0 ], w 1 ,B−1 ∈ [1 , M 1 ], j B−1 ∈ [1 , J] and z B−1 ∈ C ˆs B−1 . One can show that the decoder obtains the correct w c ,B−1 as long as n and B are large and\nStep (c): The decoder knows ˆ w c ,B−1 and can again obtain the correct s B−2 if n is large and (13) is true. This is accomplished by looking for the unique ˆs B−2 such that the vector x 2 ( ˆ w c ,B−1 , ˆs B−2 ) is jointly typical with y[B − 1].\nStep (d): Finally, the decoder, which now knows message ˆ w c ,B−1 and the cell index ˆs B−2 (but not the exact compression in- dex z B−1 ), estimates w 1 ,B−1 using y[B−1]. It declares that ˆ w 1 ,B−1 was sent if there exists a unique ˆ w 1 ,B−1 such that x 2 ( ˆ w c ,B−1 , ˆs B−2 ), v ( ˆ w c ,B−1 , ˆs B−2 , z B−1 ), u( ˆ w c ,B−1 , ˆs B−2 , z B−1 , ˆw 1 ,B−1 , j B−1 ) and y[B − 1] are jointly typical for some z B−1 ∈ C ˆs B−1 and j B−1 ∈ [1 , J].\n= z B−1 , the decoder ﬁnds the correct w 1 ,b−1 for su ﬃciently large n if\nz B−1 , the decoder ﬁnds the correct w 1 ,b−1 for su ﬃciently large n if\nNext, for b ranging from B − 1 to 2, the decoding of the pair (w c ,b−1 , w 1 ,b−1 ) is performed similarly, in ﬁve steps, by using the information y[b] received in block b and the information y [b − 1] received in block b − 1. More speciﬁcally, this is done as follows.\nStep (a): The decoder knows w c ,b and looks for the unique cell index ˆs b−1 such that the vector x 2 (w c ,b , ˆs b−1 ) is jointly typical with y[b]. The decoding error in this step is small for su ﬃciently large n if (13) is true.\nStep (b): The decoder knows ˆs b−1 and decodes message w c ,b−1 from y[b]. It looks for the unique ˆ w c ,b−1 such that x 2 ( ˆ w c ,b−1 , s b−2 ), v ( ˆ w c ,b−1 , s b−2 , z b−1 ), u( ˆ w c ,b−1 , s b−2 , z b−1 , w 1 ,b−1 , j b−1 ) and y[b − 1] are jointly typical for some s b−2 ∈ [1 , M 0 ], w 1 ,b−1 ∈ [1 , M 1 ], j b−1 ∈ [1 , J] and z b−1 ∈ C ˆs b−1 . One can show that the decoding error in this step is small for su ﬃciently large n if (14) is true. Step (c): The decoder knows ˆ w c ,b−1 and obtains ˆs b−2 by looking for the unique ˆs b−2 such that the vector x 2 ( ˆ w c ,b−1 , ˆs b−2 ) is jointly typical with y[b − 1]. For su ﬃciently large n, the decoder obtains the correct s b−2 with high probability if (13) is true.\nStep (d): Finally, the decoder, which now knows mes- sage ˆ w c ,b−1 and the cell index ˆs b−2 (but not the exact compression index z b−1 ), estimates message w 1 ,b−1 using y [b − 1]. It declares that ˆ w 1 ,b−1 was sent if there exists a unique ˆ w 1 ,b−1 such that x 2 ( ˆ w c ,b−1 , ˆs b−2 ), v( ˆ w c ,b−1 , ˆs b−2 , z b−1 ), u ( ˆ w c ,b−1 , ˆs b−2 , z b−1 , ˆw 1 ,b−1 , j b−1 ) and y[b − 1] are jointly typical for some z b−1 ∈ C ˆs b−1 and j b−1 ∈ [1 , J].\n= z b−1 , the decoder ﬁnds the correct w 1 ,b−1 for su ﬃciently large n if (15) is true.\nz b−1 , the decoder ﬁnds the correct w 1 ,b−1 for su ﬃciently large n if (16) is true.\nApplying Fourier-Motzkin Elimination (FME) to project out R 0 and ˆ R from (13),(14), (15) and (16), we get the desired result (8).\nAs we mentioned previously, the coding scheme of The- orem 2 shares elements with Cover and El Gamal original compress-and-forward [13, Theorem 7]; but di ﬀers from it mainly in two aspects. First, it uses backward decoding instead of the forward decoding of [13]; and, second, unlike [13] it does not require unique decoding of the compression indices. The second aspect is essential for getting the same rate expression as in (8), with no additional constraints. However, as we will see shortly in the corollary that will follow, one can modify the coding scheme of Theorem 2 in a way to get the compression indices decoded uniquely and still get the capacity region, at the expense of slightly larger | V| and larger | U|. The key element is the observation that the constraint introduced by getting the compression index decoded, i.e.,\nis also implicit in the converse proof of Theorem 1. That is, the auxiliary random variables U and V of the converse proof of Theorem 1 satisfy (18).\nTheorem 3: The coding scheme of Theorem 2 can be modiﬁed in a way to get the compression index decoded. The resulting coding scheme is optimal and achieves an equivalent characterization of the capacity region of the model that we study given by the set of all rate pairs (R c , R 1 ) such that\nwhere the auxiliary random variables V and U have their alphabets bounded as\nProof: The coding scheme that we use for the proof of Theorem 3 is very similar to that of Theorem 2, but with unique decoding of the compression indices. (See [11]).\nThis work has been supported by the European Com- mission in the framework of the Network of Excellence in Wireless Communications (NEWCOM#). The work of S. Shamai was supported by the Philipson Fund for Electrical Power, via the Technion research authority."},"refs":[{"authors":[{"name":"A. Lapidoth"},{"name":"Y. Steinberg"}],"title":{"text":"The multiple access channel with causal and strictly causal side information at the encoders"}},{"authors":[{"name":"G. Dueck"}],"title":{"text":"Partial feedback for two-way and broadcast channels"}},{"authors":[{"name":"A. Lapidoth"},{"name":"Y. Steinberg"}],"title":{"text":"The multiple access channel with two independent states each known causally at one encoder"}},{"authors":[{"name":"M. Li"},{"name":"O. Simeone"},{"name":"A. Yener"}],"title":{"text":"Multiple access channels with states causally known at transmitters"}},{"authors":[{"name":"S. H. Lim"},{"name":"Y.-H. Kim"},{"name":"A. E. Gamal"},{"name":"S.-Y. Chung"}],"title":{"text":"Noisy network coding"}},{"authors":[{"name":"A. D. Wyner"},{"name":"J. Ziv"}],"title":{"text":"The rate-distortion function for source coding with side information at the decoder"}},{"authors":[{"name":"A. Lapidoth"},{"name":"Y. Steinberg"}],"title":{"text":"A note on multiple access channels with strictly causal state information"}},{"authors":[{"name":"A. Zaidi"},{"name":"P. Piantanida"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"Multiple access channel with states known noncausally at one encoder and only strictly causally at the other encoder"}},{"authors":[{"name":"G. Kramer"},{"name":"J. Hou"}],"title":{"text":"On message lengths for noisy network coding"}},{"authors":[{"name":"X. Wu"},{"name":"L.-L. Xie"}],"title":{"text":"On the optimal compressions in the compress-and-forward relay schemes"}},{"authors":[{"name":"A. Zaidi"},{"name":"P. Piantanida"},{"name":"S. Shamai (Shitz)"}],"title":{"text":"Capacity re- gion of multiple access channel with states known noncausally at one encoder and only strictly causally at the other en- coder"}},{"authors":[{"name":"S. I. Gel\u2019fand"},{"name":"M. S. Pinsker"}],"title":{"text":"Coding for channel with random parameters"}},{"authors":[{"name":"T. M. Cover"},{"name":"A. El Gamal"}],"title":{"text":"Capacity theorems for the relay channel"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565033.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T2.1","endtime":"11:50","authors":"Abdellatif Zaidi, Pablo Piantanida, Shlomo (Shitz) Shamai","date":"1341401400000","papertitle":"Wyner-Ziv Type Versus Noisy Network Coding For a State-Dependent MAC","starttime":"11:30","session":"S10.T2: Multiple Access Channels with Side Information","room":"Kresge Auditorium (109)","paperid":"1569565033"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
