{"id":"1569566597","paper":{"title":{"text":"On Secure Communication with Constrained Randomization"},"authors":[{"name":"Matthieu R. Bloch"},{"name":"J¨org Kliewer"}],"abstr":{"text":"Abstract\u2014In this paper, we investigate how constraints on the randomization in the encoding process affect the secrecy rates achievable over wiretap channels. In particular, we characterize the secrecy capacity with a rate-limited local source of random- ness and a less capable eavesdropper\u2019s channel, which shows that limited rate incurs a secrecy rate penalty but does not preclude secrecy. We also show that secure communication is possible when randomizing with a non-uniform source of randomness, which suggests the possibility of designing robust coding schemes."},"body":{"text":"The wiretap channel model [1], [2] has attracted much attention in recent years because of its potential to strengthen the security of communication systems [3], [4]. Although this model provides a convenient abstraction to design codes for secure communication, it relies on two implicit simplifying assumptions. First, the model assumes that the transmitter knows the statistics of the channel. Second, the model assumes that the transmitter has access to an arbitrary local source of randomness, whose statistics can be optimized as part of the code design. In practice, however, these assumptions are unlikely to be perfectly guaranteed. For instance, an eaves- dropper has little incentive to help characterize the channel statistics and, realistically, the legitimate parties may only have approximate knowledge of the true statistics. Similarly, the statistics of the local source of randomness may be imperfectly known, or the source may only provide a limited rate of randomness.\nSecure communications with imperfect channel knowledge have already been the subject of previous investigations. For instance, several works have studied compound wiretap chan- nels (see [5] and references therein), in which the transmitter only knows that its channel belongs to a set of possible channels. Other works have investigated the secrecy capacity of state-dependent channels under different assumptions re- garding state information (see [3], [4] and references therein). Finally, some recent works have shown the existence of universal wiretap codes that guarantee secrecy as soon as the channel capacity of the eavesdropper\u2019s channel is known to be low enough [6]; for multiple-antenna systems, the existence\nof universal codes is even established without any assumption regarding the eavesdropper\u2019s channel statistics [7], [8].\nIn contrast to the problem of channel knowledge, little attention has been devoted to the problem of imperfect local sources of randomness. In particular, the questions of how much randomness is required to guarantee secrecy and how sensitive are secure communication codes to imperfections in randomness are still largely open.\nIn this paper, we provide partial answers to these questions. Our main contributions are 1) the characterization of secrecy capacity with a rate-limited source of randomness and a less capable eavesdropper\u2019s channel, and 2) the derivation of a sufﬁcient condition for secure communication with a non- uniform randomization.\nThe remainder of the paper is organized as follows. Sec- tion II introduces the wiretap channel model used to analyze the effect of constrained randomization and presents our results on the secrecy-capacity of wiretap channels with a rate- limited local source of randomness. Section III discusses the possibility of secure communication with a non-uniform local source of randomness that cannot be processed. All proofs are relegated to appendices to streamline the presentation; due to space limitations, some proof details are also omitted.\nWe \t consider \t a \t discrete \t wiretap \t channel X , W YZ |X , Y × Z , characterized by a ﬁnite input alphabet\nX , two ﬁnite output alphabets Y and Z, and transition probabilities p YZ |X . As illustrated in Figure 1, we assume that the transmitter (Alice) wishes to transmit a secret message to the receiver observing Y n (Bob), in the presence of an eavesdropper observing Z n (Eve). The channel\nX , W Y |X , Y is called the main channel while the channel X , W Z |X , Z is called the eavesdropper\u2019s channel. We\nassume the eavesdropper\u2019s channel is less capable, that is for any input X we have I(X; Z) I(X; Y). The encoding process may be stochastic, but the only source of randomness is a discrete memoryless 1 source (R, p R ) with known alphabet R and known statistics p R . This model captures a situation in\nwhich the transmitter does not have access to a inﬁnite pool of random numbers, and those must be generated on-the-ﬂy during encoding from a source of randomness (thermal noise, photon counting). In addition, it forces us to explicitly specify how to use the randomness in the encoding process.\nDeﬁnition 1: A (2 nR , n) wiretap code C n for the discrete wiretap channel X , p YZ |X , Y × Z with local source of ran- domness (R, p R ) consists of the following.\n\u2022 an encoding function e : M × R n → X n ; \u2022 a decoding function f : Y n → M ∪ {?}.\nThe performance of C n is measured in terms of the average probability of error P e (C n ) \t P M = ˆ M |C n and of the secrecy leakage L(C n ) I(M; Z n |C n )\nDeﬁnition 2: A rate R is achievable if there exists a se- quence of (2 nR , n) wiretap codes {C n } n 1 such that\nThe (strong) secrecy capacity with rate-limited randomness C s is deﬁned as the supremum of all achievable rates.\nRemark 1: The deﬁnition of a wiretap code above implic- itly allows the encoder to process the observations obtained from the local source of randomness. In particular, the encoder can remove a possible bias in the randomness. What happens when the encoder does not perfectly process the local source is discussed in Section III.\nRemark 2: The model can be viewed as a special case of wiretap channel with channel state known non-causally at the transmitter [9], in which the state is independent of the channel; however, our result does not follow from [9] because we consider a strong secrecy metric.\nProposition 1: The secrecy capacity of a wiretap channel (X , W YZ |X , Y × Z) with a rate-limited source of local ran- domness (R, p R ) and a less capable eavesdropper\u2019s channel 2 is\nwhere the set P is the set of distributions p UXYZ that factorize as p UXYZ = p U p X |U W YZ |X and with I(X; Z|U) H(R).\nRemark 3: Using standard techniques, one can show that the cardinality of U is bounded by |U | 2.\nThe expression in Proposition 1 is similar to that obtained in [2, Corollary 2]. The effect of the local source of ran- domness explicitly appears in the expression through the auxiliary time-sharing random variable U and the constraint I(X; Z|U) H(R). Proposition 1 conﬁrms the optimal struc- ture of the encoder, which performs two distinct operations:\n1) Uniformization: the encoder generates nearly-uniform random numbers K at rate H(R) from the local source of randomness;\n2) Randomization: the encoder uses a fraction I(X; Z|U) of the randomness rate to randomize the choice of a codeword;\nThe identiﬁcation of the optimal encoder structure suggests that non-uniform randomization may affect the performance of a code, which we discuss in Section III. Proposition 1 also highlights that the common folklore in information-theoretic security, according to which secrecy is achievable provided the randomization can exhaust the capacity of the Eve\u2019s channel, is somewhat misleading. If the source provides a non-zero rate of randomness (H(R) > 0), then the secrecy capacity with a rate-limited source of randomness is positive if and only if the secrecy capacity with unlimited randomness is positive. Intuitively, this happens because the channel seen by Eve is an \u201ceffective channel\u201d, which is partly controlled by Alice through time-sharing and the choice of the codebook.\nAlso note that if the rate of randomness vanishes, then no secure communication is possible. This conﬁrms that, except for pathological channels (for instance, one for which I(X; Z) = 0 for any X), one cannot replace the local source of randomness by a pseudo-random number generator without losing the information-theoretic secrecy guarantees.\nThe result of Proposition 1 suggests that one should always \u201cuniformize\u201d the local source of randomness to create uni- formly distributed random numbers. This operation, however, may be imperfect and one may wonder whether achieving secrecy is then still possible. A situation where the random numbers may not be perfectly uniform is if the local source of randomness is another message source; understanding this setting is crucial to assess whether secrecy constraints incur an overall rate loss or not [4].\nFor simplicity, we assume that the output of uniformization is a random variable K ∈ 1, 2 nR r with perhaps non-uniform distribution p K . In this case, we show that secrecy is still achievable, but at a lower rate limited by the R´enyi entropy rate of order two 1 n R 2 (K) where\nProposition 2: A secrecy rate R is achievable when ran- domization is performed with randomness K if it satisﬁes\nwhere P is the set of distributions I(X; Y|U) that factorize as p U p X |U W YZ |X and such that I(X; Z|U) < 1 n R 2 (K).\nIt is not straightforward to establish a converse for Proposi- tion 2 because typical converse arguments make no assumption regarding the internal structure of the encoder. In particular, it seems difﬁcult to include a constraint that would prevent any processing of K.\nIn general, 1 n R 2 (K) \t 1 n H(K), and the constraint in Proposition 2 is therefore more stringent than in Proposition 1. The effect can be quite dramatic, and the following example shows that the gap between the rates in Proposition 1 and Proposition 2 can be large.\nExample 1: Assume the encoder performs randomization with a biased local source of randomness, which produces random numbers K ∈ 1, 2 nR r such that\nwhere α ∈]0; 1 2 [ is a parameter that controls the uniformity of the distribution. Note that\nConsequently, without proper uniformization, the achievable rates predicted in Proposition 2 could be arbitrarily small.\nWe have shown for the classical wiretap channel that strong secrecy can be guaranteed even in the presence of non- uniform or rate-limited randomness, albeit at the expense of a lower secrecy capacity. The result of this work enables several interesting applications. For example, if the public message in the wiretap channel model is identiﬁed as the output of a source encoder, which is in general not uniformly distributed, extra information can be conveyed publicly while still providing secure communication. Another application is secure transmission in a network, in which multiple links are wiretapped by the same eavesdropper via channels with different capacities and in which only a given amount of randomness exists.\nLet > 0 and let R be an achievable rate. Then, there exists a (2 nR , n) code C n such that P e (C n ) \t and L(C n ) \t . Following the converse technique in [2], we obtain\nwhere ˜ Y i−1 \t {Y j } i−1 j=1 , ˜ Z i+1 \t {Z j } n j=i+1 and δ( ) is a function of that goes to zero with . Next, by deﬁnition of the encoder e and by independence of R n and M,\nNow, we also have 1\nwhere the last inequality follows because M → X n → Z n forms a Markov chain and H(M|X n Z n ) = H(M|X n ). Then,\n1 n\nwhere the inequality follows because conditioning does not increase entropy and ˜ Z i+1 Y i−1 → X i → Z i forms a Markov chain. Let us now deﬁne a random variable Q independent of all others and uniformly distributed on 1, n . For i ∈ 1, n , we also deﬁne U i Y i−1 ˜ Z i+1 and V i U i M. Combining inequalities (1), (2), and (3), and substituting the deﬁnition of Q, U i , V i above, we obtain\nZ Z Q . Note that U → V → X → YZ forms a Markov chain and that the statistics p YZ |X are those of the original channel W YZ |X . Substituting these deﬁnitions in (4) and (5), we obtain\nBecause the eavesdropper\u2019s channel is less capable, then I(V ; Y|U) − I(V ; Z|U) I(X; Y|U) − I(X; Z|U). Since can\nThe proof relies on binning, superposition coding, and stochastic encoding as in [2, Lemma 2]; however, since the local source of randomness is explicit and since we impose a strong secrecy criterion, some details must be laid out carefully. We denote the set of -strongly typical sequences with respect to p X by T n (X) and the set of conditional - strongly typical sequence with respect to p YX and x n ∈ T n (X) by T n (Y|x n ).\nWe ﬁrst show the existence of a code C n assuming an unlimited amount of uniform randomness is available. We ﬁx a joint distribution p UX on U × X such that 3 I(X; Z|U) H(R) and I(X; Y|U) − I(X; Z|U) > 0, and we construct a code C n for the broadcast channel with conﬁdential messages (X , p YZ |X , Y × Z). Let > 0, R > 0, R r > 0, R 0 > 0 and\nn ∈ N. We randomly construct a code as follows. We generate 2 nR 0 sequences independently at random according to p U , which we label u n (i) for i ∈ 1, 2 nR 0 . For each sequence u n (i), we generate 2 n(R+R r ) sequences independently at random according to p X |U , which we label x n (i, j, k) with j ∈ 1, 2 nR and k ∈ 1, 2 nR r . To transmit a message i ∈ 1, 2 nR 0 and j ∈ 1, 2 nR , the transmitter obtains a realization k of a uniform random number K ∈ 1, 2 nR r , and transmits x n (i, j, k) over the channel. Upon receiving y n , Bob decodes i as the received index if it is the unique one such that (u n (i), y n ) ∈ T n (U Y ); otherwise, he declares an error. Bob then decodes (j, k) as the other pair of indices if it is the unique one such that (x n (i, j, k), y n ) ∈ T n (U XY ). Similarly, upon receiving z n , Eve decodes i as the received index if it is the unique one such that (u n (i), z n ) ∈ T n (U Z); otherwise, she declares an error.\nLemma 1: If R 0 < min(I(U; Y), I(U; Z)) and R + R r < I(X; Y|U), then E(P e (C n )) 2 −αn for some α > 0.\nProof: The proof follows from a standard random coding argument and is omitted.\nLemma 2: If R r \t > I(X; Z|U), then we have E C n (V(p MZ n , p M p Z n )) 2 −βn for some β > 0, where V denotes the variational distance.\nUsing Markov\u2019s inequality, we conclude that there exists at least one code C n satisfying the rate inequalities in Lemma 1 and Lemma 2, such that P e (C n ) 3 · 2 −αn and V(p MM 0 Z n , p M p M 0 Z n ) \t 3 · 2 −βn . Finally, the uniform numbers K can be approximately obtained from (R, p R ) with an appropriate function φ.\nLemma 3 (adapted from [11]): If R r < H(R), then there exists φ such that V p φ(R n ) , p K 2 −nη for some η > 0.\nConsequently, one can show that, even if the code C n is used with φ(R n ) in place of K, then\nfor some κ > 0. The fact that L(C n ) 2 −κ n for some κ > 0 follows from [12, Lemma 1]. Combining all rate constraints in the previous lemmas, and since can be chosen arbitrarily small, we see that any rate R < I(X; Y|U) − I(X; Z|U) such that I(X; Z|U) H(R) is achievable. Note that the constraint on R 0 plays no role since it represents a negligible rate of time sharing information to synchronize transmitter and receiver.\nThe proof is similar to that Appendix B, with Lemma 4 in place of Lemma 2. Lemma 2 is obtained in the special case of K uniform.\nLemma 4: If 1 n R 2 (K) > I(X; Z|U), then we have E C n (V(p MZ n , p M p Z n )) 2 −βn for some β > 0.\nThe proof relies on a careful analysis and a slight gener- alization of the \u201ccloud-mixing\u201d lemma [13]; the notation is that of Appendix B. We deﬁne the distribution q U n X n Z n on U n × X n × Z n as\nThen, let U n 1 be the sequence in U n corresponding to M 0 = 1. By symmetry of the random code construction, the average of the variational distance V(p MZ n , p M p Z n ) over randomly generated codes C n satisﬁes\nThe average over the random codes can be further bounded by splitting the average between U n 1 and the random code C n (u n 1 ) for a ﬁxed value of u n 1 .\nwhere the last inequality follows from the fact that the varia- tional distance is always less than 2. By construction, the ﬁrst term on the right-hand side vanishes as n gets large; we now\nproceed to bound the expectation in the second term. First note that, for any z n ∈ Z n ,\nso that we can upper bound V p Z n |U n =u n 1 M =1 , q Z n |U n =u n 1 as\nOne can show that the average of the ﬁrst sum and of the last sum vanish as n goes to inﬁnity. We now focus on the average of the second sum. For z n ∈ T n (Z|u n 1 ), Jensen\u2019s inequality and the concavity of x →\nW Z n |X n (z n |x n )1{(x n , z n ) ∈ T n (XZ|u n 1 )} 2 =\nHence, if R 2 (K) n > I(X; Z|U) + δ( ), the sum vanishes as n goes to inﬁnity, which concludes the proof. Note that if K is uniform, then R 2 (K) = nR r , and we obtain Lemma 2."},"refs":[{"authors":[{"name":"A. D. Wyner"}],"title":{"text":"The Wire-Tap Channel"}},{"authors":[{"name":"I. Csisz´ar"},{"name":"J. K¨orner"}],"title":{"text":"Broadcast Channels with Conﬁdential Mes- sages"}},{"authors":[{"name":"Y. Lian"},{"name":"H. V. Poo"},{"name":"S. S. (Shitz"}],"title":{"text":"Information-Theoretic Secu- rity , ser"}},{"authors":[{"name":"M. Bloc"},{"name":"J. Barro"}],"title":{"text":"Physical-Layer Security: From Information Theory to Security Engineering "}},{"authors":[{"name":"Y. Liang"},{"name":"G. Kramer"},{"name":"H. V. Poor"},{"name":"S. S. (Shitz)"}],"title":{"text":"Compound Wiretap Channels"}},{"authors":[{"name":"J. Muramatsu"},{"name":"S. Miyake"}],"title":{"text":"Construction of Wiretap Channel Codes by Using Sparse Matrices"}},{"authors":[{"name":"X. He"},{"name":"A. Yener"}],"title":{"text":"Providing secrecy when the eavesdropper channel is arbitrarily varying: A case for multiple antennas"}},{"authors":[{"name":"X. He"},{"name":"A. Khisti"},{"name":"A. Yener"}],"title":{"text":"MIMO Broadcast Channel with Ar- bitrarily Varying Eavesdropper Channel: Secrecy Degrees of Freedom"}},{"authors":[{"name":"Y. Chen"},{"name":"A. J. Han Vinck"}],"title":{"text":"Wiretap Channel With Side Information"}},{"authors":[{"name":"S. Watanabe"},{"name":"Y. Oohama"}],"title":{"text":"Broadcast Channels with Conﬁdential Messages by Randomness Constrained Stochastic Encoder"}},{"authors":[{"name":"R. Ahlswede"},{"name":"I. Csisz´ar"}],"title":{"text":"Common Randomness in Information Theory and Cryptography. II. CR Capacity"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"Almost Independence and Secrecy Capacity"}},{"authors":[{"name":"P. W. Cuff"}],"title":{"text":"Communication in Networks for Coordinating Behavior"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566597.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T4.4","endtime":"16:00","authors":"Matthieu Bloch, Joerg Kliewer","date":"1341330000000","papertitle":"On Secure Communication with Constrained Randomization","starttime":"15:40","session":"S7.T4: Secrecy in Computation and Communication","room":"Stratton 20 Chimneys (306)","paperid":"1569566597"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
