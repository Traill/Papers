{"id":"1569566875","paper":{"title":{"text":"A Compression Algorithm Using Mis-aligned Side-information 1"},"authors":[{"name":"Nan Ma"},{"name":"Kannan Ramchandran"},{"name":"David Tse"}],"abstr":{"text":"Abstract\u2014We study the problem of compressing a source sequence in the presence of side-information that is related to the source via insertions, deletions and substitutions. We propose a simple algorithm to compress the source sequence when the side- information is present at both the encoder and decoder. A key attribute of the algorithm is that it encodes the edits contained in runs of di ﬀerent extents separately. For small insertion and deletion probabilities, the compression rate of the algorithm is shown to be asymptotically optimal."},"body":{"text":"In [1], we have studied the problem of compressing a source sequence with the help of mis-aligned decoder-only side- information, where the source and side-information are the input and output of a deletion channel, respectively. The min- imum rate is shown to correspond to the amount of information in the deleted content plus the locations of the deletions, minus the uncertainty in the locations given the source and side- information. We refer to the latter as \u201cnature\u2019s secret\u201d. This is the information that the encoder and decoder can never ﬁnd out. It represents the over-counting of information in the locations of the deletions. For example, if the input and output of a deletion channel and are (0 , 0) and (0), the encoder and decoder will never know and never need to know whether the ﬁrst or the second bit is deleted. An interesting question is: how to construct a practical compression algorithm with the optimal compression rate, where the encoded bits do not reveal \u201cnature\u2019s secret\u201d? In this paper we provide such a construction for a simpler problem where the side-information is available at both the encoder and decoder. Although the availability of the side-information is changed, the minimum rate remains the same.\nIn this paper, we study the problem of compressing a source sequence, X, with the help of side-information, Y, which is available at both the encoder and the decoder. The side- information is related to the source via insertions, deletions and substitutions. See Figure 1 for an illustration of the system. The objective of this work is to construct an encod- ing /decoding algorithm to achieve the optimal compression rate deﬁned as the minimum number of encoded bits per source bit.\n- \t - - \t -\nX = (0, 0, 1, 1, 0, 1) Y = (0, 1, 0, 0, 1, 1)\nIn order to compare these two sequences, we can insert some gaps, which are denoted by \u2018 −\u2019, to align them as follows.\nX ∗ = (0, 0, 1, 1, 0, 1, −) Y ∗ = (0, −, 1, 0, 0, 1, 1)\nThis alignment explains the X with respect to Y with an insertion, a substitution and a deletion: X 2 is inserted between Y 1 and Y 2 ; X 4 substitutes Y 3 ; Y 6 is deleted. The encoder needs to describe the above editing information using the minimum number of bits.\nThe problem of synchronizing edited sequences has been studied by [2]\u2013[4] assuming the number of edits is a constant that does not increase with the length of the sequence. Upper and lower bounds on the minimum number of encoded bits were provided as functions of the number of edits and the length of the sequence. In [5], an interactive, low-complexity and asymptotically optimal scheme was proposed. In compari- son, in this paper, we consider the case that a fraction of source bits, rather than a constant number of bits, is edited, which makes the problem more general. There are also practical synchronization algorithms. such as RSYNC [6] for generic ﬁles and VSYNC [7], which targets video applications. In the special case when the source and the side-information di ﬀer only by substitutions (side-information is aligned), a universal compression algorithm has been proposed by [8].\nIn this paper, we propose a simple compression algorithm, for which the compression rate is asymptotically optimal when the editing probability is small. The key ideas are: (1) describing the locations of insertions and deletions by\nspecifying the runs 2 of side-information in which they appear, and (2) separately encoding the edits that appears in runs of di ﬀerent extents. To explain idea (1), consider the example where the side-information is Y = (0, 0, 1, 0) and the source is X = (0, 1, 0). Neither the encoder nor the decoder knows whether the ﬁrst bit or the second bit is deleted. Therefore the encoder needs to describe the location of the deletion only up to a run, which consists of the ﬁrst two bits in this example, but not further. To explain idea (2), consider the example where the side-information Y = (0, 0, 1, 0) and the source is X = (0, 1). These sequences can be explained by two deletions, in the ﬁrst run and the third run of Y, respectively. If the deletion process is memoryless and stationary, the longer ﬁrst run is more likely to contain a deletion than the shorter third run. Therefore the two deletion events should be encoded separately, using entropy coders with di ﬀerent target distributions, or using a universal entropy coder.\nOur compression algorithm can ﬁnd applications in a num- ber of settings, for example, to compress genomic sequences, as in [9]. 3 The di ﬀerence between the genomic sequences from two individuals of the same species is a small fraction of a whole sequence, and is in the form of insertions, deletions and substitutions. If one of the genomic sequences can be used as side-information, the algorithm can be used to compress the other sequence. The algorithm can also be used in distributed ﬁle backup or ﬁle sharing systems, where di ﬀerent source nodes have di ﬀerent versions of the same ﬁle diﬀering by a small number of edits including insertions, deletions and substitutions. Here, an old version can be used as side- information that is mis-aligned to the new version of the same ﬁle.\nThe rest of this paper is organized as follows. In Section II we formally setup the problem. In Section III we consider a simple case where the source sequence is obtained from side-information by pure deletion. We present the algorithm and analyze the performance. In Section IV we present the algorithm in the general setup. All the proofs are provided in [10] due to the page limit.\nNotation: Symbols in boldface represent sequences or ma- trices, and the symbols in non-boldface represent scalars. The binary entropy function is denoted by h 2 ( ·). The notation {0, 1} n denotes the n-fold Cartesian product of {0, 1}, and {0, 1} ∗ denotes\nWe will deﬁne two sequences X and Y, which di ﬀer by insertions, deletions, and substitutions.\nFirst, consider an auxiliary length-n sequence Z X = (Z X ,1 , . . . , Z X ,n ) ∈ {0, 1} n ∼ iid Bernoulli(p), where p ∈ (0, 1). Pass Z X through a binary symmetric channel with crossover probability q to get Z Y .\nWe will then make deletions in Z X and Z Y to construct X and Y, respectively. Let the deletion pattern D X be a length- n sequence ∼ iid Bernoulli(d X ), which is independent of Z X and Z Y . The deleted sequence X ∈ {0, 1} ∗ is a subsequence of Z X , which is derived from Z X by deleting all those Z X ,i \u2019s with D X ,i = 1. Similarly, the deletion pattern D Y ∼ iid Bernoulli(d Y ) describes the deletion process from Z Y to Y.\nSince the editing process from Z X to X is a deletion process, the inverse process from X to Z X can be regarded as an insertion process. Therefore from X to Y there are insertions (from X to Z X ), substitutions (from Z Y to Z Y ) and deletions (from Z Y to Y).\nBoth sequences X and Y are available to the encoder and Y is available only to the decoder as side-information. All the other sequences, Z X , Z Y , D X , and D Y are available to neither the encoder nor the decoder. The encoder encodes X in the presence of Y and sends a bit string of variable length to the decoder so that the decoder can reproduce X without any error. The sequences X and Y are called the source sequence and the side-information, respectively. Please see Fig. 2 for the structure of the system together with the source model.\nThe performance of the encoder and the decoder is mea- sured by the expected operational rate, which is deﬁned as R op : = lim n →∞ E[L M /L Y ], where L M is the length of encoded bit string, and L Y is the length of Y.\nIf there is no computational constraint, the generally optimal approach for this conditional compression problem is to enu- merate all possible source sequences x\u2019s for each realization of the side-information sequence y and compute p X |Y (x |y), and then construct a Hu ﬀman code to encode the x\u2019s. However, the complexity is exponential in n and hence impractical in most applications. The objective of this work is to ﬁnd a practical encoder and decoder which minimize the expected operational rate.\nIn order to provide a clear presentation of our algorithm, we start by considering a special case of the general prob- lem, where the source sequence X is derived from the side- information Y only by deletion, but not substitution or in-\nsertion. Formally speaking, q = 0 and d Y = 0, which imply Z X = Z Y = Y. For the sake of simplicity, in this section, we drop the subscript X in d X and D X and denote them as d and D , respectively.\n1) Alignment: In this stage we insert some gaps in X to get X ∗ , which has the same length as Y. The fol- lowing greedy alignment algorithm described in [11, Section 3.1] is used.\nRead X and Y from left to right. Take the ﬁrst bit of X , and match it with the leftmost appearance of this bit in Y; then take the second bit of X, and match it with the subsequent leftmost appearance of this bit in Y ; and so on. All the bits in Y that are not matched with bits from X are matched with gaps denoted by \u2018 −\u2019. Let X ∗ be the aligned version of X with gaps inserted. The alignment implies a reconstructed deletion pattern D , which can explain the deletion process from Y to X, but is in general di ﬀerent from D.\nLet the maximum extent of the runs in Y be L max . For IID sequence Y, E[L max ] = Θ(log n) [12]. The encoder performs the following:\n\u2013 Compute U l , the number of runs of extent l in Y. \u2013 For i = 1, . . . , U l , compute V l ,i , the number of\ndeletions in the i-th run of extent l in Y according to D.\n3) Entropy coding: For each l = 1, . . . , L max , compress the sequence {V l ,i } U l i =1 using an entropy coder. Note that V l ,i with l = 1, . . . , L max have di ﬀerent distributions.\nThe encoded string generated by the encoder is the output of the entropy coder in stage 3).\n2) Locate deletions up to runs: For each l and each i, ﬁnd the i-th run of extent l in Y, and delete V i ,l bits in that run. The outcome is the reconstruction of X.\nSince the total number of entries in {V i ,l } is the total number of runs in Y, which is no larger than n, the size of memory the algorithm takes is O(n). Since the greedy alignment, the generation and coding of {V i ,l } take O(n) operations, the algorithm takes O(n) operations.\nLet the side-information, the hidden deletion pattern, and the source sequence be as follows for example:\nY = (1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1) D = (1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0) X = (0, 1, 0, 0, 1, 0, 1).\nStage 1): The greedy alignment algorithm aligns X and Y and generates D as follows.\nStage 2): The maximum extent of the runs in Y is L max = 3. There are U 1 = 4 runs of extent 1, U 2 = 2 runs of extent 2, and U 3 = 1 run of extent 3. For the four extent-1 runs, \u20181\u2019, \u20180\u2019, \u20181\u2019 and \u20180\u2019, only the ﬁrst one is deleted according to D, therefore we have\nFor the two extent-2 runs, \u20181 , 1\u2019 and \u20181, 1\u2019, there is a deletion in each of them. Therefore we have\nFor the only extent-3 run, \u20180 , 0, 0\u2019, there is a deletion in it. Therefore we have\nStage \t 3): \t The \t entropy \t encoder \t com- presses \t ((V 1 ,1 , V 1 ,2 , V 1 ,3 , V 1 ,4 ) , (V 2 ,1 , V 2 ,2 ) , (V 3 ,1 )) \t = ((1 , 0, 0, 0), (1, 1), (1)). Note that each entry in (V 1 ,1 , V 1 ,2 , V 1 ,3 , V 1 ,4 ) is more likely to be 0 than (V 2 ,1 , V 2 ,2 ) and (V 3 ,1 ). Therefore we should use entropy encoder with di ﬀerent target distributions to encode them, when the sequences are long.\nStage \t 1): \t The \t entropy \t decoder \t recon- structs \t ((V 1 ,1 , V 1 ,2 , V 1 ,3 , V 1 ,4 ) , (V 2 ,1 , V 2 ,2 ) , (V 3 ,1 )) \t = ((1 , 0, 0, 0), (1, 1), (1)).\nStage 2): Since (V 1 ,1 , V 1 ,2 , V 1 ,3 , V 1 ,4 ) = (1, 0, 0, 0), the de- coder deletes the ﬁrst run of extent-1, i.e., the ﬁrst bit. Since (V 2 ,1 , V 2 ,2 ) = (1, 1), the decoder deletes a bit from each of the two runs of extent-2. It does not matter which bit to delete in each run. Since (V 3 ,1 ) = (1), the decoder deletes a bit in the only extent-3 run. The deletions are represented by D and the reconstruction of the source sequence is denoted by X.\nY = (1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1) D = (1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1) X = (0, 1, 0, 0, 1, 0, 1).\nLet U : = {U l } L max l =1 and V : = {V l ,i } L max ,U l l =1,i=1 . In the limit as the lengths of the sequences tends to inﬁnity, the operational rate of this algorithm is R op = lim n →∞ H(V) /n. The optimal rate is lim n →∞ H(X |Y)/n. When the probability of deletion d is small, the following theorem shows that the algorithm is asymptotically optimal.\nTheorem 1: The gap between the operational rate of the algorithm described in Section III-A and the optimal rate\nsatisﬁes: lim n →∞ [H(V) /n − H(X|Y)/n] = O(d 2 −ϵ ), for any ϵ > 0.\nThe proof is provided in [10, Appendix 1], which can be intuitively explained as follows. When d is small, the deletions are typically far away from each other. Therefore the intervals between the deletions are so long that can be used to synchronize segments of X to segments of Y. As a result, the deletions can be located within the correct runs with high probability. The exact positions of the deletions within the runs are impossible to ﬁnd based on only X and Y. Since the goal is to reconstruct X, describing the positions within runs is unnecessary. Moreover, the description of the locations of the deletions, V, is almost independent of the decoder side- information Y. Therefore sending V is approximately optimal in terms of rate. See Section III-D-2 for more discussions about the independence between V and Y. The deletions cannot be located within the correct runs only if two or more deletions are in the same run or adjacent runs, which occurs with the probability in the order of O(d 2 ). This event a ﬀects the performance of the algorithm when d is not small. Therefore the gap between the operational rate and the optimum is in the order of O(d 2 −ϵ ).\nRemark 1: In [1], we have shown that when p = 1/2, for any ϵ > 0, lim n →∞ H(X |Y)/n = h 2 (d) −cd+O(d 2 −ϵ ), where c : = ∑\n2 −l−1 l log 2 l ≈ 1.29. 4 It captures the asymptotic expansion of the optimal rate to the precision of Θ(d) with a remainder term O(d 2 −ϵ ). Due to Theorem 1, R op = h 2 (d) − cd + O(d 2 −ϵ ), which also matches the optimal rate to the precision of Θ(d).\nRemark 2: In [1], we have shown that lim n →∞ H(X |Y)/n is also the minimum rate when the side-information is only avail- able available at the decoder but not the encoder. Although the minimum rate is the same, constructing an explicit algorithm to implement the distributed compression at the asymptotically optimal rate remains an open problem.\nLet us compare the algorithm described in Section III-A with two simpler but suboptimal algorithms in the simple case Y ∼ iid Bernoulli(1/2) (p = 1/2). The comparison reveals more intuition on why the algorithm is asymptotically optimal.\n1) Sending D directly: A simple and the most natural algorithm to compress X given Y is ﬁrst running a greedy alignment to obtain D (as in stage 1)) and then compressing D using an entropy coder (similar to stage 3)). As the lengths of the sequences tend to inﬁnity, the operational rate is lim n →∞ H(D) /n. If we approximate H(D) by H(D) 5 , the opera- tional rate is approximately h 2 (d) = −d log 2 d +d log 2 e +O(d 2 ). Therefore for small d, the operational rate of this simple algorithm matches the optimal expression up to the −d log 2 d term. But for the Θ(d) term, there is a gap cd ≈ 1.29d. That is, this compression algorithm wastes 1 .29 bits per deletion bit on average. When d is not very small, −d log 2 d and d can be\nin the same order of magnitude. Therefore the gap may not be negligible in practice.\nThe above strategy is suboptimal because D speciﬁes the exact positions of the deletions. Note that after specifying the runs that contain the deletions and specifying the number of deletions in each run, X can already be deduced from Y. How- ever, this strategy goes further and speciﬁes the exact positions within the runs, which are redundant in terms of reconstructing X . Therefore this strategy over-describes the positions of the deletions beyond what is necessary to represent X. The amount of over-description, H(D |X, Y), is called \u201cnature\u2019s secret\u201d in [1], because only the hypothetical party \u201cnature\u201d has access to D, but the encoder and decoder do not.\n2) Locating deletions up to runs: The analysis of the previous strategy suggests that the encoder should specify the location of the deletions with respect to runs. Therefore a better algorithm than the one described in Section III-D-1 is ﬁrst deﬁning a sequence W such that W i is the number of deletions in the i-th run of Y according to D, then compressing W at the entropy rate.\nSince the average extent of a run in an iid Bernoulli(1 /2) sequence is 2, the length of W is approximately half of that of D . It can be shown 6 that the operational rate can be approxi- mated by (h 2 (d) − d). There is still a linear d gap between this rate and the optimal one, given by (c − 1)d ≈ 0.29d. That is, this algorithm wastes 0 .29 bit per deletion bit.\nWhy is this algorithm suboptimal? The reason is because W is signiﬁcantly correlated with Y. If the deletion process is iid, then the longer runs of Y tend to contain more deletions and the shorter runs tend to contain less deletions. Therefore Y reveals a certain amount of information about W, that is about 0 .29 bit per deletion bit. The algorithm described above does not use this amount of information and thus is suboptimal.\nThe algorithm described in Section III-A, however, treats the deletions contained in runs of di ﬀerent extents diﬀerently. As a result the operational rate matches the optimal rate for the Θ(d) term.\nTable I provides a comparison among the performance of the two algorithms in Section III-D and the one in Section III-A for n = 1000kb and d = 0.01. Note that when Y has biased bits (p = 0.1), the beneﬁt of the proposed algorithm in Section III-A is more signiﬁcant than when p = 0.5. The reason is that when p = 0.1, the runs of Y are longer and it pays to exploit the information from the run-lengths.\nThe algorithm described in Section III-A can be extended to the general problem where Y is related to X by insertions,\nA. Algorithm for insertions, deletions and substitutions The encoder has the following stages.\n1) Alignment: align X and Y using the minimum total number of insertions, deletions and substitutions. If there are multiple such alignments, pick any one of them. This can be done by the Needleman-Wunsch algorithm [13] with the gap penalty and the substitution penalty equal to 1, with computation complexity of order O(n 2 ). The algorithm generates two sequences X ∗ and Y ∗ , which are X and Y with gaps, respectively. Then construct Z X and Z Y by replacing the gaps in X ∗ and Y ∗ by the corresponding bits in Y ∗ and X ∗ , respectively.\n2) Describing the insertions (from Y to Z Y ): The edits from Y to Z Y can be viewed as insertions. The locations of the insertions are speciﬁed by the gaps in Y ∗ . The content of the insertions is speciﬁed by the corresponding bits in Z Y .\nAll the insertions can be categorized into isolated insertions with only one bit per insertion event, and bursts of insertions with two or more consecutive bits per insertion event. For each insolated insertion, if the inserted bit is equal to the bit on the left (or right) side, the insertion is extending the run to the left (or right). If the inserted bit is not equal to the bits on either side, it is breaking an existing run and creating a new run. We will describe the isolated insertions that extend runs, then the insertions that break runs, then the bursts of insertions.\n\u2022 In order to describe the insertions that extend runs, the encoder does the following.\n\u2013 For l = 1, . . . , L max (L max is the the maximum extent of the runs in Y), for i = 1, . . . , U l (U l is the number of runs of extent l in Y), let V ins l ,i : = 1 if the i-th run of extent l in Y is extended by one bit, and V ins l ,i : = 0 otherwise.\n\u2022 In order to describe the insertions that break runs, the encoder does the following.\nIn the sequence Y \u2032 , a slot between two bits is a potential location to break a run only if the two bits are the same. The slots before the ﬁrst bit and after the last bit are also potential locations to create new runs. Let U 0 denote the total number of such potential locations in Y \u2032 . For i = 1, . . . , U 0 , let V ins 0 ,i : = 1 if a bit is inserted in the i-th potential location, and V ins 0 ,i : = 0 otherwise.\nHaving made such insertions, Y \u2032 becomes Y \u2032\u2032 . Let V ins denote all the descriptions up to this step: {V ins l ,i } l ≥0 .\n\u2022 In order to describe the bursts of insertions, the encoder creates a sequence V burst from Z Y by keeping the bursts of inserted bits and replacing the other bits by \u2018 ∗\u2019. V burst describes the insertions needed to construct Z Y from Y \u2032\u2032 .\nThe edits from Z Y to Z X can be viewed as substitutions, which can be described by V sub : = Z Y ⊕ Z X .\n4) Describe the deletions (from Z X to X) as in stage 2) of Section III-A. Denote the description by V del .\nThe decoder decodes V ins , V burst , V sub and V del by an entropy decoder, and then follow the stages 2) to 4) to construct X from Y.\nThe operational rate of the above algorithm can be analyzed for small probability of insertion, deletion and substitution as follows.\nTheorem 2: The gap between the operational rate of the algorithm described in Section IV-A and the optimal rate satisﬁes: lim n →∞ [H(V ins , V burst , V sub , V del ) /n − H(X|Y)/n] = O(d 2 −ϵ ), for any ϵ > 0, where d = max{d X , d Y , q}.\nThe proof is similar to that of Theorem 1 and is provided in [10, Appendix 2].\nWe have proposed an algorithm to compress the source sequence given the mis-aligned side-information at both the encoder and decoder. For small editing probability, the com- pression rate of the algorithm is asymptotically optimal. Di- rections for future work include (1) developing algorithms for bursty edits, and (2) developing distributed algorithms to compress a source sequence when the reference sequence is only available at the decoder side."},"refs":[{"authors":[{"name":"N. Ma"},{"name":"K. Ramchandran"},{"name":"D. Tse"}],"title":{"text":"E ﬃcient ﬁle synchronization: A distributed source coding approach"}},{"authors":[{"name":"V. L. Levenshtein"}],"title":{"text":"Binary codes capable of correcting deletions, inser- tions and reversals"}},{"authors":[{"name":"A. Orlitsky"},{"name":"K. Viswanathan"}],"title":{"text":"One-way communication and error- correcting codes"}},{"authors":[{"name":"S. Agarwal"},{"name":"V. Chauhan"},{"name":"A. Trachtenberg"}],"title":{"text":"Bandwidth e ﬃcient string reconciliation using puzzles"}},{"authors":[{"name":"R. Venkataramanan"},{"name":"H. Zhang"},{"name":"K. Ramchandran"}],"title":{"text":"Interactive low- complexity codes for synchronization from deletions and insertions"}},{"authors":[{"name":"A. Tridgell"},{"name":"P. Mackerras"}],"title":{"text":"The rsync algorithm"}},{"authors":[{"name":"H. Zhang"},{"name":"C. Yeo"},{"name":"K. Ramchandran"}],"title":{"text":"VSYNC: a novel video ﬁle synchronization protocol"}},{"authors":[{"name":"H. Cai"},{"name":"S. Kulkarni"},{"name":"S. Verdu"}],"title":{"text":"An algorithm for universal lossless compression with side information"}},{"authors":[{"name":"M. C. Brandon"},{"name":"D. C. Wallace"},{"name":"P. Baldi"}],"title":{"text":"Data structures and compression algorithms for genomic sequence data"}},{"authors":[{"name":"N. Ma"},{"name":"K. Ramchandran"},{"name":"D. Tse"}],"title":{"text":"A compression algorithm using mis-aligned side-information"}},{"authors":[{"name":"M. Mitzenmacher"}],"title":{"text":"A survey of results for deletion channels and related synchronization channels"}},{"authors":[{"name":"M. F. Schilling"}],"title":{"text":"The longest run of heads"}},{"authors":[{"name":"S. B. Needleman"},{"name":"C. D. Wunsch"}],"title":{"text":"A general method applicable to the search for similarities in the amino acid sequence of two proteins"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566875.pdf"},"links":[{"id":"1569566567","weight":7},{"id":"1569564889","weight":3},{"id":"1569566385","weight":3},{"id":"1569565067","weight":3},{"id":"1569566683","weight":3},{"id":"1569565091","weight":3},{"id":"1569566591","weight":3},{"id":"1569566571","weight":11},{"id":"1569552245","weight":3},{"id":"1569564481","weight":3},{"id":"1569566415","weight":3},{"id":"1569566469","weight":3},{"id":"1569566081","weight":3},{"id":"1569565613","weight":7},{"id":"1569565547","weight":3},{"id":"1569565461","weight":3},{"id":"1569565837","weight":3},{"id":"1569566319","weight":3},{"id":"1569566941","weight":7},{"id":"1569558459","weight":3},{"id":"1569564203","weight":3},{"id":"1569565859","weight":3},{"id":"1569565809","weight":3},{"id":"1569566843","weight":3},{"id":"1569565455","weight":7},{"id":"1569566497","weight":3},{"id":"1569566709","weight":3},{"id":"1569566895","weight":7},{"id":"1569566679","weight":7},{"id":"1569563981","weight":3},{"id":"1569566905","weight":3},{"id":"1569566733","weight":3},{"id":"1569558681","weight":3},{"id":"1569555999","weight":3},{"id":"1569559995","weight":3},{"id":"1569566511","weight":3},{"id":"1569565841","weight":3},{"id":"1569566531","weight":3},{"id":"1569567665","weight":3},{"id":"1569564611","weight":3},{"id":"1569566811","weight":3},{"id":"1569558901","weight":3},{"id":"1569553537","weight":3},{"id":"1569566403","weight":3},{"id":"1569565915","weight":3},{"id":"1569566231","weight":3},{"id":"1569564209","weight":3},{"id":"1569566425","weight":3},{"id":"1569566649","weight":3},{"id":"1569565087","weight":3},{"id":"1569566473","weight":3},{"id":"1569564857","weight":3},{"id":"1569565929","weight":3},{"id":"1569565633","weight":3},{"id":"1569566661","weight":3},{"id":"1569565279","weight":7},{"id":"1569565219","weight":3},{"id":"1569558509","weight":3},{"id":"1569566003","weight":3},{"id":"1569566037","weight":3},{"id":"1569566553","weight":3},{"id":"1569565357","weight":7},{"id":"1569566505","weight":3},{"id":"1569562207","weight":3},{"id":"1569566191","weight":3},{"id":"1569567029","weight":7},{"id":"1569566695","weight":3},{"id":"1569555787","weight":3},{"id":"1569565467","weight":3},{"id":"1569566233","weight":7},{"id":"1569566667","weight":3},{"id":"1569566407","weight":3},{"id":"1569560349","weight":3},{"id":"1569565741","weight":3},{"id":"1569566481","weight":3},{"id":"1569566831","weight":7},{"id":"1569566983","weight":3},{"id":"1569566779","weight":3},{"id":"1569566097","weight":3},{"id":"1569565925","weight":3},{"id":"1569566129","weight":3},{"id":"1569566261","weight":3},{"id":"1569565093","weight":7},{"id":"1569566927","weight":3},{"id":"1569565661","weight":3},{"id":"1569566887","weight":3},{"id":"1569565319","weight":14},{"id":"1569564919","weight":7},{"id":"1569565353","weight":3},{"id":"1569564291","weight":7},{"id":"1569566691","weight":3},{"id":"1569566595","weight":3},{"id":"1569566137","weight":3},{"id":"1569565375","weight":3},{"id":"1569566713","weight":3},{"id":"1569566771","weight":3},{"id":"1569559035","weight":3},{"id":"1569564437","weight":11},{"id":"1569564861","weight":3},{"id":"1569564787","weight":7},{"id":"1569566619","weight":3},{"id":"1569561185","weight":11},{"id":"1569566397","weight":3},{"id":"1569565669","weight":3},{"id":"1569566001","weight":3},{"id":"1569560235","weight":3},{"id":"1569566817","weight":3},{"id":"1569564157","weight":3},{"id":"1569566389","weight":3},{"id":"1569566911","weight":3},{"id":"1569566299","weight":3},{"id":"1569557851","weight":3},{"id":"1569559919","weight":3},{"id":"1569566147","weight":3},{"id":"1569563725","weight":3},{"id":"1569565635","weight":3},{"id":"1569565113","weight":7},{"id":"1569564257","weight":3},{"id":"1569564141","weight":3},{"id":"1569566973","weight":3},{"id":"1569566987","weight":3},{"id":"1569564419","weight":3},{"id":"1569566609","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S1.T1.4","endtime":"11:10","authors":"Nan Ma, Kannan Ramchandran, David Tse","date":"1341226200000","papertitle":"A Compression Algorithm Using Mis-aligned Side-information","starttime":"10:50","session":"S1.T1: Source Coding with Side Information","room":"Kresge Rehearsal B (030)","paperid":"1569566875"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
