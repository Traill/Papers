{"id":"1569564805","paper":{"title":{"text":"Update Efﬁcient Codes for Error Correction"},"authors":[{"name":"Arya Mazumdar"},{"name":"Gregory W. Wornell"},{"name":"Venkat Chandar"}],"abstr":{"text":"Abstract\u2014An update efﬁcient code is a mapping from messages to codewords such that small perturbations in the message induce only slight changes to the corresponding codeword. The parameter that captures this notion is called update-efﬁciency. In this paper we study update-efﬁcient error-correcting codes and develop their basic properties. While update-efﬁciency and error- correction are two conﬂicting objectives, we deduce conditions for existence of such codes. In particular, logarithmically growing update-efﬁciency is achievable with a capacity-achieving linear code in both binary symmetric and binary erasure channels. On the other hand we show a tight converse result. Our result implies that it is not possible to have a capacity-achieving code in binary symmetric channel that has sub-logarithmic update- efﬁciency. This is true in the case of the binary erasure channel as well for linear codes. We also discuss a number of questions related to update-efﬁcient adversarial error-correcting codes."},"body":{"text":"In a variety of applications such as distributed storage networks, there is a need for update-efﬁcient codes. Such networks consist of multiple distributed and unreliable storage devices across which dynamically changing information must be stored. Each time a portion of the information content changes, its associated reliable encoding must also change, and the contents of the storage devices updated accordingly. In such applications, to minimize the communication bandwidth and power resources required by such networks, it is desirable to have encodings that are update-efﬁcient, i.e., minimize the number of storage devices affected when the information content changes. In this paper we examine aspects of the degree to which such update-efﬁcient codes are possible.\nIn our development, a code C ∈ F n 2 is a collection of binary n-vectors with a one-to-one encoding map φ : F k 2 → C, k < n. We restrict our attention speciﬁcally to error-correcting codes (i.e., channel codes). The support of a vector x, denoted supp(x), is the set of coordinates where x has nonzero values. In turn, the weight of a vector, denoted wt(x), is the size its support.\nThe notion of update-efﬁciency for channel codes is intro- duced in [2]. A code is called update-efﬁcient if for a small change in the message x ∈ F k 2 , the corresponding codeword φ(x) changes only slightly. Formally, we have the following deﬁnition.\nDeﬁnition 1: A code (C, φ) is (u, t)-update-efﬁcient if for all x ∈ F k 2 , and for all e ∈ F k 2 such that wt(e) ≤ u, we have φ(x + e) = φ(x) + e , for some e ∈ F n 2 such that wt(e ) ≤ t. For much of our development in this paper, we focus on the case where u = 1, and equivalently refer to an (1, t)-update- efﬁcient code as one having update efﬁciency t.\nAs background for the present work, much of the initial work on update efﬁcient codes has focused on their use on the binary erasure channel (BEC), which is a natural model for capturing server failures. For example, in [2] it is shown that there exist capacity-achieving codes for the BEC with update-efﬁciency O(log n). A subsequent paper [13] shows, using the randomized codes proposed [2], that it is possible for capacity-achieving codes for the BEC to have both update- efﬁciency and repair-bandwidth efﬁciency, a property desirable in distributed storage. Yet another recent paper [9] considers the update-efﬁciency of linear codes.\nIn this paper we are concerned with update-efﬁcient codes that also correct errors. In addition to distributed storage applications, another potential application for such codes is transmitting uncompressed video over a noisy communication link. As the messages (video-frames) only change slightly from one frame to the next, update-efﬁcient codes can be an efﬁcient mechanism for encoding the sequence of frames.\nWe consider two models of error: random and adversarial. The random errors take the form of independent bit ﬂips, corresponding to a binary symmetric channel (BSC), with ﬂip probability 0 < p < 1/2. Such a channel is denoted via BSC(p).\nCorrecting errors is generally a more difﬁcult task than correcting erasures. In this paper we show that the result for erasures in the paper [2] carries over for the case of errors, viz., there exist linear codes with update efﬁciency O(log n) that achieve rates arbitrary close to the BSC(p) capacity of 1 − h(p), where h(p) = −p log p − (1 − p) log(1 − p) is the binary entropy function, with probability of error approaching zero. In addition, we show a converse result. To be speciﬁc, we show that for a suitable α > 0, within the ensemble of positive rate linear codes with update efﬁciency α log n, almost all codes have a probability of error bounded away from zero on the BSC(p). We also show a stronger version of this result, namely, that for some other α > 0, there does not exist a code with update-efﬁciency α log n that has both positive rate and arbitrarily small probability of error.\nFor the case of adversarial errors, update-efﬁciency and error correction are conﬂicting objectives. However, following [2] and [11], it can be shown that there exist codes with update- efﬁciency O(log n) that correct any pn adversarial errors if there is sufﬁcient shared randomness between the encoder and the decoder. We discuss several properties of linear codes that result in good update-efﬁcient error correcting codes, and give a number of examples. Finally, we turn our attention to general (u, t)-update-efﬁcient codes and provide bounds on the size of an update-efﬁcient code in terms of the minimum distance of the code.\nIn this paper, we focus primarily on linear codes, which are attractive in terms of representation and both encoding and decoding complexity. A linear code (C, φ) is such that φ : F k 2 → C is a homomorphism. It can always be represented by a k × n generator matrix G such that φ(x) = x T G, for any x ∈ F k 2 . Note that G for a code is not unique; distinct G give different labelings. By an [n, k, d] code we mean a linear code with length n, dimension k and minimum distance d.\nFor a linear code, the maximum number of bits that change in the codeword when one bit in the message changes is the maximum over the weights of the rows of the generator matrix. Hence, for an update-efﬁcient code, we need a representation of the linear code where the maximum weight of the rows of the generator matrix is low.\nProposition 1: A linear code C will have update-efﬁciency t if and only if there is a generator matrix G of C with maximum row weight t.\nProof: It is easy to see that if the maximum number of ones in any row is bounded above by t, then at most t bits need to be changed following a one bit change in the message.\nOn the other hand, if the code has update-efﬁciency t then there must exist a labeling φ that gives a sparse generator matrix. Speciﬁcally, the vectors (1, 0, . . . , 0), (0, 1, . . . , 0), . . . , (0, 0 . . . , 1) ∈ F k 2 must produce vectors of weight at most t under φ, so the generator matrix given by φ has row weight at most t.\nProposition II-B implies that given a linear code, to see whether it is update-efﬁcient or not, we need to ﬁnd the sparsest basis for the code. A linear code with a sparse basis is informally called a low density generator matrix (LDGM) code.\nWe start this section with a negative result regarding update- efﬁcient codes on the BSC. Suppose we want to construct a code with update efﬁciency t. We will look at the ensemble of linear codes with update-efﬁciency t and show that almost all codes in this ensemble are bad for t less than certain value. Proposition II-B shows that a linear code with update efﬁciency t always has a generator matrix with maximum row weight t. For simplicity, we consider generator matrices where all rows have weight t, but all the results can be extended to the case where the row weight is at most t.\nLet Γ n,k,t be the set of all k × n matrices over F 2 such that each row has exactly t ones. First, we recall the following lemma from [5], which shows that almost all the matrices in Γ n,k,t generate codes with dimension k (i.e., the rank of the matrix is k).\nLemma 2: Randomly and uniformly choose a matrix G from Γ n,k,t . If k is such that k ≤ 1 − e −t ln 2 − o(e −t ) n, then with probability 1 − o(1) the rank of G is k.\nThis lemma, along with the next theorem, prove that almost all codes in Γ n,k,t are bad for small t.\nTheorem 3: Fix an 0 < α < 1/2, and assume that k ≥ n α , t ≤\nproportion of the matrices in Γ n,k,t , the associated linear code has probability of error at least n α √ t 2 −λ p t over a BSC(p) for p < 1/2 and λ p = −1 − 1/2 log p − 1/2 log(1 − p) > 0.\nBefore proving this theorem, we state the following corollary. Corollary 4: For at least 1 − o(1) proportion of all linear\nlog n, α < 1/2, > 0 and dimension k, k > n α , the probability of error is 1 − o(1) over a BSC(p) for p < 1/2.\nIn particular, this shows that codes with update efﬁciency < log n/(2λ p ) and rate > n α−1 are almost always bad.\nProof of Corollary 4: From Lemma 2 it is clear that 1 − o(1) proportion of all codes in Γ n,k,t have rank k. Hence, if 1− o(1) proportion of codes in Γ n,k,t have some property, 1−o(1) proportion of codes with update-efﬁciency t and dimension k also have that property. Plugging in the value of t in the expression for the error probability from Theorem 3 gives the corollary.\nTo prove Theorem 3 we will need the following series of lemmas.\nLemma 5: Let x ∈ {0, 1} n be a vector of weight t. Let the all-zero vector of length n be transmitted over a BSC with ﬂip probability p < 1/2. If the received vector is y, then,\nwhere λ p = −1 − 1/2 log p − 1/2 log(1 − p) > 0. The proof is omitted.\nLemma 6: Suppose two random vectors x, y ∈ {0, 1} n are chosen independently and uniformly from the set of all length- n binary vectors of weight t ≤\nn − t + 1 . Proof: The probability in question equals,\n= (n − t)(n − t − 1)(n − t − 2) . . . (n − 2t + 1) n(n − 1)(n − 2) . . . (n − t + 1)\nafter the ﬁrst two terms. The inequality will be justiﬁed if the terms of the series are decreasing in absolute value. Let us verify that to conclude the proof. In the following X i denote the ith term in the series, 0 ≤ i ≤ t.\nLemma 7: For 0 < α < 1/2, choose random vectors x i , 1 ≤ i ≤ n α of weight t ≤\nThis implies all of the vectors have disjoint supports with probability at least 1 − t 2 n 2α n−t .\nProof: The claim follows from taking a union bound over all pairs of randomly chosen vectors.\nProof of Theorem 3: We begin by choosing a matrix G uniformly at random from Γ n,k,t . This is equivalent to choosing each row of G uniformly and independently from the set of all weight-t binary vectors. Now k > n α , hence there exist n α vectors among the rows of G such that any two of them have disjoint support with probability at least 1− t 2 n 2α n−t\n(from Lemma 7). Hence for at least a proportion 1 − t 2 n 2α n−t of matrices of Γ n,k,t , there are n α rows with disjoint supports. Suppose G is one such matrix. It remains to show that the code C deﬁned by G has probability of error at least n α √ t 2 −λ p t over a BSC(p).\nSuppose, without loss of generality, that the all zero vector is transmitted over a BSC(p) and y is the vector received. Let x i , 1 ≤ i ≤ n α be codewords of weight t with disjoint support, guaranteed to exist by our assumption on C. The probability that the maximum likelihood decoder incorrectly decodes y to x i satisﬁes\nfrom Lemma 5. As the codewords x 1 , . . . x n α have disjoint supports, the probability that the maximum likelihood decoder incorrectly decodes to any one of them is at least n α √ t 2 −λ p t .\nIt should be noted that the above theorem is easily extend- able to the random ensemble of matrices whose entries are independently chosen from F 2 with Pr(1) = t/n.\nB. No good codes with update efﬁciency < α log n, revisited In this subsection, for a smaller α than discussed above, we\nshow that no code can simultaneously achieve low probability of error on the binary symmetric channel and have update complexity less than α log n. More precisely, we give two results. The ﬁrst result says that linear codes cannot have low update complexity when used over the binary erasure channel (BEC). Since the binary symmetric channel is degraded with\nrespect to the binary erasure channel, this is a stronger result. To see that a BSC(p) is a degraded version of BEC with erasure probability 2p, one can concatenate a BEC(2p) with a channel with ternary input {0, 1, ?} and binary output {0, 1} such that with probability 1 the inputs {0, 1} remain the same, and with uniform probability ? goes to {0, 1}.\nOur second result says that for the binary symmetric chan- nel, even non-linear codes cannot have low update efﬁciency. We include both results because although the second result applies to more general codes, we have not been able to extend the second result to the binary erasure channel. We conjecture that for positive rates, even nonlinear codes must have logarithmic update complexity for the binary erasure channel (at zero rate, trivial counterexamples can be found).\nThe proof for linear codes used over a binary erasure chan- nel is based on Proposition , i.e., when the update complexity is low, the generator matrix G is very sparse. Let the random subset I ∈ {1, . . . , n} denote the coordinates not erased by the binary erasure channel. Let G I denote the submatrix of G induced by the unerased received symbols, i.e., the columns of G corresponding to I. Then, because G is so sparse, it is quite likely that G I has several all zero rows, and the presence of such rows implies a large error probability. We formalize the argument below.\nTheorem 8: Consider using some linear code of length n, dimension k and update-efﬁciency t, speciﬁed by generator matrix G over BEC(p). Assume that, for some > 0, t < ln −k 2 2n ln /(−2 ln p). Then, the average probability of error is at least 1/2 − .\nProof: For linear codes over the binary erasure channel, analyzing the probability of error essentially reduces to analyz- ing the probability that the matrix G I induced by the unerased columns of G has rank k (note that the rank is computed over F 2 ). To show that the rank is likely to be less than k for sufﬁciently small t, let us ﬁrst compute the expected number of all zero rows of G I . Since G has update-efﬁciency t, every row of G has weight at most t, so the expected number of all zero rows of G I is at least kp t . The rank of G I , rk(G I ), is at most k minus the number of all zero rows, so the expected rank of G I is at most k − kp t .\nNow, observe that the rank is a 1-Lipschitz functional of the independent random variables denoting the erasures introduced by the channel. Therefore, by Azuma\u2019s inequality [1, Thm. 7.4.2], the rank of G I satisﬁes\nSince even the maximum likelihood decoder makes an error with probability at least 0.5 when rk(G I ) < k, this shows that when t < ln −k 2 2n ln /(−2 ln p), the probability of error is at least 1/2− . (In fact, the average error probability converges to 1. The above argument can easily be extended to show that the probability of decoding successfully is at most e −Ω( kδ log k ) for some δ > 0, but we omit the details.)\nNow, we show that even nonlinear codes cannot have low update efﬁciency for the binary symmetric channel. The argument is based on a simple observation. If a code has dimension k and update efﬁciency t, then any given codeword has k neighboring codewords within distance t, corresponding to the k possible 1-bit changes to the information bits. If t is sufﬁciently small, it is not possible to pack k + 1 codewords into a Hamming ball of radius t and maintain a low probability of error.\nTheorem 9: Consider using some (possibly non-linear) code of length n, dimension (possibly fractional) k, and update-efﬁciency t over BSC(p). Assume that for some α > 0, t ≤ (1−α) log k/ log((1−p)/p). Then, the average probability of error is at least 1 − o(1), where o(1) denotes a quantity that goes to zero as k → ∞.\nOn the other hand, it is relatively easy to construct a code with update efﬁciency O(log n) that achieves capacity on the binary symmetric channel. One can in principle choose the rows of the generator matrix randomly from all low weight vectors and argue that this random ensemble contain many codes that achieve capacity of the binary symmetric channel (BSC). Some steps in this direction have been made in [10]. However there are easier ways to construct capacity achieving codes that have update efﬁciency O(log n). Let us describe one such construction 1 .\nIt is known that for every > 0 and any sufﬁciently large n, there exist a linear code of length n and rate 1 − h(p) − that has probability of error at most 2 −E(p, )n . There are numerous evaluations of this result and estimates of E(p, ) > 0. We refer the reader to [3] as an example.\nLet m = 1+α E(p, ) log n, for , α > 0 (we avoid using ceiling and ﬂoor to have a clean presentation). We know that for sufﬁciently large n, there exists a linear code ˆ C given by the mR × m generator matrix ˆ G with rate R = 1 − h(p) − that has probability of error at most 2 −E(p, )m .\nLet G be the nR × n matrix that is the Kronecker product of ˆ G and the n/m × n/m identity matrix I n/m , i.e.,\nClearly a codeword of the code C given by G is given by n/m codewords of the code ˆ C concatenated side-by-side. The probability of error of C is therefore, by the union bound, at\nmost n\nHowever, notice that the generator matrix has row weight bounded above by m = 1+α E(p, ) log n. Hence, we have con- structed a code with update efﬁciency 1+α E(p, ) log n, and rate 1−h(p)− that achieves a probability of error < E(p, ) (1+α)n α log n on a BSC(p).\nIn the adversarial error model, the adversary is allowed to introduce up to s errors at locations of his choice. It is known that to correct s adversarial errors the minimum distance d(C) of a code C needs to be at least 2s + 1. However, if a code has update-efﬁciency t, then there must exist two codewords within distance t of each other. Hence, small update-efﬁciency implies limited error correction capability. We investigate these observations in more detail below.\nAlthough it is impossible for a ﬁxed error-correction code with small update efﬁciency to correct a large number of errors, if we randomize the code then it is possible to fool the adversary. In fact, with a randomized code it is possible to correct pn adversarial errors with a code rate approaching the capacity of a BSC(p). This idea has been used in the case of erasures in [2]. Let ( ˆ C, ˆ φ) be a random code deﬁned as follows from another code (C, φ). Suppose σ ∈ S n is an uniform random permutation on the set {1, . . . , n}, and z ∈ F n 2 is a uniform random vector. The random encoding in ˆ C is deﬁned by ˆ φ(x) = σ(φ(x)) + z, x ∈ F k 2 . If the operation of the decoding algorithm of C and ˆ C are denoted by ψ and ˆ ψ respectively, then ˆ ψ(y) = ψ(σ −1 (y + z)), y ∈ F n 2 . We have the following theorem that stems from [11].\nTheorem 10: Let C be a codes with rate 1 − h(p) − that achieves probability of error approaching 0 as n → ∞ over a BSC(p). Suppose ˆ C is a random code formed as above. Then, against any adversarial pn errors, the code ˆ C n will have probability of error approaching 0 as n → ∞.\nIn the above theorem if we take the code C to be the explicit code designed in Section II-C, then the code ˆ C remains an update-efﬁcient code with update-efﬁciency O(log n). Hence by sharing O(n log n) bits between the encoder and decoder, it is possible to correct a large number of adversarial errors with a high rate code. We omit the proof of the above theorem here, as it follows directly from [11].\nAs noted at the start of this section, in an error correcting code minimum distance and update efﬁciency are conﬂicting objectives. A code with distance d must have update-efﬁciency at least d because the nearest codeword is at least distance d away. If the update-efﬁciency of the code C is denoted by t(C) then t(C) ≥ d(C), where d(C) is the minimum distance of the code. Hence, the aim of a code-designer would be to design\na code whose update-efﬁciency is as close to the distance as possible. As we saw in the Introduction, for a linear code C the update efﬁciency is simply the weight of the maximum weight row of a generator matrix. We recall the following theorem from [6].\nTheorem 11: Any binary linear code of length n, dimension k and distance d has a generator matrix consisting of rows of weight ≤ d + s where s = n − k−1 j=0 d 2 j is a nonnegative integer.\nThe fact that s is a non-negative integer also follows from the well-known Griesmer bound [12] that states for any linear code with length n, dimension k and distance d, n ≥ k−1 j=0 d 2 j .\nCorollary 12: For any linear [n, k, d] code C with update- efﬁciency t, d ≤ t ≤ d + n − k−1 j=0 d 2 j .\nIt is clear that for codes achieving the Griesmer bound with equality, the update-efﬁciency is exactly equal to the minimum distance, i.e., the best possible. There are a number of families of codes that achieve the Griesmer bound. For examples of such families and their characterizations we refer the reader to [4], [7].\nExample: Suppose C is an [n = 2 m − 1, k = 2 m − 1 − m, 3] Hamming code. For this code t(C) ≤ 3+(n−3−2−(k−2)) = n − k = m = log(n + 1). In fact, for all n, Hamming codes have update-efﬁciency 3. One way to prove this is by explicitly constructing a generator matrix for the Hamming code with weight 3 rows. One can also appeal to the following theorem of Simonis [14].\nTheorem 13: Any [n, k, d] binary linear code can be trans- formed in to a code with same parameters that has a generator matrix consisting of only weight d rows.\nThe implication of this theorem is that if there exists an [n, k, d] linear code, then there exists an [n, k, d] linear code with update-efﬁciency d. In his paper [14], Simonis gave an algorithm to transform any linear code into an update- efﬁcient linear code (a code with update-efﬁciency equal to the minimum distance). However, the algorithm is of expo- nential complexity. It is of interest to have a polynomial time algorithm for the procedure.\nOn the other hand, the above theorem says that there exists a linear [n = 2 m − 1, k + 2 m − 1 − m, 3] code that has update-efﬁciency only 3. All codes with these parameters are equivalent to the Hamming code of the same parameters up to a permutation of coordinates [8], providing an indirect proof that Hamming codes have update-efﬁciency 3.\nAnalysis of the update-efﬁciency of BCH codes and other linear codes is of independent interest. In general, ﬁnding a sparse basis for a linear code seems to be a hard problem.\nLet us now return now to codes satisfying Deﬁnition 1. Extending the u = 1 case, clearly any (u, t)-update-efﬁcient code must satisfy t ≥ d, the minimum distance of the code, but for general u we can strengthen this bound.\nProposition 14: Suppose a (u, t)-update-efﬁcient code of length n, dimension k and minimum distance d exists. Then\n≤ B(n, d, t), where B(n, d, w) is the size of the largest code with distance d such that each codeword has weight at most w.\nProof: Suppose C is an update-efﬁcient code where x ∈ F k 2 is mapped to y ∈ F n 2 . Now, the u i=0 k i different message vectors that are within distance u from x should map to codewords within distance t from y. Suppose these codewords are y 1 , y 2 , . . . . Consider the vectors y −y, y 1 −y, y 2 −y, . . . . These must be at least distance d apart from one another and all of their weights are at most t. This proves the claim.\nIt is not very difﬁcult to construct a (u, O(u log n)) update efﬁcient code that achieves the capacity of a BSC(p) by modifying the constructions of Section II-C. On the other hand, one expects a converse result of the form\nk i\nwhere K(n, t, p) is the maximum size of a code with code- words having weight bounded by t that achieves arbitrarily small probability of error. A formal expression for K(n, t, p) is a subject of our ongoing work.\nAcknowledgement: A. M. thanks Yury Polyanskiy and Barna Saha for useful discussions."},"refs":[{"authors":[{"name":"N. Alo"},{"name":"J. Spence"}],"title":{"text":"The Probabilistic Method, Wiley-Interscience, 2000"}},{"authors":[{"name":"N. P. Anthapadmanabhan"},{"name":"E. Soljanin"},{"name":"S. Vishwanath"}],"title":{"text":"Update-efﬁcient codes for erasure correction"}},{"authors":[{"name":"A. Barg"},{"name":"G. D. Forney"}],"title":{"text":"Random codes: minimum distances and error exponents"}},{"authors":[{"name":"B. I. Belov"},{"name":"V. N. Logachev"},{"name":"V. P. Sandimirov"}],"title":{"text":"Construction of a class of linear binary codes achieving the Varshamov/Griesmer bound"}},{"authors":[{"name":"N. Calkin"}],"title":{"text":"Dependent sets of constant weight binary vectors"}},{"authors":[{"name":"S. D. Dodunekov"},{"name":"N. L. Manev"}],"title":{"text":"An improvement of the Griesmer bound for some small minimum distances"}},{"authors":[{"name":"T. Helleseth"}],"title":{"text":"New constructions of codes meeting the Griesmer bound"}},{"authors":[{"name":"W. C. Huffma"},{"name":"V. Ples"}],"title":{"text":"Fundamentals of Error-Correcting Codes, Cambridge, 2003"}},{"authors":[{"name":"A. Jule"},{"name":"I. Andriyanova"}],"title":{"text":"Some Results on update complexity of a linear code ensemble"}},{"authors":[{"name":"A. M. Kakhaki"},{"name":"H. K. Abadi"},{"name":"P. Pad"},{"name":"H. Saeedi"},{"name":"F. Marvasti"},{"name":"K. Al- ishahi"}],"title":{"text":"Capacity achieving linear codes with binary sparse generating matrices"}},{"authors":[{"name":"M. Langberg"}],"title":{"text":"Private codes or Succinct random codes that are (almost) perfect"}},{"authors":[{"name":"F. Macwilliam"},{"name":"N. Sloan"}],"title":{"text":"The Theory of Error-Correcting Codes, North-Holland, 1977"}},{"authors":[{"name":"A. S. Rawat"},{"name":"S. Vishwanath"},{"name":"A. Bhowmick"},{"name":"E. Soljanin"}],"title":{"text":"Update efﬁcient codes for distributed storage"}},{"authors":[{"name":"J. Simonis"}],"title":{"text":"On generator matrices of codes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564805.pdf"},"links":[{"id":"1569566567","weight":3},{"id":"1569566485","weight":3},{"id":"1569566725","weight":3},{"id":"1569567049","weight":3},{"id":"1569565067","weight":3},{"id":"1569559617","weight":6},{"id":"1569566697","weight":3},{"id":"1569565355","weight":3},{"id":"1569566871","weight":3},{"id":"1569558325","weight":3},{"id":"1569565837","weight":3},{"id":"1569566119","weight":3},{"id":"1569565317","weight":3},{"id":"1569566739","weight":3},{"id":"1569565859","weight":3},{"id":"1569564387","weight":3},{"id":"1569566795","weight":3},{"id":"1569566787","weight":3},{"id":"1569566015","weight":6},{"id":"1569566895","weight":6},{"id":"1569566749","weight":3},{"id":"1569565803","weight":3},{"id":"1569566575","weight":6},{"id":"1569566733","weight":3},{"id":"1569563307","weight":3},{"id":"1569555999","weight":3},{"id":"1569566217","weight":6},{"id":"1569566511","weight":3},{"id":"1569566531","weight":3},{"id":"1569564611","weight":3},{"id":"1569566423","weight":3},{"id":"1569565257","weight":6},{"id":"1569566811","weight":3},{"id":"1569558901","weight":6},{"id":"1569565839","weight":3},{"id":"1569566139","weight":3},{"id":"1569553519","weight":3},{"id":"1569564209","weight":3},{"id":"1569554689","weight":3},{"id":"1569566445","weight":3},{"id":"1569566909","weight":3},{"id":"1569566447","weight":3},{"id":"1569566357","weight":3},{"id":"1569565847","weight":6},{"id":"1569563231","weight":3},{"id":"1569566003","weight":3},{"id":"1569565185","weight":6},{"id":"1569556671","weight":6},{"id":"1569558401","weight":3},{"id":"1569566553","weight":3},{"id":"1569565469","weight":3},{"id":"1569565933","weight":3},{"id":"1569562207","weight":3},{"id":"1569566191","weight":3},{"id":"1569566159","weight":3},{"id":"1569566297","weight":6},{"id":"1569564097","weight":3},{"id":"1569565961","weight":3},{"id":"1569551347","weight":3},{"id":"1569566383","weight":3},{"id":"1569565571","weight":3},{"id":"1569557633","weight":3},{"id":"1569559199","weight":3},{"id":"1569565665","weight":3},{"id":"1569557715","weight":3},{"id":"1569566779","weight":3},{"id":"1569556361","weight":6},{"id":"1569566129","weight":3},{"id":"1569565385","weight":3},{"id":"1569566711","weight":3},{"id":"1569565241","weight":3},{"id":"1569564131","weight":3},{"id":"1569564283","weight":3},{"id":"1569566547","weight":3},{"id":"1569566595","weight":3},{"id":"1569566529","weight":3},{"id":"1569565597","weight":3},{"id":"1569565293","weight":3},{"id":"1569566771","weight":3},{"id":"1569563975","weight":3},{"id":"1569556759","weight":3},{"id":"1569564923","weight":3},{"id":"1569564769","weight":3},{"id":"1569561713","weight":3},{"id":"1569566577","weight":6},{"id":"1569557851","weight":3},{"id":"1569566147","weight":15},{"id":"1569566457","weight":3},{"id":"1569566847","weight":3},{"id":"1569567013","weight":3},{"id":"1569566273","weight":3},{"id":"1569565165","weight":3},{"id":"1569566113","weight":3},{"id":"1569565715","weight":3},{"id":"1569560581","weight":6}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S9.T4.4","endtime":"11:10","authors":"Arya Mazumdar, Gregory Wornell, Venkat Chandar","date":"1341399000000","papertitle":"Update Efficient Codes for Error Correction","starttime":"10:50","session":"S9.T4: Joint Source-Channel Codes","room":"Stratton 20 Chimneys (306)","paperid":"1569564805"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
