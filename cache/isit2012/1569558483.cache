{"id":"1569558483","paper":{"title":{"text":"Entropy Functions and Determinant Inequalities"},"authors":[{"name":"Terence Chan"},{"name":"Dongning Guo"},{"name":"Raymond Yeung"}],"abstr":{"text":"Abstract\u2014In this paper, we show that the characterisation of all determinant inequalities for n × n positive deﬁnite matrices is equivalent to determining the smallest closed and convex cone containing all entropy functions induced by n scalar jointly Gaussian random variables. We have obtained inner and outer bounds on the cone by using representable functions and entropic functions. In particular, these bounds are tight and explicit for n ≤ 3, implying that determinant inequalities for 3 × 3 positive deﬁnite matrices are completely characterized by Shannon-type information inequalities."},"body":{"text":"Let n be a positive integer and denote the ground set by N = {1, ..., n}. Suppose K is an n×n positive deﬁnite matrix. For any subset α ⊆ N , let K α be the sub-matrix of K obtained by removing those rows and columns of K indexed by N \\ α and its determinant be denoted by |K α |. When α is the empty set, we will simply deﬁne K α as the scalar of value 1. There are many determinant inequalities in the existing literature that involve only the principle minors of the matrix. These include\n(2) for any 1 ≤ l < k.\nAs pointed out in [1], [2], many determinant inequalities (including the above two inequalities) can be proved via an information-theoretic approach. Despite that many determinant inequalities can be found in this approach, a complete char- acterisation of all determinant inequalities is still missing. In this paper, we aim to understand determinant inequalities by using the information inequality framework proposed in [3].\nCharacterisation of determinantal inequalities is closely re- lated to characterisation of jointly Gaussian random variables. In the relevant work [4], properties of (normalised) entropy function of jointly Gaussian random variables were studied. The characterisation problem was directly solved for no more than four random variables. This paper offers an alternative proof for the result in [4] by showing that all representable functions are Gaussian.\nThe work [3] provides a geometric approach to understand- ing information inequalities. See also [5, Ch. 13-16] for a comprehensive treatment. Its idea will be illustrated shortly.\nDeﬁnition 1 (Rank functions): A rank function over the ground set N is a real-valued function deﬁned on all subsets of N . The rank function space over the ground set N , denoted by R 2 n , is the set of all rank functions over N .\nAs usual, R 2 n is treated as an Euclidean space. Hence, concepts such as metric and limits can be deﬁned accordingly.\nDeﬁnition 2 (Entropic functions): Let g be a rank function. Then g is called entropic if there exists a set of discrete random variables {X i , i ∈ N } such that g(α) is the Shannon entropy 1 H(X i , i ∈ α), or H(X α ) for short, for all α ⊆ N .\nOn the other hand, if {X i , i ∈ N } is a set of continuous scalar random variables such that g(α) is the differential entropy h(X α ) for all α ⊆ N , then g is called s-entropic.\nDeﬁnition 3 (Entropic regions): Consider any nonempty ﬁ- nite ground set N . Deﬁne the following \u201centropy regions\u201d:\nΓ ∗ n = {g ∈ R 2 n : g is entropic} \t (3) γ ∗ s,n = {g ∈ R 2 n : g is s-entropic}. \t (4)\nUnderstanding the above entropic regions is one of the most fundamental problems in information theory. It is equivalent to determining the set of all information inequalities [3].\nIn this paper, we will use the following notation. For any subset S ⊆ R 2 n , W(S) is deﬁned as the set of all rank functions g ∗ such that g ∗ = c · g for some c > 0 and g ∈ S. The closure of W(S) will be denoted by W(S). Finally, the\nsmallest closed and convex cone containing S will be denoted by con(S). Clearly,\nTheorem 1 (Geometric framework [3]): A linear informa- tion inequality α⊆N c α H(X α ) ≥ 0 is valid for all discrete random variables {X 1 , . . . , X n } if and only if for all g ∈ Γ ∗ n ,\nBy Theorem 1, characterising the set of all valid information inequalities is thus equivalent to characterising the set Γ ∗ n . Similar results can be obtained for the set γ ∗ s,n . In the following, we will extend this geometric framework to study determinant inequalities.\nDeﬁnition 4 (Log-determinant function): A rank function g over N is called log-determinant if there exists an n × n positive deﬁnite matrix K such that for all α ⊆ N ,\nLet Ψ n be the set of all log-determinant functions over N . Then, we have the following theorem.\nTheorem 2: Let {c α , α ⊆ N } be any real numbers. The determinant inequality\nholds for all positive deﬁnite matrix K if and only if for all g ∈ con(Ψ n )\nIn other words, the characterisation of the set of all determi- nant inequalities is equivalent to determining con(Ψ n ). In this paper, we will obtain inner and outer bounds on con(Ψ n ). To achieve our goal, we will take an information theoretic approach [2]. The idea is very simple: Let {X 1 , . . . , X n } be a set of scalar jointly Gaussian random variables whose covariance matrix is equal to (1/2πe)K. Then the differential entropy of X α is given by\nfunction g ∈ R 2 n is called s-Gaussian if there exists scalar jointly Gaussian variables {X 1 , . . . , X n } where\nFrom (9), a rank function g is log-determinant if and only if 1 2 g is s-Gaussian. Let Υ s,n be the set of all s-Gaussian functions. Then con(Ψ n ) = con(Υ s,n ). Consequently, we have the following theorem.\nIn fact, the Hadamard inequality and Szasz inequality are respectively the counterparts of the following basic informa- tion inequalities 2 [6]\nIn the following sections, we will obtain inner and outer bounds on the set con(Υ s,n ). The following corollaries of Theorem 2 show how these bounds can be used for proving or disproving a determinant inequality.\nCorollary 1 (Proving an inequality): Suppose S contains con(Υ s,n ) as a subset. The inequality (7) holds for all positive deﬁnite matrix K if for all g ∈ S, α⊆N c α g(α) ≥ 0.\nTherefore, any explicit outer bound on con(Υ s,n ) can lead to the discovery of new determinant inequalities. On the other hand, an inner bound on con(Υ s,n ) can be used for disproving a determinant inequality.\nCorollary 2: Suppose that T ⊆ con(Υ s,n ). The determi- nant inequality (7) does not hold for all positive deﬁnite matrices if there exists g ∈ T such that α⊆N c α g(α) < 0.\nAs log-determinant functions are essentially the same as s -Gaussian functions, our objective is thus to characterise con(Υ s,n ). Since scalar Gaussian random variables are con- tinuous scalar random variables, the next lemma follows immediately from the deﬁnition.\nIt is well known that Γ ∗ n (i.e., the closure of Γ ∗ n ) is a closed and convex cone [3]. It was established in [7] that\nIn the following, we prove an inner bound on con(Υ s,n ) by using representable functions.\nDeﬁnition 6 ( s-representable function): A rank function g is called s-representable if there exists real-valued vectors (of the same length) {A 1 , . . . , A n } such that for all α ⊆ N ,\nTheorem 4 (Inner bound): If g is s-representable, then g ∈ W(Υ s,n ).\nProof: Suppose the length of each row vector A i is k. Let {W 1 , . . . , W k , V 1 , . . . , V n } be a set of independent standard Gaussian random variables. Therefore, its covariance matrix is the (n + k) × (n + k) identity matrix. Let c > 0. For each i = 1, . . . , n, deﬁne a real-valued continuous random variable\nwhere A is an n × k matrix whose i th row is A i and V = [V 1 , . . . , V n ] . Since X i is zero-mean,\nConsequently, det(Cov(X)) = det 1 c D + I where D is the diagonal matrix obtained by using singular-value decomposi- tion (SVD) over AA . Let d 1 ≥ d 2 ≥ · · · ≥ d n ≥ 0 be the diagonal entries of D and r be the rank of the matrix AA\n(or equivalently, the rank of A). Hence, d i > 0 if and only if i ≤ r. Then\nlog 1/c \t (17) = r. \t (18)\nLemma 2: Let {X 1 , . . . , X n } be a set of scalar jointly continuous random variables with differential entropy function\ng. For any c 1 , . . . , c n > 0, deﬁne the set of random variables {Y 1 , . . . , Y n } by Y i = X i /c i , ∀i ∈ N , and let g ∗ be the differential entropy function of {Y 1 , . . . , Y n }. Then\nProof: By Corollary 3, to prove the proposition, it sufﬁces to prove that for n ≤ 3,\nIn [15], the cone Γ ∗ n (when n ≤ 3) was explicitly determined by identifying the set of extreme vectors of the cone. It can be proved that all the extreme vectors are s-representable 3 and hence is a subset of con(Ω s,n ). Consequently, (23) holds and the proposition follows.\nProposition 1 does not hold when n ≥ 4. In fact, con(Ω s,n , φ n 1 , . . . , φ n n ) is in general a proper subset of con(Υ s,n ) when n ≥ 4. In [11], it was proved that all s -representable functions satisfy the Ingleton inequalities. It can also be directly veriﬁed that all the functions φ n i also satisfy the Ingleton inequalities. Therefore, all the functions in con(Ω s,n , φ n 1 , . . . , φ n n ) also satisfy the Ingleton inequalities. However, in [4], it was proved that there exists g ∈ Υ s,n for n = 4 that violates the the Ingleton inequality. Thus, con(Ω s,n , φ n 1 , . . . , φ n n ) is indeed a proper subset of con(Υ s,n ).\nRemark 1: Theorem 4 is not merely an intermediate result to proving Proposition 1. It leads to Corollary 3 which gives a non-trivial inner bound for the set of Gaussian functions.\nBy deﬁnition, con(Ψ n ) (which is the focus of our interest) is close under addition. However, this is not necessarily true for Ψ n . In fact, W(Ψ n ) is not necessarily equal to con(Ψ n ). In the previous section, we showed that the set Ψ n is essentially equivalent to the set of s-Gaussian functions, deﬁned via sets of scalar jointly Gaussian random variables. It turns out that,\nif we relax the constraint by allowing the jointly Gaussian random variables to be vectors, instead of scalars, we will obtain an outer bound for Ψ n and also con(Ψ n ).\nDeﬁnition 7 (Vector Gaussian function): A function g is called v-Gaussian if there exists n jointly Gaussian random vectors {X 1 , . . . , X n } such that g(α) = h(X α ) for all α ⊆ N .\nSo far, we have established two outer bounds (13) and (24) for con(Υ s,n ). In the following, we will prove that (24) is in fact a tighter one.\nDeﬁnition 8: A rank function g is called v-entropic if there exists a set of random vectors {X 1 , . . . , X n }, not necessarily of the same length, such that g(α) = h(X α ). Also, let\nTheorem 6 states that replacing the real-valued random variables X i in the vector X by random vectors does not enlarge the closure of the space of differential entropy vectors. The discrete counterpart of this result is trivial, because as far as the probability masses and the entropy are concerned, a discrete random vector is the same as a scalar discrete random variable. However, in the continuous domain, it is unclear how a probability density function on R 2 or more generally R m can be mapped to a pdf on R without changing the entropies. In particular, there does not exist a continuous mapping from R 2 to R [9]. The proof of Theorem 6 exploits the relationship between the differential entropy of a continuous vector and the entropy of a discrete vector obtained through quantisation: Given the n-tuple Z whose entries are vectors, we \u201cquantise\u201d Z by a discrete vector and then construct a continuous vector with n scalar entries whose entropy vector arbitrarily approximates that of Z. Before we prove the theorem, we need several intermediate supporting results.\nLemma 4 (Closeness in addition): If g 1 and g 2 are v- entropic (or entropic) functions over N , then their sum g 1 +g 2 is also v-entropic (or entropic).\nDeﬁnition 9 ( m-Quantization): Given m > 0, let the m- quantization of any real number x be denoted as:\nwhere t denotes the largest integer not exceeding t. Simi- larly, let the m-quantization of a real vector x = (x 1 , . . . , x n ) be the element-wise m-quantization of the vector, denoted by [x] m , i.e., [x] m = ([x 1 ] m , . . . , [x n ] m ).\nHence for every real-valued random variable X, [X] m is a discrete random variable taking value in the set (27).\nProposition 4 (Renyi [8]): If X is a real-valued random vector of dimension n with a probability density function, then\nUnder the assumption that the pdf of a random variable X is Riemann-integrable, Proposition 4 is established in [1] by treating H([X] m ) − n log m as the approximation of the Rie- mann integration of − f X (x) log f X (x)dx. It is nontrivial to establish the result in general, where the pdf is not necessarily Rieman-integrable. An example of such a pdf can be deﬁned by using the Smith-Volterra-Cantor set. Nonetheless (28) can be shown to hold using the Lebesgue convergence theorem along with some truncation arguments [8].\nLemma 5: Let {X 1 , . . . , X n } be a set of discrete random variables such that its entropy function is g. For any positive numbers c 1 , . . . , c n , let g ∗ be deﬁned as g ∗ (α) = g(α) −\nProof of Theorem 6: Clearly, γ ∗ s,n ⊆ γ ∗ v,n . We will now prove that γ ∗ v,n ⊆ γ ∗ s,n . Let Z = (Z 1 , . . . , Z n ) consist of n random vectors, where Z i = (Z i,1 , . . . , Z i,k i ). Let us deﬁne the m-quantization of Z i , denoted as [Z i ] m , be the element- wise m-quantization of Z i , i.e., it consists of [Z i,j ] m for j = 1, . . . , k i . By Proposition 4,\ng ∗ (α) = h(Z α ) \t (30) r m (α) = H([Z i ] m , i ∈ α) \t (31)\nBy (29), lim m→∞ g m = g ∗ . Also, since r m ∈ Γ ∗ n , g m ∈ γ ∗ s,n by Lemma 5. Consequently, g ∗ ∈ γ ∗ s,n . We have thus proved that γ ∗ v,n ⊆ γ ∗ s,n and as a result, γ ∗ v,n = γ ∗ s,n . Finally, by Proposition 3, γ ∗ v,n is a closed and convex cone and is equal\nto con(γ ∗ s,n ). Then by (14), γ ∗ v,n = con(Γ ∗ n , φ n 1 , . . . , φ n n ). The theorem is proved.\nIn Theorem 4, we have constructed an inner bound for con(Υ s,n ) by using s-representable functions. The same trick can also be used for constructing an inner bound for W(Υ v,n ).\nDeﬁnition 10: A rank function g over N is called v- representable if for i = 1, . . . , n, there exists a set of real- valued vectors (of the same length) {A i,1 , . . . A i,k i } such that for all α ⊆ N , g(α) = dim A i,j , i ∈ α, j = 1, . . . , k i .\nThe following theorem is a counterpart of Theorem 4. We will omit the proof for brevity.\nTheorem 7 (Inner bound on W(Υ v,n )): Suppose that g is v -representable, then g ∈ W(Υ v,n ) .\nCharacterising the set of v-representable functions has been a very important (and also extremely difﬁcult) problem in linear algebra and information theory. For many years, it is only known that v-representable functions are polymatroidal and satisﬁes the Ingleton inequalities [10], [11]. The set of representable functions is only known when n ≤ 4. However, there were some recent breakthrough in this areas. In [12], [13], many new subspace rank inequalities which are required to be satisﬁed by representable functions are discovered. Via a computer-assisted mechanical approach, the set of all representable functions when n ≤ 5 has been completely char- acterised. Properties about the set of v-representable functions were also obtained [14]. Theorems 4 and 7 thus opens a new door to exploit results obtained about representable functions to characterise the set of Gaussian functions.\nRemark 2: While con(Ω s,n , φ n 1 , . . . , φ n n ) ⊆ con(Υ s,n ), it is still an open question whether or not\nGaussian rank functions were studied in [4]. However, their deﬁnitions are slightly different from ours.\n{X 1 , . . . , X n } be a set of n jointly distributed vector valued Gaussian random variables such that each vector X i is a vector of length T . Its normalised Gaussian entropy function g is a function in R 2 n such that g(α) h(X α )/T .\nThe only difference between Deﬁnitions 5 and 11 is the scalar multiplier 1/T . Hence, a normalised Gaussian entropy function must be contained in the set W(Υ v,n ). In one sense, our proposed deﬁnition is slightly more general as we do not require all the random vectors X i to have the same length. On the other hand, the \u201cnormalising factor\u201d 1/T in Deﬁnition 11 can lead to some interesting results. For example, while we cannot prove that the closure of W(Υ s,n ) is closed and\nconvex, [4] proved that the closure of the set of all normalised Gaussian entropy functions is indeed closed and convex.\nProposition 5: Let Υ ∗ N,n 4 be the set of all normalised Gaussian entropy functions. Then con(Υ ∗ N,n ) = con(Υ v,n ).\nRemark 3: Our Proposition 1 can also be derived from [4, Theorem 5], which proved that for any g ∈ Υ v,n when n = 3, there exists a θ ∗ > 0 such that for all θ ≥ θ ∗ ,\ng is vector Gaussian. However, their proof techniques are completely different.\nIn this paper, we took an information theoretic approach to study determinant inequalities for positive deﬁnite matrices. We showed that characterising all such inequalities for an n × n positive deﬁnite matrix is equivalent to characterising the set of all scalar Gaussian entropy functions for n random variables. While a complete and explicit characterisation of the set is still missing, we obtained inner and outer bounds respectively by means of linearly representable functions and vector Gaussian entropy functions. It turns out that for n ≤ 3, the set of all scalar Gaussian entropy functions is the same as the set of all differential entropy functions. The latter set is completely characterized by Shannon-type information inequalities. Consequently, the aforementioned inner and outer bounds agree with each other. For n ≥ 4, we showed the contrary, and the problem is seeming very difﬁcult."},"refs":[{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, 2nd ed"}},{"authors":[{"name":"T. Cover"}],"title":{"text":"Determinant inequalities via information theory"}},{"authors":[{"name":"R. W. Yeung"}],"title":{"text":"A framework for linear information inequalities"}},{"authors":[{"name":"B. Hassibi"},{"name":"S. Shadbakht"}],"title":{"text":"The entropy region for three gaussian random variables"}},{"authors":[{"name":"R. W. Yeun"}],"title":{"text":"Information Theory and Network Coding, Springer 2008"}},{"authors":[{"name":"T. S. Han"}],"title":{"text":"Nonnegative entropy measures of multivariate symmetric correlations"}},{"authors":[{"name":"T. H. Chan"}],"title":{"text":"Balanced information inequalities"}},{"authors":[{"name":"A. Rény"}],"title":{"text":"Probability Theory"}},{"authors":[{"name":"K. Wiboonton"}],"title":{"text":"Bijections from R n to R m "}},{"authors":[{"name":"L. Guille"},{"name":"T. H. Chan"},{"name":"A. Grant"}],"title":{"text":"The minimal set of ingleton inequalities"}},{"authors":[{"name":"A. W. Ingleton"}],"title":{"text":"Representation of matroids."}},{"authors":[{"name":"R. Dougherty"},{"name":"C. Freiling"},{"name":"K. Zeger"}],"title":{"text":"Linear rank inequalities on ﬁve or more variables"}},{"authors":[{"name":"R. Kinser"}],"title":{"text":"New inequalities for subspace arrangements"}},{"authors":[{"name":"T. Chan"},{"name":"A. Grant"},{"name":"D. Pﬂüger"}],"title":{"text":"Truncation technique for character- ising linear polymatroids"}},{"authors":[{"name":"Z. Zhang"},{"name":"R. W. Yeung"}],"title":{"text":"On the characterization of entropy function via information inequalities"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569558483.pdf"},"links":[{"id":"1569566381","weight":8},{"id":"1569566485","weight":4},{"id":"1569565663","weight":8},{"id":"1569565377","weight":8},{"id":"1569567049","weight":4},{"id":"1569564635","weight":4},{"id":"1569565867","weight":4},{"id":"1569566799","weight":4},{"id":"1569565067","weight":4},{"id":"1569559665","weight":4},{"id":"1569565691","weight":4},{"id":"1569566981","weight":12},{"id":"1569559259","weight":12},{"id":"1569566697","weight":4},{"id":"1569552245","weight":4},{"id":"1569567045","weight":12},{"id":"1569564481","weight":8},{"id":"1569566081","weight":4},{"id":"1569565355","weight":8},{"id":"1569566765","weight":8},{"id":"1569565461","weight":8},{"id":"1569564227","weight":12},{"id":"1569565837","weight":8},{"id":"1569566303","weight":8},{"id":"1569564203","weight":4},{"id":"1569560613","weight":8},{"id":"1569566903","weight":4},{"id":"1569566999","weight":16},{"id":"1569565347","weight":4},{"id":"1569564387","weight":8},{"id":"1569566795","weight":4},{"id":"1569566963","weight":16},{"id":"1569561679","weight":12},{"id":"1569566709","weight":8},{"id":"1569551763","weight":4},{"id":"1569564189","weight":4},{"id":"1569566865","weight":4},{"id":"1569565321","weight":4},{"id":"1569566095","weight":4},{"id":"1569566167","weight":4},{"id":"1569566575","weight":12},{"id":"1569563981","weight":4},{"id":"1569566905","weight":8},{"id":"1569566753","weight":4},{"id":"1569566063","weight":8},{"id":"1569559995","weight":4},{"id":"1569566643","weight":4},{"id":"1569566531","weight":4},{"id":"1569561143","weight":8},{"id":"1569564611","weight":4},{"id":"1569559111","weight":4},{"id":"1569566687","weight":8},{"id":"1569553519","weight":4},{"id":"1569566231","weight":4},{"id":"1569565559","weight":4},{"id":"1569565655","weight":4},{"id":"1569558985","weight":8},{"id":"1569566473","weight":4},{"id":"1569566913","weight":4},{"id":"1569566809","weight":4},{"id":"1569566447","weight":4},{"id":"1569566721","weight":8},{"id":"1569565055","weight":4},{"id":"1569555879","weight":16},{"id":"1569565219","weight":8},{"id":"1569566003","weight":4},{"id":"1569556671","weight":8},{"id":"1569565095","weight":4},{"id":"1569566223","weight":8},{"id":"1569566593","weight":4},{"id":"1569566043","weight":4},{"id":"1569565393","weight":4},{"id":"1569566191","weight":8},{"id":"1569566695","weight":20},{"id":"1569566051","weight":8},{"id":"1569565311","weight":4},{"id":"1569566233","weight":4},{"id":"1569566297","weight":12},{"id":"1569566501","weight":4},{"id":"1569565961","weight":4},{"id":"1569565463","weight":4},{"id":"1569565439","weight":8},{"id":"1569566229","weight":4},{"id":"1569566133","weight":8},{"id":"1569562551","weight":4},{"id":"1569563395","weight":12},{"id":"1569551347","weight":8},{"id":"1569566383","weight":4},{"id":"1569565885","weight":4},{"id":"1569565549","weight":8},{"id":"1569565611","weight":8},{"id":"1569566983","weight":4},{"id":"1569565397","weight":8},{"id":"1569566873","weight":4},{"id":"1569565765","weight":8},{"id":"1569565435","weight":8},{"id":"1569566267","weight":4},{"id":"1569564131","weight":4},{"id":"1569561221","weight":4},{"id":"1569566035","weight":4},{"id":"1569566253","weight":8},{"id":"1569565353","weight":4},{"id":"1569566547","weight":4},{"id":"1569566237","weight":8},{"id":"1569566283","weight":4},{"id":"1569565375","weight":4},{"id":"1569566755","weight":4},{"id":"1569566813","weight":12},{"id":"1569565293","weight":4},{"id":"1569566771","weight":8},{"id":"1569566641","weight":4},{"id":"1569564247","weight":4},{"id":"1569563975","weight":4},{"id":"1569551905","weight":8},{"id":"1569556759","weight":8},{"id":"1569566619","weight":4},{"id":"1569561185","weight":4},{"id":"1569558779","weight":4},{"id":"1569565669","weight":12},{"id":"1569563721","weight":8},{"id":"1569566817","weight":8},{"id":"1569564923","weight":20},{"id":"1569564281","weight":4},{"id":"1569564769","weight":12},{"id":"1569565805","weight":4},{"id":"1569566933","weight":8},{"id":"1569557851","weight":8},{"id":"1569567691","weight":4},{"id":"1569565389","weight":8},{"id":"1569565537","weight":4},{"id":"1569566847","weight":4},{"id":"1569564961","weight":4},{"id":"1569559251","weight":8},{"id":"1569561861","weight":4},{"id":"1569564253","weight":4},{"id":"1569560459","weight":8},{"id":"1569565853","weight":8},{"id":"1569564505","weight":16},{"id":"1569565165","weight":8},{"id":"1569565143","weight":4},{"id":"1569564931","weight":4},{"id":"1569564141","weight":16},{"id":"1569566973","weight":4},{"id":"1569566987","weight":4},{"id":"1569551751","weight":4},{"id":"1569564419","weight":4},{"id":"1569566067","weight":4},{"id":"1569566615","weight":4},{"id":"1569566443","weight":4},{"id":"1569560581","weight":12}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S7.T8.1","endtime":"15:00","authors":"Terence H. Chan, Dongning Guo, Raymond Yeung","date":"1341326400000","papertitle":"Entropy functions and determinant inequalities","starttime":"14:40","session":"S7.T8: Information Inequalities","room":"Stratton (491)","paperid":"1569558483"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
