{"id":"1569566601","paper":{"title":{"text":"On Decoding Delay of Intra-session Random Linear Network Coding for Line Networks"},"authors":[{"name":"Vahid Shah-Mansouri"},{"name":"Vincent W.S. Wong"}],"abstr":{"text":"Abstract\u2014 Random linear network coding (RLNC) can im- prove the reliability of end-to-end communication in the presence of erasure channels. In RLNC with intra-session coding, an in- termediate node creates coded packets by combining the packets of various sources destined for a destination. Each transmitted packet contains information of multiple sources. In this paper, we study the decoding delay of RLNC with intra-session coding for a line network with S sources and a destination. Given the generation size K, we show that the expected decoding delay is upper-bounded by  η 2 K log(S) + Kη 1 + η , where η, η 1 , η 2 are constants. The analytical results are validated via simulations. We also compare the results with the case where intra-session coding is not used."},"body":{"text":"Network coding can achieve the packet-level capacity of unicast and multicast sessions in wireless networks in the presence of lossy links [1], [2]. In random linear network coding (RLNC), the source groups the packets into blocks called generations. The intermediate nodes store the packets received from one generation. The source and intermediate nodes transmit a new packet by choosing the random coefﬁ- cients from a Galois ﬁeld and linearly combining the stored packets using these coefﬁcients. The source and intermediate nodes transmit packets of a generation until the destination can fully decode all the packets of that generation. The time between sending the ﬁrst packet of a generation by the source and the decoding time of a generation at the destination is called the decoding delay.\nThe decoding delay of network coding over line networks with erasure channels is ﬁrst studied in [3]. Different coding schemes are presented and the delays are compared. It is shown that for a two-hop line network with identical erasure probabilities α, the decoding delay is equal to K/α+O( √ K) , where K is the generation size. The delay analysis of RLNC for a unicast session on line networks is presented in [4]. When the erasure probability of various links is different, the decoding delay mainly depends on the link with the minimum erasure probability. For a two parallel multihop line network, results in [5] show that the delay gap between network coding and routing is unbounded as the generation size increases. The work in [6] presents an upper bound on the expected decoding delay, which is a constant plus the generation size divided by the max-ﬂow min-cut of the relay network. RLNC for time division duplexing line networks with unicast session is studied in [7]. The source transmits packets consecutively\nand then waits for the destination to announce how many more packets it requires for decoding. In order to reduce power consumption, there is an optimal number of packets to be transmitted before waiting for the feedback.\nThe decoding delay of line networks with a pair of source and destination with multiple intermediate nodes has been studied in the literature (e.g., [3]\u2013[5], [7]). In some appli- cations, a subset of these intermediate nodes can also be sources with data to be sent to the same destination. As an example, in wireless mesh networks (e.g., [8]), all the wireless mesh routers have data to be transmitted to the same gateway (destination). In this paper, we consider a line topology where there is one destination and all other nodes are sources. This topology can be extended to the case where only a subset of intermediate nodes are sources.\nFor a network with multiple sources (or ﬂows), RLNC with intra-session coding refers to the case that an intermediate node performs coding operations on the packets of its upstream ﬂows destined for the destination [9]. A transmitted packet contains information of all upstream sources. In this paper, we study the decoding delay of RLNC with intra-session coding and derive an upper bound on the decoding delay. The contributions of our paper are as follows:\n\u2022 For RLNC with intra-session coding, we show that the expected decoding delay for a line network with S sources and generation size K is upper-bounded by \nη 2 K log(S) + Kη 1 + η , where η, η 1 , η 2 are constants. \u2022 The analytical models are validated via simulations. Re-\nsults show that as the number of sources S increases on the line, the difference between the decoding delay of RLNC with and without intra-session coding increases without bound.\nOur work is different from the previous works in various aspects. In [4] and [7], a single source and destination pair is considered, whereas our work consider multiple sources. In [7], the line network is half-duplex, whereas our work study the full-duplex scenario. In [6], the decoding delay for multiple parallel line networks between a source and destination is determined. In this paper, we consider the multihop scenario and study the beneﬁt of using intra-session coding.\nThe rest of this paper is organized as follows: The system model is presented in Section II. In Section III, we present the decoding delay model for RLNC with intra-session coding.\nPerformance evaluation and comparisons are presented in Section IV. Conclusions are given in Section V.\nWe consider a line network 1 with S source nodes and one destination d as shown in Fig. 1. Each source s ∈ {1, . . . , S} also forwards the packets of other sources with lower indices (i.e., sources 1, 2, . . . , (s − 1)) towards the destination d. Source s has one outgoing link with node s + 1 as its downstream neighbor. We assume that the packet generation rate for each source is one packet per second. Since source s forwards packets of sources 1, . . . , (s−1) as well, its aggregate sending rate is s packets per second. We assume that the capacity of each link is higher than the aggregate sending rate.\nFor RLNC with intra-session coding, packets of various ﬂows towards the destination d are combined at the intermedi- ate nodes. The intermediate nodes store the received packets of different ﬂows in a single buffer. The coded packets are created by combining packets of various ﬂows and each packet transmitted by node s contains information of all the sources in set {1, . . . , s}.\nThe ﬂux of innovative packets from source s to destination d can be modeled as a queueing system [10]. Each intermediate node can be considered as a server while innovative packets are customers traveling from servers with queues on a tandem. We notice that this queue is different from the actual buffer of the node which stores the received packets. To distinguish this queue from the buffer of node s, we call it the virtual queue of node s. The size of the virtual queue is increased by one when node s receives an innovative packet. A packet is serviced by the virtual queue of node s and is dequeued when node s successfully transmits an innovative packet to node s + 1. We notice that the packet is not removed from the actual buffer of node s. The size of the virtual queue of node s at time t is the difference between the number of packets with independent encoding vectors at the buffers of node s and downstream node s + 1.\nWhen RLNC with intra-session coding is employed, each intermediate source node keeps a shared buffer for different ﬂows towards the destination. Hence, each node has only one virtual queue. Node s transmits coded packets periodically from its buffer at a rate of s packets per second. The time inter- val between two consecutive packet transmissions is 1/s. The packet transmission is independent of receiving new packets. We assume that the packet success probability of the outgoing link of source s, which is denoted as α s , is given. Since\nvarious packet transmissions over link s experience erasure independently with probability 1 − α s , the time between two consecutive successful packet transmissions for node s is the time between two packet transmissions (i.e., 1/s) multiplied by a random variable which follows a geometric distribution. The time between two successful packet transmissions of node s is the service time for the virtual queue of node s.\nAll the sources start transmission of one generation at time 0, with the size of the virtual queue for all the sources being K . Let τ s denote the time at which node s transmits its last innovative packet (i.e., the sKth innovative packet). Consider node s has successfully transmitted u s ∈ {sK, sK + 1, . . .} packets up to time τ s . Time τ s can be written as τ s = \n, where ν j is the time interval between the (j − 1)th and jth successful packet transmission of node s. Random variable ν j follows a geometric distribution. sK transmissions out of u s successful packet transmissions contain innovative information for node s + 1. Let U s denote the set of time intervals which precedes an innovative packet transmission.\nj th transmitted packet is innovative} . (1) The total time which takes source node s to transmit sK\ninnovative packets is the summation of the members of set U s . Let ζ s denote this summation (i.e., ζ s =  j ∈U s ν j ). Since ν j is the product of a geometric random variable and 1/s, ζ s is the summation of sK independent geometric random variables with parameters α s multiplied by 1/s. The summation of sK independent geometric random variables with parameter α s constructs a negative binomial distribution with parameters sK and α s . Thus, for i = sK, sK + 1, sK + 2, . . ., we have\nP {ζ s = i/s } =  i −1 sK −1  (1 − α s ) i −sK α sK s . (2) For the line network with S sources, the time that the Sth source node transmits the last innovative packet, τ S , is the decoding delay of the system. Time τ S can be written in a recursive manner using two different equations. First, τ S can be written as the summation of τ S −1 and the time node S requires to transmit the remaining packets in its virtual queue when it receives the last innovative packet at time τ S −1 . Let n S denote the number of packets in the virtual queue of source S at time τ S −1 . The time it takes node S to successfully transmit n S packets is the product of 1/S (i.e., the time between two packet transmissions) and the number of packet transmissions needed to successfully transmit n S packets. Let t(n S ) denote the number of packet transmissions needed to successfully transmit n S packets. t(n S ) is a negative binomial random variable with parameters n S and α S . Therefore, we have τ S = τ S −1 + t(n S )/S .\nIn the second approach for deriving τ S , we use the equation τ S =  u S j=1 ν j . The decoding delay τ S can be expressed as the summation of two terms\nIn (3), the second term  j ∈ ¯ U S ν j is the portion of time within τ S that node s transmits packets without innovative information. Let c S denote the number of successful packet transmissions of node S up to time τ S which do not have innovative information.  j ∈ ¯ U S ν j is the time node S spends on transmitting c S packets. The time it takes node S to successfully transmit c S packets is the product of the time between two packet transmissions 1/S and the number of packet transmissions which is a negative binomial random variable with parameters c S and α S . Let t(c S ) denote this random variable. t(c S ) times 1/S denotes the time node S requires to transmit c S packets (i.e., t(c S )/S =  j ∈ ¯ U S ν j ). Therefore, τ S = ζ S + t(c S )/S . Using the two equations derived for τ S , one can obtain\n. (4) A similar equation can be written for τ S −1 as\n. When we substitute τ S −1 to (4), we obtain\nIf τ S −2 is substituted to (5) accordingly and it is continued until it reaches source 1, one can write τ S as\nτ S = max {ζ S , ζ S −1 , . . . , ζ 1 } + g S , \t (6) where\nand γ = arg max {ζ S , ζ S −1 , . . . , ζ 1 } (7)\nTo ﬁnd the expected value of the decoding delay, we need to determine E[g S ] and E[max {ζ S , ζ S −1 , . . . , ζ 1 }]. The follow- ing lemma presents an upper bound on E[g S ] .\nLemma 1: The expectation of g S is upper-bounded by a constant, which is independent of S and K.\nγ ) γ\n, (8) where E γ is the expectation with respect to random variable γ. E c γ |γ and E n s |γ are the conditional expectations with respect to random variables c γ and n s , given γ, respectively. The expectation of t(n s ) is equal to E[n s ]/α s . Note that n s denotes the number of packets in the virtual queue of node s at time τ s −1 . Since ζ γ > ζ s for s = γ + 1, . . . , S, the virtual queues\nof the nodes γ + 1, . . . , S are in the steady state mode 2 . This guarantees that E[n s ] is constant for s = γ + 1, . . . , S. Let F s denote the expectation of t(n s ) in this case. Then we have\nOn the other hand, since ζ γ > ζ γ −1 , the rate at which node γ receives innovative packets is higher than the rate at which it sends innovation packets. Therefore, the virtual queue of node γ does not reach a steady state until it receives the last innovative packet. Recall that c γ denotes the number of times a successfully transmitted packet by node γ does not have new information. This is equal to the number of times a packet transmitted when the virtual queue of node γ is empty. If we model the size of the virtual queue of node γ with a Markov chain, then c γ is the number of times this Markov chain visits state 0. Since this queue is unstable, state 0 is a transient state and the number of times the Markov chain visits state zero is upper-bounded. Let D γ denote the expectation of t(c γ ) when the queue is unstable. Therefore, we have\nLet η denote the upper bound on all values of F s and D γ . Using equations (9) and (10), equation (8) can be written as\nSince we have no information about the success probabilities, the probability that γ = i is equal to 1/S for i ∈ {1, . . . , S}. This results in E γ  S s=γ 1 s  = 1 and E[g S ] < η .\nNext, we present the derivation of an upper bound for E[max {ζ S , ζ S −1 , . . . , ζ 1 }]. The probability distribution func- tion of ζ s is given in (2). ζ s is the product of 1/s and a random variable with negative binomial distribution with parameters sK and α s . The probability distribution function of random variable ζ s can be approximated with a normal distribution and we have\n. \t (11) If α s is not small and for large values of K, then the\napproximation in (11) is accurate enough to derive the upper- bound. In the rest of the paper, we use the approximation of ζ s as ζ s . The following theorem presents an upper bound for the expectation of max {ζ S , ζ S −1 , . . . , ζ 1 }.\nTheorem 1: Using the approximation in (11), the expecta- tion of max {ζ S , ζ S −1 , . . . , ζ 1 } is upper-bounded by η 1 K +\nProof: Equation (11) provides a normal approximation for variable ζ s . Consider another random variable ˜ζ s , which is a normal random variable with mean K/α min and variance K(1 − α min )/(α 2 min ) , where α min = min {α 1 , . . . , α S }. The difference between two normal random variables ζ s and ˜ζ s ,\nwhich is then normalized by the generation size K, is also a normal random variable. That is,\nFor the normalized difference of random variables, the mean is always greater than one while for large values of K, the variance approaches zero. Therefore, for large values of K, the probability that ˜ζ s is greater than ζ s approaches one. Therefore, we have\n. (12) Let random variable Y S = max  ˜ ζ S , ˜ ζ S −1 , . . . , ˜ ζ 1  . Y S is\nthe maximum of S normal random variables with identical mean µ = K/α min and variance σ 2 = K(1 −α min )/α 2 min . Let F −1 (.) denote the inverse of the cumulative distribution func- tion (CDF) of the normal random variable. Then, F −1 (U 1/S ) is distributed as Y S [11], where U is a uniform (0, 1) random variable. We exploit the fact that e P is a uniform (0, 1) random variable when P is an exponential random variable with parameter 1 [11]. Using Taylor series expansion for e P , we obtain\nThe inverse of the CDF of a normal random variable can be written using the inverse of error function as F −1 (x) = µ + σ √ 2 erf −1 (2x − 1). Therefore, we have\n. \t (13) An upper bound for the inverse of error function is presented in [12] as erf −1 (x) <  − log(1 − x), for 0 < x < 1. Using this upper bound, we obtain\nRandom variable P has an exponential distribution. The probability that log  2P S − P 2  is less than 0 (i.e., P  log  2P S − P 2  < 0 | S  ) quickly approaches zero as S increases. Therefore, by taking expectation from both sides of inequality in (14), we obtain\nwhere µ = K/α min and σ =  K(1 − α min )/(α 2 min ) . Setting η 1 = 1/α min and η 2 = 4(1 − α min )/α 2 min , we obtain\nWe are now able to present an upper bound on the decoding delay τ S obtained in (6). Theorem 1 presents an upper bound on the ﬁrst term in (6) while the second term in (6) is bounded by a constant η. Therefore, the upper bound for decoding delay can be written as\nIn this section, we validate our analytical models with sim- ulations and perform performance comparison. We consider a line network with multiple sources and a destination. New data from upper layer arrives at each source at a rate of one packet per second. For the packet success probabilities, we follow the model used in [13]. For the output link of source s , the packet success probability α s is randomly chosen with probability λα (λ −1) s , where λ is a tunable design parameter. The mean and variance of the packet success probability are λ/(λ + 1) and λ/(λ+2), respectively. The higher the value of λ , the lower the rate of packet loss in the network. We set λ to be equal to 4, which results in an average success probability of 0.8 for all the links. Unless stated otherwise, the Galois ﬁeld size q = 8, and the number of sources S = 10.\nThe destination cannot decode the packets of one generation unless it has enough packets from that generation. Large values of generation size K imposes long waiting time for the destination. It is of interest to choose a small K. On the other hand, the greater the value of K, the higher the maximum achievable throughput. This trade-off encourages us to ﬁnd an optimal K. We vary the generation size K from 50 to 400 with steps of 50. Fig. 2 shows the normalized decoding delay of RLNC with and without intra-session coding 3 . The decoding delay is normalized to the generation size K at each point. We notice that the normalized decoding delay is the inverse of the throughput for the line network. For all values of K, RLNC with intra-session coding has a lower decoding\ndelay than RLNC without intra-session coding. For values of K greater than 250, the normalized decoding delay of both cases approaches a steady state.\nFig. 3 shows that the decoding delay for RLNC with intra- session coding obtained via simulation match the analytical upper bound. The gap between the analytical model and simulation results originates from (12), where ζ s is replaced with ˜ζ s . Note that the mean of ζ s is K/α s whereas the mean of ˜ζ s is K/α min and α min = min {α 1 , . . . , α S }. The analytical upper bound will match the simulation results when α 1 = α 2 = · · · = α S .\nFig. 4 compares the decoding delay of RLNC with and without intra-session coding. For values of S less than 5, the average decoding delay of RLNC without intra-session coding is slightly less than RLNC with intra-session coding. In RLNC with intra-session coding, the decoding delay of different sources is identical and depends on the link with the lowest packet success rate. Thus, the decoding delay of RLNC with intra-session coding is higher than RLNC without intra-session coding when S is small. When intra-session coding is used, the intermediate sources can opportunistically transmit packets of other ﬂows when a particular ﬂow does not have innovative packet. As the number of sources increases in the network, the aggregate sending rate of intermediate nodes increases. Thus, using intra-session coding, the number of transmitted packets which do not have innovative information is reduced as the number of sources S increases. This consequently reduces the decoding delay compared to RLNC without intra-session network coding. As the number of sources S increases, the difference between the decoding delay of RLNC with and without intra-session coding is increased without bound.\nIn this paper, we studied random linear network coding (RLNC) with intra-session coding. We considered a line network while all the nodes generate packets. RLNC with intra-session coding requires intermediate sources to perform coding operation on the packets of all the sources that they relay. The packets of various ﬂows are decoded at the same time in the destination. For a line network with S sources and\ngeneration size K, when RLNC with intra-session coding is used, we showed that the decoding delay is upper-bounded by  η 2 K log(S) + Kη 1 + η , where η, η 1 , η 2 are constants. The analytical model is validated through simulations. We compared the decoding delay in both cases for varying number of sources. The results showed that as the number of sources S increases, the difference between the decoding delay of RLNC with and without intra-session coding increases without bound."},"refs":[{"authors":[{"name":"D. S. Lun"},{"name":"M. M´edard"},{"name":"M. Effros"}],"title":{"text":"On coding for reliable com- munication over packet networks"}},{"authors":[{"name":"D. S. Lun"},{"name":"M. M´edard"},{"name":"R. Koetter"},{"name":"M. Effros"}],"title":{"text":"On coding for reliable communication over packet networks"}},{"authors":[{"name":"P. Pakzad"},{"name":"C. Fragouli"},{"name":"A. Shokrollahi"}],"title":{"text":"Coding schemes for line networks"}},{"authors":[{"name":"T. K. Dikaliotis"},{"name":"A. G. Dimakis"},{"name":"T. Ho"},{"name":"M. Effros"}],"title":{"text":"On the delay of network coding over line networks"}},{"authors":[],"title":{"text":"On the delay advantage of coding in packet erasure networks"}},{"authors":[{"name":"R. Cogill"},{"name":"B. Shrader"}],"title":{"text":"Delay bounds for random linear coding in multihop relay networks"}},{"authors":[{"name":"D. Lucai"},{"name":"M. Stojanovic"},{"name":"M. M´edard"}],"title":{"text":"Random linear network coding for time division duplexing: When to stop talking and start listening"}},{"authors":[{"name":"A. H. Mohsenian-Rad"},{"name":"S. Wong"}],"title":{"text":"Joint channel allocation, interface assignment, and MAC design for multi-channel wireless mesh networks"}},{"authors":[{"name":"H. Seferoglu"},{"name":"A. Markopoulou"},{"name":"K. K. Ramakrishnan"}],"title":{"text":"I 2 NC: Intra- and inter-session network coding for unicast ﬂows in wireless networks"}},{"authors":[{"name":"T. H"},{"name":"D. S. Lu"}],"title":{"text":"Network Coding: An Introduction"}},{"authors":[{"name":"L. Devroye"}],"title":{"text":"Generating the maximum of independent identically dis- tributed random variables"}},{"authors":[{"name":"M. Chiani"},{"name":"D. Dardari"}],"title":{"text":"Improved exponential bounds and approxi- mation for the Q-function with application to average error probability computation"}},{"authors":[{"name":"Y. Mao"},{"name":"F. R. Kschischang"},{"name":"B. Li"},{"name":"S. Pasupathy"}],"title":{"text":"A factor graph approach to link loss monitoring in wireless sensor networks"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566601.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S14.T1.4","endtime":"18:00","authors":"Vahid Shah-Mansouri, Vincent W.S. Wong","date":"1341510000000","papertitle":"On Decoding Delay of Intra-session Random Linear Network Coding for Line Networks","starttime":"17:40","session":"S14.T1: Network Erasure Correction","room":"Kresge Rehearsal B (030)","paperid":"1569566601"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
