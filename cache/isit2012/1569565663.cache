{"id":"1569565663","paper":{"title":{"text":"Bounds on the Capacity of the Additive Inverse Gaussian Noise Channel"},"authors":[{"name":"Hui-Ting Chang"},{"name":"Stefan M. Moser"}],"abstr":{"text":"Abstract\u2014A very recent and new model describing commu- nication based on the exchange of chemical molecules in a drifting liquid medium is investigated and new analytical upper and lower bounds on the capacity are presented. The bounds are asymptotically tight, i.e., if the average-delay constraint is loosened to inﬁnity or if the drift velocity of the liquid medium tends to inﬁnity, the corresponding asymptotic capacities are derived precisely."},"body":{"text":"Recently, Srinivas, Adve, and Eckford [1] presented a new intriguing channel model describing communication in a ﬂuid media via emission of molecules. The basic idea is that in certain situations like, e.g., when nano devices try to communicate with each other or with some receiving station, it might not be possible to use traditional signal propagation via electromagnetic waves because, for example, the nano device is too small to accommodate the minimal necessary antenna size or it does not possess enough power.\nSo the question arises as to how communication could be established in such a setup. If we assume that the nano device is inside a certain liquid medium (e.g., blood in a blood vessel), then one can think of communication based on the emission of chemical substances. Such a system, of course, will behave fundamentally different from the usual information transmission systems. It is therefore a very interesting task to try to model this communication scenario and analyze it.\nThe work in [1] is a ﬁrst attempt to this: there the additive inverse Gaussian noise (AIGN) channel is introduced. The model is simplistic and neglects many properties of a real system, nevertheless, it also shows a Shannon-like beauty because it simpliﬁes as much as possible without losing the essentials. It deﬁnitely is of fundamental importance with big impact on practice seeing that nano devices are a very hot topic in current research worldwide. It also has huge potential for future research as it allows to slowly incorporate additional effects that might have inﬂuence on a real system.\nWhile we believe that the main and most important con- tribution of [1] is the description of the AIGN channel, the authors also present in [1] a ﬁrst analysis of the channel\u2019s capacity behavior. In this paper, we build on these results. We will present new upper and lower bounds on capacity and establish the exact asymptotic capacity in the cases when either\nthe drift velocity of the ﬂuid or the average-delay constraint of the transmitter tends to inﬁnity.\nThe remainder of this paper is structured as follows. In Section II we will introduce the channel model and its under- lying assumptions more in detail. In Section III we summarize a couple of mathematical properties and identities related to the given model, and Section IV reviews the bounds of [1]. In Section V we present our new upper and lower bounds on capacity, while the exact asymptotic capacity formulas are given in Section VI. Finally, Section VII outlines the basic ideas underlying the derivations, and in Section VIII we discuss the results.\nIn this paper all logarithms log(·) refer to the natural logarithm, i.e., all results are stated in nats. For engineering reasons, though, we have scaled the plots given in Figs. 1\u20134 to be in bits.\nThe basic idea of the system under consideration is that a transmitter emits a molecule at a certain time into a ﬂuid that itself is drifting with constant speed. The molecule is then transported by the ﬂuid and its inherent Brownian motion into random directions. Once it reaches the receiver, the molecule is removed from the ﬂuid. To simplify our model, we make the following assumptions:\n\u2022 The Brownian motion is described by a Wiener process with variance σ 2 .\n\u2022 We only consider a one-dimensional setup where the position of the molecule is described by a single random coordinate W . The transmitter is set at coordinate 0 and the receiver is along the moving direction of the ﬂuid at a certain distance d. Without loss of generality we will set d = 1.\n\u2022 Once the molecule reaches W = d, it is absorbed and not released again.\n\u2022 The transmitter and receiver are perfectly synchronized and have potentially inﬁnite time to wait for the arrival of the molecules.\n\u2022 There are no other molecules from the same type inter- fering with the communication.\n\u2022 The channel is memoryless, i.e., the trajectories of indi- vidual information carrying molecules are independent.\nMoreover, in case a molecule overpasses another, the receiver still can distinguish between them and put them into correct order.\nThe basic ideas behind these simpliﬁcations are related to Shannon\u2019s approach when he introduced the additive Gaussian noise channel [2] and also focused solely on the impact of the noise, but neglected many other aspects like delay, synchronization, or interference.\nA Wiener process is described by independent Gaussian position increments, i.e., for any time interval τ , the increment in position ∆W is Gaussian distributed with mean vτ and variance σ 2 τ . The increments of nonoverlapping intervals are assumed to be independent. Here, v denotes the drift velocity of the ﬂuid and σ 2 is a channel parameter that describes the strength of the Brownian motion and relates to the viscosity of the ﬂuid, the temperature, the size and structure of the molecules, etc.\nIn our setup of the communication, the positions of transmit- ter and receiver are ﬁxed, i.e., we need to \u201cinvert\u201d the Wiener process to describe the random time it takes for the molecule to travel from position 0 to position d = 1. This random time N has an inverse Gaussian distribution 1 that is described by its probability density function (PDF)\n2µ 2 n \t I {n > 0}. (1) Here, I {·} denotes the indicator function that takes on the values 1 or 0 depending on whether the statement holds or not. The PDF (1) depends on two parameters: µ denotes the average traveling time\nσ 2 . \t (3) Usually we write N ∼ IG(µ, λ).\nTo transport the information, the transmitter is now assumed to modulate the emission time X of its molecule (time-position modulation). After emission, the molecule takes the random time N to travel to the receiver, i.e., the receiver registers the arrival time\nwhere X and N are independent, X ⊥ ⊥ N. Hence, given some emission time x ≥ 0, the channel output has a conditional PDF\nfor practicability reasons, the transmitter is also subject to an average-delay constraint m:\nNote that other constraints are possible, e.g., it would be quite reasonable to constrain the maximum delay. We have deferred the investigation of such assumptions to our future research.\nWe refer to the model (4) above as additive inverse Gaus- sian noise (AIGN) channel . It is straightforward to see that Shannon\u2019s Channel Coding Theorem [2] can be applied to (4) resulting in a capacity\nNote that the capacity depends strongly on the two most important channel parameters: the allowed average delay m and the ﬂuid\u2019s drift velocity v.\nIn [1] the authors state the differential entropy of N ∼ IG (µ, λ) using some complicated expression involving deriva- tives with respect to the order of modiﬁed Bessel functions of the second kind K ν . Luckily, these expressions can be simpliﬁed considerably:\nµ , \t (9) where Ei (·) denotes the exponential integral function\nτ dτ. \t (10) The general moments of N are given as\nλ 2 . \t (14) Moreover, we have\nµ . \t (15) Similarly to Gaussian random variables, inverse Gaussians\nHowever, while the sum of two independent Gaussians is Gaussian again, this property only holds for inverse Gaussians with similar parameters: if\nµ 2 2 , \t (17) then it holds that for N 1 ∼ IG(µ 1 , λ 1 ) and N 2 ∼ IG(µ 2 , λ 2 ) with N 1 ⊥ ⊥ N 2 ,\nFinally, it is interesting to note that the inverse Gaussian distribution is differential-entropy maximizing when the fol- lowing three constraints are given:\nIn [1], two analytical bounds on capacity are presented. Firstly, an upper bound is derived based on the fact that differ- ential entropy h (Y ) under an average constraint E[Y ] ≤ m+µ is maximized by an exponential distribution:\nh (Y ) ≤ 1 + log(m + µ). \t (20) This leads to the bound\nC = max{h(Y ) − h(N)} \t (21) ≤ 1 2 log λe (m + µ) 2 2πµ 3 − 3 2 e 2λ µ Ei − 2λ µ . (22)\nSecondly, a lower bound is given that is based on (18). In the deﬁnition of capacity the maximization is dropped and instead a suboptimal input X ∼ IG m, λm 2 µ 2 is chosen. Note that this choice makes sure that X and N satisfy (17), i.e., we get Y ∼ IG m + µ, λ µ 2 (m + µ) 2 . This yields\nBoth bounds are depicted in Figs. 1 and 2 as a function of the average-delay constraint m and in Figs. 3 and 4 as a function of the drift velocity v, respectively.\nIn the following we will present our bounds on capacity. Similarly to Section IV, we will state the results using only the channel parameters µ and λ as well as the delay constraint m.\nFrom an engineering point of view, there are two interesting scenarios: we can either plot the capacity as a function of the given average-delay constraint m or as a function of the given drift velocity v. The former corresponds to the traditional situation of capacity as a function of the cost, which usually is power, but here has become delay. The latter is less traditional as the drift velocity is a channel\nparameter. However, information theorists often consider the power constraint also as being \u201cpart of the channel\u201d, i.e., belonging to that part of a system that the system designer has no control over. So, it is actually not that unorthodox to plot the capacity as a function of some channel parameter.\nThe adaptations of the given analytical formulas for these two cases are straightforward using (2).\nTheorem 1: The capacity of the AIGN channel as deﬁned in (4) is upper-bounded by either of the following three bounds:\n2λ µ\n1 λ\nHere, Ei (·) is deﬁned in (10) and K ν (·) represents the order- ν modiﬁed Bessel function of the second kind. In the second bound (26), 0 ≤ η ≤ 1 is a parameter that can be optimized over. A suboptimal, but good choice for η is\nwhere γ ≈ 0.577 denotes the Euler number. These bounds are shown in Figs. 1\u20134.\nTheorem 2: The capacity of the AIGN channel as deﬁned in (4) is lower-bounded by the following bound:\n2λ µ\nλ . \t (31) Note that this lower bound can be simpliﬁed considerably, but for the price of losing its tightness for large values of m or v: it can be shown that the complicated second last term (second and third row) can be lower-bounded by − log 1 + 1 2 :\n2λ µ\nThe upper bound (22) and the lower bound (29) are asymp- totically tight, i.e., when we let v or m tend to inﬁnity, these two bounds coincide. Hence, we can state the exact asymptotic capacity.\nTheorem 3: The capacity of the AIGN channel as deﬁned in (4) is asymptotically, when the average-delay constraint m is loosened to inﬁnity while all other parameters are kept constant, as follows:\n(33) In the asymptotic regime when the drift velocity v of the ﬂuid medium tends to inﬁnity while all other parameters are kept constant, the capacity is as follows:\nThe upper bounds on capacity are all based on the duality technique that we have successfully used in our previous work,\nsee, e.g., [3] or [4]. For an arbitrary choice of a distribution R (·) on the channel output alphabet, we have\nwhere D (· ·) is the relative entropy [5] and Q ∗ denotes the (unknown!) capacity-achieving input distribution. To be able to use this technique, we need to ﬁnd an elaborate choice of R (·) that is simple enough to allow the evaluation of (35), but that at the same time is good enough to result in a close bound. Moreover, we need to ﬁnd ways of dealing with the expectation over the unknown Q ∗ .\nAs discussed in Section IV, the basic idea of the upper bound (22) was to upper-bound the output entropy by its maximum possible value, which will be achieved if the output happens to be exponentially distributed. It therefore does not come as a surprise that if we choose R (·) to be an exponential distribution, we can fully rederive (22) from (35).\nThe upper bound (25) is based on R (·) being an IG distribution, which explains why for small m it gets close to the lower (24) (which is based on the IG distribution, too).\nThe other two upper bounds (26) and (27) stem from different versions of a derivation based on R (·) being a power inverse Gaussian distribution [6]: for y > 0,\nwhere α, β > 0, and η ∈ R \\ {0} are free parameters. The family of power inverse Gaussian distributions contains the IG distribution as a special case for the choice η = 1.\nThe lower bound was inspired by the fact that (22) is implicitly based on an output that is exponentially distributed. For large v or m, the impact of the noise N will decrease, i.e., it is a good guess that an exponential input might work well. The lower bound follows from a lengthy derivation based on the PDF of Y when in (4) X ∼ Exp 1 m [7]:\nky + e kλ Q\nwhere k is deﬁned in (30) and where this form of the PDF only is valid if condition (31) is satisﬁed. Here, Q (·) denotes the Q-function deﬁned as\nWe should point out that in [1] the authors have already concluded from numerical analysis that their upper bound (22) is very tight. We have now formally proven this by providing an analytical lower bound that is tight in the asymptotic regime.\nDue to the tightness of the known upper bound (22), it obviously is very difﬁcult to ﬁnd improved upper bounds. So, our focus in the search for upper bounds lay mainly in the low- delay and in the low-velocity regimes. We tried in particular to ﬁnd a bound that would show that the strange behavior of (22) to increase again as v ↓ 0 is an artifact of the bounding technique. We were able to ﬁnd bounds that were strictly better than (22) and, in particular, we did ﬁnd bounds that grew more or less monotonically in v, as we would expect. However, there is still considerable room for improvement.\nNote that the capacity for v = 0 is strictly larger than zero because even if there is no drift, the molecules still have a positive probability of arriving due to the Brownian motion. It is very weird that in this situation of v = 0, the noise that hurts communication at large speeds becomes the only mean of communication and is therefore highly beneﬁcial!\nIn the case when the capacity is analyzed as a function of the average-delay constraint m, the general picture is better. While the bound (22) remains strictly bounded away from zero, we have found an upper bound that tends to zero as m ↓ 0 (both (25) and (26), but the former is considerably simpler). Unfortunately, the slope of convergence of our upper bound (25) and of the lower bound (24) do not coincide. The exact asymptotic capacity for v = 0 and the capacity growth rates for v ↓ 0 and m ↓ 0 are projects of our future research.\nThis work was supported by the National Science Council under NSC 100\u20132628\u2013E\u2013009\u2013003."},"refs":[{"authors":[{"name":"K. V. Srinivas"},{"name":"R. S. Adve"},{"name":"A. W. Eckford"}],"title":{"text":"Molecular communication in ﬂuid media: The additive inverse Gaussian noise channel"}},{"authors":[{"name":"C. E. Shannon"}],"title":{"text":"A mathematical theory of communication"}},{"authors":[{"name":"A. Lapidoth"},{"name":"S. M. Moser"}],"title":{"text":"Capacity bounds via duality with applications to multiple-antenna systems on ﬂat fading channels"}},{"authors":[{"name":"A. Lapidoth"},{"name":"S. M. Moser"},{"name":"M. A. Wigger"}],"title":{"text":"On the capacity of free- space optical intensity channels"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, 2nd ed"}},{"authors":[{"name":"T. Kawamura"},{"name":"K. Iwase"}],"title":{"text":"Characterizations of the distributions of power inverse Gaussian and others based on the entropy maximization principle"}},{"authors":[{"name":"W. Schwarz"}],"title":{"text":"On the convolution of inverse Gaussian and exponential random variables"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565663.pdf"},"links":[],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S2.T7.2","endtime":"12:10","authors":"Hui-Ting Chang, Stefan M. Moser","date":"1341229800000","papertitle":"Bounds on the Capacity of the Additive Inverse Gaussian Noise Channel","starttime":"11:50","session":"S2.T7: Capacity of Gaussian Channels","room":"Stratton (407)","paperid":"1569565663"},"cluster":{"jsonClass":"Map$EmptyMap$"}}
