[[[ ID ]]]
81
[[[ INDEX ]]]
0
[[[ TITLE ]]]
Degrees of Freedom of the Interference Channel: a General Formula
[[[ AUTHORS ]]]
Yihong Wu
Shlomo Shamai (Shitz)
Sergio Verd´u
[[[ ABSTR ]]]
Abstract—We give a general formula for the degrees of freedom of the K-user real additive-noise interference channel involving maximization of information dimension. Previous results are recovered, and even generalized in certain cases with simpliﬁed proofs. Connections to fractal geometry are drawn.
Index Terms—Shannon theory, interference channel, degrees of freedom, additive noise, R´enyi information dimension.
[[[ BODY ]]]
Consider a K-user real-valued memoryless Gaussian Inter- ference Channel (IC) with a ﬁxed deterministic channel matrix H = [h ij ], where the i th user transmits X i and receives
where {X i , N i } K i=1 are independent with E X 2 i ≤ 1 and N i ∼ N (0, 1).
Denote the the capacity region of (1) by C(H, snr) and the sum-rate capacity by
The degrees of freedom (DoF) or the multiplexing gain is the pre-log of the sum-rate capacity in the high-SNR regime, deﬁned by 1
Determining the degrees of freedom has been an active research subject. In [2] it is shown that DoF(H) ≤ K 2
for fully-connected H, i.e., H has no zero entries. Using Diophantine approximation, this upper bound is shown to be achievable for Lebesgue-almost every H [3]. The almost sure achievability of K 2 for vector interference channel with varying channel gains has been shown in [4]. Sufﬁcient conditions on individual H that guarantee DoF(H) = K 2 are also given in [1], [5]. On the converse side, based on additive-combinatorial
results and deterministic channel approximation, [1] showed that DoF(H) < K 2 if H consists of all rational entries.
The goal of this paper is to give a general single-letter formula for DoF(H) via the maximization of a functional involving R´enyi’s information dimension [6]. This uniﬁed approach allows us to recover previously known results that are obtained using different methods, as well as uncover new results. We also give results for the more general case in which the rates are, unlike (2), not equally weighted.
Our results apply to non-Gaussian noise as well. This is because the degrees of freedom are insensitive to the noise statistics as long as it has ﬁnite non-Gaussianness: D(N ||Φ N ) < ∞, where Φ N is a Gaussian random variable with the same mean and variance as N . In fact in our derivations we shall often assume that the noise is uniformly distributed on the unit interval.
A key concept in fractal geometry, R´enyi [6] deﬁned the information dimension (also known as the entropy dimension [8]) of a probability distribution. It measures the rate of growth of the entropy of successively ﬁner discretizations.
Deﬁnition 1. Let X be a real-valued random variable. For m ∈ N, let X m mX m . The information dimension of X is deﬁned as
log m . 	 (4) If the limit in (4) does not exist, the lim inf and lim sup are called lower and upper information dimensions of X respectively, denoted by d(X) and d(X).
Deﬁnition 1 can be readily extended to random vectors, where the ﬂoor function · is taken componentwise.
The information dimension of X is ﬁnite if and only if the mild condition
is satisﬁed [9]. One sufﬁcient condition for ﬁnite information dimension is E [log(1 + |X|)] < ∞, which is milder than ﬁnite variance. Therefore (5) is satisﬁed for all random variables considered in this paper.
• For an integer M ≥ 2, write the M -ary expansion of X as
Then d(X) is the entropy rate of the digits {(X) i } normalized by log M .
• Denote by B(x, ) the ∞ -ball of radius centered at x. Then (see [10, Deﬁnition 4.2] and [9, Appendix A])
The following are basic properties of information dimension [6], [9], the last three of which are inherited from Shannon entropy.
where ν d is a discrete probability measure, ν c is an ab- solutely continuous probability measure and 0 ≤ ρ ≤ 1. Then
In particular, if X has a density with respect to the Lebesgue measure, then d(X) = 1; if X is discrete, then d(X) = 0.
max{d(X n ), d(Y n )} ≤ d(X n + Y n ) 	 (12) ≤ d(X n ) + d(Y n ). 	 (13)
The high-SNR asymptotics of mutual information with additive noise is governed by the input information dimension. For convenience, denote
Therefore d(X) represents the single-user degrees of freedom when the input distribution is constrained to be P X . Naturally
information dimension, as we will see, also appears in the characterization of degrees of freedom in the multi-user case. In fact, (17) also holds for random vectors [12].
Next we consider the behavior of information dimension under projections. Let A ∈ R m×n with m ≤ n. Then for any X n ,
d(AX n ) ≤ min{d(X n ), rank(A)}. 	 (18) Understanding how the dimension of a measure behaves under
projections is a basic problem in fractal geometry. It is well- known that almost every projection preserves the dimension, be it Hausdorff dimension (Marstrand’s projection theorem [13, Chapter 9]) or information dimension [10, Theorems 1.1 and 4.1]. However, computing the dimension for individual projections is in general difﬁcult.
A problem closely related to determining DoF(H) is to determine the dimension difference of a product measure under two projections. Let p, q, p , q be non-zero real numbers. By (12) and (13),
Therefore the dimension of a two-dimensional product mea- sure under two projections can differ by at most one half. However, if the coefﬁcients are rational, then the dimension difference is strictly less than one half [7]:
By the limiting characterization of interference channel capacity region [14] , the sum-rate capacity is given by
where X n i = [X i,1 , . . . , X i,n ] is the input of the i th user, and the supremum is over independent X n 1 , . . . , X n K . Then
Therefore the degrees of freedom admit the following limiting characterization: 2
where the supremum is over independent X 1 , . . . , X K such that all information dimensions appearing in (26) exist.
The main difﬁculty in the converse proof lies in exchanging the supremum with the limits in (25), which amounts to proving that varying the input distribution with increasing SNR does not improve the degrees of freedom. To this end, we invoke the following non-asymptotic version of (17) whose proof can be found in [7]: for any X n and any snr > 0,
The basic idea to single-letterize (25) is as follows: Given any X n , construct a single input X whose the ﬁrst nM bits are formed by concatenating the ﬁrst M bits from each X i ; the remaining bits are independent copies of the these nM bits. Then, the information dimension of X can be made close to 1 n d(X n ) by choosing M sufﬁciently large. The same conclusion holds for the information dimensions of linear combinations.
The following are immediate consequences of Theorem 1 combined with elementary properties of information dimen- sion in Lemma 1:
• DoF(H) is invariant under row or column scaling [1, Lemma 1], in view of (11).
• DoF(H) ≤ K, with equality if and only if H is a diagonal matrix with all diagonal entries nonzero.
• Removing cross-links increases degrees of freedom: let H be obtained from H by setting some of the off- diagonal entries to zero. By (15), for any indepen- dent X K , dof(X K , H) ≤ dof(X K , H ). Therefore DoF(H) ≤ DoF(H ).
As we illustrate next, the degrees of freedom of various channels can be obtained by specializing Theorem 1.
  
2 a, d = 0, b = c = 0 1 otherwise
• Many-to-one IC : DoF(H) = K − 1 where H are all zero except for all diagonal entries and at least one off- diagonal entry in the ﬁrst row. To see this, assuming
h 12 = 0, we have dof(X K , H)
where (31) is due to (12). The upper bound K − 1 is attained by choosing X 1 discrete and the rest absolutely continuous.
• One-to-many IC : Using similar arguments, we obtain that DoF(H) = K − 1 where H are all zero except for all entries on the diagonal and in the ﬁrst column.
• Multiple-access channel (MAC) : If H is an all-one matrix, then DoF(H) = 1, because
where we have used the following additive-combinatorial result [15, p. 3]:
with {U i } taking values on an arbitrary group. More generally, DoF(H) ≤ 1 if all rows of H are identical.
Discrete-continuous mixed input distributions are usu- ally strictly suboptimal. In fact, discrete-continuous mixtures achieve at most one degree of freedom in the fully-connected case. To see this, let d(X i ) = ρ i . If H is fully-connected, then
Therefore to obtain more than one degree of freedom, it is necessary to employ input distributions with singular compo- nents.
As we will show later, singular distributions of dimension one half are crucial in achieving the maximal degrees of freedom. Next we give a family of such distributions {µ λ } λ>0 which are homogeneously self-similar [16]. For integer λ ≥ 2, µ λ is the distribution of a random variable whose λ-ary expansion has equiprobable even digits and zero odd digits. Then d(µ λ ) = 1 2 in view of (6). For non-integer valued λ, µ λ is deﬁned as the invariant measure of an iterative function system [17].
Next we prove that the number of degrees of freedom is upper bounded by K 2 under more general conditions than fully- connectedness assumed in [2].
Theorem 2. Let π be a ﬁxed-point-free permutation on {1, . . . , K}, i.e., π(i) = i for all i. If h π(i),i = 0 for each i, then
Moreover, if π is cyclic (i.e., consisting of one cycle), then dof(X K , H) = K 2 if and only if for each i,
(42) ≤ K, 	 (43)
where (42) follows from (8), (11) and (12), and (43) is due to (13).
Next we give various sufﬁcient conditions on H that guar- antee DoF(H) = K 2 .
Theorem 3. If the off-diagonal entries of H are rational and the diagonal entries are irrational, then DoF(H) = K 2 .
Theorem 3 implies the following previously known results, obtained using different methods:
• [1, Theorem 1], which relies on the Thue-Siegel-Roth theorem and requires the diagonal entries to be irrational algebraic numbers. Note that the set of real algebraic numbers is dense but countable. Therefore almost every real number is transcendental.
• [5, Theorem 1(2)], which assumes that the diagonal and off-diagonal entries are equal to one and h respectively, with h irrational. Upon scaling, this is equivalent to a channel matrix with unit off-diagonal entries and irra- tional diagonal entries h −1 .
Proof sketch of Theorem 4 based on (27): construct input distributions depending only on the off-diagonal entries of H such that (39) are satisﬁed. Using the projection theorem [10,
Theorem 1] with respect to the diagonal entries, (40) holds for almost all h ii .
The degrees of freedom of channel matrices with rational coefﬁcients are strictly less than K 2 . Next we give a sufﬁcient condition for the three-user case.
Theorem 5. Let K = 3. If there exists distinct i, j, k, such that h ij , h ii , h kj and h ki are non-zero rationals, then DoF(H) <
The following example illustrates the tightness of the con- dition in Theorem 5:
1 2 0 0 1 2 2 0 1
3 2
Consider the following lower-triangular matrix [1, Section V]:
1 0 0 1 λ 0 1 1 1
1) DoF(H λ ) ≥ 1 with equality if and only if λ = 0 or 1; DoF(H λ ) ≤ 3 2 with equality if and only if λ is irrational.
3 2
λ i
Therefore as λ → ∞, DoF(H λ ) = 3 2 − Θ 1 log λ . For λ = 2, (48) can be sharpened to
Since λ is the channel gain of the direct link for the second user, it seems that DoF(H λ ) should be increasing in |λ|. However, (47) shows that this is not the case.
Proof of (46) and (50): dof(X 3 , H λ )
+ d(X 1 + X 2 + X 3 ) − d(X 1 + X 2 ) 	 (51) = d(X 1 + X 2 + X 3 ) + d(X 1 + λX 2 ) − d(X 1 + X 2 ). (52)
To maximize (52), choosing an absolutely continuous P X 3 yields d(X 1 + X 2 + X 3 ) = 1, regardless of P X 1 or P X 2 . This proves (46). To achieve (50) for λ = 2, consider the following singular input distributions:
where {(U i , V i )} are i.i.d. copies of (U, V ), with U and V independently valued on {0, 1} and {0, 1, 2} respectively. Then X 1 + X 2 = i≥1 (U i + V i )6 −i and X 1 + 2X 2 =
(U i + 2V i )6 −i , where U + V and U + 2V are valued on {0, . . . , 3} and {0, . . . , 5} respectively. By the entropy-rate deﬁnition of information dimension in (6), we have
Next we maximize H(U )+H(V )−H(U +V ) = H(U |U +V ). It can be shown that H(U |U + V ) is concave in P U and P V individually. Moreover, H(U |U + V ) is invariant if U is replaced by 1 − U or V replaced by 2 − V . Therefore the optimal U and V are symmetric. In particular, U is equiprobable Bernoulli. Let P {V = 0} = q. Maximizing H(U |U + V ) over 0 ≤ q ≤ 1 2 , we obtain the optimal q = φ √ 5 and H(U |U + V ) = log φ, which, in view of (54)–(55), gives (50).
In addition to the sum-rate degrees of freedom, the degrees of freedom region DoF (H) obtained by allowing different weights for the rates in (2) (see [4, Section II] for deﬁnition) is characterized by the following theorem:
Theorem 7. DoF (H) is the collection of all r K ∈ [0, 1] K , such that for any probability vector w K ,
(56) where the supremum is over independent X 1 , . . . , X K such that all information dimensions appearing in (56) exist.
where e 1 , . . . , e K are standard basis and co denotes the convex hull. Moreover, (57) holds with equality for Lebesgue- a.e. H.
This paper gives a general formula for the degrees of free- dom of the K-user interference channel in terms of a single- letter optimization (over K-dimensional input distributions) of
a linear combination of information dimensions. Beneﬁts of this method include:
• It recovers and improves known results with uniﬁed and simpliﬁed proofs, many of which are consequences of the calculus of information dimension. For instance, (21) and (49) are pure additive-combinatorial results, whereas the counterpart in [1] relies on additional techniques of deterministic channel approximation.
• The power constraint becomes immaterial. In fact the same degrees of freedom holds even if E[X 2 ] = ∞, as long as (5) is satisﬁed.
• It provides achievable rates (such as (50) which improves the lower bound 2+log 2 3 3 ≈ 1.19 in [1, p. 4945] ) that are not easily obtained from constructing explicit coding or communication schemes.
[[[ REFS ]]]
R. H. Etkin
E. Ordentlich
--
The Degrees-of-Freedom of the K-User Gaussian Interference Channel Is Discontinuous at Rational Channel Coefﬁcients
----
A. Host-Madsen
A. Nosratinia
--
The multiplexing gain of wireless networks
----
A. Motahari
S. Gharan
M. Maddah-Ali
A. Khandani
--
Real inter- ference alignment: Exploiting the potential of single antenna systems
----
V. R. Cadambe
S. A. Jafar
--
Interference Alignment and the Degrees of Freedom for the K-User Interference Channel
----
A. S. Motahari
A. K. Khandani
S. O. Gharan
--
On the Degrees of Freedom of the 3-user Gaussian interference channel: The symmetric case
----
A. R´enyi
--
On the dimension and entropy of probability distributions
----
Y. Wu
S. Shamai (Shitz)
S. Verd´u
--
Degrees of freedom of the interference channel: a general formula
----
Y. Peres
B. Solomyak
--
Existence of L q dimensions and entropy di- mension for self-conformal measures
----
Y. Wu
S. Verd´u
--
R´enyi Information Dimension: Fundamental Limits of Almost Lossless Analog Compression
----
B. R. Hunt
V. Y. Kaloshin
--
How projections affect the dimension spectrum of fractal measures
----
Y. Wu
S. Verd´u
--
Functional properties of MMSE and mutual information
----
A. Guionnet
D. Shlyakhtenko
--
On classical analogues of free entropy dimension
----
P. Mattil
--
Geometry of Sets and Measures in Euclidean Spaces: Fractals and Rectiﬁability 
----
R. Ahlswede
--
Multi-way communication channels
----
M. Madiman
A. Marcus
P. Tetali
--
Entropy and set cardinality inequalities for partition-determined functions, with applications to sumsets
----
Y. Peres
B. Solomyak
--
Self-similar measures and intersections of Cantor sets
----
K. Falcone
--
Fractal Geometry: Mathematical Foundations and Appli- cations , 2nd ed
[[[ META ]]]
parsed -> yes
file -> E:\isit2011\081.pdf
[[[ LINKS ]]]

